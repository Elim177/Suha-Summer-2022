[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 963556, "reputation": 1375, "user_id": 987397, "user_type": "registered", "accept_rate": 32, "profile_image": "https://www.gravatar.com/avatar/fa7ae7d9bd13c2d04335c3209865c262?s=256&d=identicon&r=PG", "display_name": "Derk", "link": "https://stackoverflow.com/users/987397/derk"}, "is_answered": false, "view_count": 547, "answer_count": 0, "score": 2, "last_activity_date": 1534926216, "creation_date": 1534926216, "question_id": 51962659, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51962659/tf-keras-utils-multi-gpu-model-does-use-only-one-gpu", "title": "tf.keras.utils.multi_gpu_model does use only one GPU", "body": "<p>I have two GPUs available which I want to use both to get maximal throughput during inference. That why I wanted to use multi_gpu_model, it should process half of the batch on one GPU and the other half of the other.</p>\n\n<p>However, only one GPU is used (checking with nvidia-smi).</p>\n\n<p>I could reproduce it with the following code</p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.utils import multi_gpu_model\nimport numpy as np\n\nnum_samples = 32\nheight = 224\nwidth = 224\nnum_classes = 1000\n\nwith tf.device('/cpu:0'):\n    model = Xception(weights=None,input_shape=(height, width, 3),classes=num_classes)\n\n# Replicates the model on 2 GPUs.\n# This assumes that your machine has 2 available GPUs.\nparallel_model = multi_gpu_model(model, gpus=2)\nparallel_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n# Generate dummy data.\nx = np.random.random((num_samples, height, width, 3)).astype(np.float32)\n\ndataset = tf.data.Dataset.from_tensor_slices(x)\ndataset = dataset.batch(32)\ndataset = dataset.repeat()\n\niterator = dataset.make_one_shot_iterator()\nbatch = iterator.get_next()\n\ny = parallel_model(batch)\n\nsess = tf.keras.backend.get_session()\nwhile True:\n    try:\n        result = sess.run(y)\n        print(result.shape)\n    except tf.errors.OutOfRangeError:\n        break\n</code></pre>\n\n<p>I expect this to use two GPUs, but it uses only 1. Is this a bug or is this expected behaviour? How to change this example to use multiple GPUs?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 74}]
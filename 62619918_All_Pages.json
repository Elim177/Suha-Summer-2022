[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 3948330, "reputation": 5043, "user_id": 3259896, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/641c30a7b383022f22b53c8cedb04e3f?s=256&d=identicon&r=PG&f=1", "display_name": "SantoshGupta7", "link": "https://stackoverflow.com/users/3259896/santoshgupta7"}, "is_answered": false, "view_count": 346, "answer_count": 1, "score": 0, "last_activity_date": 1593494308, "creation_date": 1593332046, "last_edit_date": 1593494308, "question_id": 62619918, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62619918/tpu-nameerror-name-minimize-is-not-defined-when-defining-keras-custom-train", "title": "TPU: NameError: name &#39;_minimize&#39; is not defined when defining Keras custom train_step", "body": "<p>I have model that runs just fine on the GPU, but gives an error on TPU.</p>\n<p>I am trying to define my own custom model in Tensorflow Keras, code below:</p>\n<pre><code>class CustomModel(tf.keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x = data\n        y = tf.constant([1.0], dtype=tf.float32)\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        \n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n                self.trainable_variables)\n\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n</code></pre>\n<p>but when I try to train, I run into</p>\n<p><code>NameError: name '_minimize' is not defined</code>, even through it is <s>defined in the inherited model class</s> defined in same code as the class.  <a href=\"https://github.com/tensorflow/tensorflow/blob/2434d2401399e3973d2f704f977bd6ad2d029ca7/tensorflow/python/keras/engine/training.py#L2699\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/tensorflow/blob/2434d2401399e3973d2f704f977bd6ad2d029ca7/tensorflow/python/keras/engine/training.py#L2699</a></p>\n<p>Here is the full error message</p>\n<pre><code>---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-44-2b800165a5d8&gt; in &lt;module&gt;()\n     13         validation_data=val_dataset,\n     14         validation_steps=val_steps,\n---&gt; 15         validation_freq=1)\n\n10 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\n     64   def _method_wrapper(self, *args, **kwargs):\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\n---&gt; 66       return method(self, *args, **kwargs)\n     67 \n     68     # Running inside `run_distribute_coordinator` already.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n    846                 batch_size=batch_size):\n    847               callbacks.on_train_batch_begin(step)\n--&gt; 848               tmp_logs = train_function(iterator)\n    849               # Catch OutOfRangeError for Datasets of unknown size.\n    850               # This blocks until the batch has finished executing.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n    578         xla_context.Exit()\n    579     else:\n--&gt; 580       result = self._call(*args, **kwds)\n    581 \n    582     if tracing_count == self._get_tracing_count():\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\n    625       # This is the first call of __call__, so we have to initialize.\n    626       initializers = []\n--&gt; 627       self._initialize(args, kwds, add_initializers_to=initializers)\n    628     finally:\n    629       # At this point we know that the initialization is complete (or less\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\n    504     self._concrete_stateful_fn = (\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n--&gt; 506             *args, **kwds))\n    507 \n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n   2444       args, kwargs = None, None\n   2445     with self._lock:\n-&gt; 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\n   2447     return graph_function\n   2448 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\n   2775 \n   2776       self._function_cache.missed.add(call_context_key)\n-&gt; 2777       graph_function = self._create_graph_function(args, kwargs)\n   2778       self._function_cache.primary[cache_key] = graph_function\n   2779       return graph_function, args, kwargs\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n   2665             arg_names=arg_names,\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\n-&gt; 2667             capture_by_value=self._capture_by_value),\n   2668         self._function_attributes,\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n    979         _, original_func = tf_decorator.unwrap(python_func)\n    980 \n--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)\n    982 \n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n    440         # the function a weak reference to itself to avoid a reference cycle.\n--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\n    443 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\n    966           except Exception as e:  # pylint:disable=broad-except\n    967             if hasattr(e, &quot;ag_error_metadata&quot;):\n--&gt; 968               raise e.ag_error_metadata.to_exception(e)\n    969             else:\n    970               raise\n\nNameError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    &lt;ipython-input-4-56fa61b8449a&gt;:14 train_step  *\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n\n    NameError: name '_minimize' is not defined\n</code></pre>\n<p>I know that</p>\n<pre><code>  gradients = tape.gradient(loss, trainable_variables)\n  self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n</code></pre>\n<p>is an equivalent to _minimize, but I can't use it in my case, since I am training over the TPU, and this code gives a name error for some reason (issue here <a href=\"https://stackoverflow.com/questions/62617511/attributeerror-tensor-name-is-meaningless-when-eager-execution-is-enabled-wh\">&quot;AttributeError: Tensor.name is meaningless when eager execution is enabled.&quot; when training on TPU at &quot;self.optimizer.apply_gradients&quot;</a> )</p>\n<p>I tried a workaround where I also define <code>_minimize</code> in the class itself when I overrided the class</p>\n<pre><code>class CustomModel(tf.keras.Model):\n    def __init__(self):\n        super(CustomModel).__init__()\n        \n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x = data\n        y = tf.constant([1.0], dtype=tf.float32)\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        \n        self._minimize(self.distribute_strategy, tape, self.optimizer, loss,\n                self.trainable_variables)\n\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n        return {m.name: m.result() for m in self.metrics}\n\n    def _minimize(strategy, tape, optimizer, loss, trainable_variables):\n        with tape:\n            if isinstance(optimizer, lso.LossScaleOptimizer):\n                loss = optimizer.get_scaled_loss(loss)\n\n        gradients = tape.gradient(loss, trainable_variables)\n        gradients = [(ClipIfNotNone(grad)) for grad in gradients]\n        gradients = [(ClipIfNotNone2(grad)) for grad in gradients]\n        # Whether to aggregate gradients outside of optimizer. This requires support\n        # of the optimizer and doesn't work with ParameterServerStrategy and\n        # CentralStroageStrategy.\n        aggregate_grads_outside_optimizer = (\n            optimizer._HAS_AGGREGATE_GRAD and  # pylint: disable=protected-access\n            not isinstance(strategy.extended,\n                            parameter_server_strategy.ParameterServerStrategyExtended))\n\n        if aggregate_grads_outside_optimizer:\n            # We aggregate gradients before unscaling them, in case a subclass of\n            # LossScaleOptimizer all-reduces in fp16. All-reducing in fp16 can only be\n            # done on scaled gradients, not unscaled gradients, for numeric stability.\n            gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n                                                        trainable_variables))\n        if isinstance(optimizer, lso.LossScaleOptimizer):\n            gradients = optimizer.get_unscaled_gradients(gradients)\n        gradients = optimizer._clip_gradients(gradients)  # pylint: disable=protected-access\n        if trainable_variables:\n            if aggregate_grads_outside_optimizer:\n                optimizer.apply_gradients(\n                    zip(gradients, trainable_variables),\n                    experimental_aggregate_gradients=False)\n            else:\n                optimizer.apply_gradients(zip(gradients, trainable_variables))\n</code></pre>\n<p>But then I get</p>\n<pre><code>TypeError: tf___minimize() takes 5 positional arguments but 6 were given\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 215}]
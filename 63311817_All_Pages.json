[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning"], "owner": {"account_id": 3938390, "reputation": 11, "user_id": 3253901, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/?s=256&d=identicon&r=PG&f=1", "display_name": "user3253901", "link": "https://stackoverflow.com/users/3253901/user3253901"}, "is_answered": true, "view_count": 646, "answer_count": 1, "score": 1, "last_activity_date": 1596869593, "creation_date": 1596860992, "last_edit_date": 1596861449, "question_id": 63311817, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63311817/keras-model-fit-and-tf-tape-gradient-giving-different-results", "title": "keras model.fit() and TF tape.gradient() giving different results", "body": "<p>I have a model that I am building using the keras functional API. After defining it, I compile it with the SGD optimizer as follows.</p>\n<pre><code>opt = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9, decay=1e-3,clipnorm=1)\nmodel.compile(optimizer=opt, loss='mse')\nmodel.fit(train_datagen, epochs=50,shuffle=True,verbose=True)\n</code></pre>\n<p>This works fine and my model converges as expected.</p>\n<p>However, when I to implement the same exact functionality using TF's tape gradient, I consistently get NaN gradients which cause my weights to equate to NaN and subsequently my loss function value because NaN. Here is the code that I use:</p>\n<pre><code>opt = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.9, decay=1e-3,clipnorm=1)\nloss_fn = tf.keras.losses.MeanSquaredError()\n\nepochs = 50\n\nfor epoch in range(epochs):\n        \n    batch_list = list(range(len(train_datagen)))\n    random.shuffle(batch_list)\n\n    running_loss = 0\n    \n    for ii in batch_list:\n        x,y_true = train_datagen[ii]\n        \n        with tf.GradientTape() as tape:\n            y_pred = model(x, training=True)\n            loss_value = loss_fn(y_true,y_pred)\n            \n        grads = tape.gradient(loss_value, model.trainable_variables)\n        opt.apply_gradients(zip(grads, model.trainable_variables))\n        \n        running_loss += loss_value\n    \n    print('Epoch',epoch,'Running Loss:',running_loss.numpy()/len(batch_list))\n</code></pre>\n<p>Is the code that I wrote equivalent to the Keras model.fit() functionality? For some reason, when I use the above code, I consistently get <code>NaN</code> gradients but with model.fit() it never happens.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 199}]
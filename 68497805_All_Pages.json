[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"user_type": "does_not_exist", "display_name": "user5178150"}, "is_answered": false, "view_count": 290, "answer_count": 2, "score": 0, "last_activity_date": 1627040480, "creation_date": 1627036362, "last_edit_date": 1627040378, "question_id": 68497805, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68497805/how-relate-relu-to-sigmoid", "title": "How relate ReLU to Sigmoid", "body": "<p>Given a network,\ninput -&gt; hidden layer with ReLU activation -&gt; output layer with Sigmoid activation</p>\n<p>I wonder how that can output values between 0 and 0.5, since ReLU returns positive values by definition, and a sigmoid is defined by 1/(1+e^-x), the smallest value would be 1/(1+e^-0) = 0.5</p>\n<p>Is the sigmoid activation shifted or anything? Because I get output between 0 and 1 (as the manual says), but why?</p>\n<p>Edit</p>\n<p>here is a code snipped that prints the function output in the console , it shows what i expected: it starts with 0.5, and increases when x&gt;0.</p>\n<pre class=\"lang-py prettyprint-override\"><code>    import numpy as np\n    import tensorflow as tf\n    \n    x = np.array([x/10 for x in range(-20,20)])\n    \n    y = tf.keras.activations.sigmoid(\n        tf.keras.activations.relu(x)\n    )\n    for a,b in zip(x,y):\n        print(a,&quot;\\t&quot;,float(b))\n</code></pre>\n<p>output</p>\n<pre><code>-0.2     0.5\n-0.1     0.5\n0.0      0.5\n0.1      0.52497918747894\n0.2      0.549833997312478\n0.3      0.574442516811659\n0.4      0.5986876601124521\n</code></pre>\n<p>but if i do</p>\n<pre class=\"lang-py prettyprint-override\"><code>    x = Dense(64, activation=&quot;relu&quot;)(input)\n    output = Dense(1, activation=&quot;sigmoid&quot;)(x)\n</code></pre>\n<p>I receive values between 0 and 1.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 96}]
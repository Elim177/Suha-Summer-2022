[{"items": [{"tags": ["python", "tensorflow", "nlp"], "owner": {"user_type": "does_not_exist", "display_name": "user9750231"}, "is_answered": false, "view_count": 110, "answer_count": 0, "score": 2, "last_activity_date": 1525653837, "creation_date": 1525653837, "question_id": 50205805, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/50205805/tensorflow-dnnclassifier-getting-bad-results", "title": "Tensorflow DNNclassifier getting bad results", "body": "<p>I am trying to make a classifier to learn if a movie review was positive or negative from its contents. I have using a couple of files that are relevant, a file of the total vocabulary(one word per line) across every document, two CSVs(one for the training set, one for the testing) containing the score each document got in a specific order, and two CSVs(same as above) where on one line it is the the index of each word that appears in that review looking at the vocab as a list. So for every a review like \"I liked this movie\" have something like a score line of 1(0: dislike, 1 like) and a word line of [2,13,64,33]. I use the DNNClassifier and currently am using 1 feature which is an embedding column wrapped around a categorical_column_with_identity. My code runs but it takes absolutely terrible results and I'm not sure why. Perhaps someone with more knowledge about tensor flow could help me out. Also I don't go on here much but I honestly tried and couldn't find a post that directly helps me.</p>\n\n<pre><code>import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\n\n\n\n\nembedding_d = 18\nlabel_name = ['Label']\ncol_name = [\"Words\"]\nhidden_unit = [10]*5\nBATCH = 50\nSTEP = 5000\n\n#Ignore some warning messages but an optional compiler\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n##Function to feed into training\ndef train_input_fn(features, labels, batch_size):\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n    # Shuffle, repeat, and batch the examples.\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n    # Return the dataset.\n    return dataset\n\n##Orignal Eval. Untouched so far. Mostly likely will need to be changed.\ndef eval_input_fn(features, labels, batch_size):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    features=dict(features)\n    if labels is None:\n        # No labels, use only features.\n        inputs = features\n    else:\n        inputs = (features, labels)\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n    # Batch the examples\n    assert batch_size is not None, \"batch_size must not be None\"\n    dataset = dataset.batch(batch_size)\n    # Return the dataset.\n    return dataset\n\n## Produces dataframe for labels and features(words) using pandas\ndef loadData():\n    train_label =pd.read_csv(\"aclImdb/train/yaynay.csv\",names=label_name)\n    test_label =pd.read_csv(\"aclImdb/test/yaynay.csv\",names=label_name)\n    train_feat = pd.read_csv(\"aclImdb/train/set.csv\", names = col_name)\n    test_feat = pd.read_csv(\"aclImdb/test/set.csv\", names =  col_name)\n    train_feat[col_name] =train_feat[col_name].astype(np.int64)\n    test_feat[col_name] =test_feat[col_name].astype(np.int64)\n    return (train_feat,train_label),(test_feat,test_label)\n\n## Stuff that I believe is somewhat working\n# Get labels for test and training data\n(train_x,train_y), (test_x,test_y) = loadData()\n\n## Get the features for each document\ntrain_feature = []\n#Currently only one key but this could change in the future\nfor key in train_x.keys():\n    #Create a categorical_column column\n    idCol = tf.feature_column.categorical_column_with_identity(\n        key= key,\n        num_buckets=89528)\nembedding_column = tf.feature_column.embedding_column(\n    categorical_column= idCol,\n    dimension=embedding_d)\ntrain_feature.append(embedding_column)\n\n##Create the neural network\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=train_feature,\n    # Species no. of layers and no. of neurons in each layer\n    hidden_units=hidden_unit,\n    # Number of output options(here there are 11 for scores 0-10 inclusive)\n    n_classes= 2)\n\n    # Train the Model\n    #First numerical value is batch size, second is total steps to take.\nclassifier.train(input_fn= lambda: train_input_fn(train_x, train_y, BATCH),steps=STEP)\n\n\n#Evaluate the model\neval_result = classifier.evaluate(\n    input_fn=lambda:eval_input_fn(test_x, test_y,\n                                                BATCH), steps = STEP)\nprint('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 281}]
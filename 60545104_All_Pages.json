[{"items": [{"tags": ["python", "tensorflow2.0", "tf.keras"], "owner": {"account_id": 97477, "reputation": 4181, "user_id": 264410, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/98a0fcc9a9390a0863a901945c45212c?s=256&d=identicon&r=PG", "display_name": "michael", "link": "https://stackoverflow.com/users/264410/michael"}, "is_answered": false, "view_count": 162, "answer_count": 0, "score": 1, "last_activity_date": 1583563752, "creation_date": 1583410183, "last_edit_date": 1583563752, "question_id": 60545104, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60545104/why-does-my-model-work-with-tf-gradienttape-but-fail-when-using-keras-model", "title": "Why does my model work with `tf.GradientTape()` but fail when using `keras.models.Model.fit()`", "body": "<p>After much effort, I managed to build a <code>tensorflow 2</code> implementation of an existing <code>pytorch</code> style-transfer project. Then I wanted to get all the nice extra features that are available through Keras standard learning, e.g. <code>model.fit()</code>. </p>\n\n<p>But the same model fails when learning through <code>model.fit()</code>. The model seems to learn the content features, but is unable to learn style features. This is the diagram of the model in quesion:</p>\n\n<p><a href=\"https://i.stack.imgur.com/MMIQa.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/MMIQa.png\" alt=\"enter image description here\"></a></p>\n\n<pre><code>def vgg_layers19(content_layers, style_layers, input_shape=(256,256,3)):\n  \"\"\" creates a VGG model that returns output values for the given layers\n  see: https://keras.io/applications/#extract-features-from-an-arbitrary-intermediate-layer-with-vgg19\n  Returns: \n    function(x, preprocess=True):\n      Args: \n        x: image tuple/ndarray h,w,c(RGB), domain=(0.,255.)\n      Returns:\n        a tuple of lists, ([content_features], [style_features])\n  usage:\n    (content_features, style_features) = vgg_layers16(content_layers, style_layers)(x_train)\n  \"\"\"\n  preprocessingFn = tf.keras.applications.vgg19.preprocess_input\n  base_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=input_shape)\n  base_model.trainable = False\n  content_features = [base_model.get_layer(name).output for name in content_layers]\n  style_features = [base_model.get_layer(name).output for name in style_layers]\n  output_features = content_features + style_features\n\n  model = Model( inputs=base_model.input, outputs=output_features, name=\"vgg_layers\")\n  model.trainable = False\n\n  def _get_features(x, preprocess=True):\n    \"\"\"\n    Args:\n      x: expecting tensor, domain=255. hwcRGB\n    \"\"\"\n    if preprocess and callable(preprocessingFn): \n      x = preprocessingFn(x)\n    output = model(x) # call as tf.keras.Layer()\n    return ( output[:len(content_layers)], output[len(content_layers):] )\n\n  return _get_features \n\n\n\nclass VGG_Features():\n\"\"\" get content and style features from VGG model \"\"\"\n  def __init__(self, loss_model, style_image=None, target_style_gram=None):\n    self.loss_model = loss_model\n    if style_image is not None:\n      assert style_image.shape == (256,256,3), \"ERROR: loss_model expecting input_shape=(256,256,3), got {}\".format(style_image.shape)\n      self.style_image = style_image\n      self.target_style_gram = VGG_Features.get_style_gram(self.loss_model, self.style_image)\n    if target_style_gram is not None:\n      self.target_style_gram = target_style_gram\n\n  @staticmethod\n  def get_style_gram(vgg_features_model, style_image):\n    style_batch = tf.repeat( style_image[tf.newaxis,...], repeats=_batch_size, axis=0)\n    # show([style_image], w=128, domain=(0.,255.) )\n\n    # B, H, W, C = style_batch.shape\n    (_, style_features) = vgg_features_model( style_batch , preprocess=True ) # hwcRGB\n    target_style_gram = [ fnstf_utils.gram(value)  for value in style_features ]  # list\n    return target_style_gram  \n\n  def __call__(self, input_batch):\n    content_features, style_features = self.loss_model( input_batch, preprocess=True )\n    style_gram = tuple(fnstf_utils.gram(value)  for value in style_features)  # tuple(&lt;generator&gt;)\n    return (content_features[0],) + style_gram  # tuple = tuple + tuple\n\n\n\n\nclass TransformerNetwork_VGG(tf.keras.Model):\n  def __init__(self, transformer=transformer, vgg_features=vgg_features):\n    super(TransformerNetwork_VGG, self).__init__()\n    self.transformer = transformer \n    # type: tf.keras.models.Model\n    # input_shapes:  (None, 256,256,3)\n    # output_shapes: (None, 256,256,3)\n\n\n    style_model = {\n       'content_layers':['block5_conv2'],\n       'style_layers': ['block1_conv1',\n                  'block2_conv1',\n                  'block3_conv1', \n                  'block4_conv1', \n                  'block5_conv1']\n    }\n    vgg_model = vgg_layers19( style_model['content_layers'], style_model['style_layers'] )\n\n    self.vgg_features = VGG_Features(vgg_model, style_image=style_image, batch_size=batch_size) \n\n    # input_shapes:  (None, 256,256,3)\n    # output_shapes: [(None, 16, 16, 512),  (None, 64, 64), (None, 128, 128), (None, 256, 256), (None, 512, 512), (None, 512, 512)]\n    #                [ content_loss,        style_loss_1, style_loss_2, style_loss_3, style_loss_4, style_loss_5 ]\n\n\n  def call(self, inputs):\n    x = inputs                # shape=(None, 256,256,3)\n\n    # shape=(None, 256,256,3)\n    generated_image = self.transformer(x)                    \n\n    # shape=[(None, 16, 16, 512),  (None, 64, 64), (None, 128, 128), (None, 256, 256), (None, 512, 512), (None, 512, 512)]\n    vgg_feature_losses = self.vgg(generated_image)           \n\n    return vgg_feature_losses       # tuple(content1, style1, style2, style3, style4, style5)\n</code></pre>\n\n<p>Style Image\n<img src=\"https://raw.githubusercontent.com/iamRusty/fast-neural-style-pytorch/master/images/mosaic.jpg\" alt=\"style image\"></p>\n\n<p>FEATURE_WEIGHTS= [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</p>\n\n<h2><code>GradientTape</code> learning</h2>\n\n<p>With the <code>tf.GradientTape()</code> loop, I'm manually handling the multiple outputs, e.g. tuple of 6 tensors, from <code>TransformerNetwork_VGG(x_train)</code>. This method learns correctly.</p>\n\n<pre><code>  @tf.function()\n  def train_step(x_train, y_true, loss_weights=None, log_freq=10):\n    with tf.GradientTape() as tape:\n      y_pred = TransformerNetwork_VGG(x_train)\n      generated_content_features = y_pred[:1]\n      generated_style_gram = y_pred[1:]\n\n\n      y_true = TransformerNetwork_VGG.vgg(x_train)\n      target_content_features = y_true[:1]\n      target_style_gram = TransformerNetwork_VGG.vgg.target_style_gram\n\n      content_loss = get_MEAN_mse_loss(target_content_features, generated_content_features, weights)\n      style_loss = tuple(get_MEAN_mse_loss(x,y)*w for x,y,w in zip(target_style_gram, generated_style_gram, weights))\n\n      total_loss = content_loss + = tf.reduce_sum(style_loss)\n      TransformerNetwork = TransformerNetwork_VGG.transformer\n      grads = tape.gradient(total_loss, TransformerNetwork.trainable_weights)\n      optimizer.apply_gradients(zip(grads, TransformerNetwork.trainable_weights))\n</code></pre>\n\n<pre><code># GradientTape epoch=5: \n# losses:             [   6078.71         70.23  4495.13 13817.65 88217.99    48.36]\n</code></pre>\n\n<p><img src=\"https://i.postimg.cc/brc7LgVK/Gradient-Tape-OK.png\" alt=\"gradient tape\"></p>\n\n<h2><code>model.fit()</code> learning</h2>\n\n<p>With <code>tf.keras.models.Model.fit()</code>, the multiple outputs, e.g. tuple of 6 tensors, are fed to the loss function individually as <code>loss(y_pred, y_true)</code> and then multipled by the correct weight on <code>reduction</code>. This method does learn to approximate the content_image, but <strong>does not learn</strong> to minimize the style losses! II cannot figure out why.</p>\n\n<pre><code>  history = TransformerNetwork_VGG.fit(\n    x=train_dataset.repeat(NUM_EPOCHS),\n    epochs=NUM_EPOCHS,\n    steps_per_epoch=NUM_BATCHES,\n    callbacks=callbacks,\n  )\n</code></pre>\n\n<pre><code># model.fit() epoch=5: \n# losses:             [  4661.08       219.95   6959.01   4897.39 209201.16     84.68]]\n</code></pre>\n\n<p><img src=\"https://i.postimg.cc/3RvMgV1X/model-fit-FAIL.png\" alt=\"model-fit\"></p>\n\n<blockquote>\n  <p>50 epochs, with boosted style_weights, \n  FEATURE_WEIGHTS= [   0.1854,           1605.23,   25.08,    8.16,    1.28, 2330.79]  # boost style loss x100</p>\n</blockquote>\n\n<p><img src=\"https://i.postimg.cc/8CFzj1Fw/model-fit-50-FAIL.png\" alt=\"model-fit after 50\"></p>\n\n<blockquote>\n  <p>step=50, losses=[269899.45    337.5   69617.7   38424.96   9192.36  85903.44  66423.51]</p>\n</blockquote>\n\n<h2>check <code>mse</code> losses * weights</h2>\n\n<p>I tested my model with losses and weights fixed as follows\n* FEATURE_WEIGHTS = SEQ = [1.,2.,3.,4.,5.,6.,]\n* MSELoss(y_true, y_pred) == tf.ones() of equal shape \nand confirmed that <code>model.fit()</code> is handling multiple output losses * weights correctly</p>\n\n<p><img src=\"https://i.postimg.cc/FFVbbJVY/check-losses-x-weights.png\" alt=\"losses as ones\"></p>\n\n<p>I've checked everything I can think of, but I cannot figure out how to make the model learn correctly with <code>model.fit()</code>. What am I missing??</p>\n\n<p>The full notebook is available here: <a href=\"https://colab.research.google.com/github/mixuala/fast_neural_style_pytorch/blob/master/notebook/%5BSO%5D_FastStyleTransfer.ipynb\" rel=\"nofollow noreferrer\">https://colab.research.google.com/github/mixuala/fast_neural_style_pytorch/blob/master/notebook/%5BSO%5D_FastStyleTransfer.ipynb</a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 63}]
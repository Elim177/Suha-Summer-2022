[{"items": [{"tags": ["python-3.x", "tensorflow", "keras", "model"], "owner": {"account_id": 16714684, "reputation": 9, "user_id": 12081086, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b36a253ca1986d75eca3da1c77f8e490?s=256&d=identicon&r=PG&f=1", "display_name": "Madhuri", "link": "https://stackoverflow.com/users/12081086/madhuri"}, "is_answered": false, "view_count": 308, "answer_count": 0, "score": 1, "last_activity_date": 1598245357, "creation_date": 1598245357, "question_id": 63554992, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63554992/model-parallelism-in-keras", "title": "Model parallelism in Keras", "body": "<p>I am trying to implement model parallelism in Keras.</p>\n<p>I am using Keras-2.2.4\nTensorflor-1.13.1</p>\n<p>Rough structure of my code is :\nimport tensorflow as tf\nimport keras</p>\n<pre><code>def model_definition():\ninput0 = Input(shape = (None, None))\ninput1 = Input(shape = (None, None))\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=False, log_device_placement=True)):\nmodel = get_some_CNN_model()\nwith tf.device(tf.DeviceSpec(device_type=&quot;GPU&quot;, device_index=0)):\nop0 = model(input0)\nwith tf.device(tf.DeviceSpec(device_type=&quot;GPU&quot;, device_index=1)):\nop1 = model(input1)\nwith tf.device(tf.DeviceSpec(device_type=&quot;CPU&quot;, device_index=0)):\nconcatenated_ops = concatenate([op0, op1],axis=-1, name = 'check_conc1')\nmixmodel = Model(inputs=[input0, input1], outputs = concatenated_ops)\nreturn mixmodel\n\nmymodel = model_definition()\nmymodel.fit_generator()\n</code></pre>\n<p>Expected result: While training, computations for op0 and op1 should be done on gpu0 and gpu1, respectively.</p>\n<p>Problem 1: When I have 2 gpus available, the training works fine. nvidia-smi shows that both gpus are being used. Though I am not sure if both gpus are doing their intended work. How to confirm that?\nBecause, even I set log_device_placement as True, I don't see any task allocated to gpu 1</p>\n<p>Problem 2: When I run this code on a machine with 1 GPU available, still it runs fine. It is expected show an error because GPU 1 is not available.</p>\n<p>The example shown here works fine as expected. It doesn't show the problem 2, i.e. on single GPU, it raises an error.</p>\n<p>So I think some manipulation is happening inside keras.</p>\n<p>I have also tried using import tensorflow.python.keras instead of import keras, in case it is causing any conflict.\nHowever, the both problems persist.</p>\n<p>Would appreciate any clue about this issue. Thank you.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 49}]
[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 3948330, "reputation": 5043, "user_id": 3259896, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/641c30a7b383022f22b53c8cedb04e3f?s=256&d=identicon&r=PG&f=1", "display_name": "SantoshGupta7", "link": "https://stackoverflow.com/users/3259896/santoshgupta7"}, "is_answered": true, "view_count": 217, "accepted_answer_id": 52963207, "answer_count": 1, "score": 0, "last_activity_date": 1540433579, "creation_date": 1540360946, "last_edit_date": 1540433579, "question_id": 52961962, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/52961962/how-to-convert-a-tensorflow-graph-into-using-the-estimator-api", "title": "How to convert a tensorflow Graph into using the Estimator API", "body": "<p>I'm a little stuck on trying to convert my tensorflow code into using the Estimator API</p>\n\n<p>My graph is below</p>\n\n<pre><code>batch_size = 1024\n\nembedding_size = 500 # 2^8 Dimension of the embedding vector. Crashed at 158 for Embed size 2656016. So possible values are 154-157. Possible choices 154, 156\nnum_inputs =5\n\nnum_sampled = 128 # Number of negative examples to sample.\n\ngraph = tf.Graph()\n\nwith graph.as_default(): \n\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, num_inputs ])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n\n    epochCount = tf.get_variable( 'epochCount', initializer= 0) #to store epoch count to total # of epochs are known\n    update_epoch = tf.assign(epochCount, epochCount + 1)\n\n    embeddings = tf.get_variable( 'embeddings', dtype=tf.float32,\n        initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.float32) )\n\n    softmax_weights = tf.get_variable( 'softmax_weights', dtype=tf.float32,\n        initializer= tf.truncated_normal([vocabulary_size, embedding_size],\n                             stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float32 ) )\n\n    softmax_biases = tf.get_variable('softmax_biases', dtype=tf.float32,\n        initializer= tf.zeros([vocabulary_size], dtype=tf.float32),  trainable=False )\n\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is\n\n    embed_reshaped = tf.reshape( embed, [batch_size*num_inputs, embedding_size] )\n\n    segments= np.arange(batch_size).repeat(num_inputs)\n\n    averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)\n\n    loss = tf.reduce_mean(\n        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds, \n                                   sampled_values=tf.nn.uniform_candidate_sampler(true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled, num_true=1, unique=True, range_max=vocabulary_size, seed=None),\n                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)) \n\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) \n\n    saver = tf.train.Saver()\n</code></pre>\n\n<p>I'm reading the official Estimator guide here</p>\n\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator</a></p>\n\n<p>My best interpretation on what to do is that I have to convert my graph into a function? And set that to <code>model_fn</code>?</p>\n\n<p>'params' are the my embeddings and softmax weights. So do I explicitly name these variables here?</p>\n\n<p>Edit: </p>\n\n<p>Thanks to answers from Sorin, this is my attempt at converting this to using TF.Estimator</p>\n\n<p>This is the function I used to generate batches of data. </p>\n\n<p>data_index = 0\nepoch_index = 0\nrecEpoch_indexA = 0 #Used to help keep store of the total number of epoches with the models</p>\n\n<pre><code>def generate_batch(batch_size, inputCount): #batch size = number of labels\n  #inputCount = number of inputs per label\n    global data_index, epoch_index\n\n    batch = np.ndarray(shape=(batch_size, inputCount), dtype=np.int32) \n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n\n    #Begin New stuff\n\n    n=0\n    while n &lt; batch_size:\n      if len(    set(my_data[data_index, 1])   ) &gt;= inputCount:\n        labels[n,0] = my_data[data_index, 0]\n        batch[n] = random.sample( set(my_data[data_index, 1]),  inputCount)\n        n = n+1\n        data_index = (data_index + 1) % len(my_data) #may have to do something like len my_data[:]\n        if data_index == 0:\n          epoch_index = epoch_index + 1\n          print('Completed %d Epochs' % epoch_index)\n      else:\n        data_index = (data_index + 1) % len(my_data)\n        if data_index == 0:\n          epoch_index = epoch_index + 1\n          print('Completed %d Epochs' % epoch_index)\n\n    return batch, labels     \n</code></pre>\n\n<p>This is where I define my model function. I commented out the place holders and put in the train data and train labels as the input</p>\n\n<pre><code>def my_model( train_dataset, train_labels):\n\n#     train_dataset = tf.placeholder(tf.int32, shape=[batch_size, num_inputs ])\n#     train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n\n    epochCount = tf.get_variable( 'epochCount', initializer= 0) #to store epoch count to total # of epochs are known\n    update_epoch = tf.assign(epochCount, epochCount + 1)\n\n    embeddings = tf.get_variable( 'embeddings', dtype=tf.float32,\n        initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.float32) )\n\n    softmax_weights = tf.get_variable( 'softmax_weights', dtype=tf.float32,\n        initializer= tf.truncated_normal([vocabulary_size, embedding_size],\n                             stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float32 ) )\n\n    softmax_biases = tf.get_variable('softmax_biases', dtype=tf.float32,\n        initializer= tf.zeros([vocabulary_size], dtype=tf.float32),  trainable=False )\n\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is\n\n    embed_reshaped = tf.reshape( embed, [batch_size*num_inputs, embedding_size] )\n\n    segments= np.arange(batch_size).repeat(num_inputs)\n\n    averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)\n\n    loss = tf.reduce_mean(\n        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds, \n                                   sampled_values=tf.nn.uniform_candidate_sampler(true_classes=tf.cast(train_labels, tf.int64), num_sampled=num_sampled, num_true=1, unique=True, range_max=vocabulary_size, seed=None),\n                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)) \n\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) \n\n    saver = tf.train.Saver()\n</code></pre>\n\n<p>This is where I define the Estimator</p>\n\n<pre><code>#Define the estimator\nword2vecEstimator = tf.estimator.Estimator(\n        model_fn=my_model,\n        params={\n            'batch_size': 1024,\n            'embedding_size': 50,\n            'num_inputs': 5,\n            'num_sampled':128\n        })\n</code></pre>\n\n<p>This is where I tell the Estimator to train</p>\n\n<pre><code>#Train with Estimator\nword2vecEstimator.train(\n    input_fn=lambda:iris_data.generate_batch(batch_size, num_inputs),\n    steps=1000)\n</code></pre>\n\n<p>Does this seem right?</p>\n\n<p>Also, after a certain amount of steps, I would like to save the checkpoint and upload it to my google drive. I was wondering how/where I would place the code to do that. </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 68}]
[{"items": [{"tags": ["tensorflow", "machine-learning", "deep-learning"], "owner": {"account_id": 7060318, "reputation": 869, "user_id": 5405823, "user_type": "registered", "accept_rate": 50, "profile_image": "https://i.stack.imgur.com/5H9hi.jpg?s=256&g=1", "display_name": "thinkdeep", "link": "https://stackoverflow.com/users/5405823/thinkdeep"}, "is_answered": false, "view_count": 212, "answer_count": 1, "score": 0, "last_activity_date": 1609795184, "creation_date": 1609724778, "last_edit_date": 1609725091, "question_id": 65556758, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65556758/why-doesnt-custom-training-loop-average-loss-over-batch-size", "title": "Why doesn&#39;t custom training loop average loss over batch_size?", "body": "<p>Below code snippet is the custom training loop from Tensorflow official tutorial.https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch . Another tutorial also does not average loss over <code>batch_size</code>, as shown here <a href=\"https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough</a></p>\n<p>Why is the loss_value not averaged over batch_size at this line <code>loss_value = loss_fn(y_batch_train, logits)</code>? Is this a bug? From another question here <a href=\"https://stackoverflow.com/questions/41954308/loss-function-works-with-reduce-mean-but-not-reduce-sum\">Loss function works with reduce_mean but not reduce_sum</a>, <code>reduce_mean</code> is indeed needed to average loss over batch_size</p>\n<p>The <code>loss_fn</code> is defined in the tutorial as below. It obviously does not average over batch_size.</p>\n<pre><code>loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n</code></pre>\n<p>From documentation, <code>keras.losses.SparseCategoricalCrossentropy</code> sums loss over the batch without averaging. Thus, this is essentially <code>reduce_sum</code> instead of <code>reduce_mean</code>!</p>\n<pre><code>Type of tf.keras.losses.Reduction to apply to loss. Default value is AUTO. AUTO indicates that the reduction option will be determined by the usage context. For almost all cases this defaults to SUM_OVER_BATCH_SIZE.\n</code></pre>\n<p>The code is shown below.</p>\n<pre><code>epochs = 2\nfor epoch in range(epochs):\n    print(&quot;\\nStart of epoch %d&quot; % (epoch,))\n\n    # Iterate over the batches of the dataset.\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n\n        # Open a GradientTape to record the operations run\n        # during the forward pass, which enables auto-differentiation.\n        with tf.GradientTape() as tape:\n\n            # Run the forward pass of the layer.\n            # The operations that the layer applies\n            # to its inputs are going to be recorded\n            # on the GradientTape.\n            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n\n            # Compute the loss value for this minibatch.\n            loss_value = loss_fn(y_batch_train, logits)\n\n        # Use the gradient tape to automatically retrieve\n        # the gradients of the trainable variables with respect to the loss.\n        grads = tape.gradient(loss_value, model.trainable_weights)\n\n        # Run one step of gradient descent by updating\n        # the value of the variables to minimize the loss.\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n        # Log every 200 batches.\n        if step % 200 == 0:\n            print(\n                &quot;Training loss (for one batch) at step %d: %.4f&quot;\n                % (step, float(loss_value))\n            )\n            print(&quot;Seen so far: %s samples&quot; % ((step + 1) * 64))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 237}]
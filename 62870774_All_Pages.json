[{"items": [{"tags": ["numpy", "tensorflow", "matplotlib", "keras", "deep-learning"], "owner": {"account_id": 16230354, "reputation": 2719, "user_id": 11725056, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3fb8aa3bd56b90f894e9805de55ff840?s=256&d=identicon&r=PG&f=1", "display_name": "Deshwal", "link": "https://stackoverflow.com/users/11725056/deshwal"}, "is_answered": false, "view_count": 83, "answer_count": 0, "score": 0, "last_activity_date": 1594623806, "creation_date": 1594623806, "question_id": 62870774, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62870774/best-fit-line-is-not-according-to-the-results-while-using-np-linspace-with-mat", "title": "Best fit line is not according to the results while using np.linspace() with matlotlib", "body": "<p><strong>For those who are not familiar with the ML part, please just take a look a look at the last code section. First codes are to ask if there is some problem with plotting or the ML model</strong></p>\n<p>I am trying to learn custom training using <code>tf.GradientTape()</code> and made a custom model as:</p>\n<pre><code>class RegressionModel(tf.keras.Model): # every new model has to use Model\n    '''\n    A Model that performs Linear Regression \n    '''\n    def __init__(self,in_units,out_units):\n        '''\n        args:\n            in_units: number of input neurons\n            out_units: number of output units\n        '''\n        super(RegressionModel,self).__init__() # constructor of Parent class \n        \n        self.in_units = in_units\n        self.out_units = out_units\n        \n        self.w = tf.Variable(tf.initializers.GlorotNormal()((self.in_units,self.out_units))) \n        # make weight which has initial weights according to glorot_normal distribution\n        \n        self.b = tf.Variable(tf.zeros(self.out_units)) # bias is mostly zeros\n        \n        self.params = [self.w,self.b] # we can use the model.params directly inside the GradientTape()\n            \n    \n    def call(self,input_tensor):\n        '''\n        execurte forward pass\n        args:\n            input_tensor: input tensor which will be fed to the network\n        '''\n        return tf.matmul(input_tensor, self.w) + self.b\n</code></pre>\n<p>I made my loss and gradients functions as:</p>\n<pre><code>def compute_loss(model,x_features,y_true):\n    '''\n    Calculate the loss. You can use RMSE or MAE \n    args:\n        model:  a model that'll give  predicted values\n        x_features: Array of data points\n        y_true: respective target values\n    '''\n    y_pred = model(x_features)\n    error = y_true  - y_pred\n    return tf.reduce_mean(tf.square(error)) # MSE: Mean Squred Error\n\n\n\ndef compute_grad(model,x_features,y_true):\n    '''\n    Compute the Gradient here\n    '''\n    with tf.GradientTape() as tape:\n        loss = compute_loss(model,x_features,y_true)\n        \n    return tape.gradient(loss,model.params) # you see model.params. It'll include all the params \n</code></pre>\n<p>and I trained the model as:</p>\n<pre><code>losses = []\noptimizer = tf.keras.optimizers.SGD(lr=0.01)\nmodel = RegressionModel(1,1) # 1 feature columns and 1 output for regression easy for plotting\nprint(f&quot;Initial:\\n{model.w}\\n{model.b}\\n\\n&quot;) # model's initial weights and biases\n\nfor epoch in range(500):\n    gradients = compute_grad(model,X,y)\n    optimizer.apply_gradients(zip(gradients,model.params)) # apply back prop to all the &quot;trainable&quot; params\n    \n    losses.append(compute_loss(model,X,y)) # make a list of loss per epoch\n\nprint(f&quot;Final:\\n{model.w}\\n{model.b}&quot;)\n\n</code></pre>\n<p><strong>PLOTTING PROBLEM</strong>:</p>\n<p>Problem is that I made a method for generating the data points according to <code>y=mx+c</code> and then tried predicting it on the model. If you generate the data as:</p>\n<pre><code>def generate_random_data(shape=(100,1)):\n    '''\n    Generate correlated X and y points which are correlated according to straight line  y=mx+c\n    args:\n        feat_shape: {tuple} shape of the X data\n    '''\n    X = np.random.random(shape)* np.random.randn() - np.random.randn() # complete randomness\n    m = np.random.random((shape[1],1))\n    c = np.random.randn()\n    y = X.dot(m) +  c + np.random.randn(shape[0],1)*0.13 # add some noise too\n    return X,y\n\nX,y = generate_random_data()  # try generating the data different times to see performance of model\n\n\nX = X.astype(np.float32) # default is double or float 64 in numpy\ny = y.astype(np.float32) # tf would have converted it to float32 automatically\n\n\nreg_x_points = np.linspace(X.min(),X.max(),100)\nreg_y_points = model.predict(reg_x_points.reshape(-1,1)).flatten()\n\nplt.plot(reg_y_points,reg_y_points)\nplt.scatter(X,y)\nplt.show()\n\n</code></pre>\n<p>It is not showing the line at all? I trained the data on same X and y but the best fit line is far from the data points and showing unusual behaviour. Is it because of the network, training, data or the plotting part?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 69}]
[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "tensorflow2.0", "gradient"], "owner": {"account_id": 20129857, "reputation": 11, "user_id": 14762434, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-y7qeF_T5OvA/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclqf9EDLRIV5nyMHDyHOnmEi9Vwbw/s96-c/photo.jpg?sz=256", "display_name": "Jay Lee", "link": "https://stackoverflow.com/users/14762434/jay-lee"}, "is_answered": false, "view_count": 585, "answer_count": 0, "score": 1, "last_activity_date": 1607932652, "creation_date": 1607932652, "question_id": 65285429, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65285429/tensorflow-gradients-suddenly-became-nans-during-training", "title": "Tensorflow: gradients suddenly became NaNs during training", "body": "<p>I am training simple variational autoencoder with negative binomial likelihood for decoder. I used python 3.7.1 and tensorflow 2.0.0.</p>\n<p>The model was trained well without any problems for tens of epochs, but all weights, loss, and gradients suddenly became NaN during training. I modified the code to find which variables become NaN first (among weights, loss, and gradients) and found that gradients first became nan and this affected other variables.</p>\n<p>I have googled similar issues but most of case nan appeared in the loss, which is different from this case. I tried: smaller learning rate, clipping loss... but nothing could resolve the problem.</p>\n<p>Here is the model class for autoencoder model:</p>\n<pre><code>class Encoder(tf.keras.layers.Layer):\n    def __init__(self, hidden_dim, latent_dim):\n        super(Encoder, self).__init__()\n        self.encoder_fc_1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.leaky_relu)\n        self.encoder_fc_2 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.leaky_relu)\n        self.encoder_latent = tf.keras.layers.Dense(latent_dim + latent_dim)\n\n    def call(self, input):\n        h = tf.math.l2_normalize(input, 1)\n        h = self.encoder_fc_1(h)\n        h = self.encoder_fc_2(h)\n        return tf.split(self.encoder_latent(h), num_or_size_splits=2, axis=1)\n\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, hidden_dim, vocab_size):\n        super(Decoder, self).__init__()\n        self.decoder_fc_1 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.leaky_relu)\n        self.decoder_fc_2 = tf.keras.layers.Dense(hidden_dim, activation=tf.nn.leaky_relu)\n        self.decoder_fc_3 = tf.keras.layers.Dense(vocab_size + vocab_size)\n    \n    def call(self, z):\n        h = self.decoder_fc_1(z)\n        h = self.decoder_fc_2(h)\n        return tf.split(self.decoder_fc_3(h), num_or_size_splits=2, axis=1)\n\nclass NBVAE(tf.keras.Model):\n    def __init__(self, config):\n        super(NBVAE, self).__init__()\n        self.optimizer = tf.keras.optimizers.Adam(config[&quot;learning_rate&quot;])\n        self.encoder = Encoder(config[&quot;hidden_dim&quot;], config[&quot;latent_dim&quot;])\n        self.decoder = Decoder(config[&quot;hidden_dim&quot;], config[&quot;vocab_size&quot;])\n\n    def call(self, input):\n        mean, logvar = self.encoder(input)\n        z = reparameterize(mean, logvar)\n        h_r, h_p = self.decoder(z)\n        return mean, logvar, z, h_r, h_p\n\ndef reparameterize(mean, logvar):\n    eps = tf.random.normal(shape=mean.shape)\n    return tf.add(tf.multiply(eps, tf.math.exp( tf.math.divide(logvar, 2))), mean)\n\ndef log_normal_pdf(sample, mean, logvar, raxis=1):\n    log2pi = tf.math.log(2. * np.pi)\n    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi), axis=raxis)\n\ndef compute_logpx_z(input, h_r, h_p):\n    temp = tf.exp(-tf.multiply(tf.exp(h_r), tf.math.log(tf.exp(h_p) + 1)))\n    temp_cliped = tf.clip_by_value(temp, 1e-5, 1 - 1e-5)\n    ll = tf.multiply(input, tf.math.log(1 - temp_cliped)) + tf.multiply(1 - input, tf.math.log(temp_cliped))\n    #print(&quot;logpx_z: {}&quot;.format(tf.reduce_sum(ll, axis=-1)))\n    return tf.reduce_sum(ll, axis=-1), temp\n\ndef compute_loss(model, input):\n    mean, logvar, z, h_r, h_p = model(input)\n    logpx_z, temp = compute_logpx_z(input, h_r, h_p)\n    logpz = log_normal_pdf(z, 0., 0.)\n    logqz_x = log_normal_pdf(z, mean, logvar)\n    return tf.negative(tf.reduce_mean(logpx_z + logpz - logqz_x)), temp\n\n</code></pre>\n<p>and here is the code snippet for training the model.\nI put some if statements in the middle of the code to check which variable become NaN first.</p>\n<pre><code>    print(&quot;start training...&quot;)\n    num_batches = int(np.ceil(len(training_data) / batch_size))\n    epoch_loss = []\n\n    for epoch in range(epochs):\n        print(&quot;epoch: {}&quot;.format(epoch+1))\n        progbar = tf.keras.utils.Progbar(num_batches)\n        loss_record = []\n    \n        for i in range(num_batches):\n            x_batch = training_data[i*batch_size:(i+1)*batch_size]\n            x_batch = one_hot(x_batch, depth=len(concept2id))\n\n            with tf.GradientTape() as tape:\n                loss, temp = compute_loss(nbvae, x_batch)\n            print(&quot;step{s} loss: {l}&quot;.format(s=i, l=loss.numpy()))\n\n            # checking the loss\n            if np.isnan(loss.numpy()):\n                print(&quot;nan loss is detected&quot;)\n                detect_nan = True\n                break\n\n            loss_record.append(loss.numpy())\n            gradients = tape.gradient(loss, nbvae.trainable_variables)\n            #gradients, global_norm = tf.clip_by_global_norm(tape.gradient(loss, nbvae.trainable_variables), 10)\n\n            print(&quot;checking gradients...&quot;)\n            gradient_nancount = 0\n            for _, grad in enumerate(gradients):\n                gradient_nancount += np.sum(np.isnan(grad))\n            if gradient_nancount != 0:\n                print(&quot;nan is detected in gradients&quot;)\n                print(&quot;saving the current gradients and weights...&quot;)\n                save_data(os.path.join(output_path, &quot;error_gradients.pkl&quot;), gradients)\n                save_data(os.path.join(output_path, &quot;error_tvariables.pkl&quot;), nbvae.trainable_variables)\n                detect_nan = True\n                break\n\n            nbvae.optimizer.apply_gradients(zip(gradients, nbvae.trainable_variables))\n            \n            print(&quot;checking the updated weights...&quot;)\n            weight_nancount = 0\n            for _, weight in enumerate(nbvae.weights):\n                weight_nancount += np.sum(np.isnan(weight))\n            if weight_nancount != 0:\n                print(&quot;nan is detected in weights&quot;)\n                print(&quot;saving the current gradients and weights...&quot;)\n                save_data(os.path.join(output_path, &quot;error_gradients.pkl&quot;), gradients)\n                save_data(os.path.join(output_path, &quot;error_tvariables.pkl&quot;), nbvae.trainable_variables)\n                detect_nan = True\n                break\n            progbar.add(1)\n        \n        if detect_nan:\n            epoch_loss.append(np.nan)\n            nbvae.save_weights(os.path.join(output_path, &quot;nbvae_error{}&quot;.format(epoch+1)))\n            break\n        print(&quot;average epoch loss: {}&quot;.format(np.mean(loss_record)))\n        epoch_loss.append(np.mean(loss_record))\n</code></pre>\n<p>Anyone knows the way to resolve this problem or possible reasons? Thank you for your time in advance.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 251}]
[{"items": [{"tags": ["tensorflow"], "owner": {"account_id": 438612, "reputation": 22264, "user_id": 826983, "user_type": "registered", "accept_rate": 69, "profile_image": "https://i.stack.imgur.com/B9PSD.jpg?s=256&g=1", "display_name": "Stefan Falk", "link": "https://stackoverflow.com/users/826983/stefan-falk"}, "is_answered": true, "view_count": 582, "answer_count": 1, "score": 0, "last_activity_date": 1568380906, "creation_date": 1568361572, "last_edit_date": 1568366349, "question_id": 57919530, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57919530/getting-interrupted-by-signal-11-sigsegv", "title": "Getting interrupted by signal 11: SIGSEGV", "body": "<p>All I know is that the error occurs when this branch gets executed and the <code>weights</code> from it get passed down to <code>tf.data.experimental.sample_from_datasets</code>:</p>\n\n<pre><code># ...\nelif pretrain_cfg.schedule == PretrainSchedule.CONVERGE_LINEARLY:\n    logger.info('[%s] - Pretrain: Using CONVERGE_LINEARLY schedule' % self.name)\n    a = tf.minimum(tf.constant(1.0, dtype=tf.float64, shape=(1,)), global_step / max_pretrain_steps)\n    b = tf.maximum(tf.constant(0.0, dtype=tf.float64, shape=(1,)), 1 - global_step / max_pretrain_steps)\n    weights = a * const_task_weights + b * pretrain_task_weights\n\nreturn tf.data.experimental.sample_from_datasets(datasets, weights=weights)\n</code></pre>\n\n<p>The following works:</p>\n\n<pre><code>weights = tf.cond(\n    tf.greater(global_step, max_pretrain_steps),\n    true_fn=lambda: const_task_weights,\n    false_fn=lambda: pretrain_task_weights\n)\n</code></pre>\n\n<p>but for some reason this here causes the <code>SIGSEGV</code>:</p>\n\n<pre><code>a = tf.minimum(tf.constant(1.0, dtype=tf.float64, shape=(1,)), global_step / max_pretrain_steps)\nb = tf.maximum(tf.constant(0.0, dtype=tf.float64, shape=(1,)), 1 - global_step / max_pretrain_steps)\nweights = a * const_task_weights + b * pretrain_task_weights\n</code></pre>\n\n<p>I don't really see what the problem is but the problem comes definitely from this line: </p>\n\n<pre><code>weights = a * const_task_weights + b * pretrain_task_weights\n</code></pre>\n\n<p>The question is why. It might not be valid to have a dependency to the <code>global_step</code> in this context as since the <code>weights</code> parameter of <code>sample_from_datasets</code>.</p>\n\n<p>However, in <code>sample_from_datasets</code> I don't see anything suspicious since inside <code>sample_from_datasets</code> the first thing that happens is</p>\n\n<pre><code>weights = ops.convert_to_tensor(weights, name=\"weights\")\n</code></pre>\n\n<p>So passing a tensor to it should be fine. </p>\n\n<p>Any ideas?</p>\n\n<hr>\n\n<p>Error output:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>INFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 0 into /data/translation/multi-problem/hi2en/model/512-3-1-1024/de2en.hi2en/c19cfad259cad911/model.ckpt.\nbash: line 1:  4153 Segmentation fault      (core dumped) env \"CUDA_VISIBLE_DEVICES\"=\"0\" \"LIBRARY_ROOTS\"=\"/Users/username/Library/Caches/PyCharm2018.2/remote_sou...\n\nProcess finished with exit code 139 (interrupted by signal 11: SIGSEGV)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 261}]
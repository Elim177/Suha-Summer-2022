[{"items": [{"tags": ["tensorflow", "scikit-learn", "pytorch", "bert-language-model", "lsh"], "owner": {"account_id": 16230354, "reputation": 2719, "user_id": 11725056, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3fb8aa3bd56b90f894e9805de55ff840?s=256&d=identicon&r=PG&f=1", "display_name": "Deshwal", "link": "https://stackoverflow.com/users/11725056/deshwal"}, "is_answered": false, "view_count": 270, "answer_count": 0, "score": 2, "last_activity_date": 1609671201, "creation_date": 1609660321, "last_edit_date": 1609661575, "question_id": 65547698, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65547698/how-to-perform-the-text-similarity-using-bert-on-10m-corpus-using-lsh-annoy", "title": "How to perform the Text Similarity using BERT on 10M+ corpus? Using LSH/ ANNOY/ fiass or sklearn?", "body": "<p>My idea is to extract the <code>CLS</code> token for all the text in the DB and save it in CSV or somewhere else. So when a new text comes in, instead of using the <code>Cosine Similarity/JAccard/MAnhattan/Euclidean</code> or other distances, I have to use some approximation like <code>LSH, ANN (ANNOY, sklearn.neighbor)</code> or the one given here <code>faiss</code> . How can that be done? I have my code as:</p>\n<p><strong>PyTorch</strong>:</p>\n<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained('bert-base-uncased')\ninput_ids = torch.tensor(tokenizer.encode(&quot;Hello, I am a text&quot;)).unsqueeze(0)  # Batch size 1\noutputs = model(input_ids)\nlast_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n</code></pre>\n<p><strong>Using Tensorflow:</strong></p>\n<pre><code>import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained('bert-base-uncased')\ninput_ids = tf.constant(tokenizer.encode(&quot;Hello, my dog is cute&quot;))[None, :]  # Batch size 1\noutputs = model(input_ids)\nlast_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n</code></pre>\n<p>and I  think can get the <code>CLS</code> token as: (<strong>Please correct if wrong</strong>)</p>\n<pre><code>last_hidden_states = outputs[0]\ncls_embedding = last_hidden_states[0][0]\n</code></pre>\n<p>Please tell me if it's the right way to use and how can I use any of the <code>LSH, ANNOT, faiss</code> or something like that?</p>\n<p>So for every text, there'll a <code>768</code> length vector and we can create a <strong>N(No of texts 10M)x768</strong> matrix, how can I find the <strong>Index</strong>  of <code>top-5</code> data points (texts) which are most similar to the given <strong>image/embedding/data point</strong>?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 65}]
[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0", "huggingface-transformers"], "owner": {"account_id": 7229261, "reputation": 2422, "user_id": 5516760, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/3fJ0V.jpg?s=256&g=1", "display_name": "Marzi Heidari", "link": "https://stackoverflow.com/users/5516760/marzi-heidari"}, "is_answered": true, "view_count": 254, "accepted_answer_id": 68421933, "answer_count": 1, "score": 0, "last_activity_date": 1626536110, "creation_date": 1626529931, "question_id": 68421125, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68421125/huggingface-transformer-with-tensorflow-saves-two-files-as-model-weights", "title": "huggingface transformer with tensorflow saves two files as model weights", "body": "<p>This is how I build the model for classification task:</p>\n<pre><code>    def bert_for_classification(transformer_model_name, max_sequence_length, num_labels):\n        config = ElectraConfig.from_pretrained(\n            transformer_model_name,\n            num_labels=num_labels,\n            output_hidden_states=False,\n            output_attentions=False\n        )\n        model = TFElectraForSequenceClassification.from_pretrained(transformer_model_name, config=config)\n        # This is the input for the tokens themselves(words from the dataset after encoding):\n        input_ids = tf.keras.layers.Input(shape=(max_sequence_length,), dtype=tf.int32, name='input_ids')\n\n        # attention_mask - is a binary mask which tells BERT which tokens to attend and which not to attend.\n        # Encoder will add the 0 tokens to the some sequence which smaller than MAX_SEQUENCE_LENGTH,\n        # and attention_mask, in this case, tells BERT where is the token from the original data and where is 0 pad\n        # token:\n        attention_mask = tf.keras.layers.Input((max_sequence_length,), dtype=tf.int32, name='attention_mask')\n\n        # Use previous inputs as BERT inputs:\n        output = model([input_ids, attention_mask])[0]\n        output = tf.keras.layers.Dense(num_labels, activation='softmax')(output)\n        model = tf.keras.models.Model(inputs=[input_ids, attention_mask], outputs=output)\n\n        model.compile(loss=keras.losses.CategoricalCrossentropy(),\n                      optimizer=keras.optimizers.Adam(3e-05, epsilon=1e-08),\n                      metrics=['accuracy'])\n\n        return model\n</code></pre>\n<p>After I trained this model I save it using <code>model.save_weights('model.hd5')</code>\nBut it turns out there are two files that are saved: <code>model.hd5.index</code> and <code>model.hd5.data-00000-of-00001</code></p>\n<p>How should I load this model from the disk?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 201}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "eager-execution"], "owner": {"account_id": 11811551, "reputation": 1735, "user_id": 9277245, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/7c0a3df687d7bef3e03872a9af9908cd?s=256&d=identicon&r=PG&f=1", "display_name": "Ankish Bansal", "link": "https://stackoverflow.com/users/9277245/ankish-bansal"}, "is_answered": true, "view_count": 51965, "accepted_answer_id": 54255819, "answer_count": 1, "score": 31, "last_activity_date": 1578608139, "creation_date": 1547819571, "last_edit_date": 1578608139, "question_id": 54255431, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/54255431/invalidargumenterror-cannot-compute-matmul-as-input-0zero-based-was-expected", "title": "InvalidArgumentError: cannot compute MatMul as input #0(zero-based) was expected to be a float tensor but is a double tensor [Op:MatMul]", "body": "<p>Can somebody explain, how does TensorFlow's eager mode work? I am trying to build a simple regression as follows:</p>\n\n<pre><code>import tensorflow as tf\n\ntfe = tf.contrib.eager\ntf.enable_eager_execution()\n\nimport numpy as np\n\n\ndef make_model():\n    net = tf.keras.Sequential()\n    net.add(tf.keras.layers.Dense(4, activation='relu'))\n    net.add(tf.keras.layers.Dense(1))\n    return net\n\ndef compute_loss(pred, actual):\n    return tf.reduce_mean(tf.square(tf.subtract(pred, actual)))\n\ndef compute_gradient(model, pred, actual):\n    \"\"\"compute gradients with given noise and input\"\"\"\n    with tf.GradientTape() as tape:\n        loss = compute_loss(pred, actual)\n    grads = tape.gradient(loss, model.variables)\n    return grads, loss\n\ndef apply_gradients(optimizer, grads, model_vars):\n    optimizer.apply_gradients(zip(grads, model_vars))\n\nmodel = make_model()\noptimizer = tf.train.AdamOptimizer(1e-4)\n\nx = np.linspace(0,1,1000)\ny = x+np.random.normal(0,0.3,1000)\ny = y.astype('float32')\ntrain_dataset = tf.data.Dataset.from_tensor_slices((y.reshape(-1,1)))\n\nepochs = 2# 10\nbatch_size = 25\nitr = y.shape[0] // batch_size\nfor epoch in range(epochs):\n    for data in tf.contrib.eager.Iterator(train_dataset.batch(25)):\n        preds = model(data)\n        grads, loss = compute_gradient(model, preds, data)\n        print(grads)\n        apply_gradients(optimizer, grads, model.variables)\n#         with tf.GradientTape() as tape:\n#             loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, data))))\n#         grads = tape.gradient(loss, model.variables)\n#         print(grads)\n#         optimizer.apply_gradients(zip(grads, model.variables),global_step=None)\n</code></pre>\n\n<p><code>Gradient output: [None, None, None, None, None, None]</code>\nThe error is following:</p>\n\n<pre><code>----------------------------------------------------------------------\nValueError                           Traceback (most recent call last)\n&lt;ipython-input-3-a589b9123c80&gt; in &lt;module&gt;\n     35         grads, loss = compute_gradient(model, preds, data)\n     36         print(grads)\n---&gt; 37         apply_gradients(optimizer, grads, model.variables)\n     38 #         with tf.GradientTape() as tape:\n     39 #             loss = tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(preds, data))))\n\n&lt;ipython-input-3-a589b9123c80&gt; in apply_gradients(optimizer, grads, model_vars)\n     17 \n     18 def apply_gradients(optimizer, grads, model_vars):\n---&gt; 19     optimizer.apply_gradients(zip(grads, model_vars))\n     20 \n     21 model = make_model()\n\n~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)\n    589     if not var_list:\n    590       raise ValueError(\"No gradients provided for any variable: %s.\" %\n--&gt; 591                        ([str(v) for _, v, _ in converted_grads_and_vars],))\n    592     with ops.init_scope():\n    593       self._create_slots(var_list)\n\nValueError: No gradients provided for any variable:\n</code></pre>\n\n<h3>Edit</h3>\n\n<p>I updated my code. Now, the problem comes in gradients calculation, it is returning zero. I have checked the loss value that is non-zero.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 32}]
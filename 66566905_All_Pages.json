[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 7846906, "reputation": 2947, "user_id": 5931672, "user_type": "registered", "accept_rate": 86, "profile_image": "https://lh5.googleusercontent.com/-Ljkm-NVRzOc/AAAAAAAAAAI/AAAAAAAAAFE/EelBBzc8ji0/photo.jpg?sz=256", "display_name": "Agustin Barrachina", "link": "https://stackoverflow.com/users/5931672/agustin-barrachina"}, "is_answered": true, "view_count": 46, "accepted_answer_id": 66568298, "answer_count": 1, "score": 0, "last_activity_date": 1615391605, "creation_date": 1615386992, "last_edit_date": 1615388318, "question_id": 66566905, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66566905/debugging-tensorflow-fit-not-making-sense", "title": "Debugging tensorflow fit not making sense", "body": "<p>So I was having different results with a self-implemented code and Tensorflow results. I wanted to test each value to see where was my error (loss, gradients, optimizer, etc).</p>\n<p>Therefore I did a test code like the one in <a href=\"https://github.com/NEGU93/cvnn/blob/master/debug/mwe_testing_learning_algo.py\" rel=\"nofollow noreferrer\">this repo</a> inspired on the <a href=\"https://www.tensorflow.org/tutorials/keras/classification\" rel=\"nofollow noreferrer\">fashion mnist example</a>. Just for simplicity I will copy-paste it at the end of the question.</p>\n<p>Logic:</p>\n<p>Basically, I do 1 epoch on 1 batch. And then save:</p>\n<ol>\n<li>Weigths before training</li>\n<li>Gradients</li>\n<li>Weights after only one epoch and batch.</li>\n</ol>\n<p>As I use the default SGD TensorFlow algorithm, then the saved gradients should be equal to <code>(initial_weights - final_weights)/0.01</code>. This idea was taken from <a href=\"https://datascience.stackexchange.com/questions/77910/tensorflow-keras-how-to-get-gradient-for-an-output-class-w-r-t-a-given-input\">here</a>.</p>\n<p>However, this does not happen, what's more, results get closer if I divide by 0.0001 instead of 0.01 which is strangely enough 0.01^2.</p>\n<p>Is there an error in my logic? testing code? I cannot find it.</p>\n<p>PS: I tried using tf version 2.2.0 and 2.4.1 on Linux.</p>\n<hr />\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom pdb import set_trace\n\n\ndef get_dataset():\n    fashion_mnist = tf.keras.datasets.fashion_mnist\n    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n    return (train_images, train_labels), (test_images, test_labels)\n\n\ndef get_model(init1='glorot_uniform', init2='glorot_uniform'):\n    tf.random.set_seed(1)\n    model = tf.keras.Sequential([\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\n        tf.keras.layers.Dense(128, activation='relu', kernel_initializer=init1),\n        tf.keras.layers.Dense(10, kernel_initializer=init2)\n    ])\n    model.compile(optimizer='sgd',\n                  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n                  metrics=['accuracy'])\n    return model\n\n\ndef train(model, x_fit, y_fit):\n    np.save(&quot;initial_weights.npy&quot;, np.array(model.get_weights()))\n    with tf.GradientTape() as g:\n        y_pred = model(x_fit)\n        loss = tf.keras.losses.categorical_crossentropy(y_pred=y_pred, y_true=y_fit)\n        np.save(&quot;loss.npy&quot;, np.array(loss))\n        gradients = g.gradient(loss, model.trainable_weights)\n    np.save(&quot;gradients.npy&quot;, np.array(gradients))\n    model.fit(x_fit, y_fit, epochs=1, batch_size=100)\n    np.save(&quot;final_weights.npy&quot;, np.array(model.get_weights()))\n\n\nif __name__ == &quot;__main__&quot;:\n    (train_images, train_labels), (test_images, test_labels) = get_dataset()\n    model = get_model()\n    y_fit = np.zeros((100, 10))\n    for i, val in enumerate(train_labels[:100]):\n        y_fit[i][val] = 1.\n    train(model, train_images[:100], y_fit)\n    results = {\n        &quot;loss&quot;: np.load(&quot;loss.npy&quot;, allow_pickle=True),\n        &quot;init_weights&quot;: np.load(&quot;initial_weights.npy&quot;, allow_pickle=True),\n        &quot;gradients&quot;: np.load(&quot;gradients.npy&quot;, allow_pickle=True),\n        &quot;final_weights&quot;: np.load(&quot;final_weights.npy&quot;, allow_pickle=True)\n    }\n    for i_w, f_w, gr in zip(results[&quot;init_weights&quot;], results[&quot;final_weights&quot;], results[&quot;gradients&quot;]):\n        gr = gr.numpy()\n        print(np.allclose(gr, (i_w - f_w) / 0.01))\n    # set_trace()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 245}]
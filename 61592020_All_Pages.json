[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 7277007, "reputation": 17146, "user_id": 10133797, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ElNKG.png?s=256&g=1", "display_name": "OverLordGoldDragon", "link": "https://stackoverflow.com/users/10133797/overlordgolddragon"}, "is_answered": false, "view_count": 186, "answer_count": 0, "score": 0, "last_activity_date": 1588606428, "creation_date": 1588594505, "last_edit_date": 1588606428, "question_id": 61592020, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61592020/get-layer-from-graph-tensorflow-keras", "title": "Get layer from graph, TensorFlow / Keras", "body": "<p>I seek to access a layer's attributes from within a model's optimizer; I suppose one way to do so is via the Graph, which is accessible using e.g. <code>K.get_session()</code> (Keras/tf.Keras). However, I only find ops and tensors using <code>.get_operations()</code> or <code>._nodes_by_name</code> - not layer instances themselves. Is it possible to access layers via the default (or other) Graph?</p>\n\n<hr>\n\n<p><strong>Example</strong>: fetch <code>recurrent_regularizer</code> of the LSTM layer without accessing <code>model</code> (except <code>model.optimizer</code>):</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from keras.layers import Input, LSTM\nfrom keras.models import Model\nfrom keras.regularizers import l2\n\nipt = Input(shape=(120, 4))\nout = LSTM(60, recurrent_regularizer=l2(1e-4), name='lstm_1')(ipt)\nmodel = Model(ipt, out)\nmodel.compile('adam', loss='mse')\n</code></pre>\n\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt; print(vars(model.layers[1].cell.recurrent_regularizer))\n{'l1': array(0., dtype=float32), 'l2': array(1.e-04, dtype=float32)}\n</code></pre>\n\n<hr>\n\n<p><strong>Context</strong>: My <a href=\"https://github.com/OverLordGoldDragon/keras-adamw\" rel=\"nofollow noreferrer\">AdamW</a> implementation allows specifying which weights to decay via a <a href=\"https://github.com/OverLordGoldDragon/keras-adamw/blob/master/example.py#L18\" rel=\"nofollow noreferrer\">dictionary</a> of layer / weight names + a list of values; this does not scale well for very large or complex networks. It's more convenient for it  to work as a \"drop-in\" - only specifying decays in layer constructors, as usual. However, this information is stored in <code>layer._losses</code> or e.g. <code>layer.kernel_regularizer</code>, not in <code>weights</code> or <code>updates</code>, thus cannot be <a href=\"https://github.com/OverLordGoldDragon/keras-adamw/blob/master/keras_adamw/optimizers_225.py#L138\" rel=\"nofollow noreferrer\">accessed within</a> (from attributes).</p>\n\n<p>The optimizer will thus automatically determine <a href=\"https://github.com/OverLordGoldDragon/keras-adamw/blob/master/keras_adamw/utils.py#L73\" rel=\"nofollow noreferrer\"><code>wd</code></a>, <em>and</em> <a href=\"https://github.com/OverLordGoldDragon/keras-adamw#weight-decay-1\" rel=\"nofollow noreferrer\">zero</a> all loss-based penalties (l2, etc).</p>\n\n<hr>\n\n<p><strong>Update</strong>: \"context\" problem is solved via a wrapper. Now the question is only about the question; is what's described \"possible\" in a non overly hackish manner?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 292}]
[{"items": [{"tags": ["python", "tensorflow", "neural-network"], "owner": {"account_id": 12079507, "reputation": 37, "user_id": 10099030, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ac2d63d6352745031d202360b27e1a52?s=256&d=identicon&r=PG", "display_name": "Hassaan", "link": "https://stackoverflow.com/users/10099030/hassaan"}, "is_answered": false, "view_count": 176, "answer_count": 0, "score": 2, "last_activity_date": 1611383111, "creation_date": 1611383111, "question_id": 65856377, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65856377/calculating-gradient-with-respect-to-modified-weights-tensorflow", "title": "Calculating gradient with respect to modified weights TensorFlow", "body": "<p>I am trying to implement the <a href=\"https://arxiv.org/pdf/2010.01412.pdf\" rel=\"nofollow noreferrer\">Sharpness Aware Minimization (SAM)</a> method in a custom TensorFlow Training Loop. The algorithm follows these steps:</p>\n<ul>\n<li>Calculate gradient with respect to loss value</li>\n<li>Calculate epsilon-hat using equation in <a href=\"https://i.stack.imgur.com/V25Or.png\" rel=\"nofollow noreferrer\">2</a></li>\n<li>Calculate gradients at model.trainable_weights+epsilon-hat</li>\n<li>Update model.trainable_weights using the new gradients</li>\n</ul>\n<p>My training loop is:</p>\n<pre><code>loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\noptimizer = tf.keras.optimizers.Adam()\ntrain_acc_metric = tf.keras.metrics.CategoricalAccuracy()\nval_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n\nfor epoch in range(epochs):\n    # Iterate over the batches of train dataset\n    for batch, (inputs, targets) in enumerate(train_ds):\n        with tf.GradientTape(persistent = True) as tape:\n            # Forward pass\n            predictions = model(inputs)\n            # Compute loss value\n            loss = loss_fn(targets, predictions)\n        # Update accuracy\n        train_acc_metric.update_state(targets, predictions)\n        # Gradient wrt model's weights\n        gradient = tape.gradient(loss, model.trainable_weights)\n\n        # USING EQ 2 \n        numerator1 = list(map(lambda g: tf.math.pow(tf.math.abs(g),q-1), gradient))\n        numerator2 = list(map(lambda g: rho*tf.math.sign(g), gradient))\n        numerator = list(map(lambda n1, n2: n1*n2, numerator1,numerator2))\n        denominator = list(map(lambda g: tf.math.pow(tf.norm(g, ord=q),q), gradient))\n        epsilon = list(map(lambda n, d: n/d, numerator, denominator))\n        # Compute gradient at weights+epsilon\n        modified_weights = list(map(lambda e, w: w+e, epsilon, model.trainable_weights))\n        gradient = tape.gradient(loss, modified_weights)\n\n        # Update weights (ValueError:No gradients provided for any variable)         \n        optimizer.apply_gradients(zip(gradient, model.trainable_weights))\n</code></pre>\n<p>Upon inspecting the gradient calculated in <code>tape.gradient(loss, modified_weights)</code>, gradients for all layers are None. I am unable to figure out how to avoid disconnections in graph.</p>\n<p>A similar question has already been asked <a href=\"https://stackoverflow.com/questions/65381773/computing-gradient-of-the-model-with-modified-weights\">here</a> but without any answers.</p>\n<p>Equation <a href=\"https://i.stack.imgur.com/V25Or.png\" rel=\"nofollow noreferrer\">2</a>:\n<a href=\"https://i.stack.imgur.com/V25Or.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/V25Or.png\" alt=\"Equation 2\" /></a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 262}]
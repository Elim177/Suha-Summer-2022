[{"items": [{"tags": ["tensorflow", "keras", "lstm", "keras-layer", "lstm-stateful"], "owner": {"account_id": 5782952, "reputation": 11, "user_id": 14687806, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/726f797680251eb2503f0ce393e67943?s=256&d=identicon&r=PG&f=1", "display_name": "Stefano Mangini", "link": "https://stackoverflow.com/users/14687806/stefano-mangini"}, "is_answered": false, "view_count": 148, "answer_count": 0, "score": 1, "last_activity_date": 1606590311, "creation_date": 1606589341, "last_edit_date": 1606590311, "question_id": 65053311, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65053311/lstm-with-keras-to-optimize-a-black-box-function", "title": "LSTM with Keras to optimize a black box function", "body": "<p>I'm trying to implement the recurrent neural network architecture proposed in this paper (<a href=\"https://arxiv.org/abs/1611.03824\" rel=\"nofollow noreferrer\">https://arxiv.org/abs/1611.03824</a>), where the authors use a LSTM to minimize a black-box function (which however is assumed to be differentiable). Here is  a diagram of the proposed architecture: <a href=\"https://i.stack.imgur.com/AywF2.png\" rel=\"nofollow noreferrer\">RNN</a>. Briefly, the idea is to use an LSTM like an optimizer, which has to learn a good heuristic to propose new parameters for the unknown function y=f(parameters), so that it moves towards a minimum. Here's how the proposed procedure works:</p>\n<ol>\n<li>Select an initial value for the parameters <code>p0</code>, and for the function <code>y0 = f(p0)</code></li>\n<li>Call to LSTM cell with <code>input=[p0,y0]</code>, and whose output is a new value for the parameters <code>output=p1</code></li>\n<li>Evaluate <code>y1 = f(p1)</code></li>\n<li>Call the LSTM cell with <code>input=[p1,y1]</code>, and obtain <code>output=p2</code></li>\n<li>Evaluate <code>y2 = f(p2)</code></li>\n<li>Repeat for few times, for example stopping at fifth iteration: <code>y5 = f(p5)</code>.</li>\n</ol>\n<p>I'm trying to implement a similar model in Tensorflow/Keras but I'm having some troubles. In particular, this case is different from &quot;standard&quot; ones because we don't have a predefinite time sequence to be analyzed, but instead it is generated online, after each iteration of the LSTM cell. Thus, in this case, our input would consist of just the starting guess <code>[p0,y0=f(p0)]</code> at time <code>t=0</code>. If I understood it correctly, this model is similar to the one-to-many LSTM, but with the difference that the input to the next time step does not come from just the previous cell, but also form the output an additional function (in our case f).</p>\n<p>I managed to create a custom <code>tf.keras.layers.Layer</code> which performs the calculation for a single time step (that is it performs the LSTM cell and then use its output as input to the function f):</p>\n<pre><code>class my_layer(tf.keras.layers.Layer):\n    def __init__(self, units = 4):\n        super(my_layer, self).__init__()\n        self.cell = tf.keras.layers.LSTMCell(units)\n\n    def call(self, inputs):\n        prev_cost = inputs[0]\n        prev_params = inputs[1]\n        prev_h = inputs[2]\n        prev_c = inputs[3]\n        \n        # Concatenate the previous parameters and previous cost to create new input\n        new_input = tf.keras.layers.concatenate([prev_cost, prev_params])\n        \n        # New parameters obtained by the LSTM cell, along with new internsal states: h and c\n        new_params, [new_h, new_c] = self.cell(new_input, states = [prev_h, prev_c])\n        \n        # Function evaluation\n        new_cost = f(new_params)\n    \n        return [new_cost, new_params, new_h, new_c]\n</code></pre>\n<p>but I do not know how to build the recurrent part. I tried to do it manually, that is doing something like:</p>\n<pre><code>my_cell = my_layer(units = 4)\n\noutputs = my_cell(inputs)\noutputs1 = my_cell(outputs)\noutputs2 = my_cell(outputs1)\n</code></pre>\n<p>Is that correct? Is there some other way to do it more appropriately?</p>\n<p>Bonus question: I would like to train the LSTM to be able to optimize not only a single function f, but rather a class of different functions <code>[f1, f2, ...]</code> which share some common structure which make them similar enough to be optimized using the same LSTM. How could I implement such a training loop which takes as inputs a list of this functions <code>[f1, f2, ...]</code>, and tries to minimize them all? My first thought was to do that &quot;brute force&quot; way: use a for loop over the function and a <code>tf.GradientTape</code> which evaluates and applies the gradients for each function.</p>\n<p>Any help is much appreciated!\nThank you very much in advance! :)</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 279}]
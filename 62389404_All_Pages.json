[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0"], "owner": {"account_id": 7792192, "reputation": 21, "user_id": 5894702, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c25b33a49258eeb3cd2577ae264f59aa?s=256&d=identicon&r=PG&f=1", "display_name": "Herleik Holtan", "link": "https://stackoverflow.com/users/5894702/herleik-holtan"}, "is_answered": true, "view_count": 1084, "accepted_answer_id": 62391285, "answer_count": 1, "score": 1, "last_activity_date": 1592233808, "creation_date": 1592228294, "question_id": 62389404, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62389404/starting-training-takes-a-very-long-time-in-tensorflow-2", "title": "Starting training takes a very long time in Tensorflow 2", "body": "<p>Tensorflow 2 takes about 15 minutes to make its static graph (or whatever it's doing before the first pass). The training time after this is normal, but obviously it's hard to experiment with 15 mins of waiting for any feedback. </p>\n\n<p>The generator encoder and discriminator are RNNs (not unrolled) with GRU cells in a Keras model.</p>\n\n<p>The generator decoder is defined and called like this:</p>\n\n<pre><code>class GeneratorDecoder(tf.keras.layers.Layer):\ndef __init__(self, feature_dim):\n    super(GeneratorDecoder, self).__init__()\n    self.cell = tf.keras.layers.GRUCell(\n        GRUI_DIM, activation='tanh', recurrent_activation='sigmoid',\n        dropout=DROPOUT, recurrent_dropout=DROPOUT)\n    self.batch_normalization = tf.keras.layers.BatchNormalization()\n    self.dense = tf.keras.layers.Dense(\n        feature_dim, activation='tanh')\n\n@tf.function\ndef __call__(self, z, timesteps, training):\n    # z has shape (batch_size, features)\n    outputs = []\n    output, state = z, z\n    for i in range(timesteps):\n        output, state = self.cell(inputs=output, states=state,\n                                  training=training)\n        dense_output = self.dense(\n            self.batch_normalization(output))\n        outputs.append(dense_output)\n    return outputs\n</code></pre>\n\n<p>Here is my training loop (the mask_gt and missing_data variables are cast using tf.cast and should so already be tensors):</p>\n\n<pre><code>for it in tqdm(range(NO_ITERATIONS)):\n   print(it)\n   train_step()\n\n\n@tf.function\ndef train_step():\n    with tf.GradientTape(persistent=True) as tape:\n        generator_output = generator(missing_data, training=True)\n        imputed_data = get_imputed_data(missing_data, generator_output)\n        mask_pred = discriminator(imputed_data)\n        D_loss = discriminator.loss(mask_pred, mask_gt)\n        G_loss = generator.loss(missing_data, mask_gt,\n                                generator_output, mask_pred)\n    gen_enc_grad = tape.gradient(\n        G_loss, generator.encoder.trainable_variables)\n    gen_dec_grad = tape.gradient(\n        G_loss, generator.decoder.trainable_variables)\n    disc_grad = tape.gradient(\n        D_loss, discriminator.model.trainable_variables)\n    del tape\n\n    generator.optimizer.apply_gradients(\n        zip(gen_enc_grad, generator.encoder.trainable_variables))\n    generator.optimizer.apply_gradients(\n        zip(gen_dec_grad, generator.decoder.trainable_variables))\n    discriminator.optimizer.apply_gradients(\n        zip(disc_grad, discriminator.model.trainable_variables))\n</code></pre>\n\n<p>Note that \"0\" is printed within a few seconds, so the slow part is definitely not earlier.\nAnd this is the get_imputed_data function that is called:</p>\n\n<pre><code>def get_imputed_data(incomplete_series, generator_output):\n    return tf.where(tf.math.is_nan(incomplete_series), generator_output, incomplete_series)\n</code></pre>\n\n<p>Thanks for any answers! Hope I provided just enough code to give a sense of where the problem lies. This is my first time posting here after reading for at least five years :)</p>\n\n<p>I use Python 3.6 and Tensorflow 2.1. </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 3}]
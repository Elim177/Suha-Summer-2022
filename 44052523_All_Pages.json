[{"items": [{"tags": ["python", "tensorflow", "deep-learning"], "owner": {"user_type": "does_not_exist", "display_name": "user4911648"}, "is_answered": false, "view_count": 602, "answer_count": 0, "score": 0, "last_activity_date": 1495285542, "creation_date": 1495122800, "last_edit_date": 1495285542, "question_id": 44052523, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/44052523/how-to-do-separate-testing-after-training-validation-in-tensorflow-with-batch", "title": "How to do separate testing after training / validation in Tensorflow with Batch Normalization", "body": "<p>I am having a training script which does training and validation like this:</p>\n\n<pre><code>...\nprint(\"Initialize training\")\nlogits = utils.inference(images, FLAGS.stacks, True)\nv_logits = utils.inference(v_images, FLAGS.stacks, False)\n\nloss = utils.get_loss(logits, volumes, FLAGS.stacks, 'train')\nv_loss = utils.get_loss(v_logits, v_volumes, FLAGS.stacks, 'val')\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    train_optimizer = utils.pre_training(loss, FLAGS.learning_rate)\n\nvalidate = utils.validate(v_images, v_logits, v_volumes, FLAGS.scale)\n\ninit = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\nsaver = tf.train.Saver(max_to_keep=2)\nwith tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n    .....\n    # training\n    _, loss_list, image_batch, volume_batch, summary_str = sess.run([train_optimizer, loss, images, volumes, summary_train_op])\n\n    .....\n    # validation\n    _, val_loss_list, summary_str_val, image_input, volume_estimated, volume_ground_truth = sess.run([validate, v_loss, summary_val_op, v_images, v_logits, v_volumes])\n</code></pre>\n\n<p>My inference files creates my model with batch_normaliztion. Therefore I have to create the inference two times, the second time I have to set <code>reuse=True</code> for the weights and batch_norm. Additionally, I have to set <code>is_training=False</code> for batch_norm. I accomplish this by doing this:</p>\n\n<p>(My inference function calls the model)</p>\n\n<pre><code>class model():\n    def __init__(self, nb_stack, is_training=True, test=False):\n        self.nb_stack = nb_stack\n        self.name = name\n        self.is_training = is_training\n        self.reuse = None if is_training else True\n\n        if test:\n            self.is_training = False\n            self.reuse = None\n\n     def __call__(self, x):\n        with tf.variable_scope(self.name, reuse=self.reuse) as scope:\n            ....\n            norm1 = tf.contrib.layers.batch_norm(conv1, 0.9, epsilon=1e-5, activation_fn=tf.nn.relu, scope=sc, reuse=self.reuse, is_training=self.is_training, fused=True)\n</code></pre>\n\n<p>Then after I have finished my training I want to test my finished graph by loading it in another file and perform the test like this:</p>\n\n<pre><code>t_image, t_volume = utils.get_test_data(FLAGS.test_file_path)\n\n        #logits = utils.test_inference(t_image, FLAGS.stacks)        \n        _ = utils.inference(t_image, FLAGS.stacks, True)\n        logits = utils.inference(t_image, FLAGS.stacks, False)\n\n        print(\"Load meta graph\")\n        saver = tf.train.import_meta_graph(FLAGS.checkpoint_file_path + 'model-' + str(FLAGS.iteration) + '.meta', clear_devices=True)\n\n        with tf.Session() as sess:\n            sess.run(t_image.initializer)\n            sess.run(t_volume.initializer)\n\n            print(\"Restore graph\")\n            saver.restore(sess, FLAGS.checkpoint_file_path + \"model-\" + str(FLAGS.iteration))\n\n            print(\"Process\")\n            ....  = sess.run([logits, t_image, t_volume])\n</code></pre>\n\n<p>However, when I try to run the test program I always get something like:</p>\n\n<blockquote>\n  <p>FailedPreconditionError (see above for traceback): Attempting to use\n  uninitialized value model/preprocessing/beta\n         [[Node: model/preprocessing/beta/read = IdentityT=DT_FLOAT, _class=[\"loc:@model/preprocessing/beta\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]</p>\n</blockquote>\n\n<p>I do not really know how to set up the graph in the test file. </p>\n\n<p>As you can see I have tried:</p>\n\n<pre><code>#logits = utils.test_inference(t_image, FLAGS.stacks)  # --&gt; this set test=True in the mode\n_ = utils.inference(t_image, FLAGS.stacks, True) ## --&gt; These two lines tries to do exactly the same what I have been doing in the training file, but this does not work either\nlogits = utils.inference(t_image, FLAGS.stacks, False)\n</code></pre>\n\n<p>How can it say it is not initialised even though I call the same interference function?</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>When I use:</p>\n\n<pre><code>sess.run(tf.global_variables_initializer())\n</code></pre>\n\n<p>Then obviously I do not get the error. But that also means that the restore function does not set the variables? Might there be something wrong the two inferences functions? How am I supposed to restore the graph properly when using batch_norm? Cause I have 2 graphs after training.</p>\n\n<p>** EDIT **</p>\n\n<p>When I add:</p>\n\n<pre><code>t = tf.get_default_graph().get_tensor_by_name(\"model/preprocessing/conv1/weights/read:0\")\n\ntv = sess.run(t)\n\nprint(\"Load meta graph\")\nsaver = tf.train.import_meta_graph(FLAGS.checkpoint_file_path + 'hourglass-model-' + str(FLAGS.iteration) + '.meta', clear_devices=True)\n\nprint(\"Restore graph\")\nsaver.restore(sess, FLAGS.checkpoint_file_path + \"hourglass-model-\" + str(FLAGS.iteration))\n\n d = tf.get_default_graph().get_tensor_by_name(\"model/preprocessing/conv1/weights/read:0\")\n\n dv = sess.run(d)\n\n print(\"here:\")\n print(tv == dv)\n</code></pre>\n\n<p>I get something like this:</p>\n\n<pre><code>  [ .... values of true ....]\n  ...\n  [[ True  True  True  True  True  True  True  True  True  True  True  True\n     True  True  True  True  True  True  True  True  True  True  True  True\n     True  True  True  True  True  True  True  True  True  True  True  True\n     True  True  True  True  True  True  True  True  True  True  True  True\n     True  True  True  True  True  True  True  True  True  True  True  True\n     True  True  True  True]]]]\n</code></pre>\n\n<p>Which means that the graph is not restored correctly.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 116}]
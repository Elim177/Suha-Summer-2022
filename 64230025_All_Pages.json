[{"items": [{"tags": ["python-3.x", "keras", "tensorflow2.0", "autograd"], "owner": {"account_id": 4787209, "reputation": 23, "user_id": 3867329, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/e25460db37d1d0294e91efbd8043ce9f?s=256&d=identicon&r=PG&f=1", "display_name": "Prameesha", "link": "https://stackoverflow.com/users/3867329/prameesha"}, "is_answered": true, "view_count": 84, "accepted_answer_id": 64242233, "answer_count": 1, "score": 0, "last_activity_date": 1602114074, "creation_date": 1602001655, "question_id": 64230025, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64230025/calculate-gradient-of-validation-error-w-r-t-inputs-using-keras-tensorflow-or-au", "title": "Calculate gradient of validation error w.r.t inputs using Keras/Tensorflow or autograd", "body": "<p>I need to calculate the gradient of the validation error w.r.t inputs x. I'm trying to see how much the validation error changes when I perturb one of the training samples.</p>\n<ul>\n<li>The validation error (E) explicitly depends on the model weights (W).</li>\n<li>The model weights explicitly depend on the inputs (x and y).</li>\n<li>Therefore, the validation error implicitly depends on the inputs.</li>\n</ul>\n<p>I'm trying to calculate the gradient of <em>E</em> w.r.t <em>x</em> directly.\nAn alternative approach would be to calculate the gradient of <em>E</em> w.r.t <em>W</em> (can easily be calculated) and the gradient of <em>W</em> w.r.t <em>x</em> (cannot do at the moment), which would allow the gradient of <em>E</em> w.r.t <em>x</em> to be calculated.</p>\n<p>I have attached a toy example. Thanks in advance!</p>\n<pre><code>import numpy as np\nimport mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow as tf\nfrom autograd import grad\n\ntrain_images = mnist.train_images()\ntrain_labels = mnist.train_labels()\ntest_images = mnist.test_images()\ntest_labels = mnist.test_labels()\n\n# Normalize the images.\ntrain_images = (train_images / 255) - 0.5\ntest_images = (test_images / 255) - 0.5\n\n# Flatten the images.\ntrain_images = train_images.reshape((-1, 784))\ntest_images = test_images.reshape((-1, 784))\n\n# Build the model.\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(784,)),\n    Dense(64, activation='relu'),\n    Dense(10, activation='softmax'),\n])\n\n# Compile the model.\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\n# Train the model.\nmodel.fit(\n    train_images,\n    to_categorical(train_labels),\n    epochs=5,\n    batch_size=32,\n)\nmodel.save_weights('model.h5')\n# Load the model's saved weights.\n# model.load_weights('model.h5')\n\ncalculate_mse = tf.keras.losses.MeanSquaredError()\n\ntest_x = test_images[:5]\ntest_y = to_categorical(test_labels)[:5]\n\ntrain_x = train_images[:1]\ntrain_y = to_categorical(train_labels)[:1]\n\ntrain_y = tf.convert_to_tensor(train_y, np.float32)\ntrain_x = tf.convert_to_tensor(train_x, np.float64)\n\nwith tf.GradientTape() as tape:\n    tape.watch(train_x)\n    model.fit(train_x, train_y, epochs=1, verbose=0)\n    valid_y_hat = model(test_x, training=False)\n    mse = calculate_mse(test_y, valid_y_hat)\nde_dx = tape.gradient(mse, train_x)\nprint(de_dx)\n\n\n# approach 2 - does not run\ndef calculate_validation_mse(x):\n    model.fit(x, train_y, epochs=1, verbose=0)\n    valid_y_hat = model(test_x, training=False)\n    mse = calculate_mse(test_y, valid_y_hat)\n    return mse\n\n\ntrain_x = train_images[:1]\ntrain_y = to_categorical(train_labels)[:1]\n\nvalidation_gradient = grad(calculate_validation_mse)\nde_dx = validation_gradient(train_x)\nprint(de_dx)\n\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 51}]
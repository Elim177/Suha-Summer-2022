[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 18286815, "reputation": 21, "user_id": 13839849, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0518bfeddae282f483aabc9efc2e9899?s=256&d=identicon&r=PG&f=1", "display_name": "nvrs", "link": "https://stackoverflow.com/users/13839849/nvrs"}, "is_answered": false, "view_count": 1275, "answer_count": 0, "score": 0, "last_activity_date": 1597304610, "creation_date": 1597304610, "question_id": 63390396, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63390396/customise-train-step-in-model-fit-tensorflow-keras-invalidargumenterror-ope", "title": "Customise train_step in model.fit() Tensorflow Keras - InvalidArgumentError: Operation &#39;while&#39; has no attr named &#39;_XlaCompile&#39;", "body": "<p>I am trying to implement a custom multi-input and -output model which uses a learning algorithm as proposed in <a href=\"https://arxiv.org/pdf/1801.07593.pdf\" rel=\"nofollow noreferrer\">this</a> paper. The model itself works fine without the custom learning algorithm which I use as a baseline. The problem I encounter is that the code got stuck in the train_step function in the DebiasModel class at code line:</p>\n<pre><code>mc_pred = self.main_classifier([xu, xs], training=True)\n</code></pre>\n<p>It did not return an error. After running for an hour, I interrupted the kernel and it returns the error message saying:</p>\n<pre><code>InvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.\n\nDuring handling of the above exception, another exception occurred:\n\nInvalidArgumentError: Operation 'gradients/while_grad/Placeholder_28' has no attr named '_read_only_resource_inputs'.\n</code></pre>\n<p>I am not sure what the issue is and I have tried to use persistent=True in tf.GradientTape as well instead of declaring two gradientTapes in single watch. But, exactly the same error occurs.</p>\n<p>Does anyone have any idea what this issue is? And how it can be solved?</p>\n<p>I am using Tensorflow V2.3.0 and Keras V2.4.0</p>\n<p><strong>Source Code</strong></p>\n<pre><code>class model_components:\n\n  def mitigation_expert():\n    inputs = Input(shape=(300,), dtype=tf.int32, name=&quot;me_input&quot;)\n    x = Embedding(num_tokens, 300, weights=[embedding_matrix], input_length=max_length, trainable=False, name=&quot;me_embedding&quot;)(inputs)\n    x = LSTM(300, return_sequences=False, name=&quot;me_lstm&quot;)(x)\n\n    model = Model(inputs, x)\n\n    return model\n\n  def control_expert():\n    inputs = Input(shape=(22,), dtype=tf.int32, name=&quot;ce_input&quot;)\n    y = Dense(19, activation='relu', name=&quot;ce_hidden&quot;)(inputs)\n\n    model = Model(inputs, y)\n\n    return model\n\n  def main_classifier():\n    # Expert components\n    me = model_components.mitigation_expert()\n    ce = model_components.control_expert()\n\n    # Main classifier\n    ensemble = concatenate([me.output, ce.output], name=&quot;pred_ensemble&quot;)\n    pred_output = Dense(319, activation=&quot;relu&quot;, name=&quot;pred_hidden&quot;)(ensemble)\n    pred_output = Dense(3, activation=&quot;softmax&quot;, name=&quot;pred_output&quot;)(pred_output)\n\n    model = Model(inputs=[me.input, ce.input], outputs=pred_output, name=&quot;main_classifier&quot;)\n\n    return model\n  \n  def adversary_classifier():\n    # Mitigation Expert component\n    me = model_components.mitigation_expert()\n\n    # Adversary classifier\n    adv_output = Dense(300, activation='relu', name=&quot;adv_hidden&quot;)(me.output)\n    adv_output = Dense(1, activation='sigmoid', name=&quot;adv_output&quot;)(adv_output)\n\n    model = Model(inputs=me.input, outputs=adv_output, name=&quot;adversary_classifier&quot;)\n\n    return model\n</code></pre>\n<pre><code>def tf_normalize(x):\n  return x / (tf.norm(x) + np.finfo(np.float32).tiny)\n\nclass DebiasModel(keras.Model):\n    def __init__(self, main_classifier, adversary_classifier):\n        super(DebiasModel, self).__init__()\n        self.main_classifier = main_classifier\n        self.adversary_classifier = adversary_classifier\n\n    def compile(self, mc_optimizer, adv_optimizer, mc_loss, adv_loss, debias_param):\n        super(DebiasModel, self).compile()\n        self.mc_optimizer = mc_optimizer\n        self.adv_optimizer = adv_optimizer\n        self.mc_loss = mc_loss\n        self.adv_loss = adv_loss\n        self.debias_param = debias_param\n\n    def train_step(self, data):\n      # Unpack data from model.fit()\n      x, y, sample_weight = data\n\n      # Unpack input and output features\n      xu, xs = x\n      y_mc = y['pred_output']\n      z_adv = y['adv_output']\n\n      # Unpack sample_weights\n      mainClass_weights = sample_weight[&quot;pred_output&quot;]\n      protectClass_weights = sample_weight[&quot;adv_output&quot;]\n\n      # Generate prediction and compute loss for Main_Classifier\n      with tf.GradientTape() as mc_tape, tf.GradientTape() as me_mc_tape:\n        mc_pred = self.main_classifier([xu, xs], training=True)\n        mc_loss = self.mc_loss(y_mc, mc_pred, sample_weight=mainClass_weights)\n      \n      # Compute and Apply Gradients for CE &amp; Main Classifier\n      mc_trainable_vars = self.main_classifier.trainable_weights[3:]\n      mc_grads = mc_tape.gradient(mc_loss, mc_trainable_vars)\n      self.mc_optimizer.apply_gradients(zip(mc_grads, mc_trainable_vars))\n\n      # Generate prediction and compute loss for Adversary_Classifier\n      with tf.GradientTape() as adv_tape, tf.GradientTape() as me_adv_tape:\n        adv_pred = self.adversary_classifier(xu)\n        adv_loss = self.adv_loss(z_adv, adv_pred, sample_weight=protectClass_weights)\n      \n      # Compute and Apply Gradients for CE &amp; Main Classifier\n      adv_trainable_vars = self.adversary_classifier.trainable_weights[3:]\n      adv_grads = adv_tape.gradient(adv_loss, adv_trainable_vars)\n      self.adv_optimizer.apply_gradients(zip(adv_grads, adv_trainable_vars))\n\n      # Compute and Apply Gradients to debias ME\n      me_adv_debias_trainable_vars = self.adversary_classifier.trainable_weights[:3]\n      adv_debias_grads = me_adv_tape.gradient(adv_loss, me_adv_debias_trainable_vars)\n      adv_debias_dict = tf.lookup.StaticHashTable(\n          tf.lookup.KeyValueTensorInitializer(me_adv_debias_trainable_vars, adv_debias_grads), 0)\n      \n      me_mc_debias_trainable_vars = self.main_classifier.trainable_weights[:3]\n      mc_debias_grads = me_mc_tape.gradient(mc_loss, me_mc_debias_trainable_vars)\n\n      me_grads = []\n\n      for g, v in zip(mc_debias_grads, me_mc_debias_trainable_vars):\n        unit_adv = tf_normalize(adv_debias_dict.lookup(v))\n        g -= tf.math.reduce_sum(g * unit_adv) * unit_adv\n        g -= self.debias_param * adv_debias_dict.lookup(v)\n        me_grads.append(zip(g, v))\n      \n      self.mc_optimizer.apply_gradients(me_grads)\n      \n      return {&quot;pred_loss&quot;: mc_loss, &quot;adv_loss&quot;: adv_loss}\n</code></pre>\n<pre><code>model = DebiasModel(model_components.main_classifier(),\n                    model_components.adversary_classifier())\n\nmodel.compile(mc_optimizer=tf.keras.optimizers.Adam(),\n              adv_optimizer=tf.keras.optimizers.Adam(),\n              mc_loss=tf.keras.losses.CategoricalCrossentropy(),\n              adv_loss=tf.keras.losses.BinaryCrossentropy(),\n              debias_param=1)\n\nepoch = 5\nsample_weights = {\n    &quot;pred_output&quot;: mainClass_weight,\n    &quot;adv_output&quot;: protectClass_weight,}\n\nmodel.fit(x=[xu_train, xs_train],\n          y={&quot;pred_output&quot;: y_train, &quot;adv_output&quot;: z_train},\n          validation_data=([xu_val, xs_val], {&quot;pred_output&quot;: y_val, &quot;adv_output&quot;: z_val}),\n          sample_weight=sample_weights, epochs=epoch, batch_size=256, verbose=1)\n</code></pre>\n<p><strong>Error Traceback</strong></p>\n<pre><code>---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\n   2485       with c_api_util.tf_buffer() as buf:\n-&gt; 2486         pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)\n   2487         data = pywrap_tf_session.TF_GetBuffer(buf)\n\nInvalidArgumentError: Operation 'while' has no attr named '_XlaCompile'.\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n51 frames\nValueError: Operation 'while' has no attr named '_XlaCompile'.\n\nDuring handling of the above exception, another exception occurred:\n\nInvalidArgumentError                      Traceback (most recent call last)\nInvalidArgumentError: Operation 'gradients/while_grad/Placeholder_28' has no attr named '_read_only_resource_inputs'.\n</code></pre>\n<p>Note: I have not added the full traceback, but if needed I can provide it. Many thanks in advance!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 274}]
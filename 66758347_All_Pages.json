[{"items": [{"tags": ["tensorflow"], "owner": {"account_id": 15688505, "reputation": 1, "user_id": 15458150, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgOIPzvAsm5897oaS9xa88EMAAxWbigoTKR_f23Rg=k-s256", "display_name": "Avishek Singh", "link": "https://stackoverflow.com/users/15458150/avishek-singh"}, "is_answered": false, "view_count": 54, "answer_count": 0, "score": 0, "last_activity_date": 1616481497, "creation_date": 1616481497, "question_id": 66758347, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66758347/calculating-gradients-of-cusom-loss-function-with-gradient-tape", "title": "Calculating gradients of cusom loss function with Gradient.Tape", "body": "<p>I am trying custom traning of the network using Gradient.Tape method.\nThis traning is unsupervised.\nThe details of network and cost function is as following,\nMy Network is,</p>\n<pre><code>def CreateNetwork(inplayer, hidlayer, outlayer,seed):\n    model = keras.Sequential()\n    model.add(Dense(hidlayer, input_dim=inplayer, kernel_initializer=initializers.RandomNormal(mean=0.0,stddev=1/np.sqrt(inplayer),seed=seed), bias_initializer=initializers.RandomNormal(mean=0.0,stddev=1/np.sqrt(inplayer),seed=seed), activation='tanh'))\n    model.add(Dense(outlayer, kernel_initializer=initializers.RandomNormal(mean=0.0,stddev=1/np.sqrt(hidlayer),seed=seed), bias_initializer=initializers.Zeros(), activation='linear'))\n    return model\n</code></pre>\n<p>and my custom cost function is defined as,</p>\n<pre><code>def H_tilda(J,U,nsamples,nsites,configs,out_matrix):\n    EigenValue = 0.0\n    for k in range(nsamples):\n        config = configs[k,:]\n        out_n = out_matrix[k,:]\n        exp = 0.0\n        for i in range(nsamples):\n            n = configs[i,:]\n            out_nprime = out_matrix[i,:]\n            #------------------------------------------------------------------------------------------------\n            #    Calculation of Hopping Term\n            #------------------------------------------------------------------------------------------------\n            hop = 0.0\n            for j in range(nsites):\n                if j == 0:\n                    k = [nsites-1,j+1]\n                elif j == (nsites - 1):\n                    k = [j-1,0]\n                else:\n                    k = [j-1,j+1]\n                if n[k[0]] != 0:\n                    annihiliate1 = np.sqrt(n[k[0]])\n                    n1 = np.copy(n)\n                    n1[k[0]] = n1[k[0]] - 1\n                    n1[j] = n1[j] +1\n                    if (config == n1).all():\n                        delta1 = 1\n                    else:\n                        delta1 = 0\n                else:\n                    annihiliate1 = 0\n                    n1 = np.zeros(nsites)\n                    delta1 = 0\n                if n[k[1]] != 0:\n                    annihiliate2 = np.sqrt(n[k[1]])\n                    n2 = np.copy(n)\n                    n2[k[1]] = n2[k[1]] -1\n                    n2[j] = n2[j] + 1\n                    if (config == n2).all():\n                        delta2 = 1\n                    else:\n                        delta2 = 0\n                else:\n                    annihiliate2 = 0\n                    n2 = np.zeros(nsites)\n                    delta2 = 0\n                create = np.sqrt(n[j] + 1)\n                hop = hop + create*(annihiliate1*delta1 + annihiliate2*delta2)\n            #------------------------------------------------------------------------------------------------\n            \n            \n            #------------------------------------------------------------------------------------------------\n            #    Calculation of Onsite Term\n            #------------------------------------------------------------------------------------------------\n            if (config == n).all():\n                ons = np.sum(np.dot(np.square(n),n - 1))\n            else:\n                ons = 0.0\n            #------------------------------------------------------------------------------------------------\n            phi_value = phi(out_nprime.numpy())\n            exp = exp + ((hop + ons) * phi_value)\n        Phi_value = phi(out_n.numpy())\n        EigenValue = EigenValue + exp/Phi_value\n    return np.real(EigenValue/nsamples)\n</code></pre>\n<p>I want to do custom traning using GradientTape method, for which I used following lines ,</p>\n<pre><code>optimizer = optimizers.SGD(learning_rate=1e-3)\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n    tape.watch(tf.convert_to_tensor(configs))\n    out_matrix = model(configs)\n    print(out_matrix)\n    eival = H_tilda(J,U,nsamples,nsites,configs,out_matrix)\n    print(eival)\ngradients = tape.gradient(tf.convert_to_tensor(eival), model.trainable_weights)\nprint(gradients)\n</code></pre>\n<p>But the gradient I am getting is NONE,</p>\n<pre><code>output: [None, None, None, None]\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 178}]
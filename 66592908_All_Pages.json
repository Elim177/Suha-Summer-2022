[{"items": [{"tags": ["tensorflow", "tensorflow-probability", "bayesian-deep-learning"], "owner": {"account_id": 15340100, "reputation": 11, "user_id": 11092182, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ba2c80b462af2dcdd7eec82bed0eb4ce?s=256&d=identicon&r=PG&f=1", "display_name": "jlapin", "link": "https://stackoverflow.com/users/11092182/jlapin"}, "is_answered": false, "view_count": 119, "answer_count": 1, "score": 1, "last_activity_date": 1615652411, "creation_date": 1615512368, "last_edit_date": 1615652411, "question_id": 66592908, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66592908/how-to-calculate-gradients-on-tensorflow-probability-layers", "title": "How to calculate gradients on tensorflow_probability layers?", "body": "<p>I would like to calculate the gradients on <code>tensorflow_probability</code> layers using <code>tf.GradientTape()</code>. This is rather simple using a normal, e.g., Dense layer</p>\n<pre><code>inp = tf.random.normal((2,5))\nlayer = tf.keras.layers.Dense(10)\n\nwith tf.GradientTape() as tape:\n    out = layer(inp)\n    loss = tf.reduce_mean(1-out)\ngrads = tape.gradient(loss, layer.trainable_variables)\nprint(grads)\n[&lt;tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n array([[ 0.04086879,  0.04086879, -0.02974391,  0.04086879,  0.04086879,\n          0.04086879, -0.02974391,  0.04086879, -0.02974391, -0.07061271],\n        [ 0.01167339,  0.01167339, -0.02681615,  0.01167339,  0.01167339,\n          0.01167339, -0.02681615,  0.01167339, -0.02681615, -0.03848954],\n        [ 0.00476769,  0.00476769, -0.00492069,  0.00476769,  0.00476769,\n          0.00476769, -0.00492069,  0.00476769, -0.00492069, -0.00968838],\n        [-0.00462376, -0.00462376,  0.05914849, -0.00462376, -0.00462376,\n         -0.00462376,  0.05914849, -0.00462376,  0.05914849,  0.06377225],\n        [-0.11682947, -0.11682947, -0.06357963, -0.11682947, -0.11682947,\n         -0.11682947, -0.06357963, -0.11682947, -0.06357963,  0.05324984]],\n       dtype=float32)&gt;,\n &lt;tf.Tensor: shape=(10,), dtype=float32, numpy=\n array([-0.05, -0.05, -0.1 , -0.05, -0.05, -0.05, -0.1 , -0.05, -0.1 ,\n        -0.05], dtype=float32)&gt;]\n</code></pre>\n<p>But if I do this using DenseReparameterization, the grads register None.</p>\n<pre><code>inp = tf.random.normal((2,5))\nlayer = tfp.layers.DenseReparameterization(10)\n\nwith tf.GradientTape() as tape:\n    out = layer(inp)\n    loss = tf.reduce_mean(1-out)\ngrads = tape.gradient(loss, layer.trainable_variables)\nprint(grads)\n[None, None, None]\n</code></pre>\n<p>Can anyone tell me how to fix this issue such that the gradients are taped and register?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 186}]
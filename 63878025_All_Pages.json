[{"items": [{"tags": ["python", "tensorflow", "keras", "loss-function", "automatic-differentiation"], "owner": {"account_id": 19507904, "reputation": 1, "user_id": 14272474, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-W5_PHvzWaC8/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuclQvUAgMYF4QxQwA9L1hvj_fFhD-Q/photo.jpg?sz=256", "display_name": "Alfred", "link": "https://stackoverflow.com/users/14272474/alfred"}, "is_answered": false, "view_count": 167, "answer_count": 0, "score": 0, "last_activity_date": 1600142148, "creation_date": 1600057039, "last_edit_date": 1600142148, "question_id": 63878025, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63878025/no-gradients-provided-for-any-variable-custom-loss-function-with-random-weight", "title": "No gradients provided for any variable - Custom loss function with random weights depending on the Softmax output", "body": "<p>I have difficulties writing a custom loss function that makes use of some random weights generated according to the class/state predicted by the Softmax output. The desired property is:</p>\n<ul>\n<li>The model is a simple feedforward neural network with input-dimension as 1 and the output dimension as 6.</li>\n<li>The activation function of the output layer is Softmax, which intends to estimate the actual number of classes or states using Argmax.</li>\n<li>Note that the training data only consists of X (there is no Y).</li>\n<li>The loss function is defined according to random weights (i.e., Weibull distribution) sampled based on the predicted state number for each input sample X.</li>\n</ul>\n<p>As follows, I provided a minimal example for illustration. For simplification purposes, I only define the loss function based on the random weights for state/class-1. I get: &quot;ValueError: No gradients provided for any variable: ['dense_41/kernel:0', 'dense_41/bias:0', 'dense_42/kernel:0', 'dense_42/bias:0'].&quot;</p>\n<p>As indicated in the post below, I found out that argmax is not differntiable, and a softargmax function would help (as I implemented in the following code). However, I still get the same error.\n<a href=\"https://stackoverflow.com/questions/46926809/getting-around-tf-argmax-which-is-not-differentiable\">Getting around tf.argmax which is not differentiable</a></p>\n<pre><code>import sys\nimport time\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras import layers\nfrom scipy.stats import weibull_min\n\n###############################################################################################\n# Generate Dataset\nlb  = np.array([2.0])   # Left boundary\nub  = np.array([100.0])  # Right boundary\n# Data Points - uniformly distributed\nN_r = 50\nX_r = np.linspace(lb, ub, N_r)    \n###############################################################################################\n#Define Model\nclass DGM:\n    # Initialize the class\n    def __init__(self, X_r): \n        #Normalize training input data\n        self.Xmean, self.Xstd = np.mean(X_r), np.std(X_r)\n        X_r = (X_r - self.Xmean) / self.Xstd\n        self.X_r = X_r\n        #Input and output variable dimensions\n        self.X_dim = 1; self.Y_dim = 6\n        # Define tensors\n        self.X_r_tf = tf.convert_to_tensor(X_r, dtype=tf.float32)\n        #Learning rate\n        self.LEARNING_RATE=1e-4\n        #Feedforward neural network model\n        self.modelTest = self.test_model()\n    ###############################################\n    # Initialize network weights and biases \n    def test_model(self):\n        input_shape = self.X_dim\n        dimensionality = self.Y_dim\n        model = tf.keras.Sequential()\n        model.add(layers.Input(shape=input_shape))\n        model.add(layers.Dense(64, kernel_initializer='glorot_uniform',bias_initializer='zeros'))\n        model.add(layers.Activation('tanh'))\n        model.add(layers.Dense(dimensionality))\n        model.add(layers.Activation('softmax'))\n        return model\n    ##############################################        \n    def compute_loss(self):\n        #Define optimizer\n        gen_opt = tf.keras.optimizers.Adam(lr=self.LEARNING_RATE, beta_1=0.0,beta_2=0.9)\n        with tf.GradientTape() as test_tape:\n            ###### calculate loss\n            generated_u = self.modelTest(self.X_r_tf, training=True)\n            #number of data\n            n_data = generated_u.shape[0] \n            #initialize random weights assuming state-1 at all input samples\n            wt1 = np.zeros((n_data, 1),dtype=np.float32) #initialize weights\n            for b in range(n_data):\n                wt1[b] = weibull_min.rvs(c=2, loc=0, scale =4 , size=1)   \n            wt1 =  tf.reshape(tf.convert_to_tensor(wt1, dtype=tf.float32),shape=(n_data,1))\n            #print('-----------sampling done-----------')  \n            #determine the actual state using softargmax\n            idst = self.softargmax(generated_u)\n            idst = tf.reshape(tf.cast(idst, tf.float32),shape=(n_data,1))\n            #index state-1\n            id1 = tf.constant(0.,dtype=tf.float32)\n            #assign weights if predicted state is state-1\n            wt1_final = tf.cast(tf.equal(idst, id1), dtype=tf.float32)*wt1\n            #final loss\n            test_loss = tf.reduce_mean(tf.square(wt1_final)) \n            #print('-----------test loss calcuated-----------')\n\n        gradients_of_modelTest = test_tape.gradient(test_loss,\n                                                    [self.modelTest.trainable_variables])\n\n        gen_opt.apply_gradients(zip(gradients_of_modelTest[0],self.modelTest.trainable_variables))\n\n        return test_loss\n#reference: Getting around tf.argmax which is not differentiable\n#https://stackoverflow.com/questions/46926809/getting-around-tf-argmax-which-is-not-differentiable\n    def softargmax(self, x, beta=1e10):\n        x = tf.convert_to_tensor(x)\n        x_range = tf.range(x.shape.as_list()[-1], dtype=x.dtype)\n        return tf.reduce_sum(tf.nn.softmax(x*beta,axis=1) * x_range, axis=-1)\n\n    ##############################################\n    def train(self,training_steps=100):\n        train_start_time = time.time()\n        for step in tqdm(range(training_steps), desc='Training'):\n            start = time.time()\n            test_loss = self.compute_loss()          \n\n            if (step + 1) % 10 == 0:\n                elapsed_time = time.time() - train_start_time\n                sec_per_step = elapsed_time / step\n                mins_left = ((training_steps - step) * sec_per_step)\n                tf.print(&quot;\\nStep # &quot;, step, &quot;/&quot;, training_steps,\n                         output_stream=sys.stdout)\n                tf.print(&quot;Current time:&quot;, elapsed_time, &quot; time left:&quot;,\n                         mins_left, output_stream=sys.stdout)\n                tf.print(&quot;Test Loss: &quot;, test_loss, output_stream=sys.stdout)\n###############################################################################################\n#Define and train the model\nmodel = DGM(X_r)\nmodel.train(training_steps=100)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 271}]
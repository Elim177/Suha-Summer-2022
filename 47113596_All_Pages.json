[{"items": [{"tags": ["python", "tensorflow", "scikit-learn"], "owner": {"user_type": "does_not_exist", "display_name": "user8561138"}, "is_answered": true, "view_count": 1113, "answer_count": 1, "score": 5, "last_activity_date": 1519946446, "creation_date": 1509817106, "question_id": 47113596, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/47113596/reproducing-scikit-learns-mlpclassifier-in-tensorflow", "title": "Reproducing scikit-learn&#39;s MLPClassifier in TensorFlow", "body": "<p>I am new to Tensorflow, having previously extensively used scikit-learn. As one of my first exercises in trying to transition to TensorFlow, I'm trying to reproduce some of the results I obtained with scikit-learn's MLPClassifier.</p>\n\n<p>When I use the MLPClassifier with mostly default settings, I get up to 98% accuracy on the test set. However, when I implement what I believe is an equivalent single layer ANN in TensorFlow, I get less than 90% accuracy on the test set. The only way I can get TensorFlow to yield similar accuracy is to train over the training set multiple (> 50) times.</p>\n\n<p>Any idea on where the difference may be coming from? Or is there any implementation of the sklearn MLPClassifier in Tensorflow to which I can compare my code?</p>\n\n<p>As far as I am concerned, I am using the same optimizer (Adam), the same learning rate, L2 regularization with the same parameter, the same activation function (ReLU) and softmax evaluation at the output layer.</p>\n\n<p>My implementation of the TensorFlow graph is the following:</p>\n\n<pre><code>n_units = 500\n\nX = tf.placeholder(tf.float32, [None, n_features])\nY = tf.placeholder(tf.float32, [None, n_classes])    \n\n# Create weights for all layers\nW_input = tf.Variable(tf.truncated_normal([n_features, n_units]))\nW_out = tf.Variable(tf.truncated_normal([n_units, n_classes]))\n\n# Create biases for all layers\nb_1 = tf.Variable(tf.zeros([n_units]))\nb_2 = tf.Variable(tf.zeros(([n_classes])))\n\n# Mount layers\nhidden_layer = tf.nn.relu(tf.matmul(X, W_input) + b_1)\nlogits = tf.matmul(hidden_layer, W_out) + b_2\n\n# Get all weights into a single list\nall_weights = tf.concat([tf.reshape(W_input, [-1]), tf.reshape(W_out, [-1])], 0)\n\n# Compute loss function\ncross_entropy = tf.reduce_mean(\n    tf.losses.softmax_cross_entropy(onehot_labels=Y, logits=logits))\n\n# Compute regularization parameter\nregularizer = 0.0001*tf.nn.l2_loss(all_weights)\n\n# Train step\ntrain_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy + regularizer)\n\n# Get number of correct predictions\ncorrect_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n\n# Class prediction\nprediction = tf.argmax(tf.nn.softmax(logits), 1)\n\n# Get accuracy\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n</code></pre>\n\n<p>My implementation of the sklearn model is simply:</p>\n\n<pre><code>clf = neural_network.MLPClassifier(hidden_layer_sizes = (500,), random_state=42)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 289}]
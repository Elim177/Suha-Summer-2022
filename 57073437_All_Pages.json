[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 13764581, "reputation": 1968, "user_id": 9933449, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6130945ff1667b14dc3ca894685e46cf?s=256&d=identicon&r=PG&f=1", "display_name": "user_6396", "link": "https://stackoverflow.com/users/9933449/user-6396"}, "is_answered": true, "view_count": 150, "accepted_answer_id": 57077079, "answer_count": 1, "score": 0, "last_activity_date": 1563440272, "creation_date": 1563358077, "question_id": 57073437, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57073437/how-to-build-a-basic-mnist-neural-net-using-tensorflow-2-0", "title": "How to build a basic mnist neural net using tensorflow 2.0?", "body": "<p>I'm trying to build a neural network model using tensorflow 2.0 and I couldn't find anything online on how to do it in tensorflow 2.0</p>\n\n<p>I've tried but I couldn't figure out how to apply gradients and all etc..</p>\n\n<p>Here's what's I've tried, </p>\n\n<pre><code>import math\nimport tensorflow as tf\n\n(x_train,y_train),(x_test,y_test) = tf.keras.datasets.mnist.load_data()\n\nx_train = tf.reshape(x_train,shape=(60000,28*28))\nx_test = tf.reshape(x_test,shape=(10000,28*28))\n\nx_train = tf.cast(x_train, tf.float32)\nx_test = tf.cast(x_test, tf.float32)\n\nn_input = 784\nh1 = 512\nh2 = 128\nn_classes = 10\n\n# weights and bias initializations\nf1 = tf.Variable(tf.random.uniform(shape = (n_input,h1), minval = -(math.sqrt(6)/math.sqrt(n_input+h1)),  \n                            maxval = (math.sqrt(6)/math.sqrt(n_input+h1)))) # Xavier uniform\nf2 = tf.Variable(tf.random.uniform(shape = (h1,h2), minval = -(math.sqrt(6)/math.sqrt(h1+h2)),\n                             maxval = (math.sqrt(6)/math.sqrt(h1+h2)))) \nout = tf.Variable(tf.random.uniform(shape = (h2,n_classes), minval = -(math.sqrt(6/(h2+n_classes))),\n                                   maxval = math.sqrt(6/(h2+n_classes)) ))\n\nb1 = tf.Variable(tf.random.uniform([h1]))\nb2 = tf.Variable(tf.random.uniform([h2]))\nb_out = tf.Variable(tf.random.uniform([n_classes]))\n\ndef mlp(x):\n  input1 = tf.nn.sigmoid(tf.add(tf.matmul(x, f1), b1))\n  input2 = tf.nn.sigmoid(tf.add(tf.matmul(input1, f2), b2))  \n  output = tf.nn.softmax(tf.add(tf.matmul(input2, out), b_out))\n  return output\n\nn_shape = x_train.shape[0]\nepochs = 2\nbatch_size = 128\nlr_rate = 0.001\n\ndata_gen = tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(n_shape).batch(batch_size)\n\ndef grad(x, y):\n  with tf.GradientTape() as tape:\n    y_pred = mlp(x)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_pred)\n    loss = tf.reduce_mean(loss)\n    return tape.gradient(loss, [w, b])\n\noptimizer = tf.keras.optimizers.Adam(lr_rate)\n\nfor _ in range(epochs):\n  no_steps = int(60000/128)\n  for (batch_xs, batch_ys) in data_gen.take(no_steps):\n</code></pre>\n\n<p>I just can't figure out how to proceed further in this case? I would really appreciate the help. Thanks</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 43}]
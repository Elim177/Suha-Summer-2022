[{"items": [{"tags": ["tensorflow"], "owner": {"account_id": 438612, "reputation": 22314, "user_id": 826983, "user_type": "registered", "accept_rate": 69, "profile_image": "https://i.stack.imgur.com/B9PSD.jpg?s=256&g=1", "display_name": "Stefan Falk", "link": "https://stackoverflow.com/users/826983/stefan-falk"}, "is_answered": true, "view_count": 281, "accepted_answer_id": 68132462, "answer_count": 1, "score": 1, "last_activity_date": 1624630397, "creation_date": 1624559226, "last_edit_date": 1624625165, "question_id": 68121006, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68121006/why-are-these-gradient-accumulation-implementations-not-working", "title": "Why are these Gradient Accumulation implementations not working?", "body": "<blockquote>\n<h3>Note:</h3>\n<p>After experimenting I noticed that this problem only occurs when I am training on the GPU. I created a github issue (<a href=\"https://github.com/tensorflow/tensorflow/issues/50454\" rel=\"nofollow noreferrer\">#50454</a>). At this point I am not sure what is happening exactly.</p>\n</blockquote>\n<p>I am working on an implementation for Gradient Accumulation. However, none of the approaches seem to work. Below I am describing two approaches which could work theoretically but it appears to conflict with Tensorflow.</p>\n<h3>The idea</h3>\n<p>I want to patch an arbitrary <code>Optimizer</code>-instance by replacing its <code>apply_gradients()</code> function by my own implementation which accumulates gradients.</p>\n<pre class=\"lang-py prettyprint-override\"><code># Build model first\nmodel.build()\n\n# Patch the optimizer\noptimizer = get_patched_optimizer(optimizer, n, model.trainable_variables)\n\n# Compile the model with the patched optimizer\nmodel.compile(optimizer=optimizer)\n</code></pre>\n<p>where</p>\n<pre class=\"lang-py prettyprint-override\"><code>def get_patched_optimizer(optimizer, n, trainable_variables):\n    &quot;&quot;&quot;Patch optimizer for gradient accumulation.\n\n    :param optimizer:\n        The optimizer to patch.\n    :param n:\n        The number of accumulation steps before applying gradients.\n    :param trainable_variables:\n        Trainable parameters of the model\n    :return:\n        A patched patched optimizer for gradient accumulation.\n    &quot;&quot;&quot;\n    accumulator = _GradientAccumulationPatch(\n        n=n,\n        orig_apply_gradients=optimizer.apply_gradients,\n        trainable_variables=trainable_variables\n    )\n\n    # Replace the original function\n    optimizer.apply_gradients = accumulator.apply_gradients\n\n    return optimizer\n</code></pre>\n<h2>The happy (but not working) path</h2>\n<p>The simplest way would be to just accumulate gradients and apply gradients conditionally e.g. whenever <code>current_step % n == 0</code>.</p>\n<p>However, the problem here is that it looks like I am not able to use <code>tf.cond()</code> in this context in contrast to how they're doing it in <a href=\"https://stackoverflow.com/questions/66472201/gradient-accumulation-with-custom-model-fit-in-tf-keras\">Gradient Accumulation with Custom model.fit in TF.Keras?</a>.</p>\n<p>Using <code>tf.cond()</code> results in the following <code>RuntimeError</code></p>\n<blockquote>\n<p>RuntimeError: <code>merge_call</code> called while defining a new graph or a tf.function. This can often happen if the function <code>fn</code> passed to <code>strategy.run()</code> contains a nested <code>@tf.function</code>, and the nested <code>@tf.function</code> contains a synchronization point, such as aggregating gradients (e.g, optimizer.apply_gradients), or if the function <code>fn</code> uses a control flow statement which contains a synchronization point in the body. Such behaviors are not yet supported. Instead, please avoid nested <code>tf.function</code>s or control flow statements that may potentially cross a synchronization boundary, for example, wrap the <code>fn</code> passed to <code>strategy.run</code> or the entire <code>strategy.run</code> inside a <code>tf.function</code> or move the control flow out of <code>fn</code></p>\n</blockquote>\n<p>Here is the implementation of <code>_GradientAccumulationPatch</code> using <code>tf.cond()</code>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>class _GradientAccumulationPatch:\n\n    def __init__(\n        self,\n        n: int,\n        orig_apply_gradients,\n        trainable_variables\n    ):\n        self.n = tf.constant(n, dtype=tf.int64)\n        policy = tf.keras.mixed_precision.global_policy()\n        self.variable_dtype = policy.variable_dtype\n        self.accu_gradients = [\n            tf.Variable(\n                tf.zeros(g.shape, dtype=g.dtype),\n            ) for g in trainable_variables\n        ]\n\n        self._current_step = tf.Variable(0, dtype=tf.int64)\n        self._orig_apply_gradients = orig_apply_gradients\n\n    def apply_gradients(self, grads_and_vars, *args, **kwargs):\n\n        trainable_variables = [var for (_, var) in grads_and_vars]\n        gradients = [grad for (grad, _) in grads_and_vars]\n\n        # Always accumulate gradients\n        for i, grad in enumerate(gradients):\n            self.accu_gradients[i].assign_add(grad)\n\n        tf.cond(\n            self._can_apply_on_next_step(),\n            true_fn=lambda: self.apply_accu_gradients(trainable_variables, args, kwargs),\n            false_fn=lambda: None\n        )\n\n    def apply_accu_gradients(self, trainable_variables, *args, **kwargs):\n\n        # Call the original apply_gradients() function\n        self._orig_apply_gradients(zip(self.accu_gradients, trainable_variables), *args, **kwargs)\n\n        # Reset all accumulated gradients to zero\n        for i in range(len(self.accu_gradients)):\n            self.accu_gradients[i].assign(tf.zeros_like(trainable_variables[i]))\n\n    def _can_apply_on_next_step(self):\n        &quot;&quot;&quot;\n        :return: True if gradients should be applied; False otherwise.\n        &quot;&quot;&quot;\n        # Increment (always do this first)\n        self._current_step.assign_add(1)\n        count_mod_steps = tf.math.mod(self._current_step, self.n)\n        return tf.equal(count_mod_steps, 0)\n</code></pre>\n<h2>The more complicated path (also not working)</h2>\n<p>It is possible to remove the <code>tf.cond()</code> by simply using the signal <code>apply</code>, given by <code>_can_apply_on_next_step()</code>, as a multiplication factor and apply zero-gradients whenever we are in the accumulation-phase.</p>\n<p>The idea would be to always accumulate gradients and always apply them with one particular change:</p>\n<pre class=\"lang-py prettyprint-override\"><code>final_gradients = [grad * apply for grad in gradients]\nself._orig_apply_gradients(zip(final_gradients, trainable_variables))\n</code></pre>\n<p>This is how we'd change the <code>apply_gradients()</code> method:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def apply_gradients(self, grads_and_vars, *args, **kwargs):\n\n    can_apply = self._can_apply_on_next_step()\n    # 1.0 whenever we want to apply gradients; 0.0 otherwise\n    apply = tf.cast(can_apply, dtype=self.variable_dtype)\n    # Will be 0.0 if apply is 1.0 and vice versa\n    keep = tf.cast(tf.logical_not(can_apply), dtype=self.variable_dtype)\n\n    grads_and_vars = list(grads_and_vars)\n    gradients = [grad for (grad, _) in grads_and_vars]\n    trainable_variables = [var for (_, var) in grads_and_vars]\n\n    # Accumulate gradients\n    for i, grad in enumerate(gradients):\n        self.accu_gradients[i].assign_add(grad)\n\n    # Multiply each gradient with our apply-signal\n    final_gradients = [grad * apply for grad in self.accu_gradients]\n\n    self._orig_apply_gradients(zip(final_gradients, trainable_variables), *args, **kwargs)\n\n    # This will reset our buffer whenever &quot;keep&quot; is 0.0\n    for g in self.accu_gradients:\n        g.assign(g * keep)\n</code></pre>\n<p>But the problem is that <code>self.accu_gradients[i].assign_add(grad)</code> does not seem to have any effect. And yes, I have also tried</p>\n<pre><code>self.accu_gradients[i].assign(grad + self.accu_gradients[i])\n</code></pre>\n<p>Interestingly, the model starts to converge if I use <code>assign(grad)</code> instead as in <code>self.accu_gradients[i].assign_add(grad)</code> as you can see:</p>\n<pre><code>blue: just using assign()   # &lt;- no accumulation happening\nred:  using assign_add()\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/59X1l.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/59X1l.png\" alt=\"enter image description here\" /></a></p>\n<h2>The <code>train_step()</code></h2>\n<p>This patch should work model independently. I do have a custom <code>train_step()</code> for my model though but the implementation is pretty straight forward.</p>\n<p>Here I am just computing the <code>gradients</code> and then all the <code>apply_gradients()</code> method of the optimizer:</p>\n<pre><code>def train_step(self, data):\n\n    (inputs, (input_lengths, label_lengths), mask), y_true = data\n\n    loss, gradients = self.rnnt_gradient(\n        inputs=inputs,\n        y_true=y_true,\n        input_lengths=input_lengths,\n        label_lengths=label_lengths,\n        mask=mask\n    )\n\n    self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n    return {'loss': loss}\n\ndef test_step(self, data):\n\n    (inputs, (input_lengths, label_lengths), mask), y_true = data\n\n    val_loss = self.rnnt_loss_wrapper(\n        inputs=inputs,\n        y_true=y_true,\n        input_lengths=input_lengths,\n        label_lengths=label_lengths,\n        mask=mask\n    )\n\n    return dict(loss=val_loss)\n\n\ndef rnnt_gradient(\n    self,\n    inputs: tuple,\n    y_true: tf.Tensor,\n    input_lengths: tf.Tensor,\n    label_lengths: tf.Tensor,\n    mask=None\n):\n    with tf.GradientTape() as tape:\n        model_loss = self.rnnt_loss_wrapper(\n            inputs,\n            y_true=y_true,\n            input_lengths=input_lengths,\n            label_lengths=label_lengths,\n            mask=mask\n        )\n\n        is_mixed_precision = isinstance(self.optimizer, mixed_precision.LossScaleOptimizer)\n\n        # We always want to return the unmodified model_loss for Tensorboard\n        if is_mixed_precision:\n            loss = self.optimizer.get_scaled_loss(model_loss)\n        else:\n            loss = model_loss\n\n        gradients = tape.gradient(loss, self.trainable_variables)\n\n        if is_mixed_precision:\n            gradients = self.optimizer.get_unscaled_gradients(gradients)\n\n        return model_loss, gradients\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 210}]
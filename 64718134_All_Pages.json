[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning"], "owner": {"account_id": 929093, "reputation": 15608, "user_id": 959306, "user_type": "registered", "accept_rate": 75, "profile_image": "https://i.stack.imgur.com/MJSQv.png?s=256&g=1", "display_name": "jeffery_the_wind", "link": "https://stackoverflow.com/users/959306/jeffery-the-wind"}, "is_answered": true, "view_count": 134, "accepted_answer_id": 64736666, "answer_count": 1, "score": 2, "last_activity_date": 1604836338, "creation_date": 1604679493, "last_edit_date": 1604753235, "question_id": 64718134, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64718134/how-to-override-gradient-vector-calculation-method-for-optimization-algos-in-ker", "title": "How to override gradient vector calculation method for optimization algos in Keras, Tensorflow?", "body": "<p>So I am trying to modify a couple of the optimization algos in Keras, namely Adam or just SGD. So by default I'm pretty sure the way the parameter updates work is that the loss is averaged over the data points in the batch, and then a gradient vector is calculated based on this loss value. The other way to think of it is that you average the gradients with respect to the loss values for each data point in the batch. This is the calculation I want to change, and it is gonna be expensive, so I am trying to do it inside the optimized framework that uses GPU and all that.</p>\n<p>So, for every batch, I need to calculate the gradient with respect to the loss for each data point in the batch, then instead of taking the mean of the gradients, I will do some other average or calculation. Does anyone know how I would get access to override this functionality of Adam or SGD?</p>\n<p>After a great comment I found that there should be a way to do what I am trying to do with the <code>jacobian</code> method from <code>GradientTape</code>. However the documentation isn't so thorough, I can't figure out how it fits into the overall picture. Here I am hoping someone can help me make an adjustment to the code to use <code>jacobian</code> instead of <code>gradient</code>.</p>\n<p>As a hello world example I am trying to simply replace the <code>gradient</code> line with some code that uses <code>jacobian</code> and produce the same output. This will illustrate how to use the <code>jacobian</code> method and the connection with the output from the <code>gradient</code> method.</p>\n<p>Working Code</p>\n<pre><code>class CustomModel(keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars) # &lt;-- line to change\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(y, y_pred)\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 151}]
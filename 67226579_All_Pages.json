[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 10839573, "reputation": 209, "user_id": 7971339, "user_type": "registered", "accept_rate": 75, "profile_image": "https://www.gravatar.com/avatar/599529c1ef2114630e5a8d92b76b4299?s=256&d=identicon&r=PG&f=1", "display_name": "Vishwad", "link": "https://stackoverflow.com/users/7971339/vishwad"}, "is_answered": true, "view_count": 85, "answer_count": 1, "score": 2, "last_activity_date": 1640830083, "creation_date": 1619166646, "last_edit_date": 1619213751, "question_id": 67226579, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67226579/what-causes-stated-console-output-in-tensorflow", "title": "What causes stated console output in Tensorflow?", "body": "<p>Tensorflow puts out a lot of output in console, could you tell me what code in my program causes this output? Also, how do I suppress it?<br />\n<em>\n....\nVariable/Initializer/initial_value/7859, Variable/Initializer/initial_value/7860, Variable/Initializer/initial_value/7861, Variable/Initializer/initial_value/7862, Variable/Initializer/initial_value/7863, Variable/Initializer/initial_value/7864, Variable/Initializer/initial_value/7865, Variable/Initializer/initial_value/7866, Variable/Initializer/initial_value/7867, Variable/Initializer/initial_value/7868, Variable/Initializer/initial_value/7869, Variable/Initializer/initial_value/7870, Variable/Initializer/initial_value/7871, Variable/Initializer/initial_value/7872, Variable/Initializer/initial_value/7873, Variable/Initializer/initial_value/7874, Variable/Initializer/initial_value/7875, Variable/Initializer/initial_value/7876, Variable/Initializer/initial_value/7877, Variable/Initializer/initial_value/7878, Variable/Initializer/initial_value/7879, Variable/Initializer/initial_value/7880, Variable/Initializer/initial_value/7881, Variable/Initializer/initial_value/7882, Variable/Initializer/initial_value/7883, Variable/Initializer/initial_value/7884, Variable/Initializer/initial_value/7885, Variable/Initializer/initial_value/7886, Variable/Initializer/initial_value/7887, Variable/Initializer/initial_value/7888, Variable/Initializer/initial_value/7889, Variable/Initializer/initial_value/7890, Variable/Initializer/initial_value/7891, Variable/Initializer/initial_value/7892, Variable/Initializer/initial_value/7893, Variable/Initializer/initial_value/7894, Variable/Initializer/initial_value/7895, Variable/Initializer/initial_value/7896, Variable/Initializer/initial_value/7897, Variable/Initializer/initial_value/7898, Variable/Initializer/initial_value/7899, Variable/Initializer/initial_value/7900, Variable/Initializer/initial_value/7901, Variable/Initializer/initial_value/7902, Variable/Initializer/initial_value/7903, Variable/Initializer/initial_value/7904)' with input shapes: [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [6], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7],\n[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7],\n[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7],\n[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7],.....\n</em><br />\nHere is my code</p>\n<pre class=\"lang-py prettyprint-override\"><code>class LSTMmodel(tf.Module):\n    def __init__(self, arg_name=None):\n        super().__init__(name=arg_name)\n        self.__input = tf.Variable(initial_value=[0.0 for x in range(7)], dtype=tf.float32)\n        self.__input_reshaped = tf.reshape(self.__input, [1, 7, 1])\n        self.__network = tf.keras.layers.LSTM(units=7, input_shape=(7,1))\n        self.__output = tf.Variable(initial_value=[0.0 for x in range(7)], dtype=tf.float32)\n        self.__output = tf.reshape(self.__output, [1, 7, 1])\n    @tf.function\n    def networkTraining(self, arg_data_train, arg_labels, arg_learning_rate):\n        with tf.GradientTape() as t:\n            print('loc 1')\n            self.__input = tf.Variable(arg_data_train)\n            print('loc 2')\n            self.__input_reshaped = tf.reshape(self.__input, [1, 7, 1])\n            self.__output = self.__network(self.__input_reshaped)\n            print('loc 3')\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=arg_labels, logits=self.__output)\n            print('loc 4')\n            dw, db = t.gradient(loss, [self.w, self.b])\n            print('loc 5')\n        self.w.assign_sub(arg_learning_rate * dw)\n        self.b.assign_sub(arg_learning_rate * db)\n\n    @tf.function\n    def __call__(self, arg_input=[0 for x in range(7)]):\n        self.__input = tf.Variable(arg_input)\n        self.__output = self.__network(self.__input)\n        return self.__output\n\n# some other code\n#\n#\n#\nmodela.networkTraining(cgm ,labels, 0.4)\n</code></pre>\n<p>EDIT:<br />\nThe 'Variable/Ini..../&lt;a_number&gt;' happens right before the <code>print('loc 4')</code></p>\n<pre class=\"lang-py prettyprint-override\"><code># ###########################FULL-CODE\nimport pandas\nimport scipy.io as loader\nimport tensorflow as tf\nimport keras\nimport numpy\n\ntf.get_logger().setLevel('INFO')\nclass LSTMmodel(tf.Module):\n    def __init__(self, arg_name=None):\n        super().__init__(name=arg_name)\n        self.__input = tf.Variable(initial_value=[0.0 for x in range(7)], dtype=tf.float32)\n        self.__input_reshaped = tf.reshape(self.__input, [1, 7, 1])\n        self.__network = tf.keras.layers.LSTM(units=7, input_shape=(7,1))\n        self.__output = tf.Variable(initial_value=[0.0 for x in range(7)], dtype=tf.float32)\n        self.__output = tf.reshape(self.__output, [1, 7, 1])\n    @tf.function\n    def networkTraining(self, arg_data_train, arg_labels, arg_learning_rate):\n        with tf.GradientTape() as t:\n            print('loc 1')\n            self.__input = tf.Variable(arg_data_train)\n            print('loc 2')\n            self.__input_reshaped = tf.reshape(self.__input, [1, 7, 1])\n            self.__output = self.__network(self.__input_reshaped)\n            print('loc 3')\n            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=arg_labels, logits=self.__output)\n            print('loc 4')\n            dw, db = t.gradient(loss, [self.w, self.b])\n            print('loc 5')\n        self.w.assign_sub(arg_learning_rate * dw)\n        self.b.assign_sub(arg_learning_rate * db)\n\n    @tf.function\n    def __call__(self, arg_input=[0 for x in range(7)]):\n        self.__input = tf.Variable(arg_input)\n        self.__output = self.__network(self.__input)\n        return self.__output\n    # tf.nn.sigmoid_cross_entropy_with_logits(labels=None, logits=None)\n\ncorrect = numpy.load('save.npy')\n# link for the .mat file for your convenience\n# https://drive.google.com/file/d/1NOZOeRm1oLOU12p3J4-zw3RrPnJZvw4-/view?usp=sharing\ninsulin = loader.loadmat('InsulinGlucoseData2.mat')\nalso = pandas.read_csv('dates222.csv')\n\n# print(type(insulin['numCGM'][0:5]))\n# quit()\ncgm = []\ntemp = []\nlabels = []\n# print(insulin['numCGM'])\n# print(insulin['actBolusDelivered'])\ncounter = also.shape[0] - 1\ncounta = 0\n# print(len(insulin['numCGM']))\nwhile counter &gt;= 0:\n    if also['classification2'][counter] == 1:\n        cgm.append(insulin['numCGM'][0][counter:counter + 6])\n        labels.append([0, 0, 0, 0, 0, 0, 1])\n        counter = counter - 7\n    elif also['classification2'][counter] == 0:\n        counter = counter - 1\n        counta = counta + 1\n        temp.append(insulin['numCGM'][0][counter])\n        if counta == 7:\n            counta = 0\n            cgm.append(temp)\n            temp = []\n            labels.append([0, 0, 0, 0, 0, 0, 0])\n    if counter - 7 &lt; 0: break\n    # print(counter)\n# print(len(cgm[0]))\n# quit()\nmodela = LSTMmodel(arg_name='ghajini_disease')\n# print('------------------------------------------------')\n# cgm = numpy.array(cgm, numpy.float32)\nfor each in labels:\n    for each_one in each:\n        each_one = float(each_one)\n# print(len(labels))\n# print('------------------------------------------------')\nmodela.networkTraining(cgm ,labels, 0.4)\n\nout = modela(cgm)\nprint(out)\n\nfor each in out:\n    su = su + each[-1]\n\ntrain_accuracy = su/sum(labels) * 100\n\ntest = modela(new)\n</code></pre>\n<p>EDIT:<br />\nI suspect that this problem is being caused in the line <code>self.__input = tf.Variable(arg_data_train)</code>. <code>arg_data_train</code> is actually a python list of python lists. I still don't understand the console output though.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 158}]
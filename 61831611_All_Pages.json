[{"items": [{"tags": ["python", "gpflow"], "owner": {"account_id": 2872263, "reputation": 31, "user_id": 2465331, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b27ff71860cf7e15ab525f194c32b44e?s=256&d=identicon&r=PG", "display_name": "irum", "link": "https://stackoverflow.com/users/2465331/irum"}, "is_answered": true, "view_count": 844, "answer_count": 1, "score": 1, "last_activity_date": 1589884084, "creation_date": 1589600676, "last_edit_date": 1589884084, "question_id": 61831611, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61831611/why-does-a-gpflow-model-not-seem-to-learn-anything-with-tensorflow-optimizers-su", "title": "Why does a GPflow model not seem to learn anything with TensorFlow optimizers such as tf.optimizers.Adam?", "body": "<p>My inducing points are set to trainable but do not change when I call <code>opt.minimize()</code>. Why is it and what does it mean? Does it mean, the model is not learning?\nWhat is the difference between <code>tf.optimizers.Adam(lr)</code> and <code>gpflow.optimizers.Scipy</code>?</p>\n\n<p>The following is the simple classification example adapted from the documentation. When I run this code example with gpflow's Scipy optimizer then I get the trained results and the values for inducing variables keep changing. But when I use Adam optimizer then I get only a straight line prediction, and the values for inducing points remain the same. It indicates that the model is not learning with Adam optimizer.  </p>\n\n<p><a href=\"https://i.stack.imgur.com/tyhjw.png\" rel=\"nofollow noreferrer\">plot of data before training</a></p>\n\n<p><a href=\"https://i.stack.imgur.com/xPLmB.png\" rel=\"nofollow noreferrer\">plot of data after training with Adam</a></p>\n\n<p><a href=\"https://i.stack.imgur.com/xB9Lb.png\" rel=\"nofollow noreferrer\">plot of data after training with gpflow optimizer Scipy</a></p>\n\n<p>The link for the example is <a href=\"https://gpflow.readthedocs.io/en/develop/notebooks/advanced/multiclass_classification.html\" rel=\"nofollow noreferrer\">https://gpflow.readthedocs.io/en/develop/notebooks/advanced/multiclass_classification.html</a></p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\n\n\nimport warnings\nwarnings.filterwarnings('ignore')  # ignore DeprecationWarnings from tensorflow\n\nimport matplotlib.pyplot as plt\n\nimport gpflow\n\nfrom gpflow.utilities import print_summary, set_trainable\nfrom gpflow.ci_utils import ci_niter\n\nfrom tensorflow2_work.multiclass_classification import plot_posterior_predictions, colors\n\nnp.random.seed(0)  # reproducibility\n\n# Number of functions and number of data points\nC = 3\nN = 100\n\n# RBF kernel lengthscale\nlengthscale = 0.1\n\n# Jitter\njitter_eye = np.eye(N) * 1e-6\n\n# Input\nX = np.random.rand(N, 1)\n\nkernel_se = gpflow.kernels.SquaredExponential(lengthscale=lengthscale)\nK = kernel_se(X) + jitter_eye\n\n# Latents prior sample\nf = np.random.multivariate_normal(mean=np.zeros(N), cov=K, size=(C)).T\n\n# Hard max observation\nY = np.argmax(f, 1).reshape(-1,).astype(int)\nprint(Y.shape)\n\n# One-hot encoding\nY_hot = np.zeros((N, C), dtype=bool)\nY_hot[np.arange(N), Y] = 1\n\ndata = (X, Y)\n\nplt.figure(figsize=(12, 6))\norder = np.argsort(X.reshape(-1,))\nprint(order.shape)\n\nfor c in range(C):\n    plt.plot(X[order], f[order, c], '.', color=colors[c], label=str(c))\n    plt.plot(X[order], Y_hot[order, c], '-', color=colors[c])\n\n\nplt.legend()\nplt.xlabel('$X$')\nplt.ylabel('Latent (dots) and one-hot labels (lines)')\nplt.title('Sample from the joint $p(Y, \\mathbf{f})$')\nplt.grid()\nplt.show()\n\n\n# sum kernel: Matern32 + White\nkernel = gpflow.kernels.Matern32() + gpflow.kernels.White(variance=0.01)\n\n# Robustmax Multiclass Likelihood\ninvlink = gpflow.likelihoods.RobustMax(C)  # Robustmax inverse link function\nlikelihood = gpflow.likelihoods.MultiClass(C, invlink=invlink)  # Multiclass likelihood\nZ = X[::5].copy()  # inducing inputs\n#print(Z)\n\nm = gpflow.models.SVGP(kernel=kernel, likelihood=likelihood,\n    inducing_variable=Z, num_latent_gps=C, whiten=True, q_diag=True)\n\n# Only train the variational parameters\nset_trainable(m.kernel.kernels[1].variance, True)\nset_trainable(m.inducing_variable, True)\nprint(m.inducing_variable.Z)\nprint_summary(m)\n\n\ntraining_loss = m.training_loss_closure(data) \n\nopt.minimize(training_loss, m.trainable_variables)\nprint(m.inducing_variable.Z)\nprint_summary(m.inducing_variable.Z)\n\n\nprint(m.inducing_variable.Z)\n\n# %%\nplot_posterior_predictions(m, X, Y)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 174}]
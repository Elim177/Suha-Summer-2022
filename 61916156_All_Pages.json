[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "tensorflow2.0", "reinforcement-learning"], "owner": {"account_id": 18635854, "reputation": 21, "user_id": 13582770, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/a97aa81889d2e76a07129cc7937e499d?s=256&d=identicon&r=PG&f=1", "display_name": "DanischKhurshid", "link": "https://stackoverflow.com/users/13582770/danischkhurshid"}, "is_answered": true, "view_count": 2957, "answer_count": 2, "score": 1, "last_activity_date": 1590012822, "creation_date": 1589986258, "last_edit_date": 1590001145, "question_id": 61916156, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61916156/tensorflow-2-how-can-i-use-adamoptimizer-minimize-for-updating-weights", "title": "Tensorflow 2: How can I use AdamOptimizer.minimize() for updating weights", "body": "<p>In the first Tensorflow it was possible to just <code>minimize()</code>without any <code>var_list</code>. In Tensorflow 2 it is important to have a <code>var_list</code>included.\nIn my project I want to use the policy gradient algorithm to play TIC-TAC-TO. </p>\n\n<p>How can I tune the weights of the model by calling minimizing. \nMy idea was to do the following: <code>tf.keras.optimizers.Adam(learning_rate=self.learning_rate).minimize(loss_func, var_list=self.model.weights)</code></p>\n\n<p>This my neural network:</p>\n\n<pre><code>class NeuralNetwork():\n\n    def __init__(self, learning_rate = 0.0001):    \n        self.model = tf.keras.Sequential()\n        self.learning_rate = learning_rate\n\n        self.hidden_layer = tf.keras.layers.Dense(243, activation=tf.nn.relu, input_dim=27)\n        self.output_layer = tf.keras.layers.Dense(9)\n\n        self.model.add(self.hidden_layer)\n        self.model.add(self.output_layer)\n\n        self.model.build()\n\n    def training(self, board_state_memory, action_state_memory, G):\n        loit = []\n        for board_state in board_state_memory:\n            loit.append(self.model.predict(x=board_state)[0])\n        print(np.array(loit).shape)\n        print(np.array(action_state_memory).shape)\n\n        neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=action_state_memory, logits=loit, name=None)\n        loss_func = neg_log_prob * G\n        print(loss_func)\n\n        tf.keras.optimizers.Adam(learning_rate=self.learning_rate).minimize(loss_func, var_list=self.model.weights)\n\n    def predict(self, board_state):\n        output = self.model.predict(x=board_state)\n        actions = tf.nn.softmax(output)\n        return actions[0]\n</code></pre>\n\n<p>This did not work out and I got the error: <code>'tensorflow.python.framework.ops.EagerTensor' object is not callable</code> </p>\n\n<p>How can I tune the weights of the model by the loss function. The easiest way.</p>\n\n<p>This is my entire code:</p>\n\n<pre><code>import numpy as np\n\n%matplotlib inline\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n</code></pre>\n\n<p>Basic implementation of the game:</p>\n\n<pre><code>class TicTacTo:\n\n    def __init__(self):\n        self.printing = False\n\n    def setBoard(self):\n        self.board = [0] * 9\n\n    def getPlayerName(self, val):\n        for player_name, player_value in PLAYER.items():\n            if player_value == val:\n                return player_name\n\n    def printBoard(self):\n        if (self.printing):\n            boardDisplay = ['_'] * 9\n            for i, val in enumerate(self.board):\n                if val != 0:\n                    boardDisplay[i] = self.getPlayerName(val)\n\n            print(boardDisplay[0] + '|' + boardDisplay[1] + '|' + boardDisplay[2])\n            print(boardDisplay[3] + '|' + boardDisplay[4] + '|' + boardDisplay[5])\n            print(boardDisplay[6] + '|' + boardDisplay[7] + '|' + boardDisplay[8])\n\n            print(\"\\n\")\n    def printResult(self, result):\n        if (self.printing):\n            if result == 0:\n                print(\"DRAW!\")\n            else:\n                print(\"{} won the game!\".format(self.getPlayerName(result)))\n\n    @staticmethod\n    def check(board):\n\n        # check for diagonals\n        if board[0] != 0 and board[0] == board[4] == board[8]: # check first diagonal\n            return board[0]\n        if board[2] != 0 and board[2] == board[4] == board[6]: # check second diagonal\n            return board[2]\n\n        # check horizontal\n        for n in range(3):\n            if (board[3*n+0] != 0) and (board[3*n+0] == board[3*n+1] == board[3*n+2]):\n                return board[3*n+0]\n\n        # check vertical\n        for i in range(3): \n            if (board[i] != 0) and (board[i] == board[i+3] == board[i+6]):\n                return board[i]\n\n        # check for a draw\n        if all(i != 0 for i in board): \n            return 0\n\n        return 2\n\n    def evaluate(self):\n        result = TicTacTo.check(self.board)\n        if result != 2: # check if game is finished\n            self.printResult(result)\n            return True\n        return False\n\n    \"\"\"\n    Player can take a move\n    :param player: Object of the player\n    :position: The position in the board where the move is set\n\n    :return: Result of the game\n        [1]  =&gt; Player X won\n        [-1] =&gt; Player O won\n        [0]  =&gt; Draw\n        [2]  =&gt; Game is not finished\n    \"\"\"\n    def move(self, player, position):\n        self.board[position] = player.value\n        self.printBoard()\n\n        return self.evaluate()\n\n\n    def availableMoves(self):\n        empty = []\n        for i, val in enumerate(self.board):\n            if val == 0:\n                empty.append(i)\n        return empty\n\n\n    def simulate(self, playerA, playerB):\n\n        self.setBoard()\n        self.printBoard()\n\n        playerA.start()\n        playerB.start()\n\n        while True:\n\n            moveA = playerA.turn(self.board, self.availableMoves())\n            stop = self.move(playerA, moveA)\n            if(stop): break\n\n            moveB = playerB.turn(self.board, self.availableMoves())\n            stop = self.move(playerB, moveB)\n            if(stop): break\n\n        result = TicTacTo.check(self.board)\n        playerA.learn(result)\n        playerB.learn(result)\n\n\n    def simulations(self, playerA, playerB, games, printing):\n        self.printing = printing\n        x_win = 0\n        o_win = 0\n        draw = 0\n        for n in range(games):\n            self.simulate(playerA, playerB)\n            result = TicTacTo.check(self.board)\n            if (result == 0): draw += 1\n            elif (result == 1): x_win += 1\n            elif (result == -1): o_win += 1\n        total = x_win + o_win + draw\n        #print(\"Win X: {}%, Win O: {}%, Draw: {}%\".format(100*(x_win/total), 100*(o_win/total), 100*(draw/total)))\n        return x_win, o_win, draw\n</code></pre>\n\n<p>Evaluate the game:</p>\n\n<pre><code>def evaluu(game, playerA, playerB, num_battles, games_per_battle = 100):\n    x_wins = []\n    o_wins = []\n    draws = []\n    game_number = []\n    game_counter = 0\n    for i in range(num_battles):\n        xwin, owin, draw = game.simulations(playerA, playerB, games_per_battle, False) \n        total = xwin + owin + draw\n        print(\"End Win X: {}%, Win O: {}%, Draw: {}%\".format(100*(xwin/total), 100*(owin/total), 100*(draw/total)))\n        print(\"Round: \", game_counter)\n        x_wins.append(xwin*100.0/games_per_battle)\n        o_wins.append(owin*100.0/games_per_battle)\n        draws.append(draw*100.0/games_per_battle)\n\n        game_counter=game_counter+1\n        game_number.append(game_counter)\n\n    plt.ylabel('Game outcomes in %')\n    plt.xlabel('Game number')\n\n    plt.plot(game_number, draws, 'r-', label='Draw')\n    plt.plot(game_number, x_wins, 'g-', label='Player X wins')\n    plt.plot(game_number, o_wins, 'b-', label='Player O wins')\n    plt.legend(loc='best', shadow=True, fancybox=True, framealpha =0.7)\n</code></pre>\n\n<p>Random--Player:</p>\n\n<pre><code>class RandomPlayer:\n    def __init__(self, player_name):\n        self.name = player_name\n        self.value = PLAYER[self.name]\n\n    def start(self):\n        pass\n    def turn(self, board, availableMoves):\n        return availableMoves[random.randrange(0, len(availableMoves))]\n    def learn(self, result):\n        pass\n</code></pre>\n\n<pre><code>import tensorflow as tf\nprint(tf.__version__)\nimport numpy as np\n</code></pre>\n\n<p>NeuralNet:</p>\n\n<pre><code>class NeuralNetwork():\n\n    def __init__(self, learning_rate = 0.0001): \n\n        self.model = tf.keras.Sequential()\n        self.learning_rate = learning_rate\n\n        self.hidden_layer = tf.keras.layers.Dense(243, activation=tf.nn.relu, input_dim=27)\n        self.output_layer = tf.keras.layers.Dense(9)\n\n        self.model.add(self.hidden_layer)\n        self.model.add(self.output_layer)\n\n        #self.model.build()\n\n    def training(self, board_state_memory, action_state_memory, G):\n        loit = []\n        for board_state in board_state_memory:\n            loit.append(self.model.predict(x=board_state)[0])\n        #print(np.array(loit).shape)\n        #print(np.array(action_state_memory).shape)\n\n        def loss(): \n            with tf.GradientTape() as tape:\n                neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=loit, labels=action_state_memory, name=None)\n                print(neg_log_prob * G)\n                return neg_log_prob * G\n\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n        self.optimizer.minimize(loss, var_list=self.model.weights)\n\n    def predict(self, board_state):\n        output = self.model.predict(x=board_state)\n        actions = tf.nn.softmax(output)\n        return actions[0]\n</code></pre>\n\n<p>Policy Agent:</p>\n\n<pre><code>class PolicyAgent:\n    def __init__(self, player_name):\n        self.name = player_name\n        self.value = PLAYER[self.name]\n\n    def board_to_input(self, board):\n        input_ = np.array([0] * 27)\n        for i, val in enumerate(board):\n            if val == self.value:\n                input_[i] = 1  \n            if val == self.value * -1:\n                input_[i+9] = 1\n            if val == 0:\n                input_[i+18] = 1\n\n        return np.reshape(input_, (1,-1))\n\n    def start(self, learning_rate=0.001, gamma=0.1):\n        self.learning_rate = learning_rate\n        self.gamma = gamma\n        self.moves = list(range(0,9))\n\n        self.state_memory = []\n        self.action_memory = []\n        self.reward = [] #just one reward at end\n\n        self.nn = NeuralNetwork(self.learning_rate)\n\n    def turn(self, board, availableMoves):\n        actions_prob = self.nn.predict(self.board_to_input(board))\n        actions_prob = np.array(actions_prob)\n        actions_prob /= actions_prob.sum()  # normalize\n\n        #print(actions_prob)\n\n        move = np.random.choice(self.moves, p=actions_prob)\n        while move not in availableMoves:\n            move = np.random.choice(self.moves, p=actions_prob)\n\n        #print(\"Move: \", move)\n        self.state_memory.append(self.board_to_input(board.copy()))\n        self.action_memory.append(move)\n\n        return move\n\n    def calculateReward(self, end_reward):\n        G = 0\n        discount = 1\n\n        runing = end_reward\n        for t in range(len(self.action_memory)):\n            G += runing\n            runing = runing * discount \n            discount *= self.gamma\n\n        return G\n\n    def learn(self, result):\n        if result == 0:\n            reward = 0.5\n        elif result == self.value:\n            reward = 1\n        else:\n            reward = 0\n\n        G = self.calculateReward(reward)\n        print(\"G value: \", G)\n        self.nn.training(self.state_memory, self.action_memory, G)\n\n        self.state_memory = []\n        self.action_memory = []\n        self.reward = [] #just one reward at end\n\n</code></pre>\n\n<p>Try to simulate the game:</p>\n\n<pre><code>PLAYER = {\"X\": 1, \"O\": -1}\nplayer20 = PolicyAgent(\"X\")\n\nu = TicTacTo()\nu.simulations(player20, RandomPlayer(\"O\"), 1, True)\n</code></pre>\n\n<p>Current Error:</p>\n\n<pre><code>ValueError: No gradients provided for any variable: ['dense_2/kernel:0', 'dense_2/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0'].\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 152}]
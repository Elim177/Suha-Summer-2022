[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 19785975, "reputation": 17, "user_id": 14489928, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4d5b12d7544e1f4240beee0390345a93?s=256&d=identicon&r=PG&f=1", "display_name": "Chandra", "link": "https://stackoverflow.com/users/14489928/chandra"}, "is_answered": true, "view_count": 88, "accepted_answer_id": 64748002, "answer_count": 1, "score": 0, "last_activity_date": 1605032818, "creation_date": 1604861440, "last_edit_date": 1605032818, "question_id": 64741883, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64741883/why-am-i-getting-the-error-valueerror-no-gradients-provided-for-any-variable", "title": "Why am I getting the error &quot;ValueError: No gradients provided for any variable:&quot; while using train_step() in keras?", "body": "<p>I'm facing trouble with tensorFlow.keras. While executing the following code</p>\n<pre><code>class Whole_model(tf.keras.Model):\n    def __init__(self, EEG_gen_model, emg_feature_extractor, eeg_feature_extractor, seq2seq_model):\n        super(Whole_model, self).__init__()\n        self.EEG_gen_model= EEG_gen_model\n        self.emg_feature_extractor= emg_feature_extractor\n        self.eeg_feature_extractor= eeg_feature_extractor\n        self.seq2seq_model=seq2seq_model\n\n   def compile(self, EEG_gen_optimizer, emg_feature_optim, eeg_feature_optim, seq2seq_optim, EEG_gen_loss, seq2seq_loss_fn, gen_mae, accuracy):\n      super(Whole_model, self).compile()\n      self.EEG_gen_optimizer = EEG_gen_optimizer\n      self.emg_feature_optim=emg_feature_optim\n      self.eeg_feature_optim=eeg_feature_optim\n      self.seq2seq_optim=seq2seq_optim\n      self.EEG_gen_loss = EEG_gen_loss\n      self.seq2seq_loss_fn=seq2seq_loss_fn\n      self.gen_mae=gen_mae\n      self.accuracy=accuracy\n      #we can use diffrent optimizer for each model\n\n  def train_step(self, data):\n      no_Epochs=3\n      x_train, [y_train_eeg, y]= data\n      y = tf.reshape(y, [-1, no_Epochs , 5])\n      n_samples_per_epoch=x_train.shape[1]\n      print(n_samples_per_epoch)\n      emg_input=tf.reshape(x_train, [-1, n_samples_per_epoch, 1])\n      y_eeg_true= tf.reshape(y_train_eeg, [-1, n_samples_per_epoch, 1])\n      print(emg_input.shape, y_eeg_true.shape)\n\n      #tf.argmax(pred_classes,1)\n      # Train the EEG generator\n      with tf.GradientTape() as tape:\n          EEG_Gen= self.EEG_gen_model(emg_input)\n          print(EEG_Gen.shape, y_eeg_true.shape)\n          gen_model_loss= self.EEG_gen_loss(y_eeg_true, EEG_Gen)\n          gen_MAE= self.gen_mae(y_eeg_true, EEG_Gen)\n          grads = tape.gradient(gen_model_loss, self.EEG_gen_model.trainable_weights)\n          self.EEG_gen_optimizer.apply_gradients(zip(grads, self.EEG_gen_model.trainable_weights))\n\n          #SEQ2SEQ \n          emg_inp = x_train\n          eeg_inp = self.EEG_gen_model(emg_inp)\n          emg_enc_seq=self.emg_feature_extractor(emg_inp)\n          eeg_enc_seq=self.eeg_feature_extractor(eeg_inp)\n    \n          len_epoch=input_layer.shape[1] \n          inputs=tf.reshape(input_layer, [-1, no_Epochs ,len_epoch]) \n    \n          # Train the discriminator\n         with tf.GradientTape() as tape:\n             outputs=self.seq2seq_model(inputs)\n             seq2seq_loss= self.seq2seq_loss_fn(y, outputs)\n             print('loss',  seq2seq_loss)\n             accuracy=self.accuracy(y, outputs)\n        \n        \n      grads = tape.gradient(seq2seq_loss, self.seq2seq_model.trainable_weights)\n     self.seq2seq_optim.apply_gradients(zip(grads, self.seq2seq_model.trainable_weights))\n\n      #fEATURE EXTRACTOR\n      emg_inp = x_train\n      eeg_inp = self.EEG_gen_model(emg_inp)\n      eeg_enc_seq=self.emg_feature_extractor(emg_inp)\n   \n      with tf.GradientTape() as tape:\n          eeg_enc_seq=self.eeg_feature_extractor(eeg_inp)\n          len_epoch=input_layer.shape[1] \n          inputs=tf.reshape(input_layer, [-1, no_Epochs ,len_epoch])\n          outputs=self.seq2seq_model(inputs)\n          seq2seq_loss= self.seq2seq_loss_fn(y, outputs)\n          print('loss',  seq2seq_loss)\n     grads = tape.gradient(seq2seq_loss, self.eeg_feature_extractor.trainable_weights)\n    \n     self.eeg_feature_optim.apply_gradients(zip(grads, self.eeg_feature_extractor.trainable_weights))     \n\n\n\n       emg_inp = x_train\n       eeg_inp = self.EEG_gen_model(emg_inp)\n       with tf.GradientTape() as tape:\n            eeg_enc_seq=self.emg_feature_extractor(emg_inp)\n            eeg_enc_seq=self.eeg_feature_extractor(eeg_inp)\n            len_epoch=input_layer.shape[1] \n            inputs=tf.reshape(input_layer, [-1, no_Epochs ,len_epoch])\n            outputs=self.seq2seq_model(inputs)\n            seq2seq_loss= self.seq2seq_loss_fn(y, outputs)\n            print('loss',  seq2seq_loss)\n            accuracy=self.accuracy(y, outputs)\n      grads = tape.gradient(seq2seq_loss, self.emg_feature_extractor.trainable_weights)\n     print('check', outputs.shape, y.shape, grads)\n     self.emg_feature_optim.apply_gradients(zip(grads, self.emg_feature_extractor.trainable_weights))     \n    \n\n     return {&quot;seq2seq_loss&quot;: seq2seq_loss, 'gen_model_loss':gen_model_loss, &quot;gen_MAE&quot;: gen_MAE, \n                      &quot;accuracy&quot;: accuracy}\n\n  def test_step(self, data):\n      x_emg, y = data\n      no_Epochs=3\n      y = tf.reshape(y, [-1, no_Epochs , 5])\n      emg_inp = tf.keras.layers.Input(3000, 1)\n      eeg_inp = self.EEG_gen_model(emg_inp)\n      emg_enc_seq=self.emg_feature_extractor(emg_inp)\n      eeg_enc_seq=self.eeg_feature_extractor(eeg_inp)\n      len_epoch=input_layer.shape[1] \n      inputs=tf.reshape(input_layer, [-1, no_Epochs ,len_epoch]) \n      outputs=self.seq2seq_model(inputs)\n      sleep_classifier_model=tf.keras.Model(inputs=emg_inp, outputs=outputs)\n\n      y_pred=sleep_classifier_model(x_emg,  training=False)# Forward pass\n      # Compute our own loss\n      loss = self.seq2seq_loss_fn(y, y_pred, \n      regularization_losses=self.seq2seq_loss_fn)\n      accuracy=accuracy(y, y_pred)\n\n      return {&quot;seq2seq_loss&quot;: seq2seq_loss, &quot;accuracy&quot;: accuracy}\n\n \n\n    model = Whole_model( EEG_gen_model=EEG_gen_model, emg_feature_extractor=emg_feature_extractor, \n                eeg_feature_extractor=eeg_feature_extractor, seq2seq_model=seq2seq_model)\nmodel.compile(\nEEG_gen_optimizer=tf.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False),\n                                     \nemg_feature_optim=tf.optimizers.Adam(lr=1e-3,  beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False),\n                                     \neeg_feature_optim=tf.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False),\n                                     \nseq2seq_optim=tf.optimizers.Adam(lr=1e-3,  beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False),\nseq2seq_loss_fn=tf.keras.losses.CategoricalCrossentropy(),\nEEG_gen_loss=tf.keras.losses.MSE, \ngen_mae=tf.keras.losses.MAE,                                   \naccuracy=tf.keras.metrics.Accuracy())\nmodel.fit(x_train_emg, [x_train_eeg, y_train],  batch_size=3, epochs=1,  validation_split=None, \n                                              validation_data=(x_test_emg, y_test), shuffle=False)\n</code></pre>\n<p>After executing this code, I am getting the following error</p>\n<p>ValueError: No gradients provided for any variable: ['conv1d_8/kernel:0', 'conv1d_8/bias:0', 'conv1d_12/kernel:0', 'conv1d_12/bias:0', 'conv1d_9/kernel:0', 'conv1d_9/bias:0', 'conv1d_13/kernel:0', 'conv1d_13/bias:0', 'conv1d_10/kernel:0', 'conv1d_10/bias:0', 'conv1d_14/kernel:0', 'conv1d_14/bias:0', 'conv1d_11/kernel:0', 'conv1d_11/bias:0', 'conv1d_15/kernel:0', 'conv1d_15/bias:0'].</p>\n<p>How to fix it? Please help me. Thank you in advance.</p>\n<p>Thanks, Andrey. But I have tried by defining all submodules like self.EEG_gen_model as tf.keras.layers.Layers which has a call() method as follows:</p>\n<pre><code>    class EEG_gen_layer(tf.keras.layers.Layer):\n         def __init__(self):\n            super(EEG_gen_layer, self).__init__()\n            n_samples_per_epoch=3000\n            print(n_samples_per_epoch)\n            inputs=tf.keras.layers.Input(batch_shape=[None, \n                                      n_samples_per_epoch, 1], name=&quot;input&quot;)    \n            lstm1=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM( \n                       units=128, return_sequences=True), \n                        merge_mode='concat')(inputs)\n            lstm2=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM( \n            units=128, return_sequences=True), merge_mode='concat')(lstm1)\n            dens1=tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(32, \n            kernel_initializer=tf.keras.initializers.glorot_normal(), \n            activation=tf.nn.relu))(lstm2)\n            dens2=tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1, \n            kernel_initializer=tf.keras.initializers.glorot_normal(), \n              activation=None))(dens1)\n            self.EEG_gen_model=tf.keras.Model(inputs=inputs, outputs=dens2)\n\n        def call(self, inp_emg, training=False):\n            x = self.EEG_gen_model(inp_emg)\n            return x\n</code></pre>\n<p>Like that, I defined all submodule of the model as tf.keras.layers.Layers. But again I am getting the same error.</p>\n<p>Please help me in fixing this error.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 39}]
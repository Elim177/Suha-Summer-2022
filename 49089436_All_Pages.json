[{"items": [{"tags": ["python", "tensorflow", "batch-normalization"], "owner": {"account_id": 10716056, "reputation": 2138, "user_id": 7886651, "user_type": "registered", "accept_rate": 76, "profile_image": "https://i.stack.imgur.com/zfb59.jpg?s=256&g=1", "display_name": "I. A", "link": "https://stackoverflow.com/users/7886651/i-a"}, "is_answered": true, "view_count": 1661, "accepted_answer_id": 49453084, "answer_count": 2, "score": 0, "last_activity_date": 1521818442, "creation_date": 1520112863, "last_edit_date": 1520796159, "question_id": 49089436, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49089436/how-to-use-batchnormalization-with-tensorflow", "title": "How to use BatchNormalization with tensorflow?", "body": "<p>I am having trouble using Batch Normalization with tensorflow. I have build the following model:</p>\n\n<pre><code>def weight_variable(kernal_shape):\n    weights = tf.get_variable(name='weights', shape=kernal_shape, dtype=tf.float32, trainable=True,\n                        initializer=tf.truncated_normal_initializer(stddev=0.02))\n    return weights\ndef bias_variable(shape):\n    initial = tf.constant(0.0, shape=shape)\n    return tf.Variable(initial)\n\n# return 1 conv layer\ndef conv_layer(x, w_shape, b_shape, is_training, padding='SAME'):\n    W = weight_variable(w_shape)\n    tf.summary.histogram(\"weights\", W)\n\n    b = bias_variable(b_shape)\n    tf.summary.histogram(\"biases\", b)\n\n    # Note that I used a stride of 2 on purpose in order not to use max pool layer.\n    conv = tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding=padding) + b\n    conv = tf.contrib.layers.batch_norm(conv, scale=True, is_training=is_training)\n\n    activations = tf.nn.relu(conv)\n\n    tf.summary.histogram(\"activations\", activations)\n\n    return activations\n\n# return deconv layer\ndef deconv_layer(x, w_shape, b_shape, is_training, padding=\"SAME\", activation='relu'):\n    W = weight_variable(w_shape)\n    tf.summary.histogram(\"weights\", W)\n\n    b = bias_variable(b_shape)\n    tf.summary.histogram('biases', b)\n\n    x_shape = tf.shape(x)\n    # output shape: [batch_size, h * 2, w * 2, input_shape from w].\n    out_shape = tf.stack([x_shape[0], x_shape[1] * 2, x_shape[2] * 2, w_shape[2]])\n    # Note that I have used a stride of 2 since I used a stride of 2 in conv layer.\n\n    conv_trans = tf.nn.conv2d_transpose(x, W, out_shape, [1, 2, 2, 1], padding=padding) + b\n    conv_trans = tf.contrib.layers.batch_norm(conv_trans, scale=True, is_training=is_training)\n\n    if activation == 'relu':\n        transposed_activations = tf.nn.relu(conv_trans)\n    else:\n        transposed_activations = tf.nn.sigmoid(conv_trans)\n\n    tf.summary.histogram(\"transpose_activation\", transposed_activations)\n    return transposed_activations\n\ndef model(input):\n    with tf.variable_scope('conv1'):\n        conv1 = conv_layer(input, [4, 4, 3, 32], [32], is_training=phase_train)  # image size: [56, 56]\n    with tf.variable_scope('conv2'):\n        conv2 = conv_layer(conv1, [4, 4, 32, 64], [64], is_training=phase_train)  # image size: [28, 28]\n    with tf.variable_scope('conv3'):\n        conv3 = conv_layer(conv2, [4, 4, 64, 128], [128], is_training=phase_train)  # image size: [14, 14]\n    with tf.variable_scope('conv4'):\n        conv4 = conv_layer(conv3, [4, 4, 128, 256], [256], is_training=phase_train)  # image size: [7, 7]\n        conv4_reshaped = tf.reshape(conv4, [batch_size * num_participants, 7 * 7 * 256], name='conv4_reshaped')\n\n    w_c_mu = tf.Variable(tf.truncated_normal([7 * 7 * 256, latent_dim], stddev=0.1), name='weight_fc_mu')\n    b_c_mu = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name='biases_fc_mu')\n    w_c_sig = tf.Variable(tf.truncated_normal([7 * 7 * 256, latent_dim], stddev=0.1), name='weight_fc_sig')\n    b_c_sig = tf.Variable(tf.constant(0.1, shape=[latent_dim]), name='biases_fc_sig')\n    epsilon = tf.random_normal([1, latent_dim])\n\n    tf.summary.histogram('weights_c_mu', w_c_mu)\n    tf.summary.histogram('biases_c_mu', b_c_mu)\n    tf.summary.histogram('weights_c_sig', w_c_sig)\n    tf.summary.histogram('biases_c_sig', b_c_sig)\n\n    with tf.variable_scope('mu'):\n        mu = tf.nn.bias_add(tf.matmul(conv4_reshaped, w_c_mu), b_c_mu)\n        tf.summary.histogram('mu', mu)\n\n    with tf.variable_scope('stddev'):\n        stddev = tf.nn.bias_add(tf.matmul(conv4_reshaped, w_c_sig), b_c_sig)\n        tf.summary.histogram('stddev', stddev)\n\n    with tf.variable_scope('z'):\n        # This formula was adopted from the following paper: http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979344\n        latent_var = mu + tf.multiply(tf.sqrt(tf.exp(stddev)), epsilon)\n        tf.summary.histogram('features_sig', stddev)\n\n    with tf.variable_scope('GRU'):\n        print(latent_var.get_shape().as_list())\n        latent_var = tf.reshape(latent_var, shape=[int(batch_size / 100)* num_participants, time_steps, latent_dim])\n\n        cell = tf.nn.rnn_cell.GRUCell(cell_size)   # state_size of cell_size.\n        H, C = tf.nn.dynamic_rnn(cell, latent_var, dtype=tf.float32)  # H size: [batch_size * num_participants, SEQLEN, cell_size]\n        H = tf.reshape(H, [batch_size * num_participants, cell_size])\n\n    with tf.variable_scope('output'):\n        # output layer.\n        w_output = tf.Variable(tf.truncated_normal([cell_size, 1], mean=0, stddev=0.01, dtype=tf.float32, name='w_output'))\n        tf.summary.histogram('w_output', w_output)\n        b_output = tf.get_variable('b_output', shape=[1], dtype=tf.float32,\n                                   initializer=tf.constant_initializer(0.0))\n        predictions = tf.add(tf.matmul(H, w_output), b_output, name='softmax_output')\n        tf.summary.histogram('output', predictions)\n\n        var_list = [v for v in tf.global_variables() if 'GRU' in v.name]\n        var_list.append([w_output, b_output])\n\n    return predictions, var_list\n</code></pre>\n\n<p>In addition, I am restoring the model parameters as follows:</p>\n\n<pre><code>saver_torestore = tf.train.Saver()\n\nwith tf.Session() as sess:\n    train_writer = tf.summary.FileWriter(events_path, sess.graph)\n    merged = tf.summary.merge_all()\n\n    to_run_list = [merged, RMSE]\n\n    # Initialize `iterator` with training data.\n    sess.run(init_op)\n\n    # Note that the last name \"Graph_model\" is the name of the saved checkpoints file =&gt; the ckpt is saved\n    # under tensorboard_logs.\n    ckpt = tf.train.get_checkpoint_state(\n        os.path.dirname(model_path))\n    if ckpt and ckpt.model_checkpoint_path:\n        saver_torestore.restore(sess, ckpt.model_checkpoint_path)\n        print('checkpoints are saved!!!')\n    else:\n        print('No stored checkpoints')\n\n    counter = 0\n    for _ in range(num_epoch):\n        sess.run(iterator.initializer)\n        print('epoch:', _)\n\n        # This while loop will run indefinitly until the end of the first epoch\n        while True:\n            try:\n                summary, loss_ = sess.run(to_run_list, feed_dict={phase_train: False})\n\n                print('loss: ' + str(loss_))\n\n                losses.append(loss_)\n                counter += 1\n\n                train_writer.add_summary(summary, counter)\n\n            except tf.errors.OutOfRangeError:\n                print('error, ignore ;) ')\n                break\n\n     print('average losses:', np.average(losses))\n     train_writer.close()\n</code></pre>\n\n<p>I make sure that variables are saved. So I ran the following command:</p>\n\n<pre><code>def assign_values_to_batchNorm():\n    vars = [v for v in tf.global_variables() if \"BatchNorm\" in v.name and \"Adam\" not in v.name]\n    file_names = [(v.name[:-2].replace(\"/\", \"_\") + \".txt\") for v in vars]\n    for var, file_name in zip(vars, file_names):\n        lst = open(file_name).read().split(\";\")[:-1]\n        print(lst)\n        values = list(map(np.float32, lst))\n        tf.assign(var, values)\n</code></pre>\n\n<p><em>Please note that I have used this method in order to restore the values of moving mean and moving variance manually. But I got the same result.</em></p>\n\n<p>And I called the assign_values_to_batchNorm() under session. I got some values => It seems that the moving average, moving variance, gamma and betta are all saved. </p>\n\n<p>Now Please note that I am working on windows 10, and I have tensorflow version 1.3. </p>\n\n<p>So, whenever I run <code>summary, loss_ = sess.run(to_run_list, feed_dict={phase_train: True})</code> under the session as well, after initializing/restoring all variables, I got a RMSE of 0.022 which is the same error achieved at the end of training the model. Now, if I set <code>phase_train</code> to false, I got a <code>RMSE</code> of 0.038. Please note that I am just testing the network in the meanwhile. Therefore, even though I am using the training dataset for testing, but my purpose was just to test the behavior of the network while training/testing. So this is so weird I guess to me. And please note that the phase is placeholder. I have it in code as follows:</p>\n\n<pre><code>phase_train = tf.placeholder(dtype=tf.bool, name='phase')\n</code></pre>\n\n<p>In addition, here is the code snippet for the optimizer:</p>\n\n<pre><code>with tf.name_scope('optimizer'):\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        optimizer = tf.train.AdamOptimizer(0.00001).minimize(RMSE) \n</code></pre>\n\n<p><strong>Main Problem:</strong> RMSE = 0.038 when phase = False and 0.022 when phase = True.</p>\n\n<p>Any help is much appreciated!!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 72}]
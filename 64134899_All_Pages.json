[{"items": [{"tags": ["generator", "generative-adversarial-network", "discriminator"], "owner": {"account_id": 12457671, "reputation": 11, "user_id": 9070292, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/654dee9626a35a9ac6b8df07ec622dfb?s=256&d=identicon&r=PG&f=1", "display_name": "Peter Kim", "link": "https://stackoverflow.com/users/9070292/peter-kim"}, "is_answered": false, "view_count": 81, "answer_count": 0, "score": 1, "last_activity_date": 1601468236, "creation_date": 1601459026, "last_edit_date": 1601468236, "question_id": 64134899, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64134899/gan-generate-regression-output-by-the-real-image-not-from-the-random-noise", "title": "GAN, generate regression output by the real image, not from the random noise", "body": "<p>Is this concept possible to be implemented with the GAN algorithm?</p>\n<p>I want the GAN to generate a regression-output(G-Value) of the shape(4,) by the real-image, not from the random noise, and discriminate <code>G-Value</code> with real regression-value(<code>R-Value</code>) of the same shape(4, ). <code>R-Value</code> is of the &quot;y-train&quot; dataset.</p>\n<p>It means that if an image has a pattern like circular, it generally has the <code>4</code> features of position x, y, z, and alpha. I call it Real-Value(<code>R-Value</code>) and I want the <code>GAN</code> to generate fake value (<code>G-Value</code>) fooling the discriminator.</p>\n<p>I have tried to implement it as below.</p>\n<pre><code>class UTModel:\n    def __init__(self):\n        optimizer__ = Adam(2e-4)\n\n        self.__dropout = .3\n\n        self.optimizerGenerator = Adam(1e-4)\n        self.optimizerDiscriminator = Adam(1e-4)\n\n        self.generator, self.discriminator = self.build()\n\n    def build(self):\n        # build the generator\n        g = Sequential()\n        g.add(Conv2D(512, kernel_size=3, strides=2, input_shape=(128, 128, 1), padding='same'))\n        g.add(BatchNormalization(momentum=0.8))\n        g.add(LeakyReLU(alpha=0.2))\n        g.add(Dropout(self.__dropout))\n        g.add(Conv2D(256, kernel_size=3, strides=2, padding='same'))\n        g.add(BatchNormalization(momentum=0.8))\n        g.add(LeakyReLU(alpha=0.2))\n        g.add(Dropout(self.__dropout))\n        g.add(Conv2D(128, kernel_size=3, strides=2, padding='same'))\n        g.add(BatchNormalization(momentum=0.8))\n        g.add(LeakyReLU(alpha=0.2))\n        g.add(Dropout(self.__dropout))\n        g.add(Conv2D(64, kernel_size=3, strides=1, padding='same'))\n        g.add(BatchNormalization(momentum=0.8))\n        g.add(LeakyReLU(alpha=0.2))\n        g.add(Dropout(self.__dropout))\n        g.add(Flatten())\n        g.add(Dense(4, activation='linear'))\n\n        # build the discriminator\n        d = Sequential()\n        d.add(Dense(128, input_shape=(4,)))\n        d.add(LeakyReLU(alpha=0.2))\n        d.add(Dropout(self.__dropout))\n        d.add(Dense(64))\n        d.add(LeakyReLU(alpha=0.2))\n        d.add(Dropout(self.__dropout))\n        d.add(Dense(64))\n        d.add(LeakyReLU(alpha=0.2))\n        d.add(Dropout(self.__dropout))\n        d.add(Dense(32))\n        d.add(LeakyReLU(alpha=0.2))\n        d.add(Dropout(self.__dropout))\n        d.add(Dense(1, activation='sigmoid'))\n\n        return g, d\n\n    def computeLosses(self, rValid, fValid):\n        bce = BinaryCrossentropy(from_logits=True)\n\n        # Discriminator loss\n        rLoss = bce(tf.ones_like(rValid), rValid)\n        fLoss = bce(tf.zeros_like(fValid), fValid)\n        dLoss = rLoss + fLoss\n\n        # Generator loss\n        gLoss = bce(tf.zeros_like(fValid), fValid)\n\n        return dLoss, gLoss\n\n    def train(self, images, rValues):\n        with tf.GradientTape() as gTape, tf.GradientTape() as dTape:\n            gValues = self.generator(images, training=True)\n\n            rValid = self.discriminator(rValues, training=True)\n            fValid = self.discriminator(gValues, training=True)\n\n            dLoss, gLoss = self.computeLosses(rValid, fValid)\n\n        dGradients = dTape.gradient(dLoss, self.discriminator.trainable_variables)\n        gGradients = gTape.gradient(gLoss, self.generator.trainable_variables)\n\n        self.optimizerDiscriminator.apply_gradients(zip(dGradients, self.discriminator.trainable_variables))\n        self.optimizerGenerator.apply_gradients(zip(gGradients, self.generator.trainable_variables))\n\n        print (dLoss, gLoss)\n\n\nclass UTTrainer:\n    def __init__(self):\n        self.env = 3DPatterns()\n        self.model = UTModel()\n\n    def start(self):\n        if not self.env.available:\n            return\n\n        batch = 32\n\n        for epoch in range(1):\n            # set new episod\n            while self.env.setEpisod():\n                for i in range(0, self.env.episodelen, batch):\n                    self.model.train(self.env.episode[i:i+batch], self.env.y[i:i+batch])\n</code></pre>\n<p>But the <code>G-Values</code> have not generated as valid values. It converges the 1 or -1 always. The proper value should be like <code>[-0.192798, 0.212887, -0.034519, -0.015000]</code>. Please help me to find the right way.</p>\n<p>Thank you.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 54}]
[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "deep-learning"], "owner": {"account_id": 8323724, "reputation": 1, "user_id": 6254421, "user_type": "registered", "profile_image": "https://graph.facebook.com/489857454543397/picture?type=large", "display_name": "Salar Ahmad", "link": "https://stackoverflow.com/users/6254421/salar-ahmad"}, "is_answered": false, "view_count": 209, "answer_count": 1, "score": 0, "last_activity_date": 1624938342, "creation_date": 1594982552, "question_id": 62952341, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62952341/gan-gradient-tape-computes-zero-gradients-for-discriminator", "title": "GAN: Gradient Tape computes zero gradients for discriminator", "body": "<p>I'm computing gradients for the discriminator using gradient tape. Discriminator model has custom layers and filters, but the gradients are zero. The gradients for the generator are being computed just fine.\nThe model is processing the audios and the custom filters initialized as tf variables with trainable set as true</p>\n<hr />\n<p>This is the discriminator</p>\n<pre><code>def discriminator(images_from_before, filters):\n    #Discriminator\n    #3x1 Convolution\n    conv_3 = tf.nn.conv1d(input = images_from_before,\n                filters = filters.filters[0][0],\n                stride = 1,\n                padding = 'SAME',\n                data_format = 'NWC')\n    #9x1 Convolution\n    conv_9 = tf.nn.conv1d(input = images_from_before,\n                filters = filters.filters[0][1],\n                stride = 1,\n                padding = 'SAME',\n                data_format = 'NWC')\n    #27x1 Convolution\n    conv_27 = tf.nn.conv1d(input = images_from_before,\n                filters = filters.filters[0][2], #some of the whole tfVariable\n                stride = 1,\n                padding = 'SAME',\n                data_format = 'NWC')\n    #81x1 Convolution\n    conv_81 = tf.nn.conv1d(input = images_from_before,\n                filters = filters.filters[0][3], #some of the whole tfVariable\n                stride = 1,\n                padding = 'SAME',\n                data_format = 'NWC')\n            \n    out = tf.concat([conv_3,conv_9,conv_27,conv_81], 2)\n\n    out = leaky_relu(out,0.2)\n\n    #7x Discriminator block\n    for i in range(7):\n        out = discriminator_block(out,filters.filters[i+1],filters.BN_val[i])\n\n    #Flatten width of the out tensor    \n    mul_FC = out.shape[1] * out.shape[2]\n\n    #FC - Dropout - LeakyReLU\n    #Flatten out\n    out = tf.reshape(out, shape = [out.shape[0],mul_FC])\n    out = FClayer(out,filters.FC1, filters.bias1)\n    out = Dropout(out)\n    out = leaky_relu(out,0.2)\n\n    #FC - Sigmoid\n    out = FClayer(out, filters.FC2, filters.bias2)\n\n    out = tf.math.sigmoid(out)\n\n    #implicit mean over the minibatch samples\n    out = tf.math.reduce_mean(out)\n\n    out = tf.clip_by_value(out,0.1, 0.9)\n\n    return out\n</code></pre>\n<p>The gradients are being calculated as follow</p>\n<pre><code>with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n        gen_out = generator.generator(downsampled, num_of_blocks, gen_var)\n        \n        gen_dis = discriminator.discriminator(gen_out,dis_var)     \n        \n        #compute losses\n        gen_loss = losses.generator_loss(\n                            losses.L2_loss(upsampled, gen_out),\n                            losses.Lf_loss(upsampled, gen_out,auto_var,4),\n                            losses.Ladv_loss(gen_dis,dis_var), #updated\n                            lamda_f,\n                            lamda_adv)\n\n        dis_loss = losses.discriminator_loss(0.1, \n                                             gen_dis)\n        \n    print('Gen loss: {}'.format(gen_loss.numpy()))\n    print('Dis loss: {}'.format(dis_loss.numpy()))\n    \n\n\n    gen_grads = gen_tape.gradient(gen_loss, [gen_var.dfilters,\n                                             gen_var.ufilters,\n                                             gen_var.finalfilter,\n                                             gen_var.prelu])\n    \n    disc_grads = dis_tape.gradient(dis_loss, [dis_var.filters,\n                                              dis_var.BN_val,\n                                              dis_var.FC1,\n                                              dis_var.bias1,\n                                              dis_var.FC2,\n                                              dis_var.bias2])\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 73}]
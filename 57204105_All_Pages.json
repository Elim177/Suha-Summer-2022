[{"items": [{"tags": ["keras", "tensorflow2.0"], "owner": {"account_id": 108161, "reputation": 5951, "user_id": 287238, "user_type": "registered", "accept_rate": 75, "profile_image": "https://i.stack.imgur.com/oQJH2.jpg?s=256&g=1", "display_name": "mathtick", "link": "https://stackoverflow.com/users/287238/mathtick"}, "is_answered": false, "view_count": 405, "answer_count": 0, "score": 1, "last_activity_date": 1564064100, "creation_date": 1564064100, "question_id": 57204105, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57204105/how-to-add-batchnormalization-loss-to-gradient-calculation-in-tensorflow-2-0-usi", "title": "How to add BatchNormalization loss to gradient calculation in tensorflow 2.0 using keras subclass API", "body": "<p>Using the keras subclass API it is easy enough to add a a batch normalization layer however the layer.losses list always appears empty. What is the correct method of including in the train loss when doing <code>tape.gradient(loss, lossmodel.trainable_variables)</code> where lossmodel is some separate keras subclass model defining a more complicated loss function that must include the gradient losses?</p>\n\n<p>For example, this is minimal model with ONLY the batch norm layer. It has no loss AFAIK</p>\n\n<pre><code>class M(tf.keras.Model):\n\n    def __init__(self, axis):\n        super().__init__()\n        self.layer = tf.keras.layers.BatchNormalization(axis=axis, scale=False, center=True, virtual_batch_size=1, input_shape=(6,))\n\n    def call(self, x):\n        out = self.layer(x)\n        return out\n\nm = M(1)\nIn [77]: m.layer.losses\nOut[77]: []\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 75}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "distributed-computing"], "owner": {"account_id": 5786508, "reputation": 386, "user_id": 4565536, "user_type": "registered", "accept_rate": 78, "profile_image": "https://www.gravatar.com/avatar/a5438bfb67076b263c0ab8440e35efb0?s=256&d=identicon&r=PG&f=1", "display_name": "Edvard-D", "link": "https://stackoverflow.com/users/4565536/edvard-d"}, "is_answered": true, "view_count": 370, "accepted_answer_id": 64851842, "answer_count": 1, "score": 0, "last_activity_date": 1605492734, "creation_date": 1605453480, "last_edit_date": 1605457077, "question_id": 64846130, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64846130/how-to-use-distributed-training-with-a-custom-loss-using-tensorflow", "title": "How to use distributed training with a custom loss using Tensorflow?", "body": "<p>I have a transformer model I'd like to train distributed across several workers on the Google Cloud AI Platform using Actor-Critic RL for training. I have my data broken up into individual files by date and uploaded to Cloud Storage. Since I'm using Actor-Critic RL, I have a custom loss function that calculates and applies the gradient. All the examples I've come across for distributed training make use of <code>model.fit</code>, which I'm not going to be able to do. I haven't been able to find any information on using a custom loss instead.</p>\n<p>Along with distributing it across several machines, I'd like to know how to properly distribute training across several CPU cores as well. From my understanding <code>model.fit</code> takes care of this stuff normally.</p>\n<p>Here's the custom loss function; right now it's the equivalent of a batch size of 1 I believe:</p>\n<pre><code>def learn(self, state_value_starting: tf.Tensor, probabilities: tf.Tensor, state_new: tf.Tensor,\n            reward: tf.Tensor, is_done: tf.Tensor):\n    with tf.GradientTape() as tape:\n        state_value_starting = tf.squeeze(state_value_starting)\n        state_value_new, _ = self.call(state_new)\n        state_value_new = tf.squeeze(state_value_new)\n\n        action_probabilities = tfp.distributions.Categorical(probs=probabilities)\n        log_probability = action_probabilities.log_prob(self._last_action)\n\n        delta = reward + (self._discount_factor * state_value_new * (1 - int(is_done))) - state_value_starting\n        actor_loss = -log_probability * delta\n        critic_loss = delta ** 2\n        total_loss = actor_loss + critic_loss\n\n    gradient = tape.gradient(total_loss, self.trainable_variables)\n    self.optimizer.apply_gradients(zip(gradient, self.trainable_variables))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 140}]
[{"items": [{"tags": ["python-3.x", "optimization", "tensorflow2.0"], "owner": {"account_id": 15937098, "reputation": 11, "user_id": 11499978, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-wMFgSIGo9mo/AAAAAAAAAAI/AAAAAAAADW8/5rUsyCN-GMg/photo.jpg?sz=256", "display_name": "Mohit Anand", "link": "https://stackoverflow.com/users/11499978/mohit-anand"}, "is_answered": false, "view_count": 99, "answer_count": 0, "score": 1, "last_activity_date": 1631686493, "creation_date": 1631686493, "question_id": 69187912, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69187912/computing-jacobian-and-derivative-in-tensorflow-is-extremely-slow", "title": "Computing Jacobian and Derivative in Tensorflow is extremely slow", "body": "<p>Is there a more efficient way to compute Jacobian (there must be, it doesn't even run for a single batch)  I want to compute the loss as given in the self-explanatory neural network. Input has a shape of (32, 365, 3) where 32 is the batch size. The loss I want to minimize is Equation 3 of the <a href=\"https://papers.nips.cc/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf\" rel=\"nofollow noreferrer\">paper</a>.</p>\n<p>I believe that I am not using the GradientTape optimally.</p>\n<pre><code>def compute_loss_theta(tape, parameter, concept, output, x):\n\n    b = x.shape[0]\n    in_dim = (x.shape[1], x.shape[2])\n\n    feature_dim = in_dim[0]*in_dim[1]\n\n    J = tape.batch_jacobian(concept, x)\n    \n    grad_fx = tape.gradient(output, x)\n    grad_fx = tf.reshape(grad_fx,shape=(b, feature_dim))\n    J = tf.reshape(J, shape=(b, feature_dim, feature_dim))\n\n    parameter = tf.expand_dims(parameter, axis =1)\n\n    loss_theta_matrix = grad_fx - tf.matmul(parameter, J)\n\n    loss_theta = tf.norm(loss_theta_matrix)\n\n    return loss_theta\n\n\nfor i in range(10):\n    for x, y in train_dataset:\n\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch(x)\n            \n            parameter, concept, output = model(x)\n\n            loss_theta = compute_loss_theta(tape, parameter, concept, output , x)\n\n            loss_y = loss_object(y_true=y, y_pred=output)\n                \n            loss_value = loss_y + eps*loss_theta\n\n        gradients = tape.gradient(loss_value, model.trainable_weights)\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 30}]
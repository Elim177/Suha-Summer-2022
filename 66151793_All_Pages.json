[{"items": [{"tags": ["tensorflow", "keras", "progress-bar", "tensorflow-datasets"], "owner": {"account_id": 8122961, "reputation": 1012, "user_id": 6118987, "user_type": "registered", "accept_rate": 91, "profile_image": "https://i.stack.imgur.com/MZvTG.jpg?s=256&g=1", "display_name": "Masoud Masoumi Moghadam", "link": "https://stackoverflow.com/users/6118987/masoud-masoumi-moghadam"}, "is_answered": true, "view_count": 451, "accepted_answer_id": 66185562, "answer_count": 1, "score": 3, "last_activity_date": 1613222470, "creation_date": 1613034827, "last_edit_date": 1613199979, "question_id": 66151793, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66151793/tensorflow-custom-loop-does-not-end-in-first-epoch-and-progress-bar-runs-to-infi", "title": "tensorflow custom loop does not end in first epoch and progress bar runs to infinite", "body": "<p>I am trying to write a tensorflow custom training loop and include some tensorboard utilities.</p>\n<p>Here is the full code:</p>\n<pre><code>import tensorflow as tf\nfrom pathlib import Path\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import layers\nimport cv2\nfrom tqdm import tqdm\nfrom os import listdir\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm\nfrom random import shuffle, choice, uniform\n\nfrom os.path import isdir, dirname, abspath, join\nfrom os import makedirs\nfrom tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard,\n                                        EarlyStopping, LearningRateScheduler)\n\nimport io\nfrom natsort import natsorted\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import Sequential,Model\n\nfrom tensorflow.keras.applications import (DenseNet201, InceptionV3, MobileNetV2,\n                                           ResNet101, Xception, EfficientNetB7,VGG19, NASNetLarge)\nfrom tensorflow.keras.applications import (densenet, inception_v3, mobilenet_v2,\n                                           resnet, xception, efficientnet, vgg19, nasnet)\n\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling, Resizing\nfrom tensorflow.keras.utils import Progbar\n\n\nROOT = '/content/drive/MyDrive'\ndata_path = 'cropped/'\ntrain_path = data_path + 'train'\nval_path = data_path + 'val'\n\nlabels = {v:k for k, v in enumerate(listdir(train_path))}\n\nmodels = {\n    'densenet': DenseNet201,\n    'xception': Xception,\n    'inceptionv3': InceptionV3,\n    'effecientnetb7': EfficientNetB7,\n    'vgg19': VGG19,\n    'nasnetlarge': NASNetLarge,\n    'mobilenetv2': MobileNetV2,\n    'resnet': ResNet101\n}\n\n# models['densenet']()\n\npreprocess_pipeline = {\n    'densenet': densenet.preprocess_input,\n    'xception': xception.preprocess_input,\n    'inceptionv3': inception_v3.preprocess_input,\n    'effecientnetb7': efficientnet.preprocess_input,\n    'vgg19': vgg19.preprocess_input,\n    'nasnetlarge': nasnet.preprocess_input,\n    'mobilenetv2': mobilenet_v2.preprocess_input,\n    'resnet': resnet.preprocess_input\n}\n\n\ndef configure_for_performance(ds, buffer_size, batch_size):\n    &quot;&quot;&quot;\n    Configures caching and prefetching\n    &quot;&quot;&quot;\n    ds = ds.cache()\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=buffer_size)\n    return ds\n\n\ndef generator(tfrecord_file, batch_size, n_data, validation_ratio, reshuffle_each_iteration=False):\n    &quot;&quot;&quot;\n    Returns training and validation generators with infinite repeat.\n    &quot;&quot;&quot;\n    reader = tf.data.TFRecordDataset(filenames=[tfrecord_file])\n    reader.shuffle(n_data, reshuffle_each_iteration=reshuffle_each_iteration)\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n    val_size = int(n_data * validation_ratio)\n    train_ds = reader.skip(val_size)\n    val_ds = reader.take(val_size)\n\n    # Parsing data from tfrecord format.\n    train_ds = train_ds.map(_parse_function, num_parallel_calls=AUTOTUNE)\n    \n    # Some data augmentation.\n    train_ds = train_ds.map(_augment_function, num_parallel_calls=AUTOTUNE)\n    train_ds = configure_for_performance(train_ds, AUTOTUNE, batch_size).repeat()\n\n    val_ds = val_ds.map(_parse_function, num_parallel_calls=AUTOTUNE)\n    val_ds = val_ds.map(_augment_function, num_parallel_calls=AUTOTUNE)\n    val_ds = configure_for_performance(val_ds, AUTOTUNE, batch_size).repeat() # Is this repeat function the reason behind the issue \n    return train_ds, val_ds\n\ndef create_model(optimizer, name='densenet', include_compile=True):\n    base_model = models[name](include_top=False, weights='imagenet')\n    x = GlobalAveragePooling2D()(base_model.layers[-1].output)\n    x = Dense(1024, activation='relu')(x)\n    output = Dense(12, activation='softmax')(x)\n    model = Model(base_model.inputs, output)\n\n    if include_compile:\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=optimizer,\n                      metrics=['accuracy'])\n\n    return model\n</code></pre>\n<p>Now let's create a model and initialize:</p>\n<pre><code>n_data = len(list(Path(data_path).rglob('*.jpg'))) # Find out how many images are there\nvalidation_ratio = 0.2\nval_size = int(n_data * validation_ratio) # Find out validation image size.\ntrain_size = n_data - val_size # And train images size\nbatch_size = 64\nn_epochs = 5\n\n# Tfrecord of images\nfilename = '/content/drive/MyDrive/cropped_data.tfrecord'\n\ntrain_ds, val_ds = generator(filename,\n                            batch_size=batch_size,\n                            n_data=n_data,\n                            validation_ratio=validation_ratio,\n                            reshuffle_each_iteration=True)\n\n# Tensorboard initialization\nmodel_name = 'xception'\n\npath_to_run = &quot;runs/run_1&quot;\ntb_train_path = join(path_to_run, 'logs','train')\ntb_test_path = join(path_to_run, 'logs', 'test')\n\ntrain_writer = tf.summary.create_file_writer(tb_train_path)\ntest_writer = tf.summary.create_file_writer(tb_test_path)\ntrain_step = test_step = 0\n\nblocks_to_train = []\nlr = 1e-4\n\noptimizer = SGD(lr=lr, decay=1e-6,momentum=0.9,nesterov=True)\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\nacc_metric = tf.keras.metrics.CategoricalCrossentropy()\n\n# Create the xception model\nmodel = create_model(optimizer, name=model_name, include_compile=False)\n\nmetrics = {'acc': 0.0, 'loss': 0.0, 'val_acc': 0.0, 'val_loss': 0.0, 'lr': lr}\n</code></pre>\n<p>And this is the loop for training and testing:</p>\n<pre><code>for epoch in range(n_epochs):\n    # Iterate through the training set\n    progress_bar = Progbar(train_size, stateful_metrics=list(metrics.keys()))\n\n    for batch_idx, (x, y) in enumerate(train_ds):\n        with tf.GradientTape() as tape:\n            y_pred = model(x, training=True)\n            loss = loss_fn(y, y_pred)\n\n        gradients = tape.gradient(loss, model.trainable_weights)\n        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n        acc_metric.update_state(y, y_pred)\n        train_step += 1\n        progress_bar.update(batch_idx*batch_size, values=[('acc',acc_metric.result()),\n                                       ('loss', loss)])\n\n    with train_writer.as_default():\n        tf.summary.scalar(&quot;Loss&quot;, loss, step=epoch)\n        tf.summary.scalar(\n            &quot;Accuracy&quot;, acc_metric.result(), step=epoch\n        )\n\n    # reset accuracy between epochs (and for testing and test)\n\n    acc_metric.reset_states()\n\n\n    for batch_idx, (x,y) in enumerate(val_ds):\n        y_pred = model(x, training=False)\n        loss = loss_fn(y, y_pred)\n        acc_metric.update_state(y,\n                                y_pred)\n        confusion += get_confusion_matrix(y, y_pred, class_names=list(labels.keys()))\n\n    with test_writer.as_default():\n        tf.summary.scalar(&quot;Loss&quot;, loss, step=epoch)\n        tf.summary.scalar(&quot;Accuracy&quot;, acc_metric.result(), step=epoch)\n\n    progress_bar.update(train_size, values=[('val_acc', acc_metric.result()), ('val_loss', loss)])\n\n    # reset accuracy between epochs (and for testing and test)\n    acc_metric.reset_states()\n</code></pre>\n<p>I modified the code and removed some of tensorboard utilities. The code starts training but it does not stop in the end of predefined epochs. I see the progress bar keeps going and never stops to show the validation metrics.</p>\n<p>Can you guys help me to have the exact same progress bar like <code>keras.fit</code> function.</p>\n<p>Thanks</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 255}]
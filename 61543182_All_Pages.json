[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "autoencoder"], "owner": {"account_id": 17161244, "reputation": 107, "user_id": 12420933, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/a8e59a6120f48254498206f4ad79c150?s=256&d=identicon&r=PG&f=1", "display_name": "Jorrmungandr", "link": "https://stackoverflow.com/users/12420933/jorrmungandr"}, "is_answered": false, "view_count": 328, "answer_count": 0, "score": 1, "last_activity_date": 1620575690, "creation_date": 1588338813, "last_edit_date": 1588575758, "question_id": 61543182, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61543182/deep-learning-oscillating-loss", "title": "Deep Learning Oscillating Loss", "body": "<p>I am building an autoencoder using <code>tensorflow</code> and my loss throughout training looks like this:\n<img src=\"https://i.stack.imgur.com/HQEh4.png\" alt=\"test and training loss\"></p>\n\n<p>It's actually kinda beautiful, but i need to minimize the oscillation as much as I can, these are my hyperparameters:\n</p>\n\n<pre><code>batch_size = 64\nlearning_rate = 0.0002\nnum_epochs = 50\nn_training_samples = 480\nthreshold = 10000\n</code></pre>\n\n<p>I have tried increasing the batch_size, lowering/increasing the learning_rate, but it doesn't seem to make a difference, thank you in advance.</p>\n\n<p>This is the base model:\n</p>\n\n<pre><code>class BaseModel(Model):\n    def __init__(self, input_size):\n        super(BaseModel, self).__init__()\n        self.weight_initializer = tf.random_normal_initializer(\n            mean=0.0, stddev=0.25)\n        self.bias_initializer = tf.zeros_initializer()\n        self.input_size = input_size\n        self.layers_shape = [1000, 750, 30, 750, 1000]\n\n    def init_variables(self):\n        self.W1 = tf.compat.v1.get_variable(\n            'W1', shape=[self.input_size, self.layers_shape[0]], \n            initializer=self.weight_initializer, dtype=tf.float32)\n        self.W2 = tf.compat.v1.get_variable(\n            'W2', shape=[self.layers_shape[0], self.layers_shape[1]], \n             initializer=self.weight_initializer, dtype=tf.float32)\n        self.W3 = tf.compat.v1.get_variable(\n            'W3', shape=[self.layers_shape[1], self.layers_shape[2]],                     \n            initializer=self.weight_initializer, dtype=tf.float32)\n        self.W4 = tf.compat.v1.get_variable(\n            'W4', shape=[self.layers_shape[2], self.layers_shape[3]],         \n            initializer=self.weight_initializer, dtype=tf.float32)\n        self.W5 = tf.compat.v1.get_variable(\n            'W5', shape=[self.layers_shape[3], self.layers_shape[4]], \n            initializer=self.weight_initializer, dtype=tf.float32)\n        self.W6 = tf.compat.v1.get_variable(\n            'W6', shape=[self.layers_shape[4], self.input_size], \n            initializer=self.weight_initializer, dtype=tf.float32)\n\n        self.b1 = tf.compat.v1.get_variable(\n            'b1', shape=[self.layers_shape[0]], \n            initializer=self.bias_initializer, dtype=tf.float32)\n        self.b2 = tf.compat.v1.get_variable(\n            'b2', shape=[self.layers_shape[1]], \n            initializer=self.bias_initializer, dtype=tf.float32)\n        self.b3 = tf.compat.v1.get_variable(\n            'b3', shape=[self.layers_shape[2]], \n            initializer=self.bias_initializer, dtype=tf.float32)\n        self.b4 = tf.compat.v1.get_variable(\n            'b4', shape=[self.layers_shape[3]], \n            initializer=self.bias_initializer, dtype=tf.float32)\n        self.b5 = tf.compat.v1.get_variable(\n            'b5', shape=[self.layers_shape[4]], \n            initializer=self.bias_initializer, dtype=tf.float32)\n\ndef forward_propagation(self, x):\n    with tf.name_scope('feed_forward'):\n        # First Hidden Layer\n        z1 = tf.linalg.matmul(x, self.W1) + self.b1\n        a1 = tf.nn.relu(z1)\n\n        # Second Hidden Layer\n        z2 = tf.linalg.matmul(a1, self.W2) + self.b2\n        a2 = tf.nn.relu(z2)\n\n        # Third Hidden Layer\n        z3 = tf.linalg.matmul(a2, self.W3) + self.b3\n        a3 = tf.nn.relu(z3)\n\n        # Fourth Hidden Layer\n        z4 = tf.linalg.matmul(a3, self.W4) + self.b4\n        a4 = tf.nn.relu(z4)\n\n        # Fifth Hidden Layer\n        z5 = tf.linalg.matmul(a4, self.W5) + self.b5\n        a5 = tf.nn.relu(z5)\n\n        prediction = tf.linalg.matmul(a5, self.W6)\n    return prediction\n</code></pre>\n\n<p>This is the other model, which specifies other techniques:\n</p>\n\n<pre><code>class AnomalyDetector(BaseModel):\n    def __init__(self, input_size, num_variables):\n        super(AnomalyDetector, self).__init__(input_size)\n        self.init_variables()\n        self.num_variables = num_variables\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002)\n\n    def compute_loss(self, x_train):\n        mse = tf.keras.losses.MeanSquaredError()\n        loss = mse(x_train, self.forward_propagation(x_train))\n        return loss\n\n    def train(self, x_train):\n        with tf.GradientTape() as tape:\n            gradients = tape.gradient(self.compute_loss(x_train), self.trainable_variables)\n            gradient_variables = zip(gradients, self.trainable_variables)\n            self.optimizer.apply_gradients(gradient_variables)\n</code></pre>\n\n<p>This is how I train it:\n</p>\n\n<pre><code>for epoch in range(num_epochs):\n    temp_loss = 0\n    training_dataset.shuffle(len(list(training_dataset)))\n    for step, x_train in enumerate(training_dataset):\n        features = tf.reshape(x_train, [1, input_size])\n\n        model.train(features)\n\n        loss_values = model.compute_loss(features)\n        temp_loss += loss_values\n\n        if step &gt; 0 and step % eval_after == 0:\n            test_loss = 0\n            for step_test, x_test in enumerate(test_dataset):\n                features = tf.reshape(x_test, [1, input_size])\n                test_loss = model.compute_loss(features)\n\n            print('epoch_nr: %d, batch: %d/%d, mse_loss: %.3f' %\n                  (epoch, step, n_batches, (temp_loss/step)))\n            loss.append(temp_loss/step)\n            test_losses.append(test_loss)\n\nmodel.save(save_path)\n\nplt.plot(loss, label='Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.legend(loc=\"upper right\")\nplt.show()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 234}]
[{"items": [{"tags": ["python-3.x", "tensorflow", "machine-learning", "custom-training"], "owner": {"account_id": 20436611, "reputation": 31, "user_id": 14994311, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-QFCsnitc1BA/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucnMwyV8Lp2dRqSosAvSs6D243ZBaA/s96-c/photo.jpg?sz=256", "display_name": "gowithdaflo", "link": "https://stackoverflow.com/users/14994311/gowithdaflo"}, "is_answered": false, "view_count": 128, "answer_count": 0, "score": 1, "last_activity_date": 1619598206, "creation_date": 1619306112, "last_edit_date": 1619598206, "question_id": 67248294, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67248294/microbatching-accumulating-gradients-in-tensorflow-2-x-with-tf-function", "title": "Microbatching (accumulating gradients) in Tensorflow 2.x with tf.function", "body": "<p>How can micro batching be implemented in tensorflow 2.x? That is I would like to accumulate gradients for several batches and then update the weights with these accumulated gradients (this would virtually increase my batch size to accumulation steps * batch size).</p>\n<p>I tried with the following code:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nclass Model(tf.keras.Model):\n    def __init__(self,  ):\n        super().__init__()\n    \n        self.dense = tf.keras.layers.Dense(1)\n\n    def call(self, inputs):\n        return self.dense(inputs)\n\n\nclass Trainer:\n    def __init__(self, model, num_accumulate):\n        self.model = model\n        self.num_accumulate = num_accumulate\n        self.optimizer = tf.keras.optimizers.Adam()\n        self.accumulated_gradients = None\n\n    def _init_accumulated_gradients_maybe(self):\n        if self.accumulated_gradients is None:\n            self.accumulated_gradients = [tf.Variable(var, dtype=var.dtype, trainable=False) for var in self.model.trainable_weights]\n            self._reset_gradients()\n\n    def _reset_gradients(self):\n        for grad in self.accumulated_gradients:\n            grad.assign(tf.zeros_like(grad))\n\n    def _accumulate_gradients(self, gradients):\n        for acc_grad, grad in zip(self.accumulated_gradients, gradients):\n            acc_grad.assign_add( grad / self.num_accumulate )\n\n    def get_mae(self, targets, mean_pred):\n        return tf.reduce_mean(tf.abs(targets - mean_pred))\n\n    @tf.function\n    def train_on_batch(self, dataset_iter):\n        \n        for _ in range(self.num_accumulate): # problematic\n            inputs, target = next(dataset_iter)\n\n            with tf.GradientTape() as tape:\n                prediction = self.model(inputs, training=True)\n                loss = self.get_mae(target, prediction)\n\n            gradients = tape.gradient(loss, self.model.trainable_weights)\n\n            self._init_accumulated_gradients_maybe()\n            self._accumulate_gradients(gradients)\n            gradients = self.accumulated_gradients\n\n        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_weights))\n        self._reset_gradients()\n\n        return loss\n\nclass DataProvider:\n    def __init__(self,  \n                        batch_size: int = 1, \n                ):\n        self.batch_size = batch_size\n        self.in_data = np.random.rand(100,10)\n        self.out_data = np.random.rand(100,1)\n\n    def get_dataset(self):\n        def generator():\n            while True:\n                yield (tf.constant(self.in_data, dtype=tf.float32), tf.constant(self.out_data, dtype=tf.float32))\n\n        return tf.data.Dataset.from_generator(\n                generator,\n                output_types=(tf.float32, tf.float32),\n                output_shapes=([None,10], [None,1])\n                )\n\n\nnum_accumulate = 4\nbatch_size = 25\nnSteps = 10\n\nmodel = Model()\ntrainer = Trainer(model, num_accumulate)\ndataset_iter = iter(DataProvider(batch_size).get_dataset())\n\nfor step in range(1, nSteps):\n    trainer.train_on_batch(dataset_iter)\n</code></pre>\n<p>However, I ran into two different problems depending on if I use tf.range or range inside the tf.function decorated function.</p>\n<ol>\n<li>Using range: It works with the provided mini model but in my use case the model is significantly bigger (2.6 Mio params) and when I accumulate gradients like this the following error is raised:</li>\n</ol>\n<blockquote>\n<p>2021-04-24 18:19:28.349940: W tensorflow/core/common_runtime/process_function_library_runtime.cc:733] Ignoring multi-device function optimization failure: Deadline exceeded: meta_optimizer exceeded deadline.</p>\n</blockquote>\n<p>My guess is that using range (as far as I understood how tf.function works) every gradient accumulation step is added to the graph instead of repeating this part and adding it only once.</p>\n<ol start=\"2\">\n<li>Replacing range with tf.range raises the following error:</li>\n</ol>\n<pre><code>Traceback (most recent call last):\n  File &quot;/mydirectory/model/test_train copy.py&quot;, line 89, in &lt;module&gt;\n    trainer.train_on_batch(dataset_iter)\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py&quot;, line 580, in __call__\n    result = self._call(*args, **kwds)\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py&quot;, line 627, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py&quot;, line 505, in _initialize\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py&quot;, line 2446, in _get_concrete_function_internal_garbage_collected\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py&quot;, line 2777, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py&quot;, line 2657, in _create_graph_function\n    func_graph_module.func_graph_from_py_func(\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 981, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py&quot;, line 441, in wrapped_fn\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py&quot;, line 3299, in bound_method_wrapper\n    return wrapped_fn(*args, **kwargs)\n  File &quot;/mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 968, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\nValueError: in user code:\n\n    /mydirectory/model/test_train copy.py:40 train_on_batch  *\n        for _ in tf.range(self.num_accumulate):\n    /mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:343 for_stmt\n        _tf_range_for_stmt(\n    /mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:526 _tf_range_for_stmt\n        _tf_while_stmt(\n    /mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:862 _tf_while_stmt\n        _verify_loop_init_vars(init_vars, symbol_names)\n    /mydirectory/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/operators/control_flow.py:119 _verify_loop_init_vars\n        raise ValueError('&quot;{}&quot; must be defined before the loop.'.format(name))\n\n    ValueError: &quot;loss&quot; must be defined before the loop.\n</code></pre>\n<p>Therefore, I initialized all occuring variables such as gradients, loss and prediction and then it works but it is painfully slow (in my use case) why is that?</p>\n<p>What am I missing ? Any help is highly appreciated.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 228}]
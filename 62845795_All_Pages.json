[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0", "tf.keras"], "owner": {"account_id": 8133134, "reputation": 33, "user_id": 7083967, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0ed32787f41ce5a44d252632da4f3bf6?s=256&d=identicon&r=PG&f=1", "display_name": "rakeshKM", "link": "https://stackoverflow.com/users/7083967/rakeshkm"}, "is_answered": false, "view_count": 469, "answer_count": 0, "score": 3, "last_activity_date": 1594447555, "creation_date": 1594447555, "question_id": 62845795, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62845795/changing-learning-rate-over-epoch-in-tensorflow-2-using-custom-training-loop", "title": "changing learning rate over epoch in tensorflow 2 using custom training loop", "body": "<p>I want to optimize 2 loss components separately , so thinking of running two optimizer for them.</p>\n<p>is changing learning rate over epoch in tensorflow 2 using custom training loop,in this way ok?</p>\n<pre><code>    batch_size = 4\n    epochs = 50\n    myLearnRate1=1e-4\n    myLearnRate2=1e-4  \n    X_train,X_test=train_data,val_data\n\n    for epoch in range(0, epochs):\n      train_loss=[]\n      for i in range(0, len(X_train) // batch_size):\n          X = X_train[i * batch_size:min(len(X_train), (i+1) * batch_size)]\n          Image,Mask_p,Mask_n=create_mask(X)\n                    \n          Lr1= myLearnRate1 /(1 + (epoch/37))    \n          Lr2= myLearnRate2 /(1 + (epoch/37))   \n          optimizer1 =tf.keras.optimizers.Adam(learning_rate=Lr1)\n          optimizer2 =tf.keras.optimizers.Adam(learning_rate=Lr2)\n          \n          loss=train_on_batch(Image,Mask_p,Mask_n,optimizer1,optimizer2)  \n</code></pre>\n<p>basically i am sending the &quot; optimizer along with different learning rate&quot; in each iteration to training\nfunction</p>\n<p>training function</p>\n<pre><code>def train_on_batch(X_original,X_p,X_n,optimizer1,optimizer2):\n  with tf.GradientTape(persistent=True) as tape:\n    # Forward pass.\n    recon,latent=autoencoder_model([X_original,X_p,X_n],training=True)\n    # Loss value for this batch.\n    loss_value1_,loss_value2=assymetric_fun(X_original,X_p,X_n,recon)\n    loss_value1=-1.0*loss_value1_\n    \n  # make gradient\n  grads1 = tape.gradient(loss_value1, autoencoder_model.trainable_variables)\n  grads2 = tape.gradient(loss_value2, autoencoder_model.trainable_variables)\n  \n  #update weight\n  optimizer1.apply_gradients(zip(grads1, autoencoder_model.trainable_variables))\n  optimizer2.apply_gradients(zip(grads2, autoencoder_model.trainable_variables))\n  \n  return loss_value1_+loss_value2\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 54}]
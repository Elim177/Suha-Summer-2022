[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning"], "owner": {"account_id": 20343491, "reputation": 17, "user_id": 15384396, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/01c32d32a71aca5ff02a2fbff3fdd7a9?s=256&d=identicon&r=PG&f=1", "display_name": "skiii gairola", "link": "https://stackoverflow.com/users/15384396/skiii-gairola"}, "is_answered": true, "view_count": 145, "accepted_answer_id": 66951170, "answer_count": 1, "score": 0, "last_activity_date": 1617618289, "creation_date": 1617607541, "last_edit_date": 1617617850, "question_id": 66949567, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66949567/all-gradient-values-calculated-as-none-if-using-bce-loss-manually", "title": "All Gradient values calculated as &quot;None&quot; if using BCE loss manually", "body": "<p>I am working on a multi-output model where I need to weigh all output losses before calculating the overall loss. I have a customized <code>model. fit()</code> <a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\" rel=\"nofollow noreferrer\">training loop</a> to achieve this.</p>\n<p>As I need to calculate the sample-wise loss for all four outputs and fuse these sample-wise losses after applying weight, I have customized the standard code. Now, the loss is calculating sample-wise but while calculating the gradient, all gradient values are calculated as &quot;None&quot;. I tried to put <code>tape.watch(loss)</code> also, but it is not working. Kindly, help me to fix this issue.</p>\n<pre><code>class CustomModel(keras.Model):\n    def train_step(self, data):\n        print(tf.executing_eagerly())\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n        alpha = 0.1\n        loss = 0\n        y_pred_all = []\n\n        with tf.GradientTape() as tape:\n            bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n            for spl in range(1 if np.shape(x)[0] == None else np.shape(x)[0]):\n                tape.watch(loss)\n                tape.watch(loss_mean)\n                tape.watch(loss_element)\n                x_spl = np.reshape(x[spl], (1, np.shape(x)[1], np.shape(x)[2], np.shape(x)[3]))\n                y_pred = self(x_spl, training=True)  # Forward pass\n                y_pred_all.append(y_pred)\n                loss_element = bce(y[spl], y_pred)\n                loss_mean = [np.mean(loss_element[0]), np.mean(loss_element[1]), np.mean(loss_element[2]), np.mean(loss_element[3])]\n                id = np.argmin(loss_mean)\n                for i, ele in enumerate(loss_mean):\n                    if i == id:\n                        loss_mean[i] *= 1\n                    else:\n                        loss_mean[i] *= alpha\n\n                loss = loss + np.sum(loss_mean)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(y, y_pred_all)\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>\n<p><strong>UPDATE</strong>\nI did few changes as suggested by <strong>@rvinas</strong>\nNow it is calculating the gradient without any error but I am not sure if the changes I did are correct or not:</p>\n<pre><code>class CustomModel(keras.Model):\n    def train_step(self, data):\n        # print(tf.executing_eagerly())\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n        alpha = 0.1\n        loss = tf.Variable(0, dtype='float32')\n        y_pred_all = []\n\n        with tf.GradientTape() as tape:\n            bce = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n            for spl in tf.range(1 if tf.shape(x)[0] == None else tf.shape(x)[0]):\n                loss_mean=tf.convert_to_tensor([])\n                x_spl =  tf.reshape(x[spl], (1, tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]))\n                y_pred = self(x_spl, training=True)  # Forward pass\n                y_pred_all.append(y_pred)\n                loss_element = bce(y[spl], y_pred)\n                loss_mean = [tf.reduce_mean(loss_element[0]), tf.reduce_mean(loss_element[1]), tf.reduce_mean(loss_element[2]), tf.reduce_mean(loss_element[3])]\n\n                id = tf.argmin(loss_mean)\n                for i, ele in enumerate(loss_mean):\n                    if i == id:\n                        loss_mean[i] = tf.multiply(loss_mean[i], 1)\n                    else:\n                        loss_mean[i] = tf.multiply(loss_mean[i], alpha)\n\n                loss = tf.add(loss, tf.add(tf.add(tf.add(loss_mean[0],loss_mean[1]), loss_mean[2]), loss_mean[3]))\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(y, y_pred_all)\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 171}]
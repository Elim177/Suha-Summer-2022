[{"items": [{"tags": ["python", "tensorflow", "keras", "neural-network", "siamese-network"], "owner": {"account_id": 4390825, "reputation": 19, "user_id": 3579279, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/73904a02b14a5d8fcc1ce4fff846a1d4?s=256&d=identicon&r=PG&f=1", "display_name": "david.a.", "link": "https://stackoverflow.com/users/3579279/david-a"}, "is_answered": false, "view_count": 283, "answer_count": 0, "score": 0, "last_activity_date": 1587456362, "creation_date": 1587456362, "question_id": 61338722, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61338722/siamese-network-on-mnist-dataset-is-not-getting-trained", "title": "Siamese network on MNIST dataset is not getting trained", "body": "<p>I train Siamese network with constructive loss on two classes of MNIST dataset to identify whether two images are similar or not. Although the loss is decreasing in the beginning, it freezes later with accuracy around 0.5. </p>\n\n<p>The model is trained on pairs of images and a label (0.0 for different, 1.0 for identical). I used only two classes for simplicity (zeros and ones) and prepared the dataset, so that it contains every pair of images. I've checked that the dataset is consistent (<a href=\"https://i.stack.imgur.com/BECUx.png\" rel=\"nofollow noreferrer\">image pairs from dataset</a>). I've also experimented with data normalization, different batch sizes, learning rates, initializations and regularization constants with no luck. </p>\n\n<p>This is the model:</p>\n\n<pre><code>class Encoder(Model):\n    \"\"\"\n    A network that finds a 50-dimensional representation of the input images\n    so that the distances between them minimize the constructive loss\n    \"\"\"\n\n    def __init__(self):\n        super(Encoder, self).__init__(name='encoder')\n\n        self.cv = Conv2D(32, (3, 3), activation='relu', padding='Same',\n                         input_shape=(28, 28, 1),\n                         kernel_regularizer=tf.keras.regularizers.l2(0.01))\n        self.pool = MaxPooling2D((2, 2))\n        self.flatten = Flatten()\n        self.dense = Dense(50, activation=None,\n                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n\n    def call(self, inputs, training=None, mask=None):\n        \"\"\" Forward pass for one image \"\"\"\n        x = self.cv(inputs)\n        x = self.pool(x)\n        x = self.flatten(x)\n        x = self.dense(x)\n        return x\n\n    @staticmethod\n    def distance(difference):\n        \"\"\" The D function from the paper which is used in loss \"\"\"\n        distance = tf.sqrt(tf.reduce_sum(tf.pow(difference, 2), 0))\n        return distance\n</code></pre>\n\n<p>The loss and accuracy:</p>\n\n<pre><code>def simnet_loss(target, x1, x2):\n    difference = x1 - x2\n    distance_vector = tf.map_fn(lambda x: Encoder.distance(x), difference)\n    loss = tf.map_fn(lambda distance: target * tf.square(distance) +\n                                      (1.0 - target) * tf.square(tf.maximum(0.0, 1.0 - distance)), distance_vector)\n    average_loss = tf.reduce_mean(loss)\n    return average_loss\n\ndef accuracy(y_true, y_pred):\n    distance_vector = tf.map_fn(lambda x: Encoder.distance(x), y_pred)\n    accuracy = tf.keras.metrics.binary_accuracy(y_true, distance_vector)\n    return accuracy\n</code></pre>\n\n<p>Training:</p>\n\n<pre><code>def train_step(images, labels):\n    with tf.GradientTape() as tape:\n        x1, x2 = images[:, 0, :, :, :], images[:, 1, :, :, :]\n        x1 = model(x1)\n        x2 = model(x2)\n        loss = simnet_loss(labels, x1, x2)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    return loss\n\nmodel = Encoder()\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\nfor epoch in range(n_epoch):\n    epoch_loss = 0\n    n_batches = int(x_train.shape[0]/batch_size)\n    for indices in np.array_split(np.arange(x_train.shape[0]), indices_or_sections=n_batches):\n        x = np.take(x_train, indices, axis=0)\n        y = np.take(y_train, indices, axis=0)\n        epoch_loss += train_step(x, y)\n\n    epoch_loss = epoch_loss / n_batches\n    accuracy = test_step(x_train, y_train)\n    val_accuracy = test_step(x_test, y_test)\n    tf.print(\"epoch:\", epoch, \"loss:\", epoch_loss, \"accuracy:\", accuracy,\n             \"val_accuracy:\", val_accuracy, output_stream=sys.stdout)\n</code></pre>\n\n<p>The code above produces:</p>\n\n<blockquote>\n  <p>epoch: 0 loss: 0.755419433 accuracy: 0.318898171 val_accuracy:\n  0.310316473</p>\n  \n  <p>epoch: 1 loss: 0.270610392 accuracy: 0.369466901 val_accuracy:\n  0.360871345</p>\n  \n  <p>epoch: 2 loss: 0.262594223 accuracy: 0.430587918 val_accuracy:\n  0.418002456</p>\n  \n  <p>epoch: 3 loss: 0.258690506 accuracy: 0.428258181 val_accuracy:\n  0.427044809</p>\n  \n  <p>epoch: 4 loss: 0.25654456 accuracy: 0.43497327 val_accuracy:\n  0.44800657</p>\n  \n  <p>epoch: 5 loss: 0.255373538 accuracy: 0.444840342 val_accuracy:\n  0.454993844</p>\n  \n  <p>epoch: 6 loss: 0.254594624 accuracy: 0.453885168 val_accuracy:\n  0.454171807</p>\n</blockquote>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 133}]
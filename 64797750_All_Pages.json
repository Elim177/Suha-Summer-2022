[{"items": [{"tags": ["tensorflow", "machine-learning", "keras", "deep-learning", "openai-gym"], "owner": {"account_id": 10858883, "reputation": 2767, "user_id": 7984318, "user_type": "registered", "accept_rate": 86, "profile_image": "https://www.gravatar.com/avatar/4bdbe511507924dda59497c425c04c4b?s=256&d=identicon&r=PG&f=1", "display_name": "William", "link": "https://stackoverflow.com/users/7984318/william"}, "is_answered": true, "view_count": 512, "answer_count": 1, "score": 0, "last_activity_date": 1605975498, "creation_date": 1605153719, "question_id": 64797750, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64797750/how-to-use-own-environment-for-ddpg-without-gym", "title": "How to use own environment for DDPG without gym", "body": "<p>I'm using Keras to build a ddpg model,I followed the official instruction from here <a href=\"https://keras.io/examples/rl/ddpg_pendulum/\" rel=\"nofollow noreferrer\">enter link description here</a></p>\n<p>But I want to my own environment, not gym,here is my own environment:</p>\n<pre><code>class Environment1:\ndef __init__(self, data, history_t=90):\n    self.data = data\n    self.history_t = history_t\n    self.reset ()\n\ndef reset(self):\n    self.t = 0\n    self.done = False\n    self.profits = 0\n    self.positions = []\n    self.position_value = 0\n    self.history = [0 for _ in range (self.history_t)]\n    return [self.position_value] + self.history  # obs\n\ndef step(self, act):\n    reward = 0\n\n    # act = 0: stay, 1: buy, 2: sell\n    if act == 1:\n        self.positions.append (self.data.iloc[self.t, :]['close'])\n    elif act == 2:  # sell\n        if len (self.positions) == 0:\n            reward = -1\n        else:\n            profits = 0\n            for p in self.positions:\n                profits += (self.data.iloc[self.t, :]['close'] - p)\n            reward += profits\n            self.profits += profits\n            self.positions = []\n\n    # set next time\n    self.t += 1\n    self.position_value = 0\n    for p in self.positions:\n        self.position_value += (self.data.iloc[self.t, :]['close'] - p)\n    self.history.pop (0)\n    self.history.append (self.data.iloc[self.t, :]['close'] - self.data.iloc[(self.t - 1), :]['close'])\n\n    # clipping reward\n    if reward &gt; 0:\n        reward = 1\n    elif reward &lt; 0:\n        reward = -1\n\n    return [self.position_value] + self.history, reward, self.done  # obs, reward, done\n\n\nenv = Environment1 (train)\nprint (env.reset ())\nfor _ in range (3):\n    pact = np.random.randint (3)\n    print (env.step (pact))  \n</code></pre>\n<p>When I use my own environment as above,there is an error:</p>\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-1-a51b38095bf0&gt; in &lt;module&gt;\n    179 # env = gym.make(problem)\n    180 \n--&gt; 181 num_states = env.observation_space.shape[0]\n    182 print(&quot;Size of State Space -&gt;  {}&quot;.format(num_states))\n    183 num_actions = env.action_space.shape[0]\n\nAttributeError: 'Environment1' object has no attribute 'observation_space'\n</code></pre>\n<p>Following is the whole code:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\n\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom subprocess import check_output\n# print(check_output([&quot;ls&quot;, &quot;../input&quot;]).decode(&quot;utf8&quot;))\nimport time\nimport copy\nimport numpy as np\nimport pandas as pd\n\n\n\ndata = pd.read_csv (r'C:\\Users\\willi\\Downloads\\spyv.csv')\n\ndata = data.loc[:, ~data.columns.str.contains('^Unnamed')]\n\n\ndate_split = 377\n\ntrain = data[:date_split]\ntest = data[date_split:]\n\n\nclass Environment1:\n\n    def __init__(self, data, history_t=90):\n        self.data = data\n        self.history_t = history_t\n        self.reset ()\n\n    def reset(self):\n        self.t = 0\n        self.done = False\n        self.profits = 0\n        self.positions = []\n        self.position_value = 0\n        self.history = [0 for _ in range (self.history_t)]\n        return [self.position_value] + self.history  # obs\n\n    def step(self, act):\n        reward = 0\n\n        # act = 0: stay, 1: buy, 2: sell\n        if act == 1:\n            self.positions.append (self.data.iloc[self.t, :]['close'])\n        elif act == 2:  # sell\n            if len (self.positions) == 0:\n                reward = -1\n            else:\n                profits = 0\n                for p in self.positions:\n                    profits += (self.data.iloc[self.t, :]['close'] - p)\n                reward += profits\n                self.profits += profits\n                self.positions = []\n\n        # set next time\n        self.t += 1\n        self.position_value = 0\n        for p in self.positions:\n            self.position_value += (self.data.iloc[self.t, :]['close'] - p)\n        self.history.pop (0)\n        self.history.append (self.data.iloc[self.t, :]['close'] - self.data.iloc[(self.t - 1), :]['close'])\n\n        # clipping reward\n        if reward &gt; 0:\n            reward = 1\n        elif reward &lt; 0:\n            reward = -1\n\n        return [self.position_value] + self.history, reward, self.done  # obs, reward, done\n\n\nenv = Environment1 (train)\nprint (env.reset ())\nfor _ in range (3):\n    pact = np.random.randint (3)\n    print (env.step (pact))\n\n#above here are all my own code, under here is the code from keras\n\nnum_states = env.observation_space.shape[0]\nprint(&quot;Size of State Space -&gt;  {}&quot;.format(num_states))\nnum_actions = env.action_space.shape[0]\nprint(&quot;Size of Action Space -&gt;  {}&quot;.format(num_actions))\n\nupper_bound = env.action_space.high[0]\nlower_bound = env.action_space.low[0]\n\nprint(&quot;Max Value of Action -&gt;  {}&quot;.format(upper_bound))\nprint(&quot;Min Value of Action -&gt;  {}&quot;.format(lower_bound))\n\n\nclass OUActionNoise:\n    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n        self.theta = theta\n        self.mean = mean\n        self.std_dev = std_deviation\n        self.dt = dt\n        self.x_initial = x_initial\n        self.reset()\n\n    def __call__(self):\n        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n        x = (\n            self.x_prev\n            + self.theta * (self.mean - self.x_prev) * self.dt\n            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n        )\n        # Store x into x_prev\n        # Makes next noise dependent on current one\n        self.x_prev = x\n        return x\n\n    def reset(self):\n        if self.x_initial is not None:\n            self.x_prev = self.x_initial\n        else:\n            self.x_prev = np.zeros_like(self.mean)\n            \n            \nclass Buffer:\n    def __init__(self, buffer_capacity=100000, batch_size=64):\n        # Number of &quot;experiences&quot; to store at max\n        self.buffer_capacity = buffer_capacity\n        # Num of tuples to train on.\n        self.batch_size = batch_size\n\n        # Its tells us num of times record() was called.\n        self.buffer_counter = 0\n\n        # Instead of list of tuples as the exp.replay concept go\n        # We use different np.arrays for each tuple element\n        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n\n    # Takes (s,a,r,s') obervation tuple as input\n    def record(self, obs_tuple):\n        # Set index to zero if buffer_capacity is exceeded,\n        # replacing old records\n        index = self.buffer_counter % self.buffer_capacity\n\n        self.state_buffer[index] = obs_tuple[0]\n        self.action_buffer[index] = obs_tuple[1]\n        self.reward_buffer[index] = obs_tuple[2]\n        self.next_state_buffer[index] = obs_tuple[3]\n\n        self.buffer_counter += 1\n\n    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n    # TensorFlow to build a static graph out of the logic and computations in our function.\n    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n    @tf.function\n    def update(\n        self, state_batch, action_batch, reward_batch, next_state_batch,\n    ):\n        # Training and updating Actor &amp; Critic networks.\n        # See Pseudo Code.\n        with tf.GradientTape() as tape:\n            target_actions = target_actor(next_state_batch, training=True)\n            y = reward_batch + gamma * target_critic(\n                [next_state_batch, target_actions], training=True\n            )\n            critic_value = critic_model([state_batch, action_batch], training=True)\n            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n\n        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n        critic_optimizer.apply_gradients(\n            zip(critic_grad, critic_model.trainable_variables)\n        )\n\n        with tf.GradientTape() as tape:\n            actions = actor_model(state_batch, training=True)\n            critic_value = critic_model([state_batch, actions], training=True)\n            # Used `-value` as we want to maximize the value given\n            # by the critic for our actions\n            actor_loss = -tf.math.reduce_mean(critic_value)\n\n        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n        actor_optimizer.apply_gradients(\n            zip(actor_grad, actor_model.trainable_variables)\n        )\n\n    # We compute the loss and update parameters\n    def learn(self):\n        # Get sampling range\n        record_range = min(self.buffer_counter, self.buffer_capacity)\n        # Randomly sample indices\n        batch_indices = np.random.choice(record_range, self.batch_size)\n\n        # Convert to tensors\n        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n\n        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n\n\n# This update target parameters slowly\n# Based on rate `tau`, which is much less than one.\n@tf.function\ndef update_target(target_weights, weights, tau):\n    for (a, b) in zip(target_weights, weights):\n        a.assign(b * tau + a * (1 - tau))\n        \n        \ndef get_actor():\n    # Initialize weights between -3e-3 and 3-e3\n    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n\n    inputs = layers.Input(shape=(num_states,))\n    out = layers.Dense(256, activation=&quot;relu&quot;)(inputs)\n    out = layers.Dense(256, activation=&quot;relu&quot;)(out)\n    outputs = layers.Dense(1, activation=&quot;tanh&quot;, kernel_initializer=last_init)(out)\n\n    # Our upper bound is 2.0 for Pendulum.\n    outputs = outputs * upper_bound\n    model = tf.keras.Model(inputs, outputs)\n    return model\n\n\ndef get_critic():\n    # State as input\n    state_input = layers.Input(shape=(num_states))\n    state_out = layers.Dense(16, activation=&quot;relu&quot;)(state_input)\n    state_out = layers.Dense(32, activation=&quot;relu&quot;)(state_out)\n\n    # Action as input\n    action_input = layers.Input(shape=(num_actions))\n    action_out = layers.Dense(32, activation=&quot;relu&quot;)(action_input)\n\n    # Both are passed through seperate layer before concatenating\n    concat = layers.Concatenate()([state_out, action_out])\n\n    out = layers.Dense(256, activation=&quot;relu&quot;)(concat)\n    out = layers.Dense(256, activation=&quot;relu&quot;)(out)\n    outputs = layers.Dense(1)(out)\n\n    # Outputs single value for give state-action\n    model = tf.keras.Model([state_input, action_input], outputs)\n\n    return model\n\n\ndef policy(state, noise_object):\n    sampled_actions = tf.squeeze(actor_model(state))\n    noise = noise_object()\n    # Adding noise to action\n    sampled_actions = sampled_actions.numpy() + noise\n\n    # We make sure action is within bounds\n    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n\n    return [np.squeeze(legal_action)]\n\nstd_dev = 0.2\nou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n\nactor_model = get_actor()\ncritic_model = get_critic()\n\ntarget_actor = get_actor()\ntarget_critic = get_critic()\n\n# Making the weights equal initially\ntarget_actor.set_weights(actor_model.get_weights())\ntarget_critic.set_weights(critic_model.get_weights())\n\n# Learning rate for actor-critic models\ncritic_lr = 0.002\nactor_lr = 0.001\n\ncritic_optimizer = tf.keras.optimizers.Adam(critic_lr)\nactor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n\ntotal_episodes = 100\n# Discount factor for future rewards\ngamma = 0.99\n# Used to update target networks\ntau = 0.005\n\nbuffer = Buffer(50000, 64)\n\n\n# To store reward history of each episode\nep_reward_list = []\n# To store average reward history of last few episodes\navg_reward_list = []\n\n# Takes about 4 min to train\nfor ep in range(total_episodes):\n\n    prev_state = env.reset()\n    episodic_reward = 0\n\n    while True:\n        # Uncomment this to see the Actor in action\n        # But not in a python notebook.\n        # env.render()\n\n        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n\n        action = policy(tf_prev_state, ou_noise)\n        # Recieve state and reward from environment.\n        state, reward, done, info = env.step(action)\n\n        buffer.record((prev_state, action, reward, state))\n        episodic_reward += reward\n\n        buffer.learn()\n        update_target(target_actor.variables, actor_model.variables, tau)\n        update_target(target_critic.variables, critic_model.variables, tau)\n\n        # End this episode when `done` is True\n        if done:\n            break\n\n        prev_state = state\n\n    ep_reward_list.append(episodic_reward)\n\n    # Mean of last 40 episodes\n    avg_reward = np.mean(ep_reward_list[-40:])\n    print(&quot;Episode * {} * Avg Reward is ==&gt; {}&quot;.format(ep, avg_reward))\n    avg_reward_list.append(avg_reward)\n\n# Plotting graph\n# Episodes versus Avg. Rewards\nplt.plot(avg_reward_list)\nplt.xlabel(&quot;Episode&quot;)\nplt.ylabel(&quot;Avg. Epsiodic Reward&quot;)\nplt.show()\n\n\n# Save the weights\nactor_model.save_weights(&quot;pendulum_actor.h5&quot;)\ncritic_model.save_weights(&quot;pendulum_critic.h5&quot;)\n\ntarget_actor.save_weights(&quot;pendulum_target_actor.h5&quot;)\ntarget_critic.save_weights(&quot;pendulum_target_critic.h5&quot;)\n</code></pre>\n<p>After I run the whole code error:</p>\n<pre><code>    AttributeError: 'Environment1' object has no attribute 'observation_space'\n</code></pre>\n<p>Any friend can help?It is real hard for me.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 281}]
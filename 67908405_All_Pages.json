[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "tensorflow-datasets"], "owner": {"account_id": 20538721, "reputation": 21, "user_id": 15074102, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1fa7e0849947169f284ef0e6a0036336?s=256&d=identicon&r=PG&f=1", "display_name": "Seung-Hoon Yi", "link": "https://stackoverflow.com/users/15074102/seung-hoon-yi"}, "is_answered": false, "view_count": 69, "answer_count": 0, "score": 2, "last_activity_date": 1623256980, "creation_date": 1623256980, "question_id": 67908405, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67908405/input-data-error-in-tensorflow-based-transformer", "title": "Input data error in Tensorflow based transformer", "body": "<p>I was making a custom transformer model based on <a href=\"https://www.tensorflow.org/text/tutorials/transformer#setup_input_pipeline\" rel=\"nofollow noreferrer\">this</a> tutorial for my dataset. <br><br>\nThe overall shape of the dataset is like : <br>\n<strong>Encoder input</strong> : frame features of (Batch size, 150, 1024) <br>\n<strong>Decoder input</strong> : word sequence of (Batch size, 50), composed with [sos token, integer tokens]<br>\n<strong>Decoder output</strong> : word sequence of (Batch size, 50), composed with [integer tokens, eos token]<br>\nAnd each of them are named as &quot;encoder_ipt_train&quot;, &quot;decoder_ipt_train&quot;, &quot;decoder_opt_train&quot;.<br><br>\nHere i used the code below for <br>\n<strong>1. Generating dataset batches</strong><br></p>\n<pre><code>BATCH_SIZE = 32\nBUFFER_SIZE = 16384\n\ndataset = tf.data.Dataset.from_tensor_slices((\n{\n    'inputs': encoder_ipt_train,\n    'dec_inputs': decoder_ipt_train \n},\n{\n    'outputs': decoder_opt_train \n},\n))\n\ndataset = dataset.cache()\ndataset = dataset.shuffle(BUFFER_SIZE)\ndataset = dataset.batch(BATCH_SIZE)\ndataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n</code></pre>\n<br>\n<p><strong>2. In the training phase</strong> <br></p>\n<pre><code>EPOCHS = 20\n\ntrain_step_signature = [\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n]\n\n\n@tf.function(input_signature=train_step_signature)\ndef train_step(enc_ipt, dec_ipt, dec_opt):\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(enc_ipt, dec_ipt)\n\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(enc_ipt, dec_ipt, True,\n                                     enc_padding_mask, combined_mask, dec_padding_mask)\n        loss = loss_function(dec_opt, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(accuracy_function(dec_opt, predictions))\n\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    for (batch, (inp, tar)) in enumerate(dataset):\n        case 1 --&gt; train_step(inp, tar)\n        case 2 --&gt; train_step(inp['inputs'],inp['dec_inputs'], tar['outputs'])\n\n        if batch % 50 == 0:\n          print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n\n    if (epoch + 1) % 5 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n\n    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n\n    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')\n</code></pre>\n<p><br><br>\nBut this results a error like this, whether i use <strong>case 1</strong> or <strong>case 2</strong> in <strong>2.</strong> :\n<a href=\"https://i.stack.imgur.com/oQUO4.png\" rel=\"nofollow noreferrer\">The entire error message</a>\n<br></p>\n<pre><code>TypeError: Expected any non-tensor type, got a tensor instead.\n</code></pre>\n<br>\n<p>Now i'm doubting about my pipeline structure or the data type, but i have no idea how to change data type from 'dataset' in <strong>1</strong>. Is there anyone who had similar problems like this or solved it?<br>\nand some additional questions : <br>\nFirst, I wonder if I'm using the tf.data.Dataset package in a right way. As far as i know, 'dataset' in <strong>1</strong> returns a dictionary of tensor batches, such that if i run the code below, <br></p>\n<pre><code>for (batch, (dict_1, dict_2)) in enumerate(dataset):\n</code></pre>\n<p><strong>dict_1['inputs']</strong> returns the batch of <strong>encoder_ipt_train</strong> with size of (32, 150, 1024). I wonder whether my understanding is correct.<br></p>\n<p>Second, is there a better way(or a right way)to insert data batches to my training_step() function?</p>\n<p>Sincerely, Seunghoon.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 126}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning"], "owner": {"account_id": 16777256, "reputation": 361, "user_id": 12128237, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f56364f29ee711028529370db5673fd4?s=256&d=identicon&r=PG&f=1", "display_name": "dexter2406", "link": "https://stackoverflow.com/users/12128237/dexter2406"}, "is_answered": false, "view_count": 607, "answer_count": 0, "score": 1, "last_activity_date": 1611841026, "creation_date": 1611841026, "question_id": 65938203, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65938203/why-inference-of-savedmodel-slower-on-tf-keras-than-tf-saved-model", "title": "Why inference of SavedModel slower on tf.keras than tf.saved_model?", "body": "<p>What I did:</p>\n<ul>\n<li>convert the <code>.weights</code> to <code>SavedModel</code> (.pb), using <code>tf.keras.models.save()</code></li>\n<li>load the new model file in two ways below:</li>\n</ul>\n<pre><code># 1st way: tf.saved_model.load\nmodel = tf.saved_model.load(model_path, tags=[tag_constants.SERVING])\ninfer = model.signatures['serving_default']\nres = infer(tf.constant(batch_data))\n\n# 2nd way: tf.keras.models.load\nmodel = tf.keras.models.load_model(model_path, compile=False)\nres = model.predict(batch_data)\n</code></pre>\n<p>The first runs at 15 FPS, while the second runs at 10 FPS, 1.5x slower...</p>\n<p>My ultimate goal is to output both <em>intermediate layer outputs</em> and <em>final predictions</em>. And the only (simple) way to achieve this in TF2 is by <code>tf.keras.Modul(model.inputs, [&lt;layer&gt;.output, model.output])</code>. I need that loaded model to be an keras object to implement this.</p>\n<p>So how could I use <code>keras</code> way and maintain the same inference speed?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 261}]
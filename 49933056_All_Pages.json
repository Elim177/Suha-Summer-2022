[{"items": [{"tags": ["python", "tensorflow", "lstm", "rnn"], "owner": {"account_id": 408246, "reputation": 309, "user_id": 778800, "user_type": "registered", "accept_rate": 86, "profile_image": "https://www.gravatar.com/avatar/35bda0cfce51768d9ffe04c3cdfcdfcb?s=256&d=identicon&r=PG", "display_name": "menphix", "link": "https://stackoverflow.com/users/778800/menphix"}, "is_answered": true, "view_count": 659, "accepted_answer_id": 49933355, "answer_count": 1, "score": 0, "last_activity_date": 1524193991, "creation_date": 1524191827, "last_edit_date": 1524193034, "question_id": 49933056, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49933056/tensorflow-rnn-lm-loss-not-decreasing", "title": "Tensorflow RNN LM loss not decreasing", "body": "<p>I'm trying to train a basic unidirectional LSTM RNN language model on PennTree Bank. My neural network runs, but the loss on the test set is not decreasing at all. I'm wondering why is this? </p>\n\n<p>Network parameters:</p>\n\n<pre><code>V = 10000\nbatch_size = 20\nhidden_size = 650\nembed_size = hidden_size\nnum_unrollings = 35\nmax_epoch = 6\nlearning_rate = 1.0\n</code></pre>\n\n<p>Graph definition:</p>\n\n<pre><code>graph = tf.Graph()\nwith graph.as_default():\n  cell_state = tf.placeholder(tf.float32, shape=(batch_size, hidden_size), name=\"CellState\")\n  hidden_state = tf.placeholder(tf.float32, shape=(batch_size, hidden_size), name=\"HiddenState\")\n  curr_batch = tf.placeholder(tf.int32, shape=[num_unrollings + 1, batch_size])\n  lstm = tf.contrib.rnn.BasicLSTMCell(hidden_size)\n  embeddings = tf.Variable(tf.truncated_normal([V, embed_size], -0.1, 0.1), trainable=True, dtype=tf.float32)\n  W = tf.Variable(tf.truncated_normal([hidden_size, V], -0.1, 0.1))\n  b = tf.Variable(tf.zeros(V))\n\n  inputs = curr_batch[:num_unrollings,:] # num_unrollings x batch_size\n  labels = curr_batch[1:, :] # num_unrollings x batch_size\n\n  input_list = list()\n  for t in range(num_unrollings):\n    emb = tf.nn.embedding_lookup(embeddings, inputs[t,:])\n    input_list.append(emb)\n\n  outputs, states = tf.nn.static_rnn(lstm, input_list, initial_state=[cell_state, hidden_state])  # outputs: num_unrollings x batch_size x hidden\n  cell_state, hidden_state = states\n  outputs_flat = tf.reshape(outputs, [-1, lstm.output_size]) # output_flat: (num_unrollings x batch_size) x hidden\n  logits = tf.nn.softmax(tf.matmul(outputs_flat, W) + b)   # logits_tensor: (num_unrollings x batch_size) x V\n  logits_tensor = tf.reshape(logits, [batch_size, num_unrollings, V])\n\n  targets = tf.transpose(labels)  # targets: batch_size x num_unrollings\n  weights = tf.ones([batch_size, num_unrollings]) # weights: batch_size x num_unrollings\n  loss = tf.reduce_sum(tf.contrib.seq2seq.sequence_loss(logits_tensor, targets, weights, average_across_timesteps=False, average_across_batch=True))\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n</code></pre>\n\n<p>Session:</p>\n\n<pre><code>with tf.Session(graph=graph) as session:\n  tf.global_variables_initializer().run()\n  cstate = np.zeros([batch_size, hidden_size]).astype(np.float32)\n  hstate = np.zeros([batch_size, hidden_size]).astype(np.float32)\n  for epoch in range(max_epoch):\n    CURSOR_train = 0\n    epoch_over = False\n    steps = 0\n    average_loss = 0.0\n    while not epoch_over:\n      new_batch, epoch_over = nextBatch()\n      feed_data = {curr_batch: new_batch, \"CellState:0\": cstate, \"HiddenState:0\": hstate}\n      _, l, new_cell_state, new_hidden_state = session.run([optimizer, loss, cell_state, hidden_state], feed_dict=feed_data)\n      cstate = new_cell_state\n      hstate = new_hidden_state\n      average_loss += l\n      PRINT_INTERVAL = 200\n      if steps % PRINT_INTERVAL == 0:\n        print(\"Avg loss for last {0} batches: {1}\".format(PRINT_INTERVAL, average_loss / PRINT_INTERVAL))\n        average_loss = 0\n\n      TEST_INTERVAL = 600\n      if steps % TEST_INTERVAL == 0:\n        # Evaluate the model\n        test_over = False\n        test_loss = 0.0\n        test_batch_num = 0\n        print(\"Testing ... \")\n        while not test_over:\n          test_batch_num += 1\n          test_batch, test_over = nextBatch(setup='test')\n          feed_data_test = { curr_batch: test_batch, \"CellState:0\": cstate, \"HiddenState:0\": hstate }\n          tl, d1, d2 = session.run([loss, cell_state, hidden_state], feed_dict=feed_data_test)\n          test_loss += tl\n        test_loss = test_loss / test_batch_num\n        print(\"Avg loss on test set: {0}\".format(test_loss))\n\n      steps += 1\n      sys.stdout.write('\\rStep: {0}'.format(steps))\n</code></pre>\n\n<p>The loss on test set is always 320.2430792614422, no matter how long I train it. The loss on the training set does change. \nThanks in adavance!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 109}]
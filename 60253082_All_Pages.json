[{"items": [{"tags": ["deep-learning", "conv-neural-network", "image-segmentation", "tf.keras", "weighting"], "owner": {"account_id": 7120877, "reputation": 337, "user_id": 5445548, "user_type": "registered", "accept_rate": 40, "profile_image": "https://i.stack.imgur.com/pmGVM.png?s=256&g=1", "display_name": "maracuja", "link": "https://stackoverflow.com/users/5445548/maracuja"}, "is_answered": true, "view_count": 2195, "answer_count": 2, "score": 7, "last_activity_date": 1624962864, "creation_date": 1581886333, "last_edit_date": 1581936807, "question_id": 60253082, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60253082/weighting-samples-in-multiclass-image-segmentation-using-keras", "title": "Weighting samples in multiclass image segmentation using keras", "body": "<p>I am using a Unet based model to perform image segmentation on a biomedical image. Each image is 224x224 and I have four classes including the background class. Each mask is sized as (224x224x4) and so my generator creates batches of numpy arrays sized (16x224x224x4). I recast the values for the mask as either 1 or 0 so for each class a 1 is present in the relevant channel. The image is also scaled by 1/255. I use a dice score as the performance metric during training and 1-dice score as the loss function. I seem to be getting scores up to 0.89 during training but  I'm finding that when I predict on my test set I am always predicting the background class. I'm only training for 10 epochs on a few hundred images (although I do have access to far more) which may be affecting the model but I would have thought I'd still get predictions of other classes so i'm assuming the main problem is a class imbalance. From looking online the sample_weight argument could be the answer but I'm not sure how I'm meant to implement the actual weight part? presumably I need to apply the weights to the array of pixels at some point in the model using a layer but i'm not sure how. Any help would be much appreciated?</p>\n\n<pre><code>class DataGenerator(keras.utils.Sequence):\n     def __init__(self, imgIds, maskIds, imagePath, maskPath, batchSize=16, imageSize = (224, 224, 3), nClasses=2, shuffle=False):\n       self.imgIds = imgIds\n       self.maskIds = maskIds\n       self.imagePath = imagePath\n       self.maskPath = maskPath\n       self.batchSize = batchSize\n       self.imageSize = imageSize\n       self.nClasses = nClasses\n       self.shuffle = shuffle\n\n\n     def __load__(self, imgName, maskName):\n\n       img = cv2.imread(os.path.join(self.imagePath,imgName))\n       img = cv2.resize(img, (self.imageSize[0], self.imageSize[1]))\n\n       mask = cv2.imread(os.path.join(self.maskPath,maskName))\n       mask = np.dstack((mask, np.zeros((4000, 4000))))\n\n       mask[:,:,3][mask[:,:,0]==0]=255\n       mask = mask.astype(np.bool)\n       mask = img_as_bool(resize(mask, (self.imageSize[0], self.imageSize[1])))\n       mask = mask.astype('uint8')\n\n       img = img/255.0\n       mask = mask\n\n       return (img, mask)\n\n\n    def __getitem__(self, index):\n\n       if(index+1)*self.batchSize &gt; len(self.imgIds):\n          self.batchSize = len(self.imgIds) - index*self.batchSize\n\n       batchImgs = self.imgIds[self.batchSize*index:self.batchSize*(index+1)]\n       batchMasks = self.maskIds[self.batchSize*index:self.batchSize*(index+1)]\n\n       batchfiles = [self.__load__(imgFile, maskFile) for imgFile, maskFile in \n       zip(batchImgs, batchMasks)]\n\n       images, masks = zip(*batchfiles)\n\n       return np.array(list(images)), np.array(list(masks))\n\n\n   def __len__(self):\n       return int(np.ceil(len(self.imgIds)/self.batchSize))\n\n\nclass Unet():\n   def __init__(self, imgSize):\n       self.imgSize = imgSize\n\n\n   def convBlocks(self, x, filters, kernelSize=(3,3), padding='same', strides=1):\n\n       x = keras.layers.BatchNormalization()(x)\n       x = keras.layers.Activation('relu')(x)\n       x = keras.layers.Conv2D(filters, kernelSize, padding=padding, strides=strides)(x)\n\n       return x\n\n\n   def identity(self, x, xInput, f, padding='same', strides=1):\n\n      skip = keras.layers.Conv2D(f, kernel_size=(1, 1), padding=padding, strides=strides)(xInput)\n      skip = keras.layers.BatchNormalization()(skip)\n      output = keras.layers.Add()([skip, x])\n\n      return output\n\n\n    def residualBlock(self, xIn, f, stride):\n\n      res = self.convBlocks(xIn, f, strides=stride)\n      res = self.convBlocks(res, f, strides=1)\n      output = self.identity(res, xIn, f, strides=stride)\n\n      return output\n\n\n    def upSampling(self, x, xInput):\n\n      x = keras.layers.UpSampling2D((2,2))(x)\n      x = keras.layers.Concatenate()([x, xInput])\n\n      return x\n\n\n    def encoder(self, x, filters, kernelSize=(3,3), padding='same', strides=1):\n\n      e1 = keras.layers.Conv2D(filters[0], kernelSize, padding=padding, strides=strides)(x)\n      e1 = self.convBlocks(e1, filters[0])\n\n      shortcut = keras.layers.Conv2D(filters[0], kernel_size=(1, 1), padding=padding, strides=strides)(x)\n      shortcut = keras.layers.BatchNormalization()(shortcut)\n      e1Output = keras.layers.Add()([e1, shortcut])\n\n      e2 = self.residualBlock(e1Output, filters[1], stride=2)\n      e3 = self.residualBlock(e2, filters[2], stride=2)\n      e4 = self.residualBlock(e3, filters[3], stride=2)\n      e5 = self.residualBlock(e4, filters[4], stride=2)\n\n      return e1Output, e2, e3, e4, e5\n\n\n  def bridge(self, x, filters):\n\n      b1 = self.convBlocks(x, filters, strides=1)\n      b2 = self.convBlocks(b1, filters, strides=1)\n\n      return b2\n\n\n  def decoder(self, b2, e1, e2, e3, e4, filters, kernelSize=(3,3), padding='same', strides=1):\n\n      x = self.upSampling(b2, e4)\n      d1 = self.convBlocks(x, filters[4])\n      d1 = self.convBlocks(d1, filters[4])\n      d1 = self.identity(d1, x, filters[4])\n\n      x = self.upSampling(d1, e3)\n      d2 = self.convBlocks(x, filters[3])\n      d2 = self.convBlocks(d2, filters[3])\n      d2 = self.identity(d2, x, filters[3])\n\n      x = self.upSampling(d2, e2)\n      d3 = self.convBlocks(x, filters[2])\n      d3 = self.convBlocks(d3, filters[2])\n      d3 = self.identity(d3, x, filters[2])\n\n      x = self.upSampling(d3, e1)\n      d4 = self.convBlocks(x, filters[1])\n      d4 = self.convBlocks(d4, filters[1])\n      d4 = self.identity(d4, x, filters[1])\n\n      return d4 \n\n\n  def ResUnet(self, filters = [16, 32, 64, 128, 256]):\n\n      inputs = keras.layers.Input((224, 224, 3))\n\n      e1, e2, e3, e4, e5 = self.encoder(inputs, filters)\n      b2 = self.bridge(e5, filters[4])\n      d4 = self.decoder(b2, e1, e2, e3, e4, filters)\n\n      x = keras.layers.Conv2D(4, (1, 1), padding='same', activation='softmax')(d4)\n      model = keras.models.Model(inputs, x)\n\n      return model\n\n\nimagePath = 'output/t2'\nmaskPath = 'output/t1'\n\nimgIds = glob.glob(os.path.join(imagePath, '*'))\nmaskIds = glob.glob(os.path.join(maskPath, '*'))\n\nimgIds = [os.path.basename(f) for f in imgIds]\nmaskIds = [os.path.basename(f) for f in maskIds]\n\ntrainImgIds = imgIds[:300]\ntrainMaskIds = maskIds[:300]\nvalidImgIds = imgIds[300:350]\nvalidMaskIds = maskIds[300:350]\n\ntrainGenerator = DataGenerator(trainImgIds, trainMaskIds, imagePath, maskPath, **params)\nvalidGenerator = DataGenerator(validImgIds, validMaskIds, imagePath, maskPath)\n\ntrainSteps = len(trainImgIds)//trainGenerator.batchSize\nvalidSteps = len(validImgIds)//validGenerator.batchSize\n\nunet = Unet(224)\nmodel = unet.ResUnet()\nmodel.summary()\n\nadam = keras.optimizers.Adam()\nmodel.compile(optimizer=adam, loss=dice_coef_loss, metrics=[dice_coef])\n\nhist = model.fit_generator(trainGenerator, validation_data=validGenerator, \nsteps_per_epoch=trainSteps, validation_steps=validSteps, \n                verbose=1, epochs=6)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 51}]
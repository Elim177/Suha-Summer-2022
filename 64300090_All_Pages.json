[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning", "generative-adversarial-network"], "owner": {"account_id": 13653812, "reputation": 187, "user_id": 9850921, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/f1e283f98c88a7a5580da3afd76d9e81?s=256&d=identicon&r=PG&f=1", "display_name": "Gibser", "link": "https://stackoverflow.com/users/9850921/gibser"}, "is_answered": false, "view_count": 539, "answer_count": 0, "score": 2, "last_activity_date": 1602383246, "creation_date": 1602383246, "question_id": 64300090, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64300090/keras-notimplementederror-when-subclassing-the-model-class-you-should-impl", "title": "Keras - NotImplementedError: When subclassing the `Model` class, you should implement a `call` method", "body": "<p>I modified the WGAN-GP from the keras website (<a href=\"https://keras.io/examples/generative/wgan_gp/\" rel=\"nofollow noreferrer\">https://keras.io/examples/generative/wgan_gp/</a>) for what I needed: my network takes 3 consecutive images as input. The generator takes the one in the middle and tries to generate a similar image, the discriminator takes all 3 images and evaluates if the second is really the central one among the 3.\nHowever it gives me this error when I call fit():</p>\n<p><em>NotImplementedError: When subclassing the `Model' class, you should implement a call method.</em></p>\n<p>I am using tensorflow 2.3</p>\n<p>Model:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Conv2DTranspose\nfrom tensorflow.keras.layers import LeakyReLU, MaxPooling2D\nfrom tensorflow.keras.layers import Activation, Flatten, Dense\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import concatenate\nfrom tensorflow.keras.layers import LayerNormalization\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers import SGD\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import GaussianNoise\nfrom tensorflow import keras\nimport numpy as np\n\ninit = RandomNormal(mean=0.0, stddev=0.02)\n\nGNOISE = 0.25\nBATCH_SIZE = 32\n\ndef encodeImage(image):\n    layers = []\n\n    layer1 = Conv2D(32, 3, kernel_initializer=init, padding='same')(image)\n    layer1 = BatchNormalization()(layer1)\n    layer1 = LeakyReLU(alpha=0.01)(layer1)\n    layer1 = Conv2D(32, 3, kernel_initializer=init, padding='same')(layer1)\n    layer1 = BatchNormalization()(layer1)\n    layer1 = LeakyReLU(alpha=0.01)(layer1)\n    layer1 = GaussianNoise(GNOISE)(layer1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(layer1)\n    layers.append(layer1)\n    print(pool1.shape)\n\n    layer2 = Conv2D(64, 3, kernel_initializer=init, padding='same')(pool1)\n    layer2 = BatchNormalization()(layer2)\n    layer2 = LeakyReLU(alpha=0.01)(layer2)\n    layer2 = Conv2D(64, 3, kernel_initializer=init, padding='same')(layer2)\n    layer2 = BatchNormalization()(layer2)\n    layer2 = LeakyReLU(alpha=0.01)(layer2)\n    layer2 = GaussianNoise(GNOISE)(layer2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(layer2)\n    layers.append(layer2)\n    print(pool2.shape)\n\n    layer3 = Conv2D(64, 3, kernel_initializer=init, padding='same')(pool2)\n    layer3 = BatchNormalization(momentum=0.8)(layer3)\n    layer3 = LeakyReLU(alpha=0.01)(layer3)\n    layer3 = Conv2D(64, 3, kernel_initializer=init, padding='same')(layer3)\n    layer3 = BatchNormalization(momentum=0.8)(layer3)\n    layer3 = LeakyReLU(alpha=0.01)(layer3)\n    layer3 = GaussianNoise(GNOISE)(layer3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(layer3)\n    layers.append(layer3)\n    print(pool3.shape)\n\n    layer4 = Conv2D(128, 3, kernel_initializer=init, padding='same')(pool3)\n    layer4 = BatchNormalization(momentum=0.8)(layer4)\n    layer4 = LeakyReLU(alpha=0.01)(layer4)\n    layer4 = Conv2D(128, 3, kernel_initializer=init, padding='same')(layer4)\n    layer4 = BatchNormalization(momentum=0.8)(layer4)\n    layer4 = LeakyReLU(alpha=0.01)(layer4)\n    layer4 = GaussianNoise(GNOISE)(layer4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(layer4)\n    layers.append(layer4)\n    print(pool4.shape)\n\n    return pool4, layers\n\n\ndef decodeImage(image, layersA):\n    layer1 = Conv2DTranspose(128, 3, 2, kernel_initializer=init, padding='same')(image)\n    layer1 = Concatenate()([layer1, layersA[3]])\n    layer1 = Conv2D(128, 3, kernel_initializer=init, padding='same')(layer1)\n    layer1 = BatchNormalization(momentum=0.8)(layer1)\n    layer1 = LeakyReLU(alpha=0.01)(layer1)\n    layer1 = Dropout(0.2)(layer1, training=True)\n    layer1 = Conv2D(128, 3, kernel_initializer=init, padding='same')(layer1)\n    layer1 = BatchNormalization(momentum=0.8)(layer1)\n    layer1 = LeakyReLU(alpha=0.01)(layer1)\n    layer1 = GaussianNoise(GNOISE)(layer1)\n\n    layer2 = Conv2DTranspose(64, 3, 2, kernel_initializer=init, padding='same')(layer1)\n    layer2 = Concatenate()([layer2, layersA[2]])\n    layer2 = Conv2D(64, 3, kernel_initializer=init, padding='same')(layer2)\n    layer2 = BatchNormalization(momentum=0.8)(layer2)\n    layer2 = LeakyReLU(alpha=0.01)(layer2)\n    layer2 = Dropout(0.2)(layer2, training=True)\n    layer2 = Conv2D(64, 3, kernel_initializer=init, padding='same')(layer2)\n    layer2 = BatchNormalization(momentum=0.8)(layer2)\n    layer2 = LeakyReLU(alpha=0.01)(layer2)\n    layer2 = GaussianNoise(GNOISE)(layer2)\n\n\n    layer3 = Conv2DTranspose(64, 3, 2, kernel_initializer=init, padding='same')(layer2)\n    layer3 = Concatenate()([layer3, layersA[1]])\n    layer3 = Conv2D(64, 3, kernel_initializer=init, padding='same')(layer3)\n    layer3 = BatchNormalization(momentum=0.8)(layer3)\n    layer3 = LeakyReLU(alpha=0.01)(layer3)\n    layer3 = Dropout(0.2)(layer3, training=True)\n    layer3 = Conv2D(64, 3, kernel_initializer=init, padding='same')(layer3)\n    layer3 = BatchNormalization(momentum=0.8)(layer3)\n    layer3 = LeakyReLU(alpha=0.01)(layer3)\n    layer3 = GaussianNoise(GNOISE)(layer3)\n\n\n    layer4 = Conv2DTranspose(32, 3, 2, kernel_initializer=init, padding='same')(layer3)\n    layer4 = Concatenate()([layer4, layersA[0]])\n    layer4 = Conv2D(32, 3, kernel_initializer=init, padding='same')(layer4)\n    layer4 = BatchNormalization(momentum=0.8)(layer4)\n    layer4 = LeakyReLU(alpha=0.01)(layer4)\n    layer4 = Dropout(0.2)(layer4, training=True)\n    layer4 = Conv2D(32, 3, kernel_initializer=init, padding='same')(layer4)\n    layer4 = BatchNormalization(momentum=0.8)(layer4)\n    layer4 = LeakyReLU(alpha=0.01)(layer4)\n    layer4 = GaussianNoise(GNOISE)(layer4)\n\n    outImage = Conv2D(1, 1, kernel_initializer=init, padding='same')(layer4)\n    outImage = Activation('tanh')(outImage)\n\n    return outImage\n\n\ndef defineGenerator(inputShape):\n    realImage = Input(shape=inputShape)\n\n    encodedA, layersA = encodeImage(realImage)\n\n    combined = Conv2D(8, 3, kernel_initializer=init, input_shape=inputShape, padding='same')(encodedA)\n    combined = BatchNormalization(momentum=0.8)(combined)\n    combined = LeakyReLU(alpha=0.01)(combined)\n    combined = Conv2D(8, 3, kernel_initializer=init, input_shape=inputShape, padding='same')(combined)\n    combined = BatchNormalization(momentum=0.8)(combined)\n    combined = LeakyReLU(alpha=0.01)(combined)\n\n    outImage = decodeImage(combined, layersA)\n\n    model = Model(realImage, outImage)\n\n    return model\n\n\n\ndef defineDiscriminator(imageShape):\n\n    imageA = Input(shape=imageShape)\n    imageB = Input(shape=imageShape)\n    imageC = Input(shape=imageShape)\n\n    merged = Concatenate()([imageA, imageB, imageC])\n\n    layer1 = Conv2D(32, 5, 2, padding='same', kernel_initializer=init)(merged)\n    #layer1 = BatchNormalization(momentum=0.8)(layer1)\n    layer1 = LayerNormalization()(layer1)\n    layer1 = LeakyReLU(alpha=0.01)(layer1)\n    layer1 = Dropout(0.2)(layer1, training=True)\n    layer1 = Conv2D(32, 5, padding='same', kernel_initializer=init)(layer1)\n    #layer1 = BatchNormalization(momentum=0.8)(layer1)\n    layer1 = LayerNormalization()(layer1)\n    layer1 = LeakyReLU(alpha=0.01)(layer1)\n    layer1 = GaussianNoise(GNOISE)(layer1)\n\n    layer2 = Conv2D(64, 5, 2, padding='same', kernel_initializer=init)(layer1)\n    layer2 = LeakyReLU(alpha=0.01)(layer2)\n    #layer2 = BatchNormalization(momentum=0.8)(layer2)\n    layer2 = LayerNormalization()(layer2)\n    layer2 = Dropout(0.2)(layer2, training=True)\n    layer2 = Conv2D(64, 5, padding='same', kernel_initializer=init)(layer2)\n    layer2 = LeakyReLU(alpha=0.01)(layer2)\n    #layer2 = BatchNormalization(momentum=0.8)(layer2)\n    layer2 = LayerNormalization()(layer2)\n    layer2 = GaussianNoise(GNOISE)(layer2)\n\n    layer3 = Conv2D(64, 5, 2, padding='same', kernel_initializer=init)(layer2)\n    layer3 = LeakyReLU(alpha=0.01)(layer3)\n    #layer3 = BatchNormalization(momentum=0.8)(layer3)\n    layer3 = LayerNormalization()(layer3)\n    layer3 = Dropout(0.3)(layer3, training=True)\n    layer3 = Conv2D(64, 5, padding='same', kernel_initializer=init)(layer3)\n    layer3 = LeakyReLU(alpha=0.01)(layer3)\n    #layer3 = BatchNormalization(momentum=0.8)(layer3)\n    layer3 = LayerNormalization()(layer3)\n    layer3 = GaussianNoise(GNOISE)(layer3)\n\n    layer4 = Conv2D(128, 5, 2, padding='same', kernel_initializer=init)(layer3)\n    layer4 = LeakyReLU(alpha=0.01)(layer4)\n    #layer4 = BatchNormalization(momentum=0.8)(layer4)\n    layer4 = LayerNormalization()(layer4)\n    layer4 = Dropout(0.3)(layer4, training=True)\n    layer4 = Conv2D(128, 5, 2, padding='same', kernel_initializer=init)(layer4)\n    layer4 = LeakyReLU(alpha=0.01)(layer4)\n    #layer4 = BatchNormalization(momentum=0.8)(layer4)\n    layer4 = LayerNormalization()(layer4)\n    layer4 = GaussianNoise(GNOISE)(layer4)\n\n    output = Flatten()(layer4)\n\n    output = Dense(1, activation=None)(output)\n\n    model = Model([imageA, imageB, imageC], output)\n    \n    return model\n\nclass WGAN(Model):\n    def __init__(\n        self,\n        discriminator,\n        generator,\n        discriminator_extra_steps=3,\n        gp_weight=10.0,\n    ):\n        super(WGAN, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.d_steps = discriminator_extra_steps\n        self.gp_weight = gp_weight\n\n    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n        super(WGAN, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.d_loss_fn = d_loss_fn\n        self.g_loss_fn = g_loss_fn\n\n    def gradient_penalty(self, batch_size, imgA, imgB, imgC, fakeImg):\n        &quot;&quot;&quot; Calculates the gradient penalty.\n\n        This loss is calculated on an interpolated image\n        and added to the discriminator loss.\n        &quot;&quot;&quot;\n        # get the interplated image\n        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n        diff = fakeImg - imgB\n        interpolated = imgB + alpha * diff\n\n        with tf.GradientTape() as gp_tape:\n            gp_tape.watch(interpolated)\n            # 1. Get the discriminator output for this interpolated image.\n            pred = self.discriminator([imgA, interpolated, imgC], training=True)\n\n        # 2. Calculate the gradients w.r.t to this interpolated image.\n        grads = gp_tape.gradient(pred, [interpolated])[0]\n        # 3. Calcuate the norm of the gradients\n        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n        gp = tf.reduce_mean((norm - 1.0) ** 2)\n        return gp\n\n    @tf.function\n    def train_step(self, real_images):\n        #if isinstance(real_images, tuple):\n        #    real_images = real_images[0]\n\n        # Get the batch size\n        batch_size = tf.shape(real_images)[0]\n\n        firstImgs = real_images[0:batch_size-2:3]\n        secondImgs = real_images[1:batch_size-1:3]\n        thirdImgs = real_images[2:batch_size:3]\n\n        # For each batch, we are going to perform the\n        # following steps as laid out in the original paper.\n        # 1. Train the generator and get the generator loss\n        # 2. Train the discriminator and get the discriminator loss\n        # 3. Calculate the gradient penalty\n        # 4. Multiply this gradient penalty with a constant weight factor\n        # 5. Add gradient penalty to the discriminator loss\n        # 6. Return generator and discriminator losses as a loss dictionary.\n\n        # Train discriminator first. The original paper recommends training\n        # the discriminator for `x` more steps (typically 5) as compared to\n        # one step of the generator. Here we will train it for 3 extra steps\n        # as compared to 5 to reduce the training time.\n        for i in range(self.d_steps):\n            with tf.GradientTape() as tape:\n                # Generate fake images from the latent vector\n                fake_images = self.generator(secondImgs, training=True)\n                # Get the logits for the fake images\n                fake_logits = self.discriminator([firstImgs, fake_images, thirdImgs], training=True)\n                # Get the logits for real images\n                real_logits = self.discriminator([firstImgs, real_images, thirdImgs], training=True)\n\n                # Calculate discriminator loss using fake and real logits\n                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n                # Calculate the gradient penalty\n                gp = self.gradient_penalty(batch_size, firstImgs, secondImgs, thirdImgs, fake_images)\n                # Add the gradient penalty to the original discriminator loss\n                d_loss = d_cost + gp * self.gp_weight\n\n            # Get the gradients w.r.t the discriminator loss\n            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n            # Update the weights of the discriminator using the discriminator optimizer\n            self.d_optimizer.apply_gradients(\n                zip(d_gradient, self.discriminator.trainable_variables)\n            )\n\n        # Train the generator now.\n        with tf.GradientTape() as tape:\n            # Generate fake images using the generator\n            generated_images = self.generator(secondImgs, training=True)\n            # Get the discriminator logits for fake images\n            gen_img_logits = self.discriminator([firstImgs, generated_images, thirdImgs], training=True)\n            # Calculate the generator loss\n            g_loss = self.g_loss_fn(gen_img_logits)\n\n        # Get the gradients w.r.t the generator loss\n        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n        # Update the weights of the generator using the generator optimizer\n        self.g_optimizer.apply_gradients(\n            zip(gen_gradient, self.generator.trainable_variables)\n        )\n        return {&quot;d_loss&quot;: d_loss, &quot;g_loss&quot;: g_loss}\n</code></pre>\n<p>Training:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def imgGenerator(dataset, batchSize, count):\n    n = 0\n    while n &lt; count:\n        randomFolder = np.random.randint(0, len(dataset))\n        randomIndex = np.random.randint(0, len(dataset[randomFolder]) - batchSize)\n        n += 1\n        yield dataset[randomFolder][randomIndex:randomIndex+batchSize]\n\ndataset = loadDataset()\ngen = imgGenerator(dataset, 48, 1000)\nprint(&quot;Done&quot;)\n\n# learning_rate=0.0002, beta_1=0.5 are recommened\ngenerator_optimizer = keras.optimizers.Adam(\n    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n)\ndiscriminator_optimizer = keras.optimizers.Adam(\n    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n)\n\n# Define the loss functions to be used for discrimiator\n# This should be (fake_loss - real_loss)\n# We will add the gradient penalty later to this loss function\ndef discriminator_loss(real_img, fake_img):\n    real_loss = tf.reduce_mean(real_img)\n    fake_loss = tf.reduce_mean(fake_img)\n    return fake_loss - real_loss\n\n\n# Define the loss functions to be used for generator\ndef generator_loss(fake_img):\n    return -tf.reduce_mean(fake_img)\n\n# Epochs to train\nepochs = 15\n\nimageShape = (256, 256, 1)\ng_model = defineGenerator(imageShape)\nd_model = defineDiscriminator(imageShape)\n\n# Get the wgan model\nwgan = WGAN(\n    discriminator=d_model,\n    generator=g_model,\n    discriminator_extra_steps=3,\n)\n\n# Compile the wgan model\nwgan.compile(\n    d_optimizer=discriminator_optimizer,\n    g_optimizer=generator_optimizer,\n    g_loss_fn=generator_loss,\n    d_loss_fn=discriminator_loss,\n)\n\n# Start training\nwgan.fit(gen, epochs=epochs)\ng_model.save_weights(&quot;wgenerator_256x256_keras.h5&quot;)\nd_model.save_weights(&quot;wdiscriminator_256x256_keras_2.h5&quot;)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 284}]
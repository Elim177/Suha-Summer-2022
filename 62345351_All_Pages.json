[{"items": [{"tags": ["python", "tensorflow", "neural-network", "conv-neural-network", "tf.keras"], "owner": {"account_id": 6956681, "reputation": 1056, "user_id": 5337505, "user_type": "registered", "accept_rate": 67, "profile_image": "https://lh5.googleusercontent.com/-F4WMV67MG-o/AAAAAAAAAAI/AAAAAAAABGA/b-vMtZ0piXI/photo.jpg?sz=256", "display_name": "Siladittya", "link": "https://stackoverflow.com/users/5337505/siladittya"}, "is_answered": true, "view_count": 561, "accepted_answer_id": 62346455, "answer_count": 2, "score": 0, "last_activity_date": 1592225985, "creation_date": 1591968802, "last_edit_date": 1591971612, "question_id": 62345351, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62345351/gradients-returned-by-tape-gradient-is-none-in-custom-training-loop", "title": "Gradients returned by tape.gradient is None in Custom Training Loop", "body": "<p>I am trying to implement a weighted Binary Cross entropy loss function\nAlso, I am using a custom training loop for my training</p>\n\n<pre><code>def grads_ds(model_ds, ds_inputs,y_true,cw):\n    with tf.GradientTape() as ds_tape:\n        #ds_tape.watch(tf.convert_to_tensor(y_true.astype('float')))\n        #ds_tape.watch(tf.convert_to_tensor(ds_inputs))\n\n        y_pred = model_ds(ds_inputs)\n        #print(y_true,y_pred)\n\n        log_logits = np.append(np.log(y_pred),np.log(1-y_pred),axis=0).T\n        org_labs = np.append(y_true,1-y_true,axis=0).T\n        loss = K.sum(-1*org_labs*cw*log_logits,axis=1)\n        loss_value_ds = K.sum(loss)\n\n    ds_grads = ds_tape.gradient(loss_value_ds,model_ds.trainable_variables)\n\n    return loss_value_ds, ds_grads\n</code></pre>\n\n<p><code>y_true</code> and <code>y_pred</code> both is of shape (1,3) and <code>cw</code> is of shape (3,2)</p>\n\n<p><code>cw</code> is </p>\n\n<pre><code>[[0.5145 3.6036]\n [1.7163 0.7127]\n [2.4231 0.6708]]\n</code></pre>\n\n<p><code>ds_tape.gradient</code> is returning <code>None</code> gradient.\nI even tried adding <code>ds_tape.watch</code> for the input and the true labels <code>y_true</code>. But still receiving <code>None</code>.</p>\n\n<p>In my network, I used <code>tf.math.reduce_max</code> after a certain layer. Can that be the source of the problem?</p>\n\n<p>Or is it becasue of I used numpy functions on tensors <code>y_pred</code>?</p>\n\n<p><code>tf.GradientTape().gradient()</code> returns <code>None</code> when the target and source are <code>UNCONNECTED</code>. I cannot figure out how it is unconnected.</p>\n\n<p>NOTE : When I used <code>tf.keras.losses.binary_crossentropy(y_true,y_pred)</code> no error as such mentioned above occured. Only when I used a custom loss calculation code instead of the keras function, this error is occurring</p>\n\n<p>Any solution?\nNone of the methods I saw on the web works.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 31}]
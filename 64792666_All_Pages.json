[{"items": [{"tags": ["tensorflow", "machine-learning", "keras", "neural-network"], "owner": {"account_id": 5556872, "reputation": 952, "user_id": 4557607, "user_type": "registered", "accept_rate": 65, "profile_image": "https://graph.facebook.com/679173583/picture?type=large", "display_name": "Edv Beq", "link": "https://stackoverflow.com/users/4557607/edv-beq"}, "is_answered": true, "view_count": 119, "accepted_answer_id": 64837922, "answer_count": 1, "score": 2, "last_activity_date": 1605382902, "creation_date": 1605121652, "question_id": 64792666, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64792666/regression-custom-loss-return-value-in-keras-with-and-without-custom-loop", "title": "Regression custom loss return value in Keras with and without custom loop", "body": "<p>When a custom loss is defined in a Keras model, online sources seem to indicate that the the loss should return an array of values (a loss for each sample in the batch). Something like this</p>\n<pre><code>def custom_loss_function(y_true, y_pred):\n   squared_difference = tf.square(y_true - y_pred)\n   return tf.reduce_mean(squared_difference, axis=-1)\n\nmodel.compile(optimizer='adam', loss=custom_loss_function)\n</code></pre>\n<p>In the example above, I have no idea when or if the model is taking the batch sum or mean with  <code>tf.reduce_sum()</code> or <code>tf.reduce_mean()</code></p>\n<p>In another situation when we want to implement a custom training loop with a custom function, the template to follow according to Keras documentation is this</p>\n<pre><code>for epoch in range(epochs):\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n\n        with tf.GradientTape() as tape:\n            y_batch_pred = model(x_batch_train, training=True)  \n            loss_value = custom_loss_function(y_batch_train, y_batch_pred)\n\n        grads = tape.gradient(loss_value, model.trainable_weights)\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n</code></pre>\n<p>So by the book, if I understand correctly, we are supposed to take the mean of the batch gradients. Therefore, the loss value above should be a single value per batch.</p>\n<p>However, the example will work with both of the following variations:</p>\n<ul>\n<li><code>tf.reduce_mean(squared_difference, axis=-1) # array of loss for each sample</code></li>\n<li><code>tf.reduce_mean(squared_difference)          # mean loss for batch</code></li>\n</ul>\n<p>So, why does the first option (array loss) above still work? Is <code>apply_gradients</code> applying small changes for each value sequentially? Is this wrong although it works?</p>\n<p>What is the correct way <strong>without a custom loop</strong>, and <strong>with a custom loop</strong>?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 39}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "tensorflow2.0", "tf.keras"], "owner": {"account_id": 15116032, "reputation": 30088, "user_id": 10908375, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/L7f8w.jpg?s=256&g=1", "display_name": "Nicolas Gervais", "link": "https://stackoverflow.com/users/10908375/nicolas-gervais"}, "is_answered": true, "view_count": 2653, "accepted_answer_id": 61378290, "answer_count": 1, "score": 2, "last_activity_date": 1587608847, "creation_date": 1587605778, "last_edit_date": 1587608847, "question_id": 61377896, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61377896/tensorflow-2-1-getting-tensorstatefulpartitionedcall0-shape-dtype-flo", "title": "Tensorflow 2.1: getting &quot;Tensor(&quot;StatefulPartitionedCall:0&quot;, shape=(), dtype=float64)&quot; as loss", "body": "<p>Instead of getting a <code>Dataset</code> tensor, I am getting this, which I don't know how to handle:</p>\n\n<blockquote>\n  <p>Tensor(\"StatefulPartitionedCall:0\", shape=(), dtype=float64)</p>\n</blockquote>\n\n<p>Everything runs smoothly (I think), but this is what I get when I try to print out the loss. This is the code I'm playing with:</p>\n\n<pre><code>import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom sklearn.datasets import load_breast_cancer\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense\ntf.keras.backend.set_floatx('float64')\n\nx, y = load_breast_cancer(return_X_y=True)\n\ndata = tf.data.Dataset.from_tensors((x, y)).shuffle(len(x))\n\ntrain_data = data.take(int(8e-1*len(x))).batch(32)\ntest_data = data.skip(int(8e-1*len(x)))\n\n\nclass DenseNet(Model):\n    def __init__(self):\n        super(DenseNet, self).__init__()\n        self.D1 = Dense(8, activation=tf.keras.activations.selu)\n        self.D2 = Dense(16, activation=tf.keras.activations.elu)\n        self.D3 = Dense(32, activation=tf.keras.activations.relu)\n        self.D4 = Dense(1)\n\n    def __call__(self, x):\n        x = self.D1(x)\n        x = self.D2(x)\n        x = self.D3(x)\n        out = self.D4(x)\n        return out\n\n\nnetwork = DenseNet()\n\noptimizer = tf.keras.optimizers.Adam()\n\n\n@tf.function\ndef compute_loss(labels, logits):\n    labels = tf.cast(tf.one_hot(labels, depth=1), tf.float64)\n    return tf.reduce_mean(\n        tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits))\n\n\n@tf.function\ndef compute_accuracy(labels, logits):\n    labels = tf.cast(tf.one_hot(labels, depth=2), tf.float64)\n    return tf.reduce_mean(tf.cast(tf.equal(logits, labels), tf.float32))\n\n\n@tf.function\ndef train_step(inputs, targets):\n    with tf.GradientTape() as tape:\n        logits = network(inputs)\n        loss = compute_loss(labels=targets, logits=logits)\n\n    gradients = tape.gradient(loss, network.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, network.trainable_variables))\n    accuracy = compute_accuracy(labels=targets, logits=logits)\n    return loss, accuracy\n\n\n@tf.function\ndef train():\n    for inputs, labels in train_data:\n        loss, acc = train_step(inputs, labels)\n        print(loss, acc)\n\ndef main(epochs=5):\n    for i in range(1, epochs + 1):\n        train()\n\n\nif __name__ == '__main__':\n    main(epochs=10)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 285}]
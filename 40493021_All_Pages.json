[{"items": [{"tags": ["python", "tensorflow"], "owner": {"user_type": "does_not_exist", "display_name": "user6557479"}, "is_answered": true, "view_count": 1052, "accepted_answer_id": 40493313, "answer_count": 1, "score": 1, "last_activity_date": 1478627120, "creation_date": 1478626184, "question_id": 40493021, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/40493021/tensor-flow-tutorial-logloss-implementation", "title": "Tensor Flow tutorial logloss implementation", "body": "<p>I need to study TF in the express way and i cant understant this part:</p>\n\n<pre><code>cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n</code></pre>\n\n<p>It's explained with this: First, tf.log computes the logarithm of each element of y. Next, we multiply each element of y_ with the corresponding element of tf.log(y). <strong>Then tf.reduce_sum adds the elements in the second dimension of y, due to the reduction_indices=[1] parameter. Finally, tf.reduce_mean computes the mean over all the examples in the batch.</strong></p>\n\n<p>Why it does this manipulations, which are marked bold? why do wee need  another dimensiom? Thanks</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 298}]
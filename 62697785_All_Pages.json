[{"items": [{"tags": ["python", "tensorflow", "keras", "neural-network", "multilabel-classification"], "owner": {"account_id": 6956681, "reputation": 1056, "user_id": 5337505, "user_type": "registered", "accept_rate": 67, "profile_image": "https://lh5.googleusercontent.com/-F4WMV67MG-o/AAAAAAAAAAI/AAAAAAAABGA/b-vMtZ0piXI/photo.jpg?sz=256", "display_name": "Siladittya", "link": "https://stackoverflow.com/users/5337505/siladittya"}, "is_answered": false, "view_count": 387, "answer_count": 1, "score": 0, "last_activity_date": 1593750804, "creation_date": 1593697407, "last_edit_date": 1593745641, "question_id": 62697785, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62697785/multilabel-classification-with-imbalanced-dataset", "title": "Multilabel classification with imbalanced dataset", "body": "<p>I am trying to do a multilabel classfication problem, which has an imabalnced dataset.\nThe total number of samples is 1130, out of the 1130 samples, the first class occur in 913 of them. The second class 215 times and the third one 423 times.</p>\n<p>In the model architecture, I have 3 output nodes, and have applied sigmoid activation.</p>\n<pre><code>input_tensor = Input(shape=(256, 256, 3))\nbase_model = VGG16(input_tensor=input_tensor,weights='imagenet',pooling=None, include_top=False)\n\n#base_model.summary()\n\nx = base_model.output\n\nx = GlobalAveragePooling2D()(x)\n\nx = tf.math.reduce_max(x,axis=0,keepdims=True)\n\nx = Dense(512,activation='relu')(x)\n\noutput_1 = Dense(3, activation='sigmoid')(x)\n\nsagittal_model_abn = Model(inputs=base_model.input, outputs=output_1)\n\nfor layer in base_model.layers:\n    layer.trainable = True\n</code></pre>\n<p>I am using binary cross entropy loss, which I calculate usng this function.\nI am using weighted loss to deal with the imbalance.</p>\n<pre><code>        if y_true[0]==1:\n            loss_abn = -1*K.log(y_pred[0][0])*cwb[0][1]\n        elif y_true[0]==0:\n            loss_abn = -1*K.log(1-y_pred[0][0])*cwb[0][0]\n        if y_true[1]==1:\n            loss_acl = -1*K.log(y_pred[0][1])*cwb[1][1]\n        elif y_true[1]==0:\n            loss_acl = -1*K.log(1-y_pred[0][1])*cwb[1][0]\n        if y_true[2]==1:\n            loss_men = -1*K.log(y_pred[0][2])*cwb[2][1]\n        elif y_true[2]==0:\n            loss_men = -1*K.log(1-y_pred[0][2])*cwb[2][0]\n\n        loss_value_ds = loss_abn + loss_acl + loss_men\n</code></pre>\n<p><code>cwb</code> contains the class weights.</p>\n<p><code>y_true</code> is the ground truth labels having length 3.</p>\n<p><code>y_pred</code> is a numpy array with shape (1,3)</p>\n<p>I weight the classes individually as occurrence and non-occurrence of the classes.</p>\n<p>Like, if the label is 1, I count it as an occurrence and if it is 0, then it is a non-occurrence.</p>\n<p>So, the first class's label 1 occurs 913 times out of 1130</p>\n<p>So the class weight of label 1 for the first class is 1130/913 which is about 1.23 and the weight of the label 0 for the first class is 1130/(1130-913)</p>\n<p>When I train the model, the accuracy oscillates (or stays almost same), and the loss decreases.</p>\n<p>And I am getting predictions like this for every sample</p>\n<p><code>[[0.51018655 0.5010625  0.50482965]]</code></p>\n<p>The prediction values are in the range 0.49 - 0.51 in every iteration for all the classes</p>\n<p>Tried changing the number of nodes in the FC layer but it still behaves the same way.</p>\n<p>Can anyone help?</p>\n<p>Does using <code>tf.math,reduce_max</code> cause the problem? Should using <code>@tf.function</code> to do the operation that I am doing using <code>tf.math.reduce_max</code> be beneficial?</p>\n<p><strong>NOTE</strong>:</p>\n<p>I am weighting the labels 1 and 0 for each class separately.</p>\n<pre><code>cwb = {0: {0: 5.207373271889401, 1: 1.2376779846659365}, \n       1: {0: 1.2255965292841648, 1: 5.4326923076923075}, \n       2: {0: 1.5416098226466575, 1: 2.8463476070528966}}\n</code></pre>\n<p><strong>EDIT</strong>:</p>\n<p>The results when I train using <code>model.fit()</code>.</p>\n<pre><code>Epoch 1/20\n1130/1130 [==============================] - 1383s 1s/step - loss: 4.1638 - binary_accuracy: 0.4558 - val_loss: 5.0439 - val_binary_accuracy: 0.3944\nEpoch 2/20\n1130/1130 [==============================] - 1397s 1s/step - loss: 4.1608 - binary_accuracy: 0.4165 - val_loss: 5.0526 - val_binary_accuracy: 0.5194\nEpoch 3/20\n1130/1130 [==============================] - 1402s 1s/step - loss: 4.1608 - binary_accuracy: 0.4814 - val_loss: 5.1469 - val_binary_accuracy: 0.6361\nEpoch 4/20\n1130/1130 [==============================] - 1407s 1s/step - loss: 4.1722 - binary_accuracy: 0.4472 - val_loss: 5.0501 - val_binary_accuracy: 0.5583\nEpoch 5/20\n1130/1130 [==============================] - 1397s 1s/step - loss: 4.1591 - binary_accuracy: 0.4991 - val_loss: 5.0521 - val_binary_accuracy: 0.6028\nEpoch 6/20\n1130/1130 [==============================] - 1375s 1s/step - loss: 4.1596 - binary_accuracy: 0.5431 - val_loss: 5.0515 - val_binary_accuracy: 0.5917\nEpoch 7/20\n1130/1130 [==============================] - 1370s 1s/step - loss: 4.1595 - binary_accuracy: 0.4962 - val_loss: 5.0526 - val_binary_accuracy: 0.6000\nEpoch 8/20\n1130/1130 [==============================] - 1387s 1s/step - loss: 4.1591 - binary_accuracy: 0.5316 - val_loss: 5.0523 - val_binary_accuracy: 0.6028\nEpoch 9/20\n1130/1130 [==============================] - 1391s 1s/step - loss: 4.1590 - binary_accuracy: 0.4909 - val_loss: 5.0521 - val_binary_accuracy: 0.6028\nEpoch 10/20\n1130/1130 [==============================] - 1400s 1s/step - loss: 4.1590 - binary_accuracy: 0.5369 - val_loss: 5.0519 - val_binary_accuracy: 0.6028\nEpoch 11/20\n1130/1130 [==============================] - 1397s 1s/step - loss: 4.1590 - binary_accuracy: 0.4808 - val_loss: 5.0519 - val_binary_accuracy: 0.6028\nEpoch 12/20\n1130/1130 [==============================] - 1394s 1s/step - loss: 4.1590 - binary_accuracy: 0.5469 - val_loss: 5.0522 - val_binary_accuracy: 0.6028\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 26}]
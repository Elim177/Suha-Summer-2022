[{"items": [{"tags": ["python", "tensorflow", "tf.keras"], "owner": {"account_id": 16936736, "reputation": 615, "user_id": 12323228, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6498d422e6c38d0808483be485b0fcc4?s=256&d=identicon&r=PG&f=1", "display_name": "cmed123", "link": "https://stackoverflow.com/users/12323228/cmed123"}, "is_answered": false, "view_count": 641, "answer_count": 0, "score": 4, "last_activity_date": 1588279226, "creation_date": 1588279226, "question_id": 61532345, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61532345/tf-keras-mixed-precision-loss-scale-still-causing-underflow", "title": "tf.keras mixed precision loss scale still causing underflow", "body": "<p>When I use mixed precision for my tf.keras model, my model's loss isn't going down at all. I noticed that my gradients often either end up at \"nan\" values or \"-inf\" or \"inf\" after using mixed precision.</p>\n\n<p>To use mixed precision, I used these snippets of code:</p>\n\n<pre><code>from tensorflow.keras.mixed_precision import experimental as mixed_precision\n\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_policy(policy)\n\nself.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\nself.optimizer = mixed_precision.LossScaleOptimizer(self.optimizer, loss_scale='dynamic')\n\n\nfor batch_step, batch in enumerate(self.batch_iterator_train):\n    with tf.GradientTape() as tape:\n            logits, _ = self.model(batch['images'], is_training=True)\n            loss_value = self.loss_fn(labels=batch['labels'], logits=logits)\n            scaled_loss = self.optimizer.get_scaled_loss(loss_value)\n\n            scaled_grads = tape.gradient(scaled_loss, self.model.trainable_variables)\n            gradients = self.optimizer.get_unscaled_gradients(scaled_grads)\n            self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n</code></pre>\n\n<p>My model is pretty big, and it's using CNN layers if that makes a difference. I thought adding the lossScaleOptimizer would resolve this issue of underflow for the gradients, as described <a href=\"https://www.tensorflow.org/guide/keras/mixed_precision\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/guide/keras/mixed_precision</a>, but it doesn't seem to be working.</p>\n\n<p>While debugging, I noticed that the loss_scale starts from the default max value and then goes all the way down to 1.0; I'm guessing because it detected the nan gradients. But then even at a loss scale of 1.0, I'm still not getting any reduction in loss.</p>\n\n<p>Any ideas or thoughts here would be greatly appreciated. thanks!</p>\n\n<p>My model works fine if I don't use mixed precision by the way.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 84}]
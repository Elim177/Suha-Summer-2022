[{"items": [{"tags": ["tensorflow", "gradient", "gradienttape"], "owner": {"account_id": 18587329, "reputation": 66, "user_id": 13545353, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/15321cc5234df20fa3a6e9d86e74df1e?s=256&d=identicon&r=PG&f=1", "display_name": "keithrausch", "link": "https://stackoverflow.com/users/13545353/keithrausch"}, "is_answered": false, "view_count": 962, "answer_count": 0, "score": 3, "last_activity_date": 1589568125, "creation_date": 1589505457, "last_edit_date": 1589568125, "question_id": 61810094, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61810094/abysmal-tf-gradienttape-performance-compared-to-tf-gradients-for-computing-jac", "title": "Abysmal tf.GradientTape performance compared to tf.gradients() for computing jacobians", "body": "<p><strong>SOLUTION BELOW:</strong></p>\n\n<p><strong>Scenario:</strong></p>\n\n<p>I am trying to compute the jacobian of a user defined function many, many times in a loop. I am able to do this with TF 2's GradientTape as well as the older session based tf.gradients() method. The problem is that GradientTape is terribly slow (100x slower) than tf.gradients(). It has features i'd like to use (bath_jacobian, hessian support, etc), but if it's 100x slower then i can't use it.</p>\n\n<p><strong>The Question:</strong></p>\n\n<p>It's not clear to me if i'm simply misusing GradientTape, or if it will always be slower because it has to re-differentiate the provided function every time its called (my suspicion). I'm asking for tips to fix my use of GradientTape or a confirmation that it will always be fundamentally slower than tf.gradients by orders of magnitude.</p>\n\n<p><strong>Related Questions:</strong></p>\n\n<ul>\n<li><a href=\"https://stackoverflow.com/questions/60047705/repeated-use-of-gradienttape-for-multiple-jacobian-calculations\">Repeated use of GradientTape for multiple Jacobian calculations</a> - same scenario, unanswered</li>\n<li><a href=\"https://stackoverflow.com/questions/58854129/does-gradienttape-need-to-re-differentiate-each-evaluation-of-a-derivative\">Does `GradientTape` need to re-differentiate each evaluation of a derivative?</a> - same scenario, unanswered</li>\n<li><a href=\"https://stackoverflow.com/questions/58612362/using-one-gradienttape-with-global-context\">using one GradientTape with global context</a> - loosely related, having trouble applyng that solution to my scenario</li>\n</ul>\n\n<p><strong>Fully contained minimum example to compare GradientTape and tf.gradients():</strong></p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.framework.ops import disable_eager_execution\nimport numpy as np\n# from tensorflow.python.ops.parallel_for.gradients import jacobian, batch_jacobian\nimport timeit\n\n\nclass FunctionCaller(object):\n    def __init__(self, func, nX, dtype=tf.float64, useSessions=True):\n\n        if useSessions:\n            disable_eager_execution()\n\n        self.func = func\n        self.nX = nX\n        self.useSessions = useSessions\n        self.dtype = dtype\n        self.sess = tf.compat.v1.Session() if useSessions else None\n\n        if not useSessions:\n            return\n\n        #\n        # we are in session mode, so build the graph and take the batch-jacobian of the function's outputs\n        #\n        xTensor = tf.compat.v1.placeholder(dtype, shape=[None, nX])\n\n        # add function to graph and guarantee its output shape\n        func_tensor = tf.reshape(func(xTensor), [-1, nX])\n\n        # take the gradient for each output, one at a time, and stack the results back together\n        each_output = tf.unstack(func_tensor, nX, axis=1)\n\n        jac_x = tf.stack([tf.gradients(output, xTensor, unconnected_gradients='zero')[0]\n                          for output in each_output], axis=1)\n\n        # record these tensors so we can use them later with session.run()\n        self.xTensor = xTensor\n        self.func_tensor = func_tensor\n        self.jac_func_tensor = jac_x\n\n    def jac(self, x_i):\n        if self.useSessions:\n            return self.sess.run(self.jac_func_tensor, {self.xTensor: x_i})\n        else:\n            return self._useGradientTape(x_i)\n\n    # THIS FUNCTION IS SUPER INEFFICIENT.\n    def _useGradientTape(self, x_i):\n        with tf.GradientTape(persistent=True) as g:\n            xTensor = tf.Variable(x_i, dtype=self.dtype)  # is this my problem??? i recreate x every time?\n            y = tf.reshape(self.func(xTensor), [-1, self.nX])\n        jac_x_at_i = g.batch_jacobian(y, xTensor)\n        # del g\n        return jac_x_at_i.numpy()\n\n    def __del__(self):\n        if self.sess is not None:\n            self.sess.close()\n\n\ndef main():\n    @tf.function\n    def Xdot(x_i):\n        x_0, x_1, x_2 = tf.split(x_i, 3, axis=1)\n        return tf.concat([x_2 * tf.sin(x_2), x_2 * tf.cos(x_2), x_2], axis=1)\n\n    nT = 20\n    nX = 3\n\n    # create some trash data\n    x_i = np.arange(nT*nX).reshape([-1, nX])\n\n    nTrials = 100\n\n    # try the eager version first\n    caller_eager = FunctionCaller(Xdot, nX, useSessions=False)\n    start_time = timeit.default_timer()\n    for _ in range(nTrials):\n        jac_eager = caller_eager.jac(x_i)\n    elapsed = timeit.default_timer() - start_time\n    print(\"eager code took {} sec: {} sec/trial\".format(elapsed, elapsed/nTrials))\n\n    # now try the sessions version\n    caller_sessions = FunctionCaller(Xdot, nX, useSessions=True)\n    start_time = timeit.default_timer()\n    caller_sessions.jac(x_i)  # call it once to do its graph building stuff?\n    for _ in range(nTrials):\n        jac_session = caller_sessions.jac(x_i)\n    elapsed = timeit.default_timer() - start_time\n    print(\"session code took {} sec: {} sec/trial\".format(elapsed, elapsed/nTrials))\n\n    residual = np.max(np.abs(jac_eager - jac_session))\n    print('residual between eager and session trials is {}'.format(residual))\n\nif __name__ == \"__main__\":\n    main()\n\n</code></pre>\n\n<h1><strong>EDIT - SOLUTION:</strong></h1>\n\n<p>xdurch0 pointed out below that I should wrap _useGradientTape() in a @tf.function - something I was unsuccessful with before for other reasons. Once I did that, I had to move xTensor's definition outside the @tf.function wrapper by making it a member variable and using tf.assign(). </p>\n\n<p>With all this done, I find that GradientTape (for this simple example) is now on the same order of magnitude as tf.gradints. When running enough trials (~1E5), it's twice as fast as tf.gradients. <em>awesome!</em></p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.framework.ops import disable_eager_execution\nimport numpy as np\nimport timeit\n\n\nclass FunctionCaller(object):\n    def __init__(self, func, nT, nX, dtype=tf.float64, useSessions=True):\n\n        if useSessions:\n            disable_eager_execution()\n\n        self.func = func\n        self.nX = nX\n        self.useSessions = useSessions\n        self.dtype = dtype\n        self.sess = tf.compat.v1.Session() if useSessions else None\n\n        if not useSessions:\n            #  you should be able to create without an initial value, but tf is demanding one\n            #  despite what the docs say. bug?\n            #  tf.Variable(initial_value=None, shape=[None, nX], validate_shape=False, dtype=self.dtype)\n            self.xTensor = tf.Variable([[0]*nX]*nT, dtype=self.dtype)  # x needs to be properly sized once\n            return\n\n        #\n        # we are in session mode, so build the graph and take the batch-jacobian of the function's outputs\n        #\n        xTensor = tf.compat.v1.placeholder(dtype, shape=[None, nX])\n\n        # add function to graph and guarantee its output shape\n        func_tensor = tf.reshape(func(xTensor), [-1, nX])\n\n        # take the gradient for each output, one at a time, and stack the results back together\n        each_output = tf.unstack(func_tensor, nX, axis=1)\n\n        jac_x = tf.stack([tf.gradients(output, xTensor, unconnected_gradients='zero')[0]\n                          for output in each_output], axis=1)\n\n        # record these tensors so we can use them later with session.run()\n        self.xTensor = xTensor\n        self.func_tensor = func_tensor\n        self.jac_func_tensor = jac_x\n\n    def jac(self, x_i):\n        if self.useSessions:\n            return self.sess.run(self.jac_func_tensor, {self.xTensor: x_i})\n        else:\n            return self._useGradientTape(x_i).numpy()\n\n    @tf.function  # THIS IS CRUCIAL\n    def _useGradientTape(self, x_i):\n        with tf.GradientTape(persistent=True) as g:\n            self.xTensor.assign(x_i)  # you need to create the variable once outside the graph\n            y = tf.reshape(self.func(self.xTensor), [-1, self.nX])\n        jac_x_at_i = g.batch_jacobian(y, self.xTensor)\n        # del g\n        return jac_x_at_i\n\n    def __del__(self):\n        if self.sess is not None:\n            self.sess.close()\n\n\ndef main():\n    @tf.function\n    def Xdot(x_i):\n        x_0, x_1, x_2 = tf.split(x_i, 3, axis=1)\n        return tf.concat([x_2 * tf.sin(x_2), x_2 * tf.cos(x_2), x_2], axis=1)\n\n    nT = 20\n    nX = 3\n\n    # create some trash data\n    x_i = np.random.random([nT, nX])\n\n    nTrials = 1000  # i find that nTrials&lt;=1E3, eager is slower, it's faster for &gt;=1E4, it's TWICE as fast for &gt;=1E5\n\n    # try the eager version first\n    caller_eager = FunctionCaller(Xdot, nT, nX, useSessions=False)\n    start_time = timeit.default_timer()\n    for _ in range(nTrials):\n        jac_eager = caller_eager.jac(x_i)\n    elapsed = timeit.default_timer() - start_time\n    print(\"eager code took {} sec: {} sec/trial\".format(elapsed, elapsed/nTrials))\n\n    # now try the sessions version\n    caller_sessions = FunctionCaller(Xdot, nT, nX, useSessions=True)\n    start_time = timeit.default_timer()\n    for _ in range(nTrials):\n        jac_session = caller_sessions.jac(x_i)\n    elapsed = timeit.default_timer() - start_time\n    print(\"session code took {} sec: {} sec/trial\".format(elapsed, elapsed/nTrials))\n\n    residual = np.max(np.abs(jac_eager - jac_session))\n    print('residual between eager and session trials is {}'.format(residual))\n\nif __name__ == \"__main__\":\n    main()\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 149}]
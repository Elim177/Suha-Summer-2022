[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "generative-adversarial-network"], "owner": {"account_id": 19181146, "reputation": 135, "user_id": 14012811, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/30b93a4d52c6b79c2e8acf29c71825e9?s=256&d=identicon&r=PG&f=1", "display_name": "jakegergen", "link": "https://stackoverflow.com/users/14012811/jakegergen"}, "is_answered": true, "view_count": 112, "answer_count": 1, "score": 1, "last_activity_date": 1598402461, "creation_date": 1598395388, "last_edit_date": 1598396575, "question_id": 63588102, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63588102/tensorflow-addition-error-when-adding-results-from-binary-crossentropy-togethe", "title": "TensorFlow Addition Error when adding results from binary_crossentropy() together", "body": "<p>I was in the middle of training my gan when a very unexpected error came up. I have no idea how to fix it. The error doesn't come right away it happens about 2-3 minutes into my training. Here is the Error</p>\n<pre><code>Traceback (most recent call last):\n\n\nFile &quot;gan.py&quot;, line 103, in &lt;module&gt;\n    train(X_train_dataset,200)\n  File &quot;gan.py&quot;, line 80, in train\n    train_step(images) # takes images and improves both the generator and the discriminator\n  File &quot;gan.py&quot;, line 91, in train_step\n    discriminator_loss = get_discriminator_loss(real_output,fake_output)\n  File &quot;gan.py&quot;, line 48, in get_discriminator_loss\n    return fake_loss+real_loss\n  File &quot;/home/jake/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py&quot;, line 1125, in binary_op_wrapper\n    return func(x, y, name=name)\n  File &quot;/home/jake/.local/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py&quot;, line 201, in wrapper\n    return target(*args, **kwargs)\n  File &quot;/home/jake/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py&quot;, line 1447, in _add_dispatch\n    return gen_math_ops.add_v2(x, y, name=name)\n  File &quot;/home/jake/.local/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py&quot;, line 486, in add_v2\n    _ops.raise_from_not_ok_status(e, name)\n  File &quot;/home/jake/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py&quot;, line 6843, in raise_from_not_ok_status\n    six.raise_from(core._status_to_exception(e.code, message), None)\n  File &quot;&lt;string&gt;&quot;, line 3, in raise_from\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [100] vs. [13] [Op:AddV2]\n</code></pre>\n<p>So from I can tell from this call back my error occures during my get_discriminator_loss() so here is that code.</p>\n<pre><code>def get_discriminator_loss(real_predictions,fake_predictions):\n    real_predictions = tf.sigmoid(real_predictions)\n    fake_predictions = tf.sigmoid(fake_predictions)\n    real_loss=tf.losses.binary_crossentropy(tf.ones_like(real_predictions),real_predictions)\n    fake_loss=tf.losses.binary_crossentropy(tf.zeros_like(fake_predictions),fake_predictions)\n    return fake_loss+real_loss\n</code></pre>\n<p>Does anyone have any ideas? And remember this is after running successfully for about 2-3 minutes. The error doesn't occur in the first many passes.</p>\n<p>I've found the source of my error but I don't know why it's occuring.</p>\n<p>My real loss at one of the passes has only 13 values instead of the normal 100</p>\n<p>How can this be?</p>\n<p>Here is my full code.</p>\n<pre><code>    import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\nimport pickle\n\npickle_in_X = open(&quot;X.pickle&quot;,&quot;rb&quot;)\npickle_in_y = open(&quot;y.pickle&quot;,&quot;rb&quot;)\n\nX=pickle.load(pickle_in_X)\ny = pickle.load(pickle_in_y)\ny = np.array(y)\n\n\nX_train = X[  int(len(X)*.3):  ]\ny_train = y[  int(len(y)*.3 ):  ]\n\nX_test = X[  :int(len(X)*.3)  ]\ny_test = X[  :int(len(y)*.3)    ]\n\nX_train = (X_train-127.5)/127.5\n\nBATCH_SIZE = 100\nX_train_dataset = tf.data.Dataset.from_tensor_slices(X_train).batch(BATCH_SIZE)\n\n#creates a discriminator model.\n#discriminator will ouput 0-1 which represents the probability that the image is real\ndef make_discriminator():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv2D(7,(3,3),padding=&quot;same&quot;,input_shape=(40,40,1)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dense(50,activation=&quot;relu&quot;))\n    model.add(tf.keras.layers.Dense(1))\n    return model\n\nmodel_discriminator = make_discriminator()\ndiscriminator_optimizer = tf.optimizers.Adam(1e-3)\n\n#real_loss is the amount of error when trying to guess that the real images are in fact real. i.e loss will be if our discriminator guesses that there is a 100% chance that this real image is real\n#fake_loss is the amount of error when trying to guess that the fake images are in fact fake. i.e loss will be zero if our discriminator guesses there is a 0% chance that this fake image is fake\n#returns the total of our loss\ndef get_discriminator_loss(real_predictions,fake_predictions):\n    real_predictions = tf.sigmoid(real_predictions)\n    fake_predictions = tf.sigmoid(fake_predictions)\n    real_loss=tf.losses.binary_crossentropy(tf.ones_like(real_predictions),real_predictions)\n    fake_loss=tf.losses.binary_crossentropy(tf.zeros_like(fake_predictions),fake_predictions)\n    return fake_loss+real_loss\n\n#take an input of a random string of numbers. and output either a dog or a cat\ndef make_generator():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(10*10*256,input_shape = (100,)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Reshape((10,10,256)))\n    model.add(tf.keras.layers.Conv2DTranspose(128,(3,3),padding=&quot;same&quot;))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2DTranspose(64,(3,3),strides=(2,2),padding=&quot;same&quot;))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Conv2DTranspose(1,(3,3),strides=(2,2),padding=&quot;same&quot;))\n    return model\n\nmodel_generator = make_generator()\n\n#generator gets rewarded when it fools the discriminator\ndef get_generator_loss(fake_predictions):\n    fake_predictions = tf.sigmoid(fake_predictions)\n    fake_loss=tf.losses.binary_crossentropy(tf.ones_like(fake_predictions),fake_predictions)\n    return fake_loss\n\ngenerator_optimizer = tf.optimizers.Adam(1e-3)\n\n\n#training\n\ndef train(X_train_dataset,epochs):\n    for _ in range(epochs):\n        for images in X_train_dataset:\n            images = tf.cast(images,tf.dtypes.float32)\n            train_step(images) # takes images and improves both the generator and the discriminator\n\n\ndef train_step(images):\n    fake_image_noise = np.random.randn(BATCH_SIZE,100)#produces 100 random numbers that wll be converted to images\n    with tf.GradientTape() as generator_gradient, tf.GradientTape() as discriminator_gradient:\n        generated_images = model_generator(fake_image_noise)\n        real_output = model_discriminator(images)\n        fake_output = model_discriminator(generated_images)\n\n        generator_loss = get_generator_loss(fake_output)\n        discriminator_loss = get_discriminator_loss(real_output,fake_output)\n\n        gradients_of_generator = generator_gradient.gradient(generator_loss,model_generator.trainable_variables)#gradient of gen loss with respect to trainable variables\n        gradients_of_discriminator = discriminator_gradient.gradient(discriminator_loss,model_discriminator.trainable_variables)\n        \n        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,model_discriminator.trainable_variables))\n        generator_optimizer.apply_gradients(zip(gradients_of_generator,model_generator.trainable_variables))\n\n        print(&quot;generator loss: &quot;, np.mean(generator_loss))\n        print(&quot;discriminator loss: &quot;,np.mean(discriminator_loss))\n\n\ntrain(X_train_dataset,200)\n\nmodel_generator.save('genModel')\n\nmodel_discriminator.save('discModel')\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 188}]
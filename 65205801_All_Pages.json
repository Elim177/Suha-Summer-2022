[{"items": [{"tags": ["tensorflow", "pytorch"], "owner": {"account_id": 963556, "reputation": 1375, "user_id": 987397, "user_type": "registered", "accept_rate": 32, "profile_image": "https://www.gravatar.com/avatar/fa7ae7d9bd13c2d04335c3209865c262?s=256&d=identicon&r=PG", "display_name": "Derk", "link": "https://stackoverflow.com/users/987397/derk"}, "is_answered": true, "view_count": 791, "accepted_answer_id": 65206933, "answer_count": 1, "score": 2, "last_activity_date": 1607469024, "creation_date": 1607456527, "last_edit_date": 1607458238, "question_id": 65205801, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65205801/pytorch-alternative-for-tf-data-experimental-sample-from-datasets", "title": "PyTorch alternative for tf.data.experimental.sample_from_datasets", "body": "<p>Suppose I have two datasets, dataset one with 100 items and dataset two with 5000 items.</p>\n<p>Now I want that during training my model sees as much items from dataset one as from dataset two.</p>\n<p>In Tensorflow I can do:</p>\n<pre><code>dataset = tf.data.experimental.sample_from_datasets(\n    [dataset_one, dataset_two], weights=[50,1], seed=None\n)\n</code></pre>\n<p>Is there an alternative in PyTorch that does the same?</p>\n<p>I think this is not too difficult to implement by creating a custom dataset (not working example)</p>\n<pre><code>from torch.utils.data import Dataset\n\nclass SampleDataset(Dataset):\n    def __init__(self, datasets, weights):\n        self.datasets = datasets\n        self.weights = weights\n\n    def __len__(self):\n        return sum([len(dataset) for dataset in self.datasets])\n\n    def __getitem__(self, idx):\n        # sample a random number and based on that sample an item\n        \n        return self.datasets[dataset_idx][sample_idx]\n</code></pre>\n<p>However, this seems quite common. Is there already something like this available?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 278}]
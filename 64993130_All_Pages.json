[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "tensorflow2.0", "hidden-markov-models"], "owner": {"account_id": 3293578, "reputation": 4183, "user_id": 2771315, "user_type": "registered", "accept_rate": 84, "profile_image": "https://www.gravatar.com/avatar/1e99583e5df621df8d52ce0579a0affd?s=256&d=identicon&r=PG&f=1", "display_name": "Black", "link": "https://stackoverflow.com/users/2771315/black"}, "is_answered": false, "view_count": 450, "answer_count": 1, "score": 6, "last_activity_date": 1607474367, "creation_date": 1606244366, "last_edit_date": 1607380596, "question_id": 64993130, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64993130/how-to-get-hmm-working-with-real-valued-data-in-tensorflow", "title": "How to get HMM working with real-valued data in Tensorflow", "body": "<p>I'm working with a dataset that contains data from IoT devices and I have found that Hidden Markov Models work pretty well for my use case. As such, I'm trying to alter some code from a Tensorflow tutorial I've found <a href=\"https://www.tensorflow.org/probability/examples/Multiple_changepoint_detection_and_Bayesian_model_selection\" rel=\"nofollow noreferrer\">here</a>. The dataset contains real-values for the observed variable compared to the count data shown in the tutorial.</p>\n<p>In particular, I believe the following needs to be changed so that the HMM has Normally distributed emissions. Unfortunately, I can't find any code on how to alter the model to have a different emission other than Poisson.</p>\n<p>How should I change the code to emit normally distributed values?</p>\n<pre><code># Define variable to represent the unknown log rates.\ntrainable_log_rates = tf.Variable(\n  np.log(np.mean(observed_counts)) + tf.random.normal([num_states]),\n  name='log_rates')\n\nhmm = tfd.HiddenMarkovModel(\n  initial_distribution=tfd.Categorical(\n      logits=initial_state_logits),\n  transition_distribution=tfd.Categorical(probs=transition_probs),\n  observation_distribution=tfd.Poisson(log_rate=trainable_log_rates),\n  num_steps=len(observed_counts))\n\nrate_prior = tfd.LogNormal(5, 5)\n\ndef log_prob():\n return (tf.reduce_sum(rate_prior.log_prob(tf.math.exp(trainable_log_rates))) +\n         hmm.log_prob(observed_counts))\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n\n@tf.function(autograph=False)\ndef train_op():\n  with tf.GradientTape() as tape:\n    neg_log_prob = -log_prob()\n  grads = tape.gradient(neg_log_prob, [trainable_log_rates])[0]\n  optimizer.apply_gradients([(grads, trainable_log_rates)])\n  return neg_log_prob, tf.math.exp(trainable_log_rates)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 132}]
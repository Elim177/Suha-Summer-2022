[{"items": [{"tags": ["python", "tensorflow", "google-colaboratory", "training-data", "checkpoint"], "owner": {"account_id": 19206077, "reputation": 23, "user_id": 14032931, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-gstHpcNkXX4/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucmGa2F1T6frs3OvhOv1gMqgA9_tYg/photo.jpg?sz=256", "display_name": "user14032931", "link": "https://stackoverflow.com/users/14032931/user14032931"}, "is_answered": true, "view_count": 1769, "accepted_answer_id": 63205192, "answer_count": 1, "score": 2, "last_activity_date": 1655649133, "creation_date": 1596284115, "last_edit_date": 1596286315, "question_id": 63205089, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63205089/i-am-trying-to-resume-training-from-a-certain-checkpoint-tensorflow-because-i", "title": "I am trying to resume training from a certain checkpoint (Tensorflow) because I&#39;m using Colab and 12 hours aren&#39;t enough", "body": "<p>This is some part of the code I'm using</p>\n<pre><code>checkpoint_dir = 'training_checkpoints1'\ncheckpoint_prefix = os.path.join(checkpoint_dir, &quot;ckpt&quot;)\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                             encoder=encoder,\n                             decoder=decoder)\n</code></pre>\n<p>Now this is the training part</p>\n<pre><code>EPOCHS = 900\n\nfor epoch in range(EPOCHS):\n  start = time.time()\n\n  hidden = encoder.initialize_hidden_state()\n  total_loss = 0\n\n  for (batch, (inp, targ)) in enumerate(dataset):\n      loss = 0\n    \n      with tf.GradientTape() as tape:\n          enc_output, enc_hidden = encoder(inp, hidden)\n        \n          dec_hidden = enc_hidden\n        \n          dec_input = tf.expand_dims([targ_lang.word2idx['&lt;start&gt;']] * batch_size, 1)       \n        \n          # Teacher forcing - feeding the target as the next input\n          for t in range(1, targ.shape[1]):\n              # passing enc_output to the decoder\n              predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n            \n              loss += loss_function(targ[:, t], predictions)\n            \n              # using teacher forcing\n              dec_input = tf.expand_dims(targ[:, t], 1)\n    \n      batch_loss = (loss / int(targ.shape[1]))\n    \n      total_loss += batch_loss\n    \n      variables = encoder.variables + decoder.variables\n    \n      gradients = tape.gradient(loss, variables)\n    \n      optimizer.apply_gradients(zip(gradients, variables))\n    \n      if batch % 100 == 0:\n          print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n                                                     batch,\n                                                     batch_loss.numpy()))\n  # saving (checkpoint) the model every 2 epochs\n  if (epoch + 1) % 2 == 0:\n    checkpoint.save(file_prefix = checkpoint_prefix)\n\n  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n                                    total_loss / num_batches))\n  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n</code></pre>\n<p>Now I want to restore for exp this checkpoint and start training from there but I don't know how.</p>\n<pre><code>path=&quot;/content/drive/My Drive/training_checkpoints1/ckpt-9&quot;\ncheckpoint.restore(path)\n</code></pre>\n<p>Result</p>\n<pre><code>&lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6653263048&gt;\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 70}]
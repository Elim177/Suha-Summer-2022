[{"items": [{"tags": ["tensorflow", "reinforcement-learning", "sarsa"], "owner": {"account_id": 4380857, "reputation": 51, "user_id": 3572543, "user_type": "registered", "profile_image": "https://graph.facebook.com/1496762446/picture?type=large", "display_name": "Ralf", "link": "https://stackoverflow.com/users/3572543/ralf"}, "is_answered": false, "view_count": 318, "answer_count": 1, "score": 0, "last_activity_date": 1622784166, "creation_date": 1607523338, "question_id": 65218524, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65218524/sarsa-implementation-with-tensorflow", "title": "SARSA implementation with tensorflow", "body": "<p>I try to learn the concept of reinforcement learning at the moment. Hereby, I tried to implement the SARSA algorithm for the cart pole example using tensorflow. I compared my algorithm to algorithms which use a linear approximation function for the q-value function and find my algorithm to be very similar. Unfortunately, my implementation seems to be false or inefficient as the learning success is rather limited. Is there anyone who can tell me if I am doing something wrong and what it is? The code of my implementation is:</p>\n<pre><code>import numpy as np\nimport matplotlib.pylab as plt\nimport random\nimport gym\n\n\n#define a neural network which returns two action dependent q-values given a state\nneural_net = tf.keras.Sequential([\n    tf.keras.layers.Dense(10, activation = 'relu', input_shape = [4]),\n    tf.keras.layers.Dense(2)\n])\n\n#return the neural network's q-value for a specific action\ndef q_value(state, action):\n    return neural_net(tf.convert_to_tensor([state]))[0, action]\n\n#act either randomly or choose the action which maximizes the q-value\ndef policy(state, epsilon):\n    values = neural_net(tf.convert_to_tensor([state]))\n    if np.random.rand() &lt; epsilon:\n        return random.choice([0, 1])\n    else:\n        return np.argmax(values)\n\n#intialize gym environment\nenv = gym.make('CartPole-v0')\n\n#hyperparameters\ndiscount = 0.99\noptimizer = tf.keras.optimizers.Adam()\nepisodes = 1000\nepsilon = 0.30\n\n#collect reward for each episode\nrewards = []\nfor episode in range(episodes):\n\n    #start trajectory for episode\n    state = env.reset()\n\n    #record rewards during episode\n    sum_returns = 0\n\n    #decrease random action after the first 100 episodes\n    if episode == 100:\n        epsilon = 0.10\n\n    #Q-learning\n    while True:\n        action = policy(state, epsilon)\n        next_state, reward, done, _ = env.step(action)\n        next_action = policy(next_state, epsilon)\n        sum_returns += 1\n\n        if done:\n            with tf.GradientTape() as tape:\n                tape.watch(neural_net.trainable_variables)\n                q_hat = q_value(state, action)\n                y = reward\n                loss = tf.square(y - q_hat)\n\n            gradients = tape.gradient(loss, neural_net.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, neural_net.trainable_variables))\n            break\n        else:\n            with tf.GradientTape() as tape:\n                tape.watch(neural_net.trainable_variables)\n                q_hat = q_value(state, action)\n                y = reward + discount * q_value(next_state, next_action)\n                loss = tf.square(y - q_hat)\n\n            gradients = tape.gradient(loss, neural_net.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, neural_net.trainable_variables))\n            state = next_state\n\n    rewards.append(sum_returns)\n\n#plot learning over time\nplt.plot([episode for episode in range(episodes)], rewards)\nplt.show()```\n\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 253}]
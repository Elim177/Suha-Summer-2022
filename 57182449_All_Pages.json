[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning", "conv-neural-network"], "owner": {"account_id": 16229349, "reputation": 155, "user_id": 11718632, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-I15iX58nloc/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rcKzOHQ3G3rWFH4sxD-VOPfzw-_9Q/photo.jpg?sz=256", "display_name": "Will", "link": "https://stackoverflow.com/users/11718632/will"}, "is_answered": true, "view_count": 1354, "answer_count": 3, "score": 6, "last_activity_date": 1636903603, "creation_date": 1563969732, "last_edit_date": 1563971542, "question_id": 57182449, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57182449/loss-is-nan-on-image-classification-task", "title": "Loss is NaN on image classification task", "body": "<p>I'm trying to train a basic CNN on the image dataset that contains faces of celebrities with the class assigned corresponding to each person. Given that there are about 10,000 classes I used sparse_categorical_crossentropy rather than one-hot encoding the classes, however as soon as the network starts training the loss is stuck at one number and after several batches is goes to NaN I tried different scaling of the images and a smaller network but with no luck. Any clues on what might be causing the NaN?</p>\n\n<p>Function that generates batches:</p>\n\n<pre><code>def Generator(data, label, batch_size):\n    url = \"../input/celeba-dataset/img_align_celeba/img_align_celeba/\"\n    INPUT_SHAPE = (109, 109)\n    i = 0\n    while True:\n        image_batch = [ ]\n        label_batch = [ ]\n        for b in range(batch_size):\n            if i == len(data):\n                i = 0\n                data, label = shuffle(data, label)\n            sample = data[i]\n            label_batch.append(label[i])\n            i += 1\n            image = cv2.resize(cv2.imread(url + sample), INPUT_SHAPE)\n            image_batch.append((image.astype(float)) / 255)\n\n        yield (np.array(image_batch), np.array(label_batch))\n</code></pre>\n\n<p>The model:</p>\n\n<pre><code>class CNN():\n\ndef __init__(self, train, val, y_train, y_val, batch_size):\n    ## Load the batch generator\n    self.train_batch_gen = Generator(train, y_train, batch_size)\n    self.val_batch_gen = Generator(val, y_val, batch_size)\n\n    self.input_shape = (109, 109, 3)\n    self.num_classes = len(np.unique(y_train))\n    self.len_train = len(train)\n    self.len_val = len(val)\n\n    self.batch_size = batch_size\n    self.model = self.buildModel()\n\ndef buildModel(self):\n\n    model = models.Sequential()\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding=\"same\", input_shape=self.input_shape))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=self.input_shape))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(96, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.Conv2D(192, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.Conv2D(256, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(160, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.Conv2D(320, (3, 3), activation='relu', padding=\"same\"))\n    model.add(layers.AveragePooling2D(pool_size=(4, 4)))\n    model.add(layers.Flatten())\n    model.add(layers.Dense(128, activation='tanh'))\n    model.add(layers.Dropout(rate=0.1))\n    model.add(layers.Dense(self.num_classes, activation = \"softmax\")) #Classification layer or output layer\n    opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    return model\n\ndef trainModel(self, epochs):\n\n    self.model.fit_generator(generator=self.train_batch_gen,\n                            steps_per_epoch = int(self.len_train // self.batch_size),\n                            epochs=epochs,\n                            validation_data = self.val_batch_gen,\n                            validation_steps = int(self.len_val // self.batch_size))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 40}]
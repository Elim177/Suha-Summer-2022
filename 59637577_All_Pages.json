[{"items": [{"tags": ["tensorflow", "keras"], "owner": {"account_id": 8104758, "reputation": 85, "user_id": 6106700, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-JiMqGd6gSv4/AAAAAAAAAAI/AAAAAAAAAAA/iwiM1q9U9b0/photo.jpg?sz=256", "display_name": "James", "link": "https://stackoverflow.com/users/6106700/james"}, "is_answered": false, "view_count": 303, "answer_count": 0, "score": 3, "last_activity_date": 1578441191, "creation_date": 1578440087, "last_edit_date": 1578441191, "question_id": 59637577, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59637577/how-do-you-set-a-tensorflow-2-keras-optimizer-to-a-state-before-youve-applied", "title": "How do you set a Tensorflow 2 Keras optimizer to a state, before you&#39;ve applied grads with it?", "body": "<p>I'm working at a slightly lower-level of Keras than the Model fit API. I  would like to be able to set the state of a newly constructed optimizer to the state of it from previous training.</p>\n\n<p>The <code>get_weights</code> and <code>set_weights</code> methods seem promising; they just return and receive numpy arrays or standard scalar data for the state of the optimizer. However, the problem is you cannot <code>set_weights</code> if the weights have not yet been created, and as far as I can tell, the only <em>public</em> way they get created is on the first call to <code>apply_gradients</code>.</p>\n\n<p>For example, the following fails because <code>opt2</code> will not have its weights created.</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nopt1 = tf.keras.optimizers.Adam()\nopt2 = tf.keras.optimizers.Adam()\nlayer = tf.keras.layers.Dense(1)\n\n# dummy data\nx = np.array([[-1, 1], [1, 1]])\ny = np.array([[-1], [1]])\n\n# do one optimization step\nwith tf.GradientTape() as tape:\n    loss = (layer(x) - y)**2\ngrads = tape.gradient(loss, layer.trainable_weights)\nopt1.apply_gradients(zip(grads, layer.trainable_weights))\n\n# copy state to optimizer 2\nopt2.set_weights(opt1.get_weights())  # this fails!\n\n</code></pre>\n\n<p>Lets assume I do have on hand the relevant model weights on which the optimizer operates. What is the right way restore state? Based on the implementation of the <code>apply_gradients</code> method, it seems like this is the path:</p>\n\n<pre><code>_ = opt2.iterations  # must be called to make this weight appear\nopt2._create_hypers()\nopt2._create_slots(layer.trainable_weights)\n\n# now we can safely set weights\nopt2.set_weights(opt1.get_weights())\n</code></pre>\n\n<p>But that feels really hacky to me and prone to fail if implementation details change at a future point. Are there better approaches that I'm missing?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 29}]
[{"items": [{"tags": ["tensorflow", "keras", "tensorflow2.0", "tf.keras", "tensorflow2.x"], "owner": {"account_id": 8931902, "reputation": 11, "user_id": 12666811, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-qxntK5r7iqs/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rf-DHWpR5382RXaQ69ySWQGTM4AfA/mo/photo.jpg?sz=256", "display_name": "Mahdi Heidarpoor", "link": "https://stackoverflow.com/users/12666811/mahdi-heidarpoor"}, "is_answered": false, "view_count": 1180, "answer_count": 3, "score": 0, "last_activity_date": 1584179666, "creation_date": 1579967401, "last_edit_date": 1579968849, "question_id": 59910845, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59910845/customized-tf2-model-save", "title": "Customized TF2 Model Save", "body": "<p>I write customize model with TF2 </p>\n\n<pre><code>class NN(tf.keras.Model):\n\ndef __init__(self,\n             output_dim: int, \n             controller_dime:int=128,\n             interface_dim: int=35,\n             netsize: int=100, \n             degree: int=20, \n             k:float=2,\n             name:str='dnc_rn')-&gt;None:\n</code></pre>\n\n<hr>\n\n<p>Its full of random parametrs that is not trainable!\nso I need to save model completely, and I cant use save_weights because train of each model dependes on its self random parameters...</p>\n\n<hr>\n\n<p>the file of trainer is like :</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\n\ndef trainer(model: tf.keras.Model,\n        loss_fn: tf.keras.losses,\n        X_train: np.ndarray,\n        y_train: np.ndarray = None,\n        optimizer: tf.keras.optimizers = tf.keras.optimizers.Adam(learning_rate=1e-3),\n        loss_fn_kwargs: dict = None,\n        epochs: int = 1000000,\n        batch_size: int = 1,\n        buffer_size: int = 2048,\n        shuffle: bool = False,\n        verbose: bool = True,\n        show_model_interface_vector: bool = False\n        ) -&gt; None:\n\n\"\"\"\nTrain TensorFlow model.\n\nParameters\n----------\nmodel\n    Model to train.\nloss_fn\n    Loss function used for training.\nX_train\n    Training batch.\ny_train\n    Training labels.\noptimizer\n    Optimizer used for training.\nloss_fn_kwargs\n    Kwargs for loss function.\nepochs\n    Number of training epochs.\nbatch_size\n    Batch size used for training.\nbuffer_size\n    Maximum number of elements that will be buffered when prefetching.\nshuffle\n    Whether to shuffle training data.\nverbose\n    Whether to print training progress.\n\"\"\"\n\nmodel.show_interface_vector=show_model_interface_vector\n\n# Create dataset\nif y_train is None:  # Unsupervised model\n    train_data = X_train\nelse:\n    train_data = (X_train, y_train)\ntrain_data = tf.data.Dataset.from_tensor_slices(train_data)\nif shuffle:\n    train_data = train_data.shuffle(buffer_size=buffer_size).batch(batch_size)\n\n# Iterate over epochs\nhistory=[]\nfor epoch in range(epochs):\n    if verbose:\n        pbar = tf.keras.utils.Progbar(target=epochs, width=40, verbose=1, interval=0.05)\n\n    # Iterate over the batches of the dataset\n    for step, train_batch in enumerate(train_data):\n\n        if y_train is None:\n            X_train_batch = train_batch\n        else:\n            X_train_batch, y_train_batch = train_batch\n\n        with tf.GradientTape() as tape:\n            preds = model(X_train_batch)\n\n            if y_train is None:\n                ground_truth = X_train_batch\n            else:\n                ground_truth = y_train_batch\n\n            # Compute loss\n            if tf.is_tensor(preds):\n                args = [ground_truth, preds]\n            else:\n                args = [ground_truth] + list(preds)\n\n            if loss_fn_kwargs:\n                loss = loss_fn(*args, **loss_fn_kwargs)\n            else:\n                loss = loss_fn(*args)\n\n            if model.losses:  # Additional model losses\n                loss += sum(model.losses)\n\n        grads = tape.gradient(loss, model.trainable_weights)\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n    if verbose:\n            loss_val = loss.numpy().mean()\n            pbar_values = [('loss', loss_val)]\n            pbar.update(epoch+1, values=pbar_values)\n\n    history.append(loss.numpy().mean())\n\nmodel.show_interface_vector= not show_model_interface_vector\nreturn history\n</code></pre>\n\n<hr>\n\n<p>after training very I tried to save model but when I call TF2 .save :</p>\n\n<pre><code>model.save('a.h5')\n</code></pre>\n\n<p>I have an error:</p>\n\n<pre><code>NotImplementedError: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.\n</code></pre>\n\n<p>I change it to .tf format but again:</p>\n\n<pre><code>ValueError: Model &lt;model2.NN object at 0x11448b390&gt; cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling .fit() or .predict(). To manually set the shapes, call model._set_inputs(inputs).\n</code></pre>\n\n<p>but its already trained, and if i _set_inputs</p>\n\n<pre><code>ValueError: Cannot infer num from shape (None, 12, 4)\n</code></pre>\n\n<hr>\n\n<p>I don't know what should I do?\nI am sciences and amateur with TF2\nHelp me it's important for my project...</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 65}]
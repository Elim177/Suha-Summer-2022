[{"items": [{"tags": ["tensorflow", "neural-network", "embedding", "bert-language-model", "dropout"], "owner": {"account_id": 9968983, "reputation": 2869, "user_id": 7375754, "user_type": "registered", "accept_rate": 77, "profile_image": "https://www.gravatar.com/avatar/830f6d236ab0502a68fd9b9daf7f729f?s=256&d=identicon&r=PG&f=1", "display_name": "Jane Sully", "link": "https://stackoverflow.com/users/7375754/jane-sully"}, "is_answered": true, "view_count": 301, "accepted_answer_id": 69336770, "answer_count": 1, "score": -1, "last_activity_date": 1632673218, "creation_date": 1631570637, "last_edit_date": 1631630471, "question_id": 69169595, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69169595/should-feature-embeddings-be-taken-before-or-after-dropout-layer-in-neural-netwo", "title": "Should feature embeddings be taken before or after dropout layer in neural network?", "body": "<p>I am training a binary text classification model using BERT as follows:</p>\n<pre><code>def create_model():\n   text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n   preprocessed_text = bert_preprocess(text_input)\n   outputs = bert_encoder(preprocessed_text)\n\n   # Neural network layers\n   l1 = tf.keras.layers.Dropout(0.1, name=&quot;dropout&quot;)(outputs['pooled_output'])\n   l2 = tf.keras.layers.Dense(1, activation='sigmoid', name=&quot;output&quot;)(l1)\n\n   # Use inputs and outputs to construct a final model\n   model = tf.keras.Model(inputs=[text_input], outputs=[l2])\n   return model\n</code></pre>\n<p>This code is borrowed from the example on tfhub: <a href=\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\" rel=\"nofollow noreferrer\">https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4</a>.</p>\n<p>I want to extract feature embeddings from the penultimate layer and use them for comparison, clustering, visualization, etc between examples. Should this be done before dropout (l1 in the model above) or after dropout (l2 in the model above)?</p>\n<p>I am trying to figure out whether this choice makes a significant difference, or is it fine either way? For example, if I extract feature embeddings after dropout and compute feature similarities between two examples, this might be affected by which nodes are randomly set to 0 (but perhaps this is okay).</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 76}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning", "eager-execution"], "owner": {"account_id": 17112942, "reputation": 346, "user_id": 12383245, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/5a0269ec6540035ab911e2f1585acf7a?s=256&d=identicon&r=PG&f=1", "display_name": "thijsvdp", "link": "https://stackoverflow.com/users/12383245/thijsvdp"}, "is_answered": false, "view_count": 974, "answer_count": 1, "score": 1, "last_activity_date": 1603033356, "creation_date": 1601829059, "last_edit_date": 1601829535, "question_id": 64197151, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64197151/error-when-using-run-eagerly-false-in-model-compile-custom-keras-model-in-tensor", "title": "Error when using run_eagerly=False in model.compile custom Keras Model in Tensorflow", "body": "<p>I am developing a custom model in Tensorflow. I am trying to implement a Virtual Adversarial Training (VAT) model from <a href=\"https://arxiv.org/abs/1704.03976\" rel=\"nofollow noreferrer\">https://arxiv.org/abs/1704.03976</a>. The model makes use of both labeled and unlabeled data in its classification task. Therefore, in the <code>train_step</code> of the model, I need to divide the data of the batch into labeled (0, or 1), or unlabeled (-1).  It seems to work as expected when compiling the model using run_eagerly=True, but when I use run_eagerly=False, it gives me the following error:</p>\n<pre><code>ValueError: Number of mask dimensions must be specified, even if some dimensions are None.  E.g. shape=[None] is ok, but shape=None is not.\n</code></pre>\n<p>which seems to be produced in:</p>\n<pre><code>X_l, y_l = tf.boolean_mask(X, tf.logical_not(missing)), tf.boolean_mask(y, tf.logical_not(missing))\n</code></pre>\n<p>I am not sure what is causing the error, but it seems to have something to do with a weird tensor shape issues that only occur during <code>run_eagerly=False</code>. I need the <code>boolean_mask</code> functionality in order to distinguish the labeled and unlabeled data. I hope someone can help me out. In order to reproduce the errors, I added the model, and a small simulation example. The simulation will produce the error I have, when <code>run_eagerly=False</code> is set.</p>\n<p>Thanks in advance.</p>\n<p><strong>Model defintion:</strong></p>\n<pre><code>from tensorflow import keras\nimport tensorflow as tf\n\n\nmetric_acc = keras.metrics.BinaryAccuracy()\nmetric_loss = keras.metrics.Mean('loss')\n\n\nclass VAT(keras.Model):\n\n    def __init__(self, units_1=16, units_2=16, dropout=0.3, xi=1e-6, epsilon=2.0, alpha=1.0):\n        super(VAT, self).__init__()\n\n        # Set model parameters\n        self.units_1 = units_1\n        self.units_2 = units_2\n        self.dropout = dropout\n        self.xi = xi\n        self.epsilon = epsilon\n        self.alpha = alpha\n\n        # First hidden\n        self.dense1 = keras.layers.Dense(self.units_1)\n        self.activation1 = keras.layers.Activation(tf.nn.leaky_relu)\n        self.dropout1 = keras.layers.Dropout(self.dropout)\n\n        # Second hidden\n        self.dense2 = keras.layers.Dense(self.units_2)\n        self.activation2 = keras.layers.Activation(tf.nn.leaky_relu)\n        self.dropout2 = keras.layers.Dropout(self.dropout)\n\n        # Output layer\n        self.dense3 = keras.layers.Dense(1)\n        self.activation3 = keras.layers.Activation(&quot;sigmoid&quot;)\n\n    def call(self, inputs, training=None, mask=None):\n\n        x1 = self.dense1(inputs)\n        x2 = self.activation1(x1)\n        x3 = self.dropout1(x2, training=True)\n\n        x4 = self.dense2(x3)\n        x5 = self.activation2(x4)\n        x6 = self.dropout2(x5, training=True)\n\n        x7 = self.dense3(x6)\n        x8 = self.activation3(x7)\n\n        return x8\n\n    def generate_perturbation(self, inputs):\n\n        # Generate normal vectors\n        d = tf.random.normal(shape=tf.shape(inputs))\n\n        # Normalize vectors\n        d = tf.math.l2_normalize(d, axis=1)\n\n        # Calculate r\n        r = self.xi * d\n\n        # Make predictions\n        p = self(inputs, training=True)\n\n        # Tape gradient\n        with tf.GradientTape() as tape:\n            tape.watch(r)\n\n            # Perturbed predictions\n            p_perturbed = self(inputs + r, training=True)\n\n            # Calculate divergence\n            D = keras.losses.KLD(p, p_perturbed) + keras.losses.KLD(1 - p, 1 - p_perturbed)\n\n        # Calculate gradient\n        gradient = tape.gradient(D, r)\n\n        # Calculate r_vadv\n        r_vadv = tf.math.l2_normalize(gradient, axis=1)\n\n        # Return virtual adversarial perturbation\n        return r_vadv\n\n    @tf.function\n    def train_step(self, data):\n\n        # Unpack data\n        X, y = data\n\n        # Missing label boolean indices\n        missing = tf.squeeze(tf.equal(y, -1))\n\n        # Split data into labeled and unlabeled data\n        X_l, y_l = tf.boolean_mask(X, tf.logical_not(missing)), tf.boolean_mask(y, tf.logical_not(missing))\n        X_u = tf.boolean_mask(X, missing)\n\n        # Calculate virtual perturbations for labeled and unlabeled\n        r_l = self.generate_perturbation(X_l)\n        r_u = self.generate_perturbation(X_u)\n\n        # Tape gradient\n        with tf.GradientTape() as model_tape:\n            model_tape.watch(self.trainable_variables)\n\n            # Calculate probabilities real data\n            prob_l, prob_u = self(X_l, training=True), self(X_u, training=True)\n\n            # Calculate probabilities perturbed data\n            prob_r_l, prob_r_u = self(X_l + self.epsilon * r_l, training=True), self(X_u + self.epsilon * r_u, training=True)\n\n            # Calculate loss\n            loss = vat_loss(y_l, prob_l, prob_u, prob_r_l, prob_r_u, self.alpha)\n\n        # Calculate gradient\n        model_gradient = model_tape.gradient(loss, self.trainable_variables)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(model_gradient, self.trainable_variables))\n\n        # Compute metrics\n        metric_acc.update_state(y_l, prob_l)\n        metric_loss.update_state(loss)\n\n        return {'loss': metric_loss.result(), 'accuracy': metric_acc.result()}\n\n    @property\n    def metrics(self):\n        return [metric_loss, metric_acc]\n\n\ndef vat_loss(y_l, prob_l, prob_u, prob_r_l, prob_r_u, alpha):\n    N_l = tf.cast(tf.size(prob_l), dtype=tf.dtypes.float32)\n    N_u = tf.cast(tf.size(prob_u), dtype=tf.dtypes.float32)\n\n    if tf.equal(N_l, 0):\n        # No labeled examples: get contribution from unlabeled data using perturbations\n        R_vadv = tf.reduce_sum(\n            keras.losses.KLD(prob_u, prob_r_u)\n            + keras.losses.KLD(1 - prob_u, 1 - prob_r_u)\n        )\n\n        return alpha * R_vadv / N_u\n\n    elif tf.equal(N_u, 0):\n\n        # No unlabeled examples: get contribution from labeled data\n        R = tf.reduce_sum(keras.losses.binary_crossentropy(y_l, prob_l))\n\n        R_vadv = tf.reduce_sum(\n            keras.losses.KLD(prob_l, prob_r_l)\n            + keras.losses.KLD(1 - prob_l, 1 - prob_r_l)\n        )\n\n        return R / N_l + alpha * R_vadv / N_l\n\n    else:\n        # Get contribution from labeled data\n        R = tf.reduce_sum(keras.losses.binary_crossentropy(y_l, prob_l))\n\n        # Get contribution from labeled and unlabeled data using perturbations\n        R_vadv = tf.reduce_sum(\n            keras.losses.KLD(prob_l, prob_r_l)\n            + keras.losses.KLD(1 - prob_l, 1 - prob_r_l)\n        ) + tf.reduce_sum(\n            keras.losses.KLD(prob_u, prob_r_u)\n            + keras.losses.KLD(1 - prob_u, 1 - prob_r_u)\n        )\n\n        return R / N_l + alpha * R_vadv / (N_l + N_u)\n\n</code></pre>\n<p><strong>Simulation example:</strong>\nTo show that the model/code works as desired (when using <code>run_eagerly=True</code>, I made a simulation example. In this example, I bias when observations are labeled/unlabeled. The figure below illustrates the labeled observations used by the model (yellow or purple), and the unlabeled observations (blue)<a href=\"https://i.stack.imgur.com/InxjZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/InxjZ.png\" alt=\"Labeled and unlabeled data\" /></a>.</p>\n<p>The VAT produces an accuracy of around ~0.75, whereas the reference model produces an accuracy of around ~0.58. These accuracies are produced without hyperparameter tuning.</p>\n<pre><code>from modules.vat import VAT\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\n\ndef create_biased_sample(x, proportion_labeled):\n    labeled =  np.random.choice([True, False], p=[proportion_labeled, 1-proportion_labeled])\n    if x[0] &lt; 0.0:\n        return False\n    elif x[0] &gt; 1.0:\n        return False\n    else:\n        return labeled\n\n\n# Simulation parameters\nN = 2000\nproportion_labeled = 0.15\n\n# Model training parameters\nBATCH_SIZE = 128\nBUFFER_SIZE = 60000\nEPOCHS = 100\n\n# Generate a dataset\nX, y = datasets.make_moons(n_samples=N, noise=.05, random_state=3)\nX, y = X.astype('float32'), y.astype('float32')\ny = y.reshape(-1, 1)\n\n\n# Split in train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5)\n\n# Simulate missing labels\nsample_biased = lambda x: create_biased_sample(x, proportion_labeled)\nlabeled = np.array([sample_biased(k) for k in X_train])\ny_train[~ labeled] = -1\n\n# Estimate VAT model\nvat = VAT(dropout=0.2, units_1=16, units_2=16, epsilon=0.5)\nvat.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), run_eagerly=True)\nvat.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True)\n\n# Estimate a reference model\nreference = keras.models.Sequential([\n    keras.layers.Input(shape=(2,)),\n    keras.layers.Dense(16),\n    keras.layers.Activation(tf.nn.leaky_relu),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(16),\n    keras.layers.Activation(tf.nn.leaky_relu),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(1),\n    keras.layers.Activation(&quot;sigmoid&quot;)\n])\nreference.compile(optimizer=keras.optimizers.Adam(learning_rate=0.01), loss=keras.losses.binary_crossentropy, run_eagerly=False)\nreference.fit(X_train[y_train.flatten() != -1, :], y_train[y_train.flatten() != -1], batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True)\n\n# Calculate out-of-sample accuracies\ntest_acc_vat = tf.reduce_mean(keras.metrics.binary_accuracy(y_test, vat(X_test, training=False)))\ntest_acc_reference = tf.reduce_mean(keras.metrics.binary_accuracy(y_test, reference(X_test, training=False)))\n\n# Print results\nprint('Test accuracy of VAT: {}'.format(test_acc_vat))\nprint('Test accuracy of reference model: {}'.format(test_acc_reference))\n\n# Plot scatter\nplt.scatter(X_test[:, 0], X_test[:, 1])\nplt.scatter(X_train[y_train.flatten() != -1, 0], X_train[y_train.flatten() != -1, 1], c=y_train.flatten()[y_train.flatten() != -1])\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 175}]
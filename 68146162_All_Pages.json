[{"items": [{"tags": ["python", "tensorflow", "keras", "pycharm"], "owner": {"account_id": 1053202, "reputation": 54354, "user_id": 1056563, "user_type": "registered", "accept_rate": 90, "profile_image": "https://www.gravatar.com/avatar/bc888aa658ab0e0d2a4d305ad7f69123?s=256&d=identicon&r=PG", "display_name": "WestCoastProjects", "link": "https://stackoverflow.com/users/1056563/westcoastprojects"}, "is_answered": true, "view_count": 342, "answer_count": 1, "score": 1, "last_activity_date": 1624768407, "creation_date": 1624740782, "last_edit_date": 1624768407, "question_id": 68146162, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68146162/debug-into-a-custom-metrics-function-in-keras-tensorflow-in-pycharm", "title": "Debug into a custom metrics function in Keras/Tensorflow (in Pycharm)?", "body": "<p>I have created a simple <em>ConfusionMatrix</em> custom metric and am running into a problem in Tensor conversion.  It would speed up fixing if I could set a breakpoint. However breakpoints are not being respected by <em>PyCharm</em> .</p>\n<p>Here is the code</p>\n<pre><code>def multiConfusion(expectsIn, actsIn):\n  expects = tf.keras.backend.eval(expectsIn)  # Error occurs here\n  acts = tf.keras.backend.eval(actsIn)\n  classes = sorted(list(set(expects).union(set(acts))))\n  \n  from collections import defaultdict\n  mx = defaultdict(lambda: defaultdict(int))\n  for e,a in list(zip(expects,acts)):\n    mx[e][a] += 1\n  hdr = &quot;Exp/Act&quot; + ''.join([ f'\\t\\t{lab}' for lab in classes])\n  ll = '\\n'.join([\n                  '\\t\\t\\t' + '\\t\\t'.join([ str(mx[e][a]) for k,a in list(mx[e].items())])\n                  for e in classes\n                 ])\n  mat =  f&quot;{hdr}\\n{ll}&quot;\n  print(mat)\n  return mat\n\ndef confusionMat(x,y,num_classes=NClasses):\n      from tensorflow import math as tfmath\n      cmat = None\n      if NClasses == 2:\n        cmat = binaryConfusion(x,y)\n      else:\n        cmat = multiConfusion(x,y) # Breakpoint set here but gets skipped\n      print(repr(cmat))\n      return cmat\n\nmodel.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=opt, \n              metrics=[confusionMat,&quot;accuracy&quot;])\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/ivOqC.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ivOqC.png\" alt=\"enter image description here\" /></a></p>\n<p>Some thoughts I had on trying to re-enable the debugger:</p>\n<ul>\n<li>set single threaded mode</li>\n</ul>\n<p><code>tf.config.threading.set_inter_op_parallelism_threads(1)</code></p>\n<ul>\n<li>set eager execution mode</li>\n</ul>\n<p><code>vgg_model.run_eagerly = True</code></p>\n<p>But still the breakpoint is not respected. Any thoughts?</p>\n<p><strong>Update</strong>  I have tested / updated / expanded the above code a fair bit. It does generate a confusion matrix properly</p>\n<p><a href=\"https://i.stack.imgur.com/tJ1mA.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/tJ1mA.png\" alt=\"enter image description here\" /></a></p>\n<p>Debugging into the code works fine when called with a constant Tensorflow Tensor directly.</p>\n<p>But nothing I can do will make the debugger activate when the code is invoked via the <em>metrics</em> machinery within the Tensorflow <em>model</em> training. (via <em>model.fit()</em>)</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 206}]
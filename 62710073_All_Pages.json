[{"items": [{"tags": ["tensorflow", "keras", "google-cloud-platform", "google-cloud-storage", "tpu"], "owner": {"account_id": 3948330, "reputation": 5043, "user_id": 3259896, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/641c30a7b383022f22b53c8cedb04e3f?s=256&d=identicon&r=PG&f=1", "display_name": "SantoshGupta7", "link": "https://stackoverflow.com/users/3259896/santoshgupta7"}, "is_answered": false, "view_count": 670, "answer_count": 1, "score": 3, "last_activity_date": 1595554308, "creation_date": 1593759533, "question_id": 62710073, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62710073/tpu-training-in-colab-custom-model-data-from-my-own-gcp-account-cell-just-see", "title": "TPU training in colab, custom model, data from my own GCP account: Cell just seems to hang, no progress or error message", "body": "<p>I am trying to train on a colab TPU using data from my GCP account.</p>\n<p>When I run the cell that starts the training, the cell just seems to hang, with no progress. I put a very low number of steps, so that the training should complete pretty quickly, about a minute on GPU, but it never finishes on TPU.</p>\n<p>I am using a custom model, and I am using files saved on GCP using the solution given in this stackoverflow answer <a href=\"https://stackoverflow.com/questions/61448884/how-to-connect-to-private-storage-bucket-using-the-google-colab-tpu\">How to connect to private storage bucket using the Google Colab TPU</a></p>\n<p>The model trains/runs just fine on GPU/CPU.</p>\n<p>The full code is in this colab notebook here</p>\n<p><a href=\"https://colab.research.google.com/drive/13HgRJru0glOzn7m0b7tmVCO_VrRpa1XS?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/13HgRJru0glOzn7m0b7tmVCO_VrRpa1XS?usp=sharing</a></p>\n<p>And here's a google drive link to the sample data file</p>\n<p><a href=\"https://drive.google.com/file/d/10EFyxau97jLfeGaKugMevIyX-bobsFe5/view?usp=sharing\" rel=\"nofollow noreferrer\">https://drive.google.com/file/d/10EFyxau97jLfeGaKugMevIyX-bobsFe5/view?usp=sharing</a></p>\n<p>And below is the code from the colab notebook</p>\n<pre><code>!pip install transformers --q\n%tensorflow_version 2.x\n\n!gcloud auth login\n\n'''NEED TO RUN THIS CELL TWICE TO AVOID ERROR'''\n\nfrom google.colab import auth\nauth.authenticate_user()\n\nproject_id = 'machinelearning-264918'\n!gcloud config set project {project_id}\n\n!pip install tfa-nightly\nimport tensorflow_addons as tfa\n\nfrom transformers import TFBertModel, AutoModel\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (Dense,\n                                     Dropout)\nimport os\nimport tensorflow_addons as tfa\n\nlogger = tf.get_logger()\nlogger.info(tf.__version__)\n\nautotune = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    logger.info('Running with TPUStrategy on TPU {} with {} cores '\n                .format(tpu.cluster_spec().as_dict()['worker'],\n                        strategy.num_replicas_in_sync))\n    batch_size = 3 * strategy.num_replicas_in_sync\nexcept Exception:\n    # raise ValueError\n    strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n    logger.warning('Failed initializing TPU! Running on GPU')\n    batch_size = 3\n\nfrom tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer as lso\nfrom tensorflow.python.distribute import parameter_server_strategy\n\ndef _minimize(strategy, tape, optimizer, loss, trainable_variables):\n    with tape:\n        if isinstance(optimizer, lso.LossScaleOptimizer):\n            loss = optimizer.get_scaled_loss(loss)\n\n    gradients = tape.gradient(loss, trainable_variables)\n    # Whether to aggregate gradients outside of optimizer. This requires support\n    # of the optimizer and doesn't work with ParameterServerStrategy and\n    # CentralStroageStrategy.\n    aggregate_grads_outside_optimizer = (\n        optimizer._HAS_AGGREGATE_GRAD and  # pylint: disable=protected-access\n        not isinstance(strategy.extended,\n                        parameter_server_strategy.ParameterServerStrategyExtended))\n\n    if aggregate_grads_outside_optimizer:\n        # We aggregate gradients before unscaling them, in case a subclass of\n        # LossScaleOptimizer all-reduces in fp16. All-reducing in fp16 can only be\n        # done on scaled gradients, not unscaled gradients, for numeric stability.\n        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n                                                    trainable_variables))\n    if isinstance(optimizer, lso.LossScaleOptimizer):\n        gradients = optimizer.get_unscaled_gradients(gradients)\n    gradients = optimizer._clip_gradients(gradients)  # pylint: disable=protected-access\n    if trainable_variables:\n        if aggregate_grads_outside_optimizer:\n            optimizer.apply_gradients(\n                zip(gradients, trainable_variables),\n                experimental_aggregate_gradients=False)\n        else:\n            optimizer.apply_gradients(zip(gradients, trainable_variables))\n\nclass CustomModel(tf.keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x, y = data\n        batch_label = tf.reshape(y, (tf.size(y)/2, 2), name=None)\n\n        rs = tf.ragged.stack(x, axis=0)\n        reg = rs.to_tensor()\n        batch_input = tf.reshape(reg, (tf.shape(reg)[0]*tf.shape(reg)[1], tf.shape(reg)[2]))\n\n        with tf.GradientTape() as tape:\n            y_pred = self(batch_input, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(batch_label, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n                self.trainable_variables)\n        # Update weights\n        # self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(y, y_pred)\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n\ndef get_model(drop_out):\n    sciBert = TFBertModel.from_pretrained('bert-base-uncased', from_pt=True)\n\n    allFinal = tf.keras.Input(shape=(None,), dtype=tf.int32, name='inputN') \n\n    '''Should posFinal and negFinal be concatenated, so there's only one call to sciBert'''\n    allBertOut = sciBert(allFinal, training=True)\n\n    allPoolConcat = tf.concat([\n                    allBertOut[0][:, 0], #output of ff layer after last hidden state since it seems to be untrained in roberta\n                    tf.reduce_mean(allBertOut[0][:, 1:-1], axis=1)\n                    ],axis=1) \n\n    postLayer = tf.keras.layers.Dense(768, activation='swish', name='postff')\n    LayerNorm = tf.keras.layers.LayerNormalization(epsilon=1e-12, name=&quot;LayerNormO&quot;)\n    postLayer2 = tf.keras.layers.Dense(768, activation='swish', name='2postff')\n    classifier = tf.keras.layers.Dense(2, name='classifierff')\n\n    postWeights = postLayer(allPoolConcat) \n    postWeights = LayerNorm(postWeights)\n    postWeights = Dropout(drop_out)(postWeights)\n\n    postWeights2 = postLayer2(postWeights) \n    allScores = classifier(postWeights2) \n\n    model = CustomModel(inputs=allFinal, outputs=allScores)\n    return model\n\n@tf.function\ndef _parse_example(example_proto):\n    features = {\n        'sciBert_SentenceIndex': tf.io.VarLenFeature( dtype=tf.int64),\n        'SciBert_IDs': tf.io.VarLenFeature(dtype=tf.int64),\n    }\n\n    parsed_example_dict = tf.io.parse_single_example(example_proto, features)\n    sentencePositions = parsed_example_dict['sciBert_SentenceIndex']\n    passageIds = parsed_example_dict['SciBert_IDs']\n\n    sentencePositions = tf.sparse.to_dense(sentencePositions)\n    bertIds = tf.sparse.to_dense(passageIds)\n\n    sentencePositions = tf.cast(sentencePositions, dtype=tf.int32)\n    passageIds = tf.cast(passageIds, dtype=tf.int32)\n    length = tf.shape(\n                        sentencePositions, out_type=tf.dtypes.int32, name='shape'\n                    )\n\n    lengthMinusOne = tf.math.subtract(\n                            length, 1, name='SubtractOne'\n                            )\n\n    # creage random numbers for a sentence index up to 2nd to last index\n    # the last index is just the last position of the non-padded bertID\n    startRandSentIndex = tf.random.uniform(\n            shape=[1], minval=0, maxval=lengthMinusOne[0], dtype=tf.dtypes.int32, seed=None, name=None)\n    # Get the end point for that sentence \n    endRandSentIndex = tf.math.add(startRandSentIndex, 1, name=None)\n    # last position of the non-padded bertID\n    lastPosition = length-1\n    # extract BertID positions for sentence start/end and bertID end\n    startSentencePosit = tf.gather_nd(sentencePositions, [startRandSentIndex], batch_dims=0)\n    endSentencePosit = tf.gather_nd(sentencePositions, [endRandSentIndex], batch_dims=0)\n    lastPassagePosit = tf.gather_nd(sentencePositions, [lastPosition], batch_dims=0)\n    # Get slices of BertIDs for the query, and the rest\n    firstPiece = tf.slice(bertIds, [0], [startSentencePosit[0]] )\n    queryPiece = tf.slice(bertIds, [startSentencePosit[0]], [endSentencePosit[0]-startSentencePosit[0]] )\n    lastPiece = tf.slice(bertIds, [endSentencePosit[0]], [lastPassagePosit[0]-endSentencePosit[0]] )\n    # concat rest of passage\n    restPassagePiece = tf.concat( [firstPiece,lastPiece], axis=0 )\n    # Clip\n    queryPiece = queryPiece[0:256]\n\n    restPassagePiece = restPassagePiece[0:510]\n    # add special tokens for proper input into the model \n    return tf.cast(queryPiece, dtype=tf.int32), tf.cast(restPassagePiece, dtype=tf.int32)\n\n@tf.function\ndef clip_seq_to_len(seq, num_tokens=512):\n    seq_len = tf.shape(seq)[0]\n    if seq_len &gt; 511:\n        return seq[:511]\n    return seq[:]\n\n@tf.function\ndef make_samples(query_a, passage_a, query_b, passage_b):\n    CLS_inputID = tf.constant([102])\n    SEP_inputID = tf.constant([103])\n\n    positive_sample_a = clip_seq_to_len(tf.concat([CLS_inputID, query_a, SEP_inputID, passage_a], axis=-1))\n    positive_sample_b = clip_seq_to_len(tf.concat([CLS_inputID, query_b, SEP_inputID, passage_b], axis=-1))\n\n    negative_sample_a = clip_seq_to_len(tf.concat([CLS_inputID, query_a, SEP_inputID, passage_b], axis=-1))\n    negative_sample_b = clip_seq_to_len(tf.concat([CLS_inputID, query_b, SEP_inputID, passage_a], axis=-1))\n    \n    positive_sample_a = tf.concat([positive_sample_a, SEP_inputID], axis=-1)\n    positive_sample_b = tf.concat([positive_sample_b, SEP_inputID], axis=-1)\n    negative_sample_a = tf.concat([negative_sample_a, SEP_inputID], axis=-1)\n    negative_sample_b = tf.concat([negative_sample_b, SEP_inputID], axis=-1)\n    return positive_sample_a, positive_sample_b, negative_sample_a, negative_sample_b\n\n@tf.function\ndef get_samples(example_a, example_b):\n    samples = make_samples(*_parse_example(example_a), *_parse_example(example_b))\n    return samples\n\nconfig = {\n  'drop_out':0.1\n}\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n\nwith strategy.scope():\n    model = get_model(**config)\n    model.compile(loss=loss_fn,\n                  optimizer=tfa.optimizers.AdamW(weight_decay=1e-5, learning_rate=3e-4, epsilon=1e-07), run_eagerly=False)\n\nconfig_name = 'model_b'\nbase_dir = 'gs://bdora-semanticscholar'\nmodel_dir = os.path.join(base_dir, config_name)\n# tensorboard_dir = os.path.join(model_dir, 'logs_' + str(time()))\ntfrecords_pattern_train = os.path.join(base_dir, 'VersionB_00022*')\ntfrecords_pattern_train2 = os.path.join(base_dir, 'VersionB_00022*')\n\n@tf.function\ndef gen():\n    while True:\n        yield ([1, 0], [1, 0], [0, 1], [0, 1] )\n\nbatchNumber = batch_size\nrun_eagerly = False\n\nwith strategy.scope():\n    filenames = tf.io.gfile.glob(tfrecords_pattern_train)\n    train_dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=autotune)\n\n    filenames = tf.io.gfile.glob(tfrecords_pattern_train)\n    neg_dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=autotune)\n    \n    train_dataset = train_dataset.shuffle(150_000, seed=1000, reshuffle_each_iteration=True)\n    neg_dataset = neg_dataset.shuffle(150_000, seed=2000, reshuffle_each_iteration=True)\n\n    train_datasetC = tf.data.Dataset.zip((train_dataset, neg_dataset))\n    train_datasetC = train_datasetC.map(get_samples, num_parallel_calls=autotune)\n    \n    train_datasetC = train_datasetC.shuffle(1024, seed=1000, reshuffle_each_iteration=True)\n    train_datasetC = train_datasetC.padded_batch(batchNumber, padding_values=(0, 0, 0, 0))\n\n    datasetLabels = tf.data.Dataset.from_generator(\n        gen,\n        (tf.int32, tf.int32, tf.int32, tf.int32),\n        (tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None]), tf.TensorShape([None])))\n    \n    datasetLabels = datasetLabels.batch(batchNumber)\n\n    train_datasetFinal = tf.data.Dataset.zip((train_datasetC, datasetLabels))\n    train_datasetFinal = train_datasetFinal.prefetch(autotune)\n    train_datasetFinal = train_datasetFinal.repeat()\n    train_datasetFinal = train_datasetFinal.apply(tf.data.experimental.ignore_errors())\n\nmodel.fit(train_datasetFinal, steps_per_epoch=100, epochs=3)\n</code></pre>\n<p>And this is the only output I get</p>\n<pre><code>Epoch 1/3\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 215}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "neural-network", "tensorboard"], "owner": {"account_id": 1898217, "reputation": 7385, "user_id": 1714692, "user_type": "registered", "accept_rate": 43, "profile_image": "https://www.gravatar.com/avatar/bd72cbc200eb7c9aa507624f154b21a7?s=256&d=identicon&r=PG", "display_name": "Francesco Boi", "link": "https://stackoverflow.com/users/1714692/francesco-boi"}, "is_answered": false, "view_count": 245, "answer_count": 0, "score": 1, "last_activity_date": 1592572147, "creation_date": 1592564022, "last_edit_date": 1592572147, "question_id": 62468697, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62468697/how-to-group-the-histograms-shown-on-tensorboard-in-keras", "title": "How to group the histograms shown on tensorboard in Keras?", "body": "<p>I am trying to come to grips with the graphs shown on tensorboard when using Keras and trying to tune hyperparameters using <code>HPARAMS</code>.</p>\n\n<p>As an example take a NN with dense layers and batch normalization layers to classify the MNIST digits from <code>0</code> to <code>4</code>.\nThe function producing the summaries shown on tensorboard is the following:</p>\n\n<pre><code>def run(run_dir, hparams):\n    with tf.summary.create_file_writer(run_dir).as_default() as summ:\n        hp.hparams(hparams)  # record the values used in this trial\n        num_layers = 5\n        m = train_test_model(hparams, run_dir, num_layers=num_layers)\n        with tf.name_scope(\"init_lr_{}\".format(\n                 hparams[HP_INITIAL_LEARNING_RATE])):\n            for i,l in enumerate(m.layers[1:]):\n                #print(l.get_config())\n                if \"batch_normalization\" in l.get_config()['name']:\n                    w, b,c,d = l.get_weights()\n                    tf.summary.histogram(name='W%d'%i, data=w, step=1, description=\"weigths of \" + l.get_config()['name'])\n                    tf.summary.histogram(name='B%d'%i, data=b, step=1, description=\"biases of \" + l.get_config()['name'])\n                else:\n                    w, b = l.get_weights()\n                    tf.summary.histogram(name='W%d'%i, data=w, step=1, description=\"weigths of \" + l.get_config()['name'])\n                    tf.summary.histogram(name='B%d'%i, data=b, step=1, description=\"biases of \" + l.get_config()['name'])\n</code></pre>\n\n<p><code>train_test_model</code> is a function that trains different models using <code>hp.Hparam</code> for the hyperparameter variables. The problem is with the histograms. With this code the histogram graphs (of weights and biases) are named sequentially: so for example for the first model I have from <code>W_1</code> to <code>W_6</code> and from <code>B1</code> to <code>B_6</code>, for the second from <code>W7</code> to <code>W_12</code>. The model they belong to is shown at the top of the corresponding figure, but I think it is better to group the weights and biases according to the model they belong to and within each model have the histograms names from <code>W_1</code> to <code>W_6</code> and from <code>B1</code> to <code>B_6</code>.</p>\n\n<p>An example of what I get is the following where each model actually has only 5 Dense layers and 5 Batch normalization layers (excluding the one after the flatten layer):\n<a href=\"https://i.stack.imgur.com/Dje3r.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Dje3r.png\" alt=\"enter image description here\"></a></p>\n\n<p>To solve this problem I tried to add <code>with tf.name_scope(\"init_lr_{}\".format(\nhparams[HP_INITIAL_LEARNING_RATE])):</code> but this does not solve the problems: it keeps showing the previous graphs with the previous names and shows other graphs as in the following picture, having a different layout:</p>\n\n<p><a href=\"https://i.stack.imgur.com/ZCg7I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ZCg7I.png\" alt=\"enter image description here\"></a></p>\n\n<p>How can I make tensorboard group the histograms according to the NN model they belong to? Should I create a specific folder for each model trained?</p>\n\n<p><strong>FULL CODE</strong></p>\n\n<pre><code>#SET UP\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorboard.plugins.hparams import api as hp\nimport os\nkeras.backend.clear_session()\n\n#LOAD DATA\n(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\nX_train_reduced = X_train_full[y_train_full&lt;5]\ny_train_reduced = y_train_full[y_train_full&lt;5]\nX_test_reduced = X_test[y_test&lt;5]\ny_test_reduced = y_test[y_test&lt;5]\nX_train = X_train_reduced[5000:]\ny_train = y_train_reduced[5000:]\nX_valid = X_train_reduced[:5000]\ny_valid = y_train_reduced[:5000]\n\n#set the hyperparameters to tune\nkeras.backend.clear_session()\nHP_INITIAL_LEARNING_RATE = hp.HParam(\"initial_learning_rate\", hp.Discrete([0.0001, 0.00012, 0.00015]))\nHP_NUM_BATCH_SIZE        = hp.HParam(\"batch_size\", hp.Discrete([32]))\nHP_NUM_EPOCHS            = hp.HParam(\"epochs\", hp.Discrete([180]))\nHP_BETA_1                = hp.HParam(\"beta_1\", hp.Discrete([0.95]))\nHP_BETA_2                = hp.HParam(\"beta_2\", hp.Discrete([0.9994]))\nHP_DECAY_STEP            = hp.HParam(\"decay_step\", hp.Discrete([10000]))\nHP_DECAY_RATE            = hp.HParam(\"decay_rate\", hp.Discrete([0.8]))\n\n\n#function creating the summaries \ndef run(run_dir, hparams):\n    with tf.summary.create_file_writer(run_dir).as_default() as summ:\n        hp.hparams(hparams)  # record the values used in this trial\n        num_layers = 5\n        m = train_test_model(hparams, run_dir, num_layers=num_layers)\n        with tf.name_scope(\"init_lr_{}\".format(\n                 hparams[HP_INITIAL_LEARNING_RATE])):\n            for i,l in enumerate(m.layers[1:]):\n                #print(l.get_config())\n                if \"batch_normalization\" in l.get_config()['name']:\n                    w, b,c,d = l.get_weights()\n                    tf.summary.histogram(name='W%d'%i, data=w, step=1, description=\"weigths of \" + l.get_config()['name'])\n                    tf.summary.histogram(name='B%d'%i, data=b, step=1, description=\"biases of \" + l.get_config()['name'])\n                    #tf.summary.histogram(name='3rd type of weight in batch %d'%i, data=c, step=1, description=\"3rd argument of \" + l.get_config()['name'])\n                    #tf.summary.histogram(name='4th type of weight in batch %d'%i, data=d, step=1, description=\"4th argument of \" + l.get_config()['name'])\n                else:\n                    w, b = l.get_weights()\n                    tf.summary.histogram(name='W%d'%i, data=w, step=1, description=\"weigths of \" + l.get_config()['name'])\n                    tf.summary.histogram(name='B%d'%i, data=b, step=1, description=\"biases of \" + l.get_config()['name'])\n\n#function creating different models according to the hyperparameters passed with \n# hparams\ndef train_test_model(hparams, folder, num_layers=5):\n    act_fun = \"elu\"\n    initializer = \"he_normal\"\n    NAME = \"MNIST_0-4-earl_stp-nDens_{}-{}-{}-batch_norm-btch_sz_{}-epo_{}-Adam_opt-b1_{}-b2_{}-lr_exp_dec-in_lr_{}-lr_decRate_{}-lr_decStep_{}\".format(\n        num_layers, act_fun, initializer, hparams[HP_NUM_BATCH_SIZE], hparams[HP_NUM_EPOCHS], hparams[HP_BETA_1], \n               hparams[HP_BETA_2], hparams[HP_INITIAL_LEARNING_RATE], hparams[HP_DECAY_RATE], hparams[HP_DECAY_STEP])\n    model = keras.models.Sequential()\n    model.add(keras.layers.Flatten(input_shape=[28,28]))\n    input_size=28**2\n    model.add(keras.layers.BatchNormalization())\n    hidden_layer_neurons=100\n    for i in range(num_layers):\n        model.add(keras.layers.Dense(hidden_layer_neurons, activation=act_fun, kernel_initializer=\"he_normal\"))\n        model.add(keras.layers.BatchNormalization())\n\n    model.add(keras.layers.Dense(5, activation='softmax'))\n    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate = hparams[HP_INITIAL_LEARNING_RATE],\n        decay_steps=hparams[HP_DECAY_STEP],\n        decay_rate=hparams[HP_DECAY_RATE])\n    my_opt = keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=hparams[HP_BETA_1], beta_2 = hparams[HP_BETA_2])\n    early_stopping_cb =   keras.callbacks.EarlyStopping(patience = 10, restore_best_weights=True)\n    #model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"GridSearchCV.h5\", save_best_only=True)\n    run_index = 1 # increment every time you train the model\n    logdir = os.path.join(os.curdir, folder, NAME)\n    tensorboard_cb = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n    callbacks = [\n        early_stopping_cb,\n        tensorboard_cb,  \n        hp.KerasCallback(logdir, hparams),  # log hparams\n        ]\n    model.compile(loss='sparse_categorical_crossentropy', metrics=[\"accuracy\"], optimizer=my_opt)\n    model.fit(X_train, y_train, epochs=hparams[HP_NUM_EPOCHS], batch_size=hparams[HP_NUM_BATCH_SIZE],\n              validation_data=(X_valid, y_valid), callbacks=callbacks)\n    return model\n\n\nsession_num = 0\n\nLOG_FOLDER='logs/batch_normalization/'\nfor num_batch in HP_NUM_BATCH_SIZE.domain.values:\n    for epoch in HP_NUM_EPOCHS.domain.values:\n        for beta_1 in HP_BETA_1.domain.values:\n            for beta_2 in HP_BETA_2.domain.values:\n                for initial_learning_rate in HP_INITIAL_LEARNING_RATE.domain.values:\n                    for decay_rate in HP_DECAY_RATE.domain.values:\n                        for decay_step in HP_DECAY_STEP.domain.values:\n                            hparams = {\n                                HP_NUM_BATCH_SIZE: num_batch,\n                                HP_NUM_EPOCHS: epoch,\n                                HP_BETA_1: beta_1,\n                                HP_BETA_2: beta_2,\n                                HP_INITIAL_LEARNING_RATE: initial_learning_rate,\n                                HP_DECAY_RATE: decay_rate,\n                                HP_DECAY_STEP: decay_step\n                            }\n                            #print('--- Starting trial: %s' % run_name)\n                            print({h.name: hparams[h] for h in hparams})\n                            run(LOG_FOLDER , hparams)\n                            session_num += 1\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 282}]
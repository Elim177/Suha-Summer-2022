[{"items": [{"tags": ["tensorflow", "cluster-analysis", "autoencoder", "encoder", "dimensionality-reduction"], "owner": {"account_id": 15246842, "reputation": 461, "user_id": 11001493, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/a0124f494cd5bb28e0618650e575ccab?s=256&d=identicon&r=PG&f=1", "display_name": "user026", "link": "https://stackoverflow.com/users/11001493/user026"}, "is_answered": true, "view_count": 116, "accepted_answer_id": 63133577, "answer_count": 1, "score": 1, "last_activity_date": 1595937372, "creation_date": 1595887852, "question_id": 63124350, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63124350/why-do-i-get-unstable-values-in-an-encoded-dataframe-for-each-time-i-run-an-auto", "title": "Why do I get unstable values in an encoded dataframe for each time I run an autoencoder?", "body": "<p>I'm trying to find an optimal number of clusters on my data with elbow method and silhouette score while using KMeans. Although, I'm testing these methods using dimensionality reduction.</p>\n<p>If I try PCA several times, I will get the same graphs for elbow method and silhouette every time. But if I try an encoder with a neural net structure for the same purpose, I get different graphs every time. And consequentely, I don't have confidence to use this encoder technique as it results differents optimal clusters numbers.</p>\n<p>Why this happens? Even if I normalize my data, the results keep varying.</p>\n<p>What can I do to use this encoder technique properly? I know I could simply choose PCA for this, but I would like to understand and see if I'm doing something wrong.</p>\n<p>Here is my code and you can run it several times to see what I'm talking about. I used iris dataset as an example:</p>\n<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn import datasets\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import backend as K\n\niris = datasets.load_iris()\nX = pd.DataFrame(iris.data)\n\ndef autoencoding(data):\n    n_input_layer = data.shape[1]\n    n_encoding_layer = 2\n    n_output_layer = n_input_layer\n\n    # AUTOENCODER\n    autoencoder = tf.keras.models.Sequential([\n        # ENCODER\n        Dense(n_input_layer, input_shape = (n_input_layer,), activation = 'relu'),   # Input layer    \n    \n        # CENTRAL LAYER\n        Dense(n_encoding_layer, activation = 'relu', name = 'central_layer'), \n    \n        # DECODER\n        Dense(n_output_layer, activation = 'relu')  # Output layer\n    ])\n\n    n_epochs = 2000\n    loss = tf.keras.losses.MeanSquaredError()\n    optimizer = tf.optimizers.Adam(learning_rate = 0.001, decay = 0.0001, clipvalue = 0.5)\n\n    loss_history = []  # save loss improvement\n\n    data = np.array(data, dtype=np.float)\n\n    for epoch in range(n_epochs):\n    \n        with tf.GradientTape() as tape:\n            current_loss = loss(autoencoder(data), data)\n        \n        gradients = tape.gradient(current_loss, autoencoder.trainable_variables)    # get the gradient of the loss function\n        optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))  # update the weights\n    \n        loss_history.append(current_loss.numpy())  # save current loss in its history\n    \n        # show loss improvement every 200 epochs\n        if (epoch+1) % 200 == 0:\n            print(str(epoch+1) + '.\\tLoss: ' + str(current_loss.numpy()))\n\n    print('\\nEncoding complete')\n    return autoencoder\n\nX_autoencoded = autoencoding(X)\n\n# ENCODER EXTRACTION\ndef encoded(autoencoder, data):\n\n    # create a Keras function\n    extract_encoded_data = K.function(inputs = autoencoder.layers[0].input, \n                                  outputs = autoencoder.layers[1].output)\n    # extract encoded dataframe\n    encoded_dataframe = extract_encoded_data(data.values)\n    encoded_data = pd.DataFrame(encoded_dataframe)\n    return encoded_data\n\nX_encoded = encoded(X_autoencoded, X)\n\n# ELBOW METHOD AND SILHOUETTE SCORE\ninertia =[]\nsil =[]\n\nfor k in range(2,14):\n    kmeans_rand = KMeans(n_clusters=k, init='k-means++', random_state=42)\n    kmeans_rand.fit(X_encoded)\n    y_pred = kmeans_rand.predict(X_encoded)\n\n    inertia.append(kmeans_rand.inertia_)\n    sil.append((k, silhouette_score(X_encoded, y_pred)))\n\nsil_samples = silhouette_samples(X_encoded, y_pred)\n\nfig, ax = plt.subplots(1, 2, figsize=(12,4))\nax[0].plot(range(2,14), inertia)\nax[0].set_title('Elbow Method')\nax[0].set_xlabel('Number of clusters')\nax[0].set_ylabel('Inertia')\n\nx_sil = [x[0] for x in sil]\ny_sil = [x[1] for x in sil]\nax[1].plot(x_sil, y_sil)\nax[1].set_xlabel('Number of Clusters')\nax[1].set_ylabel('Silhouetter Score')\nax[1].set_title('Silhouetter Score Curve')\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 205}]
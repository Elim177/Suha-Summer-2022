[{"items": [{"tags": ["tensorflow", "machine-learning", "linear-regression", "porting"], "owner": {"account_id": 8886555, "reputation": 41, "user_id": 6634635, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-N-flIG9bSvg/AAAAAAAAAAI/AAAAAAAAARM/NtuaXAIhjEU/photo.jpg?sz=256", "display_name": "Commander Asdasd", "link": "https://stackoverflow.com/users/6634635/commander-asdasd"}, "is_answered": true, "view_count": 143, "accepted_answer_id": 64942713, "answer_count": 1, "score": 0, "last_activity_date": 1616900404, "creation_date": 1604071156, "last_edit_date": 1616900404, "question_id": 64611137, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64611137/port-tensorflow-1-code-to-tensorflow-2-model-learning-process-without-sess-run", "title": "port TensorFlow 1 code to TensorFlow 2 (model learning process without sess.run)", "body": "<p>I have this piece of tf1 code, which was taken from nice book &quot;Deep Learning&quot; by S. Nikolenko.</p>\n<p>It's a simple linear regression that learns <code>k</code> and <code>b</code> to 2 and 1 respectively.</p>\n<pre><code>%tensorflow_version 1.x\n\nimport numpy as np,tensorflow as tf\nimport pandas as pd\n\nn_samples, batch_size, num_steps = 1000, 100, 20000 #set learning constants\nX_data = np.random.uniform(1, 10, (n_samples, 1)) #generate array x from 1 to 10 of shape (1000,1)\nprint(X_data.shape)\ny_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1)) #generate right answer and add noise to it (to make it scatter)\n\nX = tf.placeholder(tf.float32, shape=(batch_size, 1)) #defining placeholders to put into session.run\ny = tf.placeholder(tf.float32, shape=(batch_size, 1))\n\nwith tf.variable_scope('linear-regression'):\n  k = tf.Variable(tf.random_normal((1, 1)), name='slope') #defining 2 variables with shape (1,1)\n  b = tf.Variable(tf.zeros((1,)), name='bias') # and (1,)\n  print(k.shape,b.shape)\n\ny_pred = tf.matmul(X, k) + b # all predicted y in batch, represents linear formula k*x + b\nloss = tf.reduce_sum((y - y_pred) ** 2)  # mean square\noptimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(loss)\ndisplay_step = 100\n\nwith tf.Session() as sess:\n  sess.run(tf.initialize_variables([k,b]))\n  for i in range(num_steps):\n    indices = np.random.choice(n_samples, batch_size) # taking random indices\n    X_batch, y_batch = X_data[indices], y_data[indices] # taking x and y from generated examples\n    _, loss_val, k_val, b_val = sess.run([optimizer, loss, k, b ],\n      feed_dict = { X : X_batch, y : y_batch })\n    if (i+1) % display_step == 0:\n      print('Epoch %d: %.8f, k=%.4f, b=%.4f' %\n        (i+1, loss_val, k_val, b_val))\n\n</code></pre>\n<p>I'm striving to port it on TensorFlow 2</p>\n<p>And for long time I can't wrap my head what should I use instead of <code>sess.run()</code> and <code>feed_dict</code>, which doing magic behind the scenes, official documentation go into to details with writing model class and so on, but I'm want to keep this as flat as possible.</p>\n<p>Also it's suggested to calculate derivatives with <code>tf.GradientTape</code>, but I'm struggling with applying it right to my example</p>\n<pre><code>%tensorflow_version 2.x\n\nimport numpy as np,tensorflow as tf\nimport pandas as pd\n\nn_samples, batch_size, num_steps = 1000, 100, 200\nX_data = np.random.uniform(1, 10, (n_samples, 1))\ny_data = 2 * X_data + 1 + np.random.normal(0, 2, (n_samples, 1))\n\nX = tf.Variable(tf.zeros((batch_size, 1)), dtype=tf.float32, shape=(batch_size, 1))\ny = tf.Variable(tf.zeros((batch_size, 1)), dtype=tf.float32, shape=(batch_size, 1))\n\nk = tf.Variable(tf.random.normal((1, 1)), name='slope')\nb = tf.Variable(tf.zeros((1,)), name='bias')\n\nloss = lambda: tf.reduce_sum((y - (tf.matmul(X, k) + b)) ** 2)\noptimizer = tf.keras.optimizers.SGD(0.01).minimize(loss, [k, b, X, y])\ndisplay_step = 100\n\n\nfor i in range(num_steps):\n  indices = np.random.choice(n_samples, batch_size)\n  X_batch, y_batch = X_data[indices], y_data[indices]\n  \n</code></pre>\n<p>I need SGD optimizer minimize that given loss function and learn k and b values, how can I achieve it from this point?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 154}]
[{"items": [{"tags": ["tensorflow", "deep-learning", "tensorflow2.0"], "owner": {"account_id": 13492482, "reputation": 437, "user_id": 9734248, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c8fd8584be5278402a0034f62976afbe?s=256&d=identicon&r=PG&f=1", "display_name": "DY92", "link": "https://stackoverflow.com/users/9734248/dy92"}, "is_answered": true, "view_count": 202, "accepted_answer_id": 62397783, "answer_count": 1, "score": 1, "last_activity_date": 1592258819, "creation_date": 1592256021, "last_edit_date": 1592257678, "question_id": 62397236, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62397236/shapes-are-incompatible-at-the-last-records-of-tf-data-dataset-from-tensor-slice", "title": "Shapes are incompatible at the last records of tf.data.Dataset.from_tensor_slices", "body": "<p>I have implemented seq2seq translation model in Tensorflow 2.0</p>\n\n<p>But during training I get the following error:</p>\n\n<pre><code>ValueError: Shapes (2056, 10, 10000) and (1776, 10, 10000) are incompatible\n</code></pre>\n\n<p>I have 10000 records in my dataset. Starting from the first record untill 8224 records dimensions matches. But for the last 1776 records I get the error mentioned above just because my batch_size is bigger than remaining number of records. Here is my code:</p>\n\n<pre><code>max_seq_len_output = 10\nn_words = 10000\nbatch_size = 2056\n\nmodel = Model_translation(batch_size = batch_size,embed_size = embed_size,total_words = n_words , dropout_rate = dropout_rate,num_classes = n_words,embedding_matrix = embedding_matrix)\ndataset_train = tf.data.Dataset.from_tensor_slices((encoder_input,decoder_input,decoder_output))\ndataset_train = dataset_train.shuffle(buffer_size = 1024).batch(batch_size)\n\n\nloss_object = tf.keras.losses.CategoricalCrossentropy()#used in backprop\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')#mean of the losses per observation\ntrain_accuracy =tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n\n\n##### no @tf.function here \ndef training(X_1,X_2,y):\n    #creation of one-hot-encoding, because if I would do it out of the loop if would have RAM problem\n    y_numpy = y.numpy()\n    Y = np.zeros((batch_size,max_seq_len_output,n_words),dtype='float32')\n    for i, d in enumerate(y_numpy):\n        for t, word in enumerate(d):\n            if word != 0:\n                Y[i, t, word] = 1\n\n    Y = tf.convert_to_tensor(Y)\n    #predictions\n    with tf.GradientTape() as tape:#Trainable variables (created by tf.Variable or tf.compat.v1.get_variable, where trainable=True is default in both cases) are automatically watched. \n        predictions =  model(X_1,X_2)\n        loss = loss_object(Y,predictions)\n\n    gradients = tape.gradient(loss,model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n    train_loss(loss) \n    train_accuracy(Y,predictions)\n    del Y\n    del y_numpy\n\n\nEPOCHS = 70\n\nfor epoch in range(EPOCHS):\n    for X_1,X_2,y in dataset_train:\n       training(X_1,X_2,y)\n    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n    print(template.format(epoch+1,train_loss.result(),train_accuracy.result()*100))\n    # Reset the metrics for the next epoch\n    train_loss.reset_states()\n    train_accuracy.reset_states() \n</code></pre>\n\n<p>How can I fixt this problem?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 6}]
[{"items": [{"tags": ["tensorflow", "customization", "loss-function"], "owner": {"account_id": 15085414, "reputation": 147, "user_id": 11057642, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bcc1147639cc503c5323ed4503df7c2c?s=256&d=identicon&r=PG", "display_name": "Keren", "link": "https://stackoverflow.com/users/11057642/keren"}, "is_answered": false, "view_count": 1183, "answer_count": 1, "score": 1, "last_activity_date": 1596040038, "creation_date": 1565122913, "last_edit_date": 1565207703, "question_id": 57383467, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57383467/getting-an-all-none-gradient-in-my-custom-loss-and-gradient-code-in-tensorflow-2", "title": "Getting an all None gradient in my custom loss and gradient code in tensorflow 2.0", "body": "<p>I am trying to write a pretty darn basic loss function in tensorflow 2.0. In summary, I have 5 classes and I want to train without grouping any of them, using one hot encoding. I want my model to predict each input with a value for each of the 5 classes. Afterwards, I would like to try and get the two highest values, and if they are either 3 or 4, I would like to classify it as \"good\" and if it's not then \"bad.\" Lastly, I want my loss to be 1-precision where the precision as I said has true positives in the following situations:\n1. Model guessed 3 and real class was 3\n2. Model guessed 3 and real class was 4\n3. Model guessed 4 and real class was 3\n4. Model guessed 4 and real class was 4</p>\n\n<p>Again, I know I could just change the labels of my data but I would rather not do that.\nI used some nice already written metric to write my loss, here it is:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#@tf.function\ndef my_loss(output,real,threeandfour=1,weights=loss_weights,mod=m):\n  m = tf.keras.metrics.TruePositives(thresholds=0.5)\n  m.update_state(real,output,sample_weight=weights)\n  shape_0=tf.shape(output)[0]\n  #shape_1=tf.constant(2,dtype=tf.int32)\n  shape_1=2\n  halfs=tf.math.multiply(tf.constant(0.5,dtype=tf.float32),tf.ones((shape_0,shape_1),dtype=tf.float32))\n  thrsfrs_1=output[:,2:4]\n  thrsfrs=tf.cast(thrsfrs_1,dtype=tf.float32)\n  logs_1=tf.math.greater(thrsfrs,halfs)\n  logs=tf.cast(logs_1,dtype=tf.float32)\n  print('shape of log: ',np.shape(logs))\n  print('few logs: ',logs,)\n\n  num_of_3_4s_in_model=tf.reduce_sum(logs)\n  prec_1=tf.math.divide(m.result(),num_of_3_4s_in_model)\n  prec=tf.cast(prec_1,dtype=tf.float32)\n  return tf.math.subtract(tf.constant(1,dtype=tf.float32),prec)\n</code></pre>\n\n<p>The gradient function:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>with tf.GradientTape() as tape:\n      tape.watch(model.trainable_variables)\n      y_=model(X_train)\n      print('y_: ',y_)\n      loss_value=my_loss(y_,tf_one_hot_train,mod=m,weights=loss_weights)\n      #loss_value=tf.cast(loss_value,dtype=tf.float32)\n      print('loss_value: ',loss_value)\ngrads=tape.gradient(loss_value,model.trainable_variables)\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\n</code></pre>\n\n<p>It does succeed in getting a loss value which is tensorflow and seems alright. This is the gradient and error I'm getting:</p>\n\n<pre><code>python\ngot grads\n[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-370-2f8f4b783a7b&gt; in &lt;module&gt;()\n     23 \n     24 #optimizer.apply_gradients(zip(grads, model.trainable_variables), global_step)\n---&gt; 25 optimizer.apply_gradients(zip(grads, model.trainable_variables))\n     26 \n     27 #print(\"Step: {},         Loss: {}\".format(global_step.numpy(),\n\n1 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)\n    973   if not filtered:\n    974     raise ValueError(\"No gradients provided for any variable: %s.\" %\n--&gt; 975                      ([v.name for _, v in grads_and_vars],))\n    976   if vars_with_empty_grads:\n    977     logging.warning(\n\nValueError: No gradients provided for any variable: ['dense_40/kernel:0', 'dense_40/bias:0', 'dense_41/kernel:0', 'dense_41/bias:0', 'dense_42/kernel:0', 'dense_42/bias:0', 'dense_43/kernel:0', 'dense_43/bias:0', 'dense_44/kernel:0', 'dense_44/bias:0', 'dense_45/kernel:0', 'dense_45/bias:0', 'dense_46/kernel:0', 'dense_46/bias:0', 'dense_47/kernel:0', 'dense_47/bias:0']\n\n</code></pre>\n\n<p>I've tried to include the @tf.function, I tried to turn the 2 into an int, etc. I also tried to do it with many different other functions like the tf.confusion_matrix or even without anything, including just tf.arg_max and stuff like that. Nothing seemed to work.</p>\n\n<p>I'm adding the most tensorflow-y code for my loss that I could think of. The same thing keeps happening. I use it with tensorflow objects, numpy objects, I checked that my inputs are from zero to one, still None gradients. Here is my tensorflowy loss:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#@tf.function\ndef my_loss(real,output):\n  threeandfour=tf.constant(1,dtype=tf.float32)\n  #turning real into real classes (opposite of one hot encoding)\n  real_classes=tf.argmax(real,axis=1)\n  real_classes=tf.cast(real_classes,dtype=tf.float32)\n  #tf.print('real_classes: ',real_classes)\n\n  pred_classes=tf.argmax(output,axis=1)\n  pred_classes=tf.cast(pred_classes,dtype=tf.float32)\n  #tf.print('pred_classes: ',pred_classes)\n\n  #checking how many 3s and 4s there are in both\n  good_real=(tf.logical_or(tf.equal(real_classes,3),tf.equal(real_classes,4)))\n  good_real=tf.cast(good_real,dtype=tf.float32)\n  #tf.print('good_real: ',good_real)\n\n  good_pred=(tf.logical_or(tf.equal(pred_classes,3),tf.equal(pred_classes,4)))\n  good_pred=tf.cast(good_pred,dtype=tf.float32)\n  #tf.print('good_pred: ',good_pred)\n\n  #which ones do the real and model agree on\n  same=tf.math.equal(good_pred,good_real)\n  same=tf.cast(same,dtype=tf.float32)\n  #print('same: ',same)\n\n  #which ones do they both think are good (3 and 4)\n  same_goods=tf.math.multiply(same,good_pred)\n  same_goods=tf.cast(same_goods,dtype=tf.float32)\n  #print('same goods: ',same_goods)\n\n  #number of ones they both think are good\n  num_same_goods=tf.reduce_sum(same_goods)\n  num_same_goods=tf.cast(num_same_goods,dtype=tf.float32)\n  #print('num_same_goods: ',num_same_goods)\n\n  #number of ones model thinks are good\n  num_pred_goods=tf.reduce_sum(good_pred)\n  num_pred_goods=tf.cast(num_pred_goods,dtype=tf.float32)\n  #print('num_pred_goods: ',num_pred_goods)\n\n  #making sure not to divide by 0\n  non_zero_num=tf.math.add(num_pred_goods,tf.constant(0.0001,dtype=tf.float32))\n  #precision\n  prec=tf.math.divide(num_same_goods,non_zero_num)\n  prec=tf.cast(prec,dtype=tf.float32)\n  #tf.print('prec: ',prec)\n  #1-precision\n  one_minus_prec=tf.math.subtract(tf.constant(1,dtype=tf.float32),prec)\n  one_minus_prec=tf.cast(one_minus_prec,dtype=tf.float32)\n\n  return one_minus_prec\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 98}]
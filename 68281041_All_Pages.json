[{"items": [{"tags": ["python", "tensorflow", "lstm"], "owner": {"account_id": 18007049, "reputation": 130, "user_id": 13087576, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GipGpDYDFj3dehQ5o2CUsRi7fvGwahd0fx_ZAtc=k-s256", "display_name": "Sai Prashanth", "link": "https://stackoverflow.com/users/13087576/sai-prashanth"}, "is_answered": false, "view_count": 479, "answer_count": 1, "score": 3, "last_activity_date": 1626433570, "creation_date": 1625639202, "question_id": 68281041, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68281041/invalidargumenterror-on-custom-tensorflow-model", "title": "InvalidArgumentError on custom tensorflow model", "body": "<p>As a part of <a href=\"https://www.kaggle.com/c/tabular-playground-series-jul-2021\" rel=\"nofollow noreferrer\">TPS July challenge</a>, I was trying to implement a custom tensorflow model based on Recurrent neural networks</p>\n<p><em>Idea:</em> I wanted to include an RNN, which predicts values at current iteration, based the model's prediction at previous iteration. So, I implemented a custom Model, which saves output of current iteration, to be fed to the model's LSTM layer in the next Iteration.</p>\n<p>However, if I call the model's fit method, I got the following error</p>\n<pre><code>InvalidArgumentError                      Traceback (most recent call last)\n&lt;ipython-input-29-0457da000b62&gt; in &lt;module&gt;\n----&gt; 1 model.fit(train_x,train_labels,epochs=100)\n\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n   1098                 _r=1):\n   1099               callbacks.on_train_batch_begin(step)\n-&gt; 1100               tmp_logs = self.train_function(iterator)\n   1101               if data_handler.should_sync:\n   1102                 context.async_wait()\n\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n    826     tracing_count = self.experimental_get_tracing_count()\n    827     with trace.Trace(self._name) as tm:\n--&gt; 828       result = self._call(*args, **kwds)\n    829       compiler = &quot;xla&quot; if self._experimental_compile else &quot;nonXla&quot;\n    830       new_tracing_count = self.experimental_get_tracing_count()\n\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\n    886         # Lifting succeeded, so variables are initialized and we can run the\n    887         # stateless function.\n--&gt; 888         return self._stateless_fn(*args, **kwds)\n    889     else:\n    890       _, _, _, filtered_flat_args = \\\n\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)\n   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n   2942     return graph_function._call_flat(\n-&gt; 2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n   2944 \n   2945   @property\n\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\n   1917       # No tape is watching; skip to running the function.\n   1918       return self._build_call_outputs(self._inference_function.call(\n-&gt; 1919           ctx, args, cancellation_manager=cancellation_manager))\n   1920     forward_backward = self._select_forward_and_backward_functions(\n   1921         args,\n\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)\n    558               inputs=args,\n    559               attrs=attrs,\n--&gt; 560               ctx=ctx)\n    561         else:\n    562           outputs = execute.execute_with_cancellation(\n\n/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)\n     58     ctx.ensure_initialized()\n     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n---&gt; 60                                         inputs, attrs, num_outputs)\n     61   except core._NotOkStatusException as e:\n     62     if name is not None:\n\nInvalidArgumentError:  Can not squeeze dim[0], expected a dimension of 1, got 32\n     [[{{node lstm_model/weight_normalization_15/cond/else/_1/lstm_model/weight_normalization_15/cond/data_dep_init/moments/Squeeze}}]] [Op:__inference_train_function_11775]\n\nFunction call stack:\ntrain_function\n</code></pre>\n<p>Is my method of utilizing the model correct? If not, what would be a better implementation of my Idea?</p>\n<p><em>My Code:</em></p>\n<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\n\ntrain_labels = train_data[['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']].copy()\ntrain_x = train_data.drop(['target_carbon_monoxide','target_benzene','target_nitrogen_oxides','date_time'],axis=1)\ntrain_x.head()\ntrain_labels = np.asarray(train_labels).reshape(-1,1,3)\n\n\ncurroutput = tf.Variable(shape=(1,3),initial_value=[[0.0,0.0,0.0]],dtype=tf.float32)\nclass CompleteModel(keras.Model):\n    def train_step(self, data):\n        x,y = data\n#         x = tf.reshape(self.curroutput,shape=(1,1,3))\n        \n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            loss = self.compiled_loss(y_pred,y, regularization_losses=self.losses)\n\n        global curroutput\n        curroutput.assign(y_pred)\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        self.compiled_metrics.update_state(y, y_pred)\n        return {m.name: m.result() for m in self.metrics}\n\n\n\nclass RNNInputLayer(keras.layers.Layer):\n    def __init__(self):\n        super(RNNInputLayer,self).__init__()\n    def call(self,inputs):\n        global curroutput\n        return tf.reshape(curroutput,shape=(1,1,3))\n\n\n\ndef make_model():\n    input_layer = layers.Input(shape=8,batch_size=1)\n    dense_in = tfa.layers.WeightNormalization(layers.Dense(16,activation='selu'))(input_layer)\n    dense_in2 = tfa.layers.WeightNormalization(layers.Dense(32,activation='selu'))(dense_in)\n    dense_out = tfa.layers.WeightNormalization(layers.Dense(8,activation='selu'))(dense_in)\n    rnn_input = RNNInputLayer()(input_layer)\n    lstm_layer = layers.LSTM(units=16,input_shape=(1,3))(rnn_input)\n    lstm_dense = tfa.layers.WeightNormalization(layers.Dense(16,activation='selu'))(lstm_layer)\n    finalconcat = layers.Concatenate()([dense_out,lstm_dense])\n    final_dense = tfa.layers.WeightNormalization(layers.Dense(16,activation='selu'))(finalconcat)\n    output_layer = layers.Dense(3)(final_dense)\n\n    model = CompleteModel(inputs=input_layer,outputs=output_layer,name='lstm_model')\n    return model\nmodel = make_model()\nmodel.compile(loss=tf.keras.losses.MeanSquaredLogarithmicError(),optimizer='Adam')\n\n\n\nmodel.fit(train_x,train_labels,epochs=100) #error\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 110}]
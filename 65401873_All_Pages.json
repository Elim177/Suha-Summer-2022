[{"items": [{"tags": ["tensorflow", "keras", "deep-learning"], "owner": {"account_id": 3732765, "reputation": 281, "user_id": 3910261, "user_type": "registered", "accept_rate": 83, "profile_image": "https://www.gravatar.com/avatar/44a8664e824e47be9e3e7a78250b23f6?s=256&d=identicon&r=PG&f=1", "display_name": "jackaraz", "link": "https://stackoverflow.com/users/3910261/jackaraz"}, "is_answered": true, "view_count": 400, "accepted_answer_id": 65426108, "answer_count": 1, "score": 0, "last_activity_date": 1608734487, "creation_date": 1608597835, "last_edit_date": 1608638176, "question_id": 65401873, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65401873/problem-with-custom-loss-functions-to-solve-differential-equations-with-tensorfl", "title": "Problem with custom loss functions to solve differential equations with Tensorflow/Keras", "body": "<p>I'm trying to reproduce the results from <a href=\"https://arxiv.org/abs/1902.05563\" rel=\"nofollow noreferrer\">1902.05563</a>. One can find an example in <a href=\"https://www.programmersought.com/article/4460831401/\" rel=\"nofollow noreferrer\">this link</a> which is solving the eq. 4 in the paper. However, they are writing the layers by hand via variables and placeholders (note that it's using tf v1). Instead of writing the network from scratch, I tried to use Keras to achieve the same thing;</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy      as np\nimport tensorflow as tf\nx1     = tf.keras.Input(name='x_1',shape=(1),dtype=tf.dtypes.float32)\ndense1 = tf.keras.layers.Dense(10,activation=tf.nn.sigmoid,name='l1')(x1)\noutput = tf.keras.layers.Dense(1,activation=None,name='output')(dense1)\nmodel  = tf.keras.Model(inputs=x1,outputs=output)\nmodel.summary()\n### output\nModel: &quot;functional_42&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nx_1 (InputLayer)             [(None, 1)]               0         \n_________________________________________________________________\nl1 (Dense)                   (None, 10)                20        \n_________________________________________________________________\noutput (Dense)               (None, 1)                 11        \n=================================================================\nTotal params: 31\nTrainable params: 31\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n<p>which I believe has the same structure as the link given above. Then I tried to construct the loss and optimizer as follows;</p>\n<pre class=\"lang-py prettyprint-override\"><code>x_train  = np.linspace(0,2,100,endpoint=True)#Generate 100 points in the [0,2] interval\nx_t      = np.zeros((len(x_train),1))\nx_t[:,0] = x_train\n\nx = tf.constant(x_t)\nwith tf.GradientTape(persistent=True) as tape:\n    variables = model.trainable_variables\n    tape.watch(x)\n    tape.watch(variables)\n    y_pred  = model(x,training=True)\n    #dy_dx = tape.gradient(y_pred,x)\n    #print(dy_dx)\ny = tf.reshape(y_pred,x.shape).numpy()\ndy_dx = tape.gradient(y_pred,x)\nA = (1+3*(x**2))/(1+x+x**3)\nt_loss = tf.reshape(dy_dx,x.shape) + (x + A)*y - x**3 - 2*x - A * x**2\nloss = tf.reshape(tf.reduce_mean(t_loss)+(y[0]-1)**2,())\n\nwith tf.GradientTape() as tape2:\n    tape2.watch(model.trainable_weights)\n    logits  = model(x,training=True)\ngrads = tape.gradient(loss,model.trainable_weights) # it has been tested with trainable_variables as well\n</code></pre>\n<p>Here <code>t_loss</code> is simply eq. 4 in <a href=\"https://arxiv.org/abs/1902.05563\" rel=\"nofollow noreferrer\">1902.05563</a> and <code>loss</code> additionally has the boundary condition implemented as <code>(y[0]-1)**2</code>. However since my <code>grads</code> are <code>[None, None, None, None]</code> my optimizer fails.</p>\n<pre class=\"lang-py prettyprint-override\"><code>optimizer = tf.keras.optimizers.Adam(0.01)\noptimizer.apply_gradients(zip(grads, model.trainable_weights))\n### output\nValueError: No gradients provided for any variable: ['l1/kernel:0', 'l1/bias:0', 'output/kernel:0', 'output/bias:0'].\n</code></pre>\n<p>would be great if anybody can show me how to write a working example of this differential eq solver. Thanks!</p>\n<hr />\n<p><strong>Additional Info</strong></p>\n<p>A similar question has been asked in <a href=\"https://stackoverflow.com/questions/50288258/derivative-in-loss-function-in-keras\">this thread</a> which I tried to rewrite my approach using it. So the modified approach is as follows using the same <code>model</code></p>\n<pre class=\"lang-py prettyprint-override\"><code>def _loss_tensor(y_true,y_pred,x_train):\n    x = tf.constant(x_train)\n    dy_dx = tf.keras.backend.gradients(y_pred,x)\n    lq = (1+3*(x1**2))/(1+x1+x1**3)\n    t_loss = (dy_dx+(x+lq)*y_pred-x**3-2*x-lq*x*x)\n    return tf.reduce_mean(t_loss)+(y_pred[0]-1)**2\ndef loss_func(x_train):\n    def loss(y_true,y_pred):\n        return _loss_tensor(y_true,y_pred,x_train)\n    return loss\noptimizer = tf.keras.optimizers.Adam(1e-2)\nmodel.compile(loss=loss_func(model.inputs), # tried x_t, model.input as well but all gave same result\n              optimizer=optimizer)\nmodel.train_on_batch(x_t)\n#also tried\n#model.fit(x_t,epochs=5,verbose=1)\n### Output\nValueError: No gradients provided for any variable: ['l1/kernel:0', 'l1/bias:0', 'output/kernel:0', 'output/bias:0'].\n</code></pre>\n<p>However I'm still getting the exact same error.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 247}]
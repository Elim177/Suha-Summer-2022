[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 17056333, "reputation": 11, "user_id": 12441811, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AAuE7mBTCPhiBaPorxxiNJSYa1o3heMvUAFWgwDJfuVU=k-s256", "display_name": "Milad Toutounchian", "link": "https://stackoverflow.com/users/12441811/milad-toutounchian"}, "is_answered": false, "view_count": 43, "answer_count": 1, "score": 1, "last_activity_date": 1589422675, "creation_date": 1582334194, "last_edit_date": 1582334675, "question_id": 60348152, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60348152/when-call-keras-model-then-there-is-no-difference-between-having-tf-function-o", "title": "when call Keras Model, then there is no difference between having @tf.function or not. But different when build low-level model", "body": "<p>Is this issue a bug?\nCompared the following two codes. If include @tf.function then both works well. If not include @tf.fucntion then the custom low-level model does not train.</p>\n\n<pre><code>model = tf.keras.Model(inputs=inputs, outputs=outputs)\n# @tf.function\ndef propagate(x_batch, y_batch):\n    \"\"\"\n    Complete both forward and backward propagation on our\n    batches.\n    \"\"\"    \n    # Record operations to automatically obtain the gradients\n    with tf.GradientTape() as tape:\n        logits = model(x_batch)\n        # Calculates the total loss of the entire network\n        loss = loss_fn(y_batch, tf.nn.softmax(logits))\n        # Compute the accuracy of our model\n        # (Convert our logits to softmax distribution)\n        accuracy(y_batch, tf.nn.softmax(logits))\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    return loss\n</code></pre>\n\n<h2>Compared to when we define custom low-level model:</h2>\n\n<pre><code>class Model(object):\n    def __init__(self):\n      self.weights, self.biases = self.initialize_weights_and_biases()\n      self.trainable_vars = list(self.weights.values()) + list(self.biases.values())\n    def initialize_weights_and_biases(self):\n\n      return out_layer\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 146}]
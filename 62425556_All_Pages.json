[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "deep-learning"], "owner": {"account_id": 5615269, "reputation": 391, "user_id": 6057582, "user_type": "registered", "profile_image": "https://graph.facebook.com/848350359/picture?type=large", "display_name": "Borun Chowdhury", "link": "https://stackoverflow.com/users/6057582/borun-chowdhury"}, "is_answered": false, "view_count": 933, "answer_count": 0, "score": 5, "last_activity_date": 1592392051, "creation_date": 1592385348, "last_edit_date": 1592392051, "question_id": 62425556, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62425556/decorating-a-custom-loss-with-tf-function-changes-the-training-results-completel", "title": "Decorating a custom loss with tf.function changes the training results completely, both in keras model.fit method as well as custom training loop", "body": "<p>I was trying to implement class_weight myself as I saw some behavior in the usual way of doing it (class_weight in model.fit method) and it led me to a see a behavior that is very confusing and after thinking about it for quite some time I cannot understand what is happening. </p>\n\n<p><em>The whole post is long so I give a brief description before giving the details for those interested</em></p>\n\n<p><strong>Brief Description</strong></p>\n\n<p>In brief, I define two custom loss functions that differ only in the tf.function decorator. The first one does not have the decorator while the second one does. Using keras model.fit, the training does not converge with the one that has the decorator. When I try the same with custom training loop the behavior is reversed. </p>\n\n<p>I can obviously see which is the right behavior by running everything in eager mode and so I can see that the custom loop with the decorate function is the right one. Its results also match the usual keras model fit method with class_weight. All the other three methods give wrong results but I cannot understand why. </p>\n\n<p><strong>Detailed description</strong></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def customloss1(y_true,y_pred,sample_weight=None):\n    weights=tf.constant([1.,1.,.1])[tf.newaxis,...]\n    y_true_one_hot=tf.one_hot(tf.cast(y_true,tf.uint8),3)\n    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true_one_hot*weights,\n                                                                   y_pred,\n                                                                   from_logits=False))\n\n@tf.function\ndef customloss2(y_true,y_pred,sample_weight=None):\n    weights=tf.constant([1.,1.,.1])[tf.newaxis,...]\n    y_true_one_hot=tf.one_hot(tf.cast(y_true,tf.uint8),3)\n    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true_one_hot*weights,\n                                                                   y_pred,\n                                                                   from_logits=False))\n</code></pre>\n\n<p>I have a function to create initial models in identical states and the model is very simple</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def make_model():\n    tf.random.set_seed(42)\n    np.random.seed(42)\n    model=tf.keras.Sequential([\n        tf.keras.layers.Dense(3,'softmax',input_shape=[1024,])\n    ])\n    return model\n</code></pre>\n\n<p>I instantiate a RMSProp optimizer (code not shown) and call it <em>optimizer</em> and then train the two models so</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model1=make_model()\nmodel2=make_model()\nmodel1.compile(loss=customloss1,optimizer=optimizer)\nmodel2.compile(loss=customloss2,optimizer=optimizer)\n\nhistory1 = model1.fit(x,y,epochs=100,batch_size=50, verbose=0)\nhistory2 = model2.fit(x,y,epochs=100,batch_size=50, verbose=0)\n</code></pre>\n\n<p>and the results are plotted below</p>\n\n<p><a href=\"https://i.stack.imgur.com/YDLjX.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/YDLjX.png\" alt=\"enter image description here\"></a></p>\n\n<p>as can be seen the model using the customloss with the decorator doesn't converge after a few epochs. I do not understand what is happening. To dig in to this deeper I decided to do the training loop manually so I made two training step functions that use two copies of the model and differ only in which custom loss function they use</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model1=make_model()\nmodel2=make_model()\n\n@tf.function\ndef train_step1(x,y):\n\n    with tf.GradientTape() as tape:\n        predictions  = model1(x)\n        loss = customloss1(y, predictions)\n\n    gradients = tape.gradient(loss, model1.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, model1.trainable_variables))\n    return loss\n\n@tf.function\ndef train_step2(x,y):\n\n    with tf.GradientTape() as tape:\n        predictions  = model2(x)\n        loss = customloss2(y, predictions)\n\n    gradients = tape.gradient(loss, model2.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, model2.trainable_variables))\n    return loss\n</code></pre>\n\n<p>This time the behavior is reversed! </p>\n\n<p><a href=\"https://i.stack.imgur.com/INkMR.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/INkMR.png\" alt=\"enter image description here\"></a></p>\n\n<p>In this case I can understand which one is the right answer by removing all tf.function decorators and running pure python with eager execution and it is the manual training loop with the decorated custom loss. Its results match the keras model fit with class_weight set to the correct values (The curves exactly match).</p>\n\n<p>But I have no idea why the aforementioned combination is giving the correct results. I was under the impression that if the training loop function has a decorator then all the function in the function stack automatically get converted to graph mode and any lower level tf.function decorators are redundant.</p>\n\n<p><strong>Extra Info</strong></p>\n\n<p>I investigated a bit more and turns out that for the custom look the <em>gradients that come out of the two loss functions are different</em>!!!</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model3=make_model()\n\n@tf.function\ndef get_gradients(x,y):\n    with tf.GradientTape() as tape1:\n        p1=model3(x)\n        l1=customloss1(y,p1)\n    with tf.GradientTape() as tape2:\n        p2=model3(x)\n        l2=customloss2(y,p2)\n\n    gradients1=tape1.gradient(l1,model3.trainable_variables)\n    gradients2=tape2.gradient(l2,model3.trainable_variables)\n\n    return gradients1, gradients2\n</code></pre>\n\n<p>gives</p>\n\n<pre><code>([&lt;tf.Tensor: shape=(1024, 3), dtype=float32, numpy=\n  array([[-0.01336379,  0.10262163, -0.01915502],\n         [ 0.07654451, -0.0181675 , -0.04819181],\n         [ 0.00367431, -0.06802277,  0.05872081],\n         ...,\n         [-0.13633026,  0.01184574,  0.02273583],\n         [ 0.02155258, -0.04340569,  0.0841853 ],\n         [ 0.17315787, -0.12444994, -0.04137734]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.05435816, 0.02056887, 0.28507292], dtype=float32)&gt;],\n [&lt;tf.Tensor: shape=(1024, 3), dtype=float32, numpy=\n  array([[-0.00059669,  0.06096834, -0.06037165],\n         [ 0.03078352,  0.00224353, -0.03302703],\n         [ 0.01101062, -0.04670432,  0.03569368],\n         ...,\n         [-0.12952083,  0.06808907,  0.06143178],\n         [ 0.03497609, -0.09745409,  0.06247801],\n         [ 0.131704  , -0.12800933, -0.00369466]], dtype=float32)&gt;,\n  &lt;tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.05939803, -0.10304251,  0.16244057], dtype=float32)&gt;])\n</code></pre>\n\n<p>The lower one is the correct one and if I remove the decorator on get_gradients then I get the same gradients for both the loss functions. So its some kind of weird interplay of tf.function in the gradient tape it seems.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 43}]
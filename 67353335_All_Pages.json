[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 10839573, "reputation": 209, "user_id": 7971339, "user_type": "registered", "accept_rate": 75, "profile_image": "https://www.gravatar.com/avatar/599529c1ef2114630e5a8d92b76b4299?s=256&d=identicon&r=PG&f=1", "display_name": "Vishwad", "link": "https://stackoverflow.com/users/7971339/vishwad"}, "is_answered": false, "view_count": 26, "answer_count": 1, "score": 0, "last_activity_date": 1619937163, "creation_date": 1619934220, "last_edit_date": 1619935628, "question_id": 67353335, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67353335/is-my-rnn-being-trained-only-on-1-or-2-samples", "title": "Is my RNN being trained only on 1 or 2 samples?", "body": "<p>I have created an LSTM-RNN of 7 cells. It reduces the loss but accuracy remains zero. I have been unable to find out why until I saw the keras training console output. Below is a sample from the latest training run.</p>\n<pre><code>Epoch 500/500\n2/2 [==============================] - 0s 13ms/step - loss: 0.1505 - accuracy: 0.0000e+00\n</code></pre>\n<p>Does 2/2 mean that training is happening only on two samples? I have 7168 datapoints and my batch size is explicitly stated as 7168 then why does it happen? Below is my code</p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas\nimport scipy.io as loader\nimport tensorflow as tf\nimport keras\nimport numpy\nimport time\nimport math\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nadditional_metrics = ['accuracy']\n\nloss_function = BinaryCrossentropy()\n\nnumber_of_epochs = 500\noptimizer = SGD()\nvalidation_split = 0.20\nverbosity_mode = 1\nmini = 0\nmaxi  = 0\nmean = 0\n&quot;&quot;&quot;\n\n&quot;&quot;&quot;\ndef myfunc(arg):\n    global mini, maxi, mean\n    return (arg - mean) / (maxi - mini)\n\n# k = 0\ncgm = numpy.load('cgm_train_new.npy')\nlabels = numpy.load('labels_train_new.npy')\nlabs = list()\ncgm_flat = cgm.flatten()\nmini = min(cgm_flat)\nmaxi = max(cgm_flat)\nmean = sum(cgm_flat) / len(cgm_flat)\ncgm = numpy.apply_along_axis(myfunc, 0, cgm)\n\nfor each in labels:\n    # suma = suma + sum(each)\n    if each[-1] == 1: labs.append(.99)\n    else: labs.append(.01)\n\nRNNmodel = Sequential()\n\nRNNmodel.add(LSTM(7, activation='tanh'))\nRNNmodel.add(Dense(1, activation='sigmoid'))\nRNNmodel.compile(optimizer=optimizer, loss=loss_function, metrics=additional_metrics)\ncgm_rs = numpy.reshape(cgm, [len(cgm), 7, 1])\nans = numpy.reshape(labs, [len(labs), 1, 1])\n\nhistory = RNNmodel.fit(\n    cgm_rs,\n    ans,\n    batch_size=7168,\n    epochs=number_of_epochs)#,\n    # verbose=verbosity_mode)#,\n                  #  validation_split=validation_split)\n\n\n\ntf.keras.utils.plot_model(\n    RNNmodel,\n    to_file=&quot;RNNmodel.png&quot;)\nanswers = RNNmodel.predict(cgm_rs)\n# for each in answers:\n    # print(each)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 153}]
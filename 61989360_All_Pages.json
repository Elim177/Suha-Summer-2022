[{"items": [{"tags": ["python", "list", "keras", "tuples", "tensorflow2.0"], "owner": {"account_id": 18588633, "reputation": 58, "user_id": 13546344, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6c02d180ea1149ee5e0d0dec963131da?s=256&d=identicon&r=PG&f=1", "display_name": "abhishek", "link": "https://stackoverflow.com/users/13546344/abhishek"}, "is_answered": false, "view_count": 553, "answer_count": 0, "score": 1, "last_activity_date": 1590339427, "creation_date": 1590339427, "question_id": 61989360, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61989360/error-in-using-model-fit-in-keras-with-custom-train-step", "title": "Error in using model.fit in keras with custom train_step", "body": "<pre><code>BATCH_SIZE =32\ndataset = tf.data.Dataset.from_tensor_slices((question, ans)).shuffle(1000)\ndataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\nclass EncoderDecoder(tf.keras.Model):\n  def __init__(self,vocab_input=1000,vocab_output=1000,BATCH_SIZE=32):\n    super().__init__()\n    #Encoder\n    self.encoder_vector = tokens.TextVectorization(vocab_input,output_sequence_length=24)\n    self.encoder_embedding = tf.keras.layers.Embedding(vocab_input,256)\n    self.encoder_lstm = tf.keras.layers.LSTM(512,return_sequences=True)\n\n    self.attention = tf.keras.layers.Attention()\n\n    #Decoder\n    self.decoder_vector = tokens.TextVectorization(vocab_output,output_sequence_length=20)\n    self.decoder_embedding = tf.keras.layers.Embedding(vocab_output,256)\n    self.fc = tf.keras.layers.Dense(vocab_output)\n    self.decoder_lstm = tf.keras.layers.LSTM(512,return_sequences=True)\n\n\n  def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n\n\n  def train_step(self,data):\n    loss =0\n    input = p[0] \n    targ = p[1]\n\n    input = self.encoder_vector(input)\n    targ = self.decoder_vector(targ)\n\n    with tf.GradientTape() as tape:\n      input = self.encoder_embedding(input)\n      enc_output,enc_h,enc_c = self.encoder_lstm(input)\n\n      attn_output = self.attention([enc_output,enc_h])\n\n\n      dec_h = tf.concat([tf.expand_dims(attn_output, 1), dec_h], axis=-1)\n      dec_input = tf.expand_dims([self.decoder_vector(['SOS'])]*BATCH_SIZE,1)\n      predictions = []\n\n      for t in range(1,targ.shape[1]):\n        dec_input = self.decoder_embedding(dec_input)\n        dec_output,dec_h,dec_c = self.decoder_lstm(dec_input,initial_states=[dec_h,enc_c])\n        predictions = self.fc(dec_output)\n\n        loss+=loss_function(targ[:,t],predictions)\n\n        dec_input = tf.expand_dims(targ[:, t], 1)\n\n      gradients = tape.gradient(loss,trainable_variables)\n\n      self.optimezer.apply_gradients(zip(gradients,trainable_variables))\n\n      self.compiled_metrics.update_state(targ,predictions)\n\n      return {m.name:m.result() for m in self.metrics}\n\nmodel = EncoderDecoder()\nmodel.compile(optimizer = 'adam',metrics = [tf.keras.metrics.SparseCategoricalCrossentropy()])\n\nmodel.fit(dataset,epochs=10)\n\nEpoch 1/10\n---------------------------------------------------------------------------\nStagingError                              Traceback (most recent call last)\n&lt;ipython-input-153-573fadf7e010&gt; in &lt;module&gt;()\n----&gt; 1 model.fit(dataset,epochs=10)\n\n10 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\n    966           except Exception as e:  # pylint:disable=broad-except\n    967             if hasattr(e, \"ag_error_metadata\"):\n--&gt; 968               raise e.ag_error_metadata.to_exception(e)\n    969             else:\n    970               raise\n\nStagingError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    &lt;ipython-input-151-46787da81d42&gt;:33 train_step  *\n        input = self.encoder_vector(input)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:897 __call__  **\n        self._maybe_build(inputs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2416 _maybe_build\n        self.build(input_shapes)  # pylint:disable=not-callable\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/preprocessing/text_vectorization.py:528 build\n        if self._split is not None and not input_shape[1] == 1:  # pylint: disable=g-comparison-negation\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:870 __getitem__\n        return self._dims[key].value\n\n    IndexError: list index out of range\n</code></pre>\n\n<p>I am trying to override the keras model train_step in my custom class.\nquestion is set of English text questions and ans is their respective answers. They are of random length and are not padded as TextVectorization will add padding. So dataset contains question answer pair. Now I batch this and use model.fit.\nThe error is when assigning input = data[0] and targ = data[1] in train_step. I need help on how to assign the input and output in the train step for a batch.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 154}]
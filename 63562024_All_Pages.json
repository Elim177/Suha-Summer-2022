[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning", "generative-adversarial-network"], "owner": {"account_id": 18453043, "reputation": 33, "user_id": 13442613, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-NQwfwSjGd5o/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMeEdT62MPF0QPgzXq3LYU9nHUTVQ/photo.jpg?sz=256", "display_name": "Lasse Veenstra", "link": "https://stackoverflow.com/users/13442613/lasse-veenstra"}, "is_answered": true, "view_count": 216, "accepted_answer_id": 63601374, "answer_count": 1, "score": 1, "last_activity_date": 1599250390, "creation_date": 1598275670, "last_edit_date": 1598278651, "question_id": 63562024, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63562024/tensorflow-gans-discriminator-doesnt-learn", "title": "Tensorflow GANs discriminator doesn&#39;t learn", "body": "<p>I am trying to make a basic GAN that tries to learn a simple 3 by 3 matrix with a plus in it.</p>\n<p>However, for some reason, the discriminator loss doesn't change.</p>\n<p>For example:</p>\n<blockquote>\n<pre><code>[[0.0, 0.98, 0,01]\n\n[0.95, 0.97, 0.99]\n\n[0.02, 0.99, 0.02]]\n</code></pre>\n</blockquote>\n<p>Here is the code:</p>\n<p>GENERATOR AND DISCRIMINATOR:</p>\n<pre><code>\ndef make_generator():\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(10, activation='relu', input_shape=(5, )))\n    model.add(keras.layers.Dense(20, activation='relu'))\n    model.add(keras.layers.Dense(9, activation='relu'))\n    model.add(keras.layers.Reshape((3, 3)))\n    return model\n\n\ndef make_discriminator():\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(10, activation='relu', input_shape=[3, 3]))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.Dense(20, activation='relu'))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.Dense(9, activation='relu'))\n    model.add(keras.layers.Dropout(0.2))\n    model.add(keras.layers.Flatten())\n    model.add(keras.layers.Dense(1, activation='softmax'))\n    return model\n\ngenerator = make_generator()\ndiscriminator = make_discriminator()\n\n\n</code></pre>\n<p>I think that the problem lies in the training, but I'm not sure.</p>\n<p>The training program:</p>\n<pre><code>generator_optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\ndiscriminator_optimizer = tf.keras.optimizers.SGD(learning_rate = 0.1)\n\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\n\ndef generator_loss(generated_im):\n    loss = cross_entropy(tf.ones_like(generated_im), generated_im)\n    return loss\n\n\ndef discriminator_loss(real_im_pred, generated_im_pred):\n    loss_on_real = cross_entropy(tf.ones_like(real_im_pred), real_im_pred)\n    loss_on_generated = cross_entropy(tf.zeros_like(generated_im_pred), generated_im_pred)\n    loss = loss_on_generated + loss_on_real\n    return loss\n\n@tf.function\ndef train_step(images, batch_size):\n        \n    noise = tf.random.normal([batch_size, 5])\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n        classification_on_real = discriminator(images, training=True)\n        classification_on_fake = discriminator(generated_images, training=True)\n        \n        gen_loss = generator_loss(generated_images)\n        disc_loss = discriminator_loss(classification_on_real, classification_on_fake)\n\n        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n    \n    return gen_loss, disc_loss\n        \ndef train(data, epochs, batch_size):\n    for epoch in range(epochs):\n        start = time.time()\n        \n        # Keep track of the total loss and accuracy\n        total_gen_loss = 0\n        total_disc_loss = 0\n        \n        for image_batch in data:\n            gen_loss, disc_loss = train_step(image_batch, batch_size)\n            \n            total_gen_loss += gen_loss\n            total_disc_loss += disc_loss\n            \n        print ('Time for epoch {} is {} sec, generator loss: {}, discriminator loss: {}'\n               .format(epoch + 1, round(time.time()-start), round(float(total_gen_loss), 2), round(float(total_disc_loss), 2)))\n\n</code></pre>\n<p>The output I get when running this code is the following:</p>\n<pre><code>Time for epoch 1 is 3 sec, generator loss: 346.15, discriminator loss: 3252.97\nTime for epoch 2 is 2 sec, generator loss: 308.61, discriminator loss: 3252.97\nTime for epoch 3 is 2 sec, generator loss: 308.33, discriminator loss: 3252.97\nTime for epoch 4 is 2 sec, generator loss: 308.24, discriminator loss: 3252.97\nTime for epoch 5 is 2 sec, generator loss: 308.19, discriminator loss: 3252.97\nTime for epoch 6 is 2 sec, generator loss: 308.16, discriminator loss: 3252.97\nTime for epoch 7 is 2 sec, generator loss: 308.14, discriminator loss: 3252.97\nTime for epoch 8 is 2 sec, generator loss: 308.13, discriminator loss: 3252.97\nTime for epoch 9 is 2 sec, generator loss: 308.12, discriminator loss: 3252.97\nTime for epoch 10 is 2 sec, generator loss: 308.11, discriminator loss: 3252.97\nTime for epoch 11 is 2 sec, generator loss: 308.11, discriminator loss: 3252.97\nTime for epoch 12 is 2 sec, generator loss: 308.11, discriminator loss: 3252.97\nTime for epoch 13 is 2 sec, generator loss: 308.1, discriminator loss: 3252.97\nTime for epoch 14 is 2 sec, generator loss: 308.1, discriminator loss: 3252.97\nTime for epoch 15 is 2 sec, generator loss: 308.1, discriminator loss: 3252.97\nTime for epoch 16 is 2 sec, generator loss: 308.1, discriminator loss: 3252.97\nTime for epoch 17 is 2 sec, generator loss: 308.09, discriminator loss: 3252.97\nTime for epoch 18 is 2 sec, generator loss: 308.09, discriminator loss: 3252.97\nTime for epoch 19 is 2 sec, generator loss: 308.09, discriminator loss: 3252.97\nTime for epoch 20 is 2 sec, generator loss: 308.09, discriminator loss: 3252.97\n</code></pre>\n<p>If you're interested in the code for making the data, here it is:</p>\n<pre><code>def plus():\n    array = np.array([[np.random.normal(0.05, 0.01, 1)[0], np.random.normal(0.95, 0.01, 1)[0], np.random.normal(0.05, 0.01, 1)[0]],\n                     [np.random.normal(0.95, 0.01, 1)[0], np.random.normal(0.95, 0.01, 1)[0], np.random.normal(0.95, 0.01, 1)[0]],\n                     [np.random.normal(0.05, 0.01, 1)[0], np.random.normal(0.95, 0.01, 1)[0], np.random.normal(0.05, 0.01, 1)[0]]])\n    return array\n\n\ndef dataset(size):\n    X = []\n    \n    for _ in range(size):\n        x = plus()\n        X.append(x)\n    return np.array(X)\n\n\ndef get_batches(x, batch_size):\n    batches = []\n    for i in range(0, x.shape[0], batch_size):\n        batch = x[i:i + batch_size]\n        batches.append(batch)\n    \n    random.shuffle(batches)\n    return np.array(batches)\n\nBATCH_SIZE = 10\ndata = dataset(20000)\ndata = get_batches(data, BATCH_SIZE)\n</code></pre>\n<p>I hope you can help!\nThanks a lot.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 191}]
[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 10572314, "reputation": 1, "user_id": 7788650, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-5J5I37hyYfE/AAAAAAAAAAI/AAAAAAAAACM/9cz13VODguk/photo.jpg?sz=256", "display_name": "\u041c\u0438\u0445\u0430\u0438\u043b \u0414\u0435\u0440\u0435\u0432\u044f\u043d\u043d\u044b\u0445", "link": "https://stackoverflow.com/users/7788650/%d0%9c%d0%b8%d1%85%d0%b0%d0%b8%d0%bb-%d0%94%d0%b5%d1%80%d0%b5%d0%b2%d1%8f%d0%bd%d0%bd%d1%8b%d1%85"}, "is_answered": true, "view_count": 155, "answer_count": 1, "score": 0, "last_activity_date": 1616754435, "creation_date": 1610276887, "question_id": 65652577, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65652577/tensorflow-keras-gradient-tape-returns-none-for-a-trainable-variable-of-one-mode", "title": "Tensorflow Keras Gradient Tape returns None for a trainable variable of one model which is impacted by trainable variable of other model", "body": "<p>Just simple code generates None gradients.\nIf i use other variable instead of &quot;model_tmp.trainable_variables[0]&quot; (tf.Variable b) everything would be ok and I get correct gradient</p>\n<pre><code>@tf.function\ndef cat(model, model_tmp):\n    with tf.GradientTape(persistent=True, watch_accessed_variables=False) as g:\n        g.watch(model.trainable_variables[0])\n        model_tmp.trainable_variables[0] = tf.multiply(model.trainable_variables[0], 2)        \n        a = tf.reduce_mean(model_tmp.trainable_variables[0])\n        grads_out = g.gradient(a, model.trainable_variables[0])\n        tf.print(grads_out) \n        return grads_out\n\ncat(model, model2)\n</code></pre>\n<p>output:</p>\n<pre><code>None\n</code></pre>\n<p>model is a custom Keras model.\nmodel2 is a clone of first model (model2 = tf.keras.models.clone_model(model))\nWhat's a possible root of this problem? Thanks</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 234}]
[{"items": [{"tags": ["python-3.x", "tensorflow", "tensorflow2.0"], "owner": {"account_id": 4275271, "reputation": 750, "user_id": 3496060, "user_type": "registered", "accept_rate": 44, "profile_image": "https://graph.facebook.com/16917224/picture?type=large", "display_name": "user3496060", "link": "https://stackoverflow.com/users/3496060/user3496060"}, "is_answered": false, "view_count": 106, "answer_count": 0, "score": 1, "last_activity_date": 1575360227, "creation_date": 1575360227, "question_id": 59152555, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59152555/tf2-0-memory-leak-eager-associated-with-timedistributedlayer", "title": "tf2.0 memory leak (eager) associated with timedistributedlayer", "body": "<p>The memory usage doubles every time I run the tape.gradients part. I think it's connected to the timedistributedlayers...?  Any ideas?</p>\n\n<pre><code>###### create model \n\n    inputs = tf.keras.Input(shape=(6, *data_loader_train[0][0][0].shape), name='img') ## (108, 192, 3)\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(inputs)\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(x)\n    block_1_output = layers.TimeDistributed(layers.MaxPooling2D(2))(x)\n\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu', padding='same'))(block_1_output)\n    block_3_output = layers.add([x, block_1_output])\n    block_3_output = layers.TimeDistributed(layers.MaxPooling2D(2))(block_3_output)\n\n    x = layers.TimeDistributed(layers.Conv2D(16, 3, activation='relu'))(block_3_output)\n    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)\n\n    x = layers.Flatten()(x)\n    x = layers.Dense(16, activation='relu')(x)\n    x = layers.Dense(1)(x)\n    counts = tf.keras.activations.softplus(x)\n\n    model = tf.keras.Model(inputs, counts, name='toy_resnet')\n    model.summary()\n\n    ### run model\n####### running this part doubles memory every two times ##########\nfor x_ in batch(np.random.uniform(size=(100,6,108,192,3)).astype(np.float32), 10):\n     with tf.GradientTape() as tape:\n             count_ = tf.reduce_sum(model(x_))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 22}]
[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0", "recommender-systems"], "owner": {"account_id": 9436478, "reputation": 814, "user_id": 7018323, "user_type": "registered", "accept_rate": 86, "profile_image": "https://i.stack.imgur.com/kQFMt.jpg?s=256&g=1", "display_name": "Kartikey Singh", "link": "https://stackoverflow.com/users/7018323/kartikey-singh"}, "is_answered": false, "view_count": 37, "answer_count": 1, "score": 1, "last_activity_date": 1589350049, "creation_date": 1589259280, "question_id": 61744101, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61744101/updating-specific-rows-of-a-tensor-matrix-during-gradient-updation", "title": "Updating specific rows of a tensor matrix during gradient updation?", "body": "<p>I have been trying to implement the paper: <a href=\"https://arxiv.org/pdf/1907.01640.pdf\" rel=\"nofollow noreferrer\">SeER: An Explainable Deep Learning MIDI-based Hybrid Song Recommender System</a>.</p>\n\n<p>So, what I have been doing is this:</p>\n\n<p>Model Code:</p>\n\n<pre><code>class HybridFactorization(tf.keras.layers.Layer):\n    # embedding_size is also the number of lstm units\n    # num_users, num_movies = input_shape\n    # required_users: (batch_size, embedding_size)\n    # songs_output: (batch_size, embedding_size)\n    def __init__(self, embedding_size, num_users, num_tracks):        \n        super(HybridFactorization, self).__init__()\n        self.embedding_size = embedding_size    \n        self.num_users = num_users\n        self.num_tracks = num_tracks  \n        self.required_users = None         \n        self.U = self.add_weight(\"U\", \n                                shape=[self.num_users, self.embedding_size], \n                                dtype=tf.float32,\n                                initializer=tf.initializers.GlorotUniform)                        \n        self.lstm = tf.keras.layers.LSTM(self.embedding_size) \n\n    def call(self, user_index, songs_batch):\n        output_lstm = self.lstm(songs_batch)\n\n        self.required_users = self.U.numpy()\n        self.required_users = tf.convert_to_tensor(self.required_users[np.array(user_index)],\n                                              dtype=tf.float32)                             \n        return tf.matmul(self.required_users, output_lstm, transpose_b=True)        \n\n\nclass HybridRecommender(tf.keras.Model):\n    def __init__(self, embedding_size, num_users, num_tracks):\n        super(HybridRecommender, self).__init__()\n        self.HybridFactorization = HybridFactorization(embedding_size, \n                                                       num_users, num_tracks)        \n\n    def call(self, user_index, songs_batch):\n        output = self.HybridFactorization(user_index, songs_batch)        \n        return output\n</code></pre>\n\n<p>Utility Functions and running the model:</p>\n\n<pre><code>def loss_fn(source, target):            \n    mse = tf.keras.losses.MeanSquaredError()        \n    return mse(source, target)\n\nmodel = HybridRecommender(EMBEDDING_SIZE, num_users, num_tracks)\nXhat = model(user_index, songs_batch)        \n\ntf.keras.backend.clear_session()\n\noptimizer = tf.keras.optimizers.Adam()\nEPOCHS = 1\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (input_batch, target_batch)) in enumerate(train_dataset):            \n        songs_batch = create_songs_batch(input_batch)\n        user_index = input_batch[:, 0].numpy()\n        X = create_pivot_batch(input_batch, target_batch)        \n\n        with tf.GradientTape() as tape:\n            Xhat = model(user_index, songs_batch)\n            batch_loss = loss_fn(X, Xhat)\n\n        variables = model.trainable_variables\n        gradients = tape.gradient(batch_loss, variables)\n        optimizer.apply_gradients(zip(gradients, variables))\n\n        total_loss += batch_loss\n</code></pre>\n\n<p>Now, various functions like <code>create_songs_batch(input_batch)</code> and <code>create_pivot_batch(input_batch, target_batch)</code> just provide data in the required format.</p>\n\n<p>My model runs but I get the warning:\n<code>WARNING:tensorflow:Gradients do not exist for variables ['U:0'] when minimizing the loss.</code></p>\n\n<p>Now, I can see why variable <code>U</code> is not being updated as there is no direct path to it.\nI want to update some specific rows of <code>U</code> which are mentioned in <code>user_index</code> in every batch call.</p>\n\n<p>Is there a way to do it?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 144}]
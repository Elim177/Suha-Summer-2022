[{"items": [{"tags": ["python", "machine-learning", "keras", "scikit-learn", "regression"], "owner": {"account_id": 1484715, "reputation": 1099, "user_id": 1393214, "user_type": "registered", "accept_rate": 85, "profile_image": "https://www.gravatar.com/avatar/63c9e6d4bcd174aba5445c7fc62acfb1?s=256&d=identicon&r=PG&f=1", "display_name": "User", "link": "https://stackoverflow.com/users/1393214/user"}, "is_answered": false, "view_count": 40, "answer_count": 0, "score": 0, "last_activity_date": 1626459915, "creation_date": 1626459915, "question_id": 68413712, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68413712/why-is-the-score-of-the-tuned-model-of-mine-obviously-non-optimal-negative", "title": "Why is the score of the tuned model of mine obviously non-optimal (negative)?", "body": "<p>I am trying to tune a stacking regressor which includes a layer constituting a decision tree, a random forest, and a deep network, and a xgbregressor as a blender.</p>\n<p>My tuning procedure below</p>\n<pre><code>import os\nimport random\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom scikeras.wrappers import KerasRegressor\nimport xgboost\n\n########## Reproducibility\nseed_value= 0\nos.environ['PYTHONHASHSEED']=str(seed_value)\nrandom.seed(seed_value)\nnp.random.seed(seed_value)\ntf.random.set_seed(seed_value)\n\n########## Data pre-processing\nhousing = fetch_california_housing()\n\nX_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target, train_size=0.8, test_size=0.2)\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n########## Models synthesis\ndef build_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.InputLayer(input_shape=X_train.shape[1:]))\n    model.add(tf.keras.layers.BatchNormalization(momentum=0.999))\n    model.add(tf.keras.layers.Dense(48, tf.keras.activations.selu, kernel_initializer=&quot;lecun_normal&quot;))\n    model.add(tf.keras.layers.BatchNormalization(momentum=0.999))\n    model.add(tf.keras.layers.Dense(48, tf.keras.activations.selu, kernel_initializer=&quot;lecun_normal&quot;))\n    model.add(tf.keras.layers.BatchNormalization(momentum=0.999))\n    model.add(tf.keras.layers.Dense(1, kernel_initializer=&quot;lecun_normal&quot;))\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n    model.compile(loss=&quot;mae&quot;, optimizer=optimizer, metrics=[&quot;mse&quot;])\n    return model\n\ndnn_reg = KerasRegressor(model=build_model())\n\nrnd_reg = DecisionTreeRegressor(max_depth=5,\n                            min_samples_leaf=1,\n                            max_leaf_nodes=9)\n\nrf_reg = RandomForestRegressor(n_estimators=500, max_leaf_nodes=16)\n\nstack_reg = StackingRegressor(estimators=[(&quot;rnd&quot;, rnd_reg), (&quot;rf&quot;, rf_reg), (&quot;dnn&quot;, dnn_reg)],\n                              final_estimator=xgboost.XGBRegressor())\n\nparam_spec = {\n    &quot;final_estimator__max_depth&quot;: np.arange(1,7).tolist(),\n    &quot;final_estimator__learning_rate&quot;: np.arange(3e-4, 7e-2).tolist(),\n}\n\nstack_search_cv = RandomizedSearchCV(stack_reg, param_spec, n_jobs=-1, n_iter=10, cv=5, verbose=True)\n\nstack_search_cv.fit(X_train, y_train)\n\nprint(stack_search_cv.best_params_)\nprint(stack_search_cv.best_score_)\n</code></pre>\n<p>reports the following values for the desired hyperparameters</p>\n<p><code>max_depth: 5, learning_rate: 0.0003</code>.</p>\n<p>However the score of the model is negative. It is weird because the score of the raw model (including all default-valued hyperparameters) is positive. Can one explain what I am doing wrong here?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 102}]
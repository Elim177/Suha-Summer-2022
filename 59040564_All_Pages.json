[{"items": [{"tags": ["python", "tensorflow", "neural-network"], "owner": {"account_id": 4441934, "reputation": 1952, "user_id": 3616293, "user_type": "registered", "accept_rate": 35, "profile_image": "https://www.gravatar.com/avatar/cf7556b4227065cec9496375d64fea3d?s=256&d=identicon&r=PG&f=1", "display_name": "Arun", "link": "https://stackoverflow.com/users/3616293/arun"}, "is_answered": true, "view_count": 243, "accepted_answer_id": 59040887, "answer_count": 1, "score": 1, "last_activity_date": 1574726309, "creation_date": 1574718343, "last_edit_date": 1574726309, "question_id": 59040564, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59040564/tensorflow-2-0-gradienttape-nonetype-error", "title": "TensorFlow 2.0 GradientTape NoneType error", "body": "<p>I am trying to do MNIST classification using TensorFlow 2.0.</p>\n\n<p>The architecture of my neural network is as follows:</p>\n\n<p>The input layer has 784 neurons (28 * 28)</p>\n\n<p>The hidden layer has 512 neurons</p>\n\n<p>The output layer has 10 neurons</p>\n\n<p>The hidden layer uses ReLU activation function and output layer has 10 neurons.</p>\n\n<p>and my code to do so is as follows:</p>\n\n<pre><code># Load and prepare the MNIST dataset-\nmnist = tf.keras.datasets.mnist\n\n# type(mnist)\n# module\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n# type(X_train), type(y_train), type(X_test), type(y_test)\n# (numpy.ndarray, numpy.ndarray, numpy.ndarray, numpy.ndarray)\n\n\n# Normalize and convert samples from integers to floating-point numbers-\nX_train, X_test = X_train / 255.0, X_test / 255.0\n\nX_train = tf.cast(X_train, dtype=tf.float32)\nX_test = tf.cast(X_test, dtype=tf.float32)\ny_train = tf.cast(y_train, dtype=tf.float32)\ny_test = tf.cast(y_test, dtype=tf.float32)\n\nprint(\"\\nShapes of training and testing sets are:\")\nprint(\"X_train.shape = {0}, y_train.shape = {1}, X_test.shape = {2} &amp; y_test.shape = {3}\\n\".format(X_train.shape, y_train.shape, X_test.shape, y_test.shape))\n# Shapes of training and testing sets are:\n# X_train.shape = (60000, 28, 28), y_train.shape = (60000,), X_test.shape = (10000, 28, 28) &amp; y_test.shape = (10000,)\n\n\n# Reshape training and testing sets-\nX_train = tf.reshape(X_train, shape=(X_train.shape[0], 784))\nX_test = tf.reshape(X_test, shape=(X_test.shape[0], 784))\n\nprint(\"\\nDimensions of training and testing sets AFTER reshaping are:\")\nprint(\"X_train.shape = {0} and X_test.shape = {1}\\n\".format(X_train.shape, X_test.shape))\n# Dimensions of training and testing sets AFTER reshaping are:\n# X_train.shape = (60000, 784) and X_test.shape = (10000, 784)\n\n\ndef relu(x):\n    '''\n    Function to calculate ReLU for\n    given 'x'\n    '''\n    # return np.maximum(x, 0)\n    return tf.cast(tf.math.maximum(x, 0), dtype = tf.float32)\n\n\ndef relu_derivative(x):\n    '''\n    Function to calculate derivative\n    of ReLU\n    '''\n    # return np.where(x &lt;= 0, 0, 1)\n    # return tf.where(x &lt;=0, 0, 1)\n    return tf.cast(tf.where(x &lt;=0, 0, 1), dtype=tf.float32)\n\n\ndef softmax_stable(z):\n    '''\n    Function to compute softmax activation function.\n    Numerically stable\n    '''\n    # First cast 'z' to floating type-\n    z = tf.cast(z, dtype = tf.float32)\n\n    # Get largest element in 'z'-\n    largest = tf.math.reduce_max(z)\n\n    # Raise each value to exp('z - largest')-\n    z_exp = tf.math.exp(z - largest)\n\n    # Compute softmax activation values-\n    s = z_exp / tf.math.reduce_sum(z_exp)\n\n    return s\n\n\ndef initialize_parameters():\n    W1 = tf.Variable(tf.random.uniform(shape=(784, 512), minval=0, maxval=1))\n    b1 = tf.Variable(tf.random.uniform(shape = (1, 512), minval = 0, maxval=1))\n    W2 = tf.Variable(tf.random.uniform(shape = (512, 10), minval=0, maxval=1))\n    b2 = tf.Variable(tf.random.uniform(shape = (1, 10), minval=0, maxval=1))\n\n    return {'W1': W1, 'W2': W2,\n        'b1': b1, 'b2': b2}\n\n\ndef forward_propagation(parameters, X, Y):\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n\n    Z1 = tf.matmul(X_train, W1) + b1    # (6000, 512)\n    A1 = relu(Z1)                       # (6000, 512)\n\n    Z2 = tf.matmul(A1, W2) + b2         # (6000, 10)\n    # A2 = softmax(Z2)                  # (6000, 10)\n    # OR-\n    A2 = tf.nn.softmax(Z2)          # (6000, 10)\n\n    return A2\n\n\ndef cost(parameters, X, Y):\n    y_pred_temp = forward_propagation(parameters, X, Y)\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n    return loss_fn(y_true = Y, y_pred = y_pred_temp)\n\n\n\ndef train_model(parameters, X, Y, learning_rate):\n\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    b1 = parameters['b1']\n    b2 = parameters['b2']\n\n    with tf.GradientTape(persistent = True) as t:\n        current_loss = cost(parameters, X_train, y_train)\n\n    dW2, dW1, db2, db1 = t.gradient(current_loss, [W2, W1, b2, b1])\n\n    W2 = W2 - (learning_rate * dW2)\n    W1 = W1 - (learning_rate * dW1)\n    b2 = b2 - (learning_rate * db2)\n    b1 = b1 - (learning_rate * db1)\n\n    updated_params = {'W1': W1, 'W2': W2,\n        'b1': b1, 'b2': b2}\n\n    return updated_params\n\nparams = initialize_parameters()\n\nupdated_params, cost_val = train_model(params, X_train, y_train, 0.01)\n</code></pre>\n\n<p>Now, if I want to use \"train_model()\" in a loop where I update it's values as follows:</p>\n\n<pre><code>for epoch in range(100):\n    updated_params, cost_val = train_model(updated_params, X_train, y_train, 0.01)\n</code></pre>\n\n<p>Subsequent calls to \"train_model()\" returns \"dW2\", \"dW1\", \"db2\", \"db1\"\nas \"NoneType\"</p>\n\n<p>What's going wrong?</p>\n\n<p>Thanks!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 26}]
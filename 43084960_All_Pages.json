[{"items": [{"tags": ["python", "python-3.x", "tensorflow", "distributed"], "owner": {"account_id": 5664091, "reputation": 1050, "user_id": 4480756, "user_type": "registered", "accept_rate": 38, "profile_image": "https://graph.facebook.com/1515949422/picture?type=large", "display_name": "Ruofan Kong", "link": "https://stackoverflow.com/users/4480756/ruofan-kong"}, "is_answered": true, "view_count": 3052, "accepted_answer_id": 43100415, "answer_count": 1, "score": 5, "last_activity_date": 1523297172, "creation_date": 1490764852, "last_edit_date": 1490771948, "question_id": 43084960, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43084960/tensorflow-variables-are-not-initialized-using-between-graph-replication", "title": "Tensorflow Variables are Not Initialized using Between-graph Replication", "body": "<p>I have Python code <code>test.py</code> as below, which uses \"Between-graph Replication\" for Distributed Tensorflow:</p>\n\n<pre><code>import argparse\nimport logging\n\nimport tensorflow as tf\n\nlog = logging.getLogger(__name__)\n\n# Job Names\nPARAMETER_SERVER = \"ps\"\nWORKER_SERVER = \"worker\"\n\n# Cluster Details\nCLUSTER_SPEC = {\n    PARAMETER_SERVER: [\"localhost:2222\"],\n    WORKER_SERVER: [\"localhost:1111\", \"localhost:1112\"]}\n\n\ndef parse_command_arguments():\n    \"\"\" Set up and parse the command line arguments passed for experiment. \"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Parameters and Arguments for the Test.\")\n    parser.add_argument(\n        \"--job_name\",\n        type=str,\n        default=\"\",\n        help=\"One of 'ps', 'worker'\"\n    )\n    # Flags for defining the tf.train.Server\n    parser.add_argument(\n        \"--task_index\",\n        type=int,\n        default=0,\n        help=\"Index of task within the job\"\n    )\n\n    return parser.parse_args()\n\n\ndef start_server(job_name, task_index):\n    \"\"\" Create a server based on a cluster spec. \"\"\"\n    cluster = tf.train.ClusterSpec(CLUSTER_SPEC)\n    server = tf.train.Server(\n        cluster, job_name=job_name, task_index=task_index)\n\n    return server, cluster\n\n\ndef model():\n    \"\"\" Build up a simple estimator model. \"\"\"\n    # Build a linear model and predict values\n    W = tf.Variable([.3], tf.float32)\n    b = tf.Variable([-.3], tf.float32)\n    x = tf.placeholder(tf.float32)\n    linear_model = W * x + b\n    y = tf.placeholder(tf.float32)\n    global_step = tf.get_variable('global_step', [],\n                                  initializer=tf.constant_initializer(0),\n                                  trainable=False)\n\n    # Loss sub-graph\n    loss = tf.reduce_sum(tf.square(linear_model - y))\n\n    # optimizer\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    train = optimizer.minimize(loss, global_step=global_step)\n\n    init_op = tf.global_variables_initializer()\n    log.info(\"Variables initialized ...\")\n\n    return W, b, loss, x, y, train, global_step, init_op\n\n\nif __name__ == \"__main__\":\n    # Initializing logging with level \"INFO\".\n    logging.basicConfig(level=logging.INFO)\n\n    # Parse arguments from command line.\n    arguments = parse_command_arguments()\n    job_name = arguments.job_name\n    task_index = arguments.task_index\n\n    # Start a server.\n    server, cluster = start_server(job_name, task_index)\n\n    if job_name == \"ps\":\n        server.join()\n    else:\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % task_index,\n                cluster=cluster)):\n            W, b, loss, x, y, train, global_step, init_op = model()\n        with tf.train.MonitoredTrainingSession(\n                master=server.target,\n                is_chief=(arguments.task_index == 0 and (\n                            arguments.job_name == 'worker'))) as sess:\n            step = 0\n            # training data\n            x_train = [1, 2, 3, 4]\n            y_train = [0, -1, -2, -3]\n            while not sess.should_stop() and step &lt; 1000:\n                _, step = sess.run(\n                    [train, global_step], {x: x_train, y: y_train})\n\n            # evaluate training accuracy\n            curr_W, curr_b, curr_loss = sess.run(\n                [W, b, loss], {x: x_train, y: y_train})\n            print(\"W: %s b: %s loss: %s\" % (curr_W, curr_b, curr_loss))\n</code></pre>\n\n<p>I ran the code with 3 different processes in a single machine (MacPro with only CPUs) following the order below:</p>\n\n<ol>\n<li>Parameter Server: <code>$ python test.py  --task_index 0 --job_name ps</code></li>\n<li>Worker 1: <code>$ python test.py  --task_index 0 --job_name worker</code></li>\n<li>Worker 2: <code>$ python test.py  --task_index 1 --job_name worker</code></li>\n</ol>\n\n<p>and I found that the process for \"Worker 2\" hit an error:</p>\n\n<pre><code>$ python test.py --task_index 1 --job_name worker\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2222}\nI tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:197] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:1111, 1 -&gt; localhost:1112}\nI tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:211] Started server with target: grpc://localhost:1112\nINFO:__main__:Variables initialized ...\nI tensorflow/core/distributed_runtime/master_session.cc:993] Start master session 9912c75f2921fe13 with config: \n\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: Variable, Variable_1, global_step\nINFO:tensorflow:Waiting for model to be ready.  Ready_for_local_init_op:  None, ready: Variables not initialized: Variable, Variable_1, global_step\n</code></pre>\n\n<p>and that process for \"Worker 2\" was just frozen there. The error shows Tensorflow variables for \"Worker 2\" are unsuccessfully initialized, so I wonder if there is a bug for <code>MonitoredTrainingSession</code> in terms of coordinating variable initializations across Tensorflow Sessions or somewhere else, or I missed things in my code.</p>\n\n<p><code>NOTE: The code was running with Tensorflow 0.12</code></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 297}]
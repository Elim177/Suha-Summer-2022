[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 3948330, "reputation": 5043, "user_id": 3259896, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/641c30a7b383022f22b53c8cedb04e3f?s=256&d=identicon&r=PG&f=1", "display_name": "SantoshGupta7", "link": "https://stackoverflow.com/users/3259896/santoshgupta7"}, "is_answered": true, "view_count": 935, "accepted_answer_id": 50975762, "answer_count": 2, "score": 3, "last_activity_date": 1529607352, "creation_date": 1529604079, "question_id": 50974976, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/50974976/tensorflow-why-must-saver-tf-train-saver-be-declared-after-variables-are", "title": "Tensorflow: Why must `saver = tf.train.Saver()` be declared after variables are declared?", "body": "<p>Important clarification: I was only running this section, the graph definition, in a notebook enviroment. I had not run an actual session yet. </p>\n\n<p>When running this code:</p>\n\n<pre><code>with graph.as_default(): #took out \" , tf.device('/cpu:0')\"\n\n  saver = tf.train.Saver()\n  valid_examples = np.array(random.sample(range(1, valid_window), valid_size)) #put inside graph to get new words each time\n\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, cbow_window*2 ])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n  valid_datasetSM = tf.constant(valid_examples, dtype=tf.int32)\n\n  embeddings = tf.get_variable( 'embeddings', \n    initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n\n  softmax_weights = tf.get_variable( 'softmax_weights',\n    initializer= tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n\n  softmax_biases = tf.get_variable('softmax_biases', \n    initializer= tf.zeros([vocabulary_size]),  trainable=False )\n\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is\n\n  embed_reshaped = tf.reshape( embed, [batch_size*cbow_window*2, embedding_size] )\n\n\n  segments= np.arange(batch_size).repeat(cbow_window*2)\n\n  averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)\n\n    #return tf.reduce_mean( tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,\n                               #labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,\n                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n  normSM = tf.sqrt(tf.reduce_sum(tf.square(softmax_weights), 1, keepdims=True))\n\n  normalized_embeddings = embeddings / norm\n  normalized_embeddingsSM = softmax_weights / normSM\n\n  valid_embeddings = tf.nn.embedding_lookup(\n    normalized_embeddings, valid_dataset)\n  valid_embeddingsSM = tf.nn.embedding_lookup(\n    normalized_embeddingsSM, valid_datasetSM)\n\n  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n  similaritySM = tf.matmul(valid_embeddingsSM, tf.transpose(normalized_embeddingsSM))\n</code></pre>\n\n<p>I got this error</p>\n\n<p>ValueError: No variables to save</p>\n\n<p>while pointing to this line</p>\n\n<pre><code>saver = tf.train.Saver()\n</code></pre>\n\n<p>I searched stack overflow and found this answer</p>\n\n<p><a href=\"https://stackoverflow.com/questions/38626435/tensorflow-valueerror-no-variables-to-save-from/38627631\">Tensorflow ValueError: No variables to save from</a></p>\n\n<p>So I simply put that line at the bottom of the graph definition like so</p>\n\n<pre><code>with graph.as_default(): #took out \" , tf.device('/cpu:0')\"\n\n  valid_examples = np.array(random.sample(range(1, valid_window), valid_size)) #put inside graph to get new words each time\n\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, cbow_window*2 ])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n  valid_datasetSM = tf.constant(valid_examples, dtype=tf.int32)\n\n  embeddings = tf.get_variable( 'embeddings', \n    initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n  softmax_weights = tf.get_variable( 'softmax_weights',\n    initializer= tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n\n  softmax_biases = tf.get_variable('softmax_biases', \n    initializer= tf.zeros([vocabulary_size]),  trainable=False )\n\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is\n  embed_reshaped = tf.reshape( embed, [batch_size*cbow_window*2, embedding_size] )\n\n  segments= np.arange(batch_size).repeat(cbow_window*2)\n\n  averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)\n\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,\n                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n  normSM = tf.sqrt(tf.reduce_sum(tf.square(softmax_weights), 1, keepdims=True))\n\n  normalized_embeddings = embeddings / norm\n  normalized_embeddingsSM = softmax_weights / normSM\n\n  valid_embeddings = tf.nn.embedding_lookup(\n    normalized_embeddings, valid_dataset)\n  valid_embeddingsSM = tf.nn.embedding_lookup(\n    normalized_embeddingsSM, valid_datasetSM)\n\n  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n  similaritySM = tf.matmul(valid_embeddingsSM, tf.transpose(normalized_embeddingsSM))\n\n  saver = tf.train.Saver()\n</code></pre>\n\n<p>And then there were no errors!</p>\n\n<p>Why is this so? The graph definition is only defining the graph, not running anything. Perhaps it's an bug prevention measure?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 77}]
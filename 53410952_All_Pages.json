[{"items": [{"tags": ["tensorflow", "neural-network", "deep-learning", "computer-vision", "conv-neural-network"], "owner": {"account_id": 5637924, "reputation": 315, "user_id": 4462831, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9a490ac73292775d912bb3dd955b6d97?s=256&d=identicon&r=PG&f=1", "display_name": "EdoardoG", "link": "https://stackoverflow.com/users/4462831/edoardog"}, "is_answered": false, "view_count": 239, "answer_count": 1, "score": 2, "last_activity_date": 1543403214, "creation_date": 1542799011, "last_edit_date": 1542799619, "question_id": 53410952, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/53410952/how-to-sweep-a-neural-network-through-an-image-with-tensorflow", "title": "How to sweep a neural-network through an image with tensorflow?", "body": "<p>My question is about finding an efficient (mostly in term of parameters count) way to implement a sliding window in tensorflow (1.4) in order to apply a neural network through the image and produce a 2-d map with each pixel (or region) representing the network output for the corresponding receptive field (which in this case is the sliding window itself).</p>\n\n<p>In practice, I'm trying to implement either a <a href=\"https://www.ncbi.nlm.nih.gov/pubmed/12906178\" rel=\"nofollow noreferrer\">MTANN</a> or a <a href=\"https://arxiv.org/abs/1604.04382\" rel=\"nofollow noreferrer\">PatchGAN</a> using tensorflow, but I cannot understand the implementation I found.</p>\n\n<p>The two architectures can be briefly described as:</p>\n\n<ul>\n<li><p>MTANN: A linear neural network with input size of [1,N,N,1] and output size [ ] is applied to an image of size [1,M,M,1] to produce a map of size [1,G,G,1], in which every pixel of the generated map corresponds to a likelihood of the corresponding NxN patch to belong to a certain class.</p></li>\n<li><p>PatchGAN Discriminator: More general architecture, as I can understand the network that is strided through the image outputs a map itself instead of a single value, which then is combined with adjacent maps to produce the final map.</p></li>\n</ul>\n\n<p>While I cannot find any tensorflow implementation of MTANN, I found the PatchGAN <a href=\"https://github.com/affinelayer/pix2pix-tensorflow/blob/master/pix2pix.py#L395\" rel=\"nofollow noreferrer\">implementation</a>, which is <a href=\"https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/issues/39\" rel=\"nofollow noreferrer\">considered as a convolutional network</a>, but I couldn't figure out how to implement this in practice.</p>\n\n<p>Let's say I got a pre-trained network of which I got the output tensor. I understand that convolution is the way to go, since a convolutional layer operates over a local region of the input and what is I'm trying to do can be clearly represented as a convolutional network. However, what if I already have the network that generates the sub-maps from a given window of fixed-size?</p>\n\n<p>E.g. I got a tensor</p>\n\n<pre><code>sub_map = network(input_patch)\n</code></pre>\n\n<p>which returns a [1,2,2,1] maps from a [1,8,8,3] image (corresponding to a 3-layer FCN with input size 8, filter size 3x3).\nHow can I sweep this network on [1,64,64,3] images, in order to produce a [1,64,64,1] map composed of each spatial contribution, like it happens in a convolution?</p>\n\n<p>I've considered these solutions:</p>\n\n<ul>\n<li><p>Using <code>tf.image.extract_image_patches</code> which explicitly extract all the image patches and channels in the depth dimension, but I think it would consume too many resources, as I'm switching to PatchGAN Discriminator from a full convolutional network due to memory constraints - also the composition of the final map is not so straight-forward. </p></li>\n<li><p>Adding a convolutional layer before the network I got, but I cannot figure out what the filter (and its size) should be in this case in order to keep the pretrained model work on 8x8 images while integrating it in a model which works on bigger images. \nFor what I can get it should be something like <code>whole_map = tf.nn.convolution(input=x64_images, filter=sub_map, ...)</code> but I don't think this would work as the filter is an operator which depends on the receptive field itself.</p></li>\n</ul>\n\n<p>The ultimate goal is to apply this small network to big images (eg. 1024x1024) in an efficient way, since my current model downscales progressively the images and wouldn't fit in memory due to the huge number of parameters. </p>\n\n<p>Can anyone help me to get a better understanding of what I am missing?</p>\n\n<p>Thank you</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 68}]
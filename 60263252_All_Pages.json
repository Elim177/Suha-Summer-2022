[{"items": [{"tags": ["tensorflow", "gpflow"], "owner": {"account_id": 4412016, "reputation": 119, "user_id": 3594642, "user_type": "registered", "profile_image": "https://graph.facebook.com/1042805390/picture?type=large", "display_name": "Fabricio", "link": "https://stackoverflow.com/users/3594642/fabricio"}, "is_answered": true, "view_count": 355, "accepted_answer_id": 60286567, "answer_count": 1, "score": 2, "last_activity_date": 1582047465, "creation_date": 1581945118, "question_id": 60263252, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60263252/optimization-in-gpflow-2-why-set-autograph-false", "title": "optimization in gpflow 2: Why set autograph=False?", "body": "<p>in the current notebook tutorials (gpflow 2.0), all @tf.function tags include the option\nautograph=False, e.g. (<a href=\"https://gpflow.readthedocs.io/en/2.0.0-rc1/notebooks/advanced/gps_for_big_data.html\" rel=\"nofollow noreferrer\">https://gpflow.readthedocs.io/en/2.0.0-rc1/notebooks/advanced/gps_for_big_data.html</a>):</p>\n\n<pre><code>@tf.function(autograph=False)\ndef optimization_step(optimizer, model: gpflow.models.SVGP, batch):\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n        tape.watch(model.trainable_variables)\n        objective = - model.elbo(*batch)\n        grads = tape.gradient(objective, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    return objective\n</code></pre>\n\n<p>Does anyone know why that is the case, or what the reasoning behind this is? \nAs far as I understood, <code>autograph=True</code> simply allows for python control flow to be translated to a graph structure. Does setting/leaving it to true, even if the functionality is not required, have any drawbacks?</p>\n\n<p>My guess would have been that its just a small overhead at compile time of the graph, but should be negligible. Is that wrong?</p>\n\n<p>Thanks</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 47}]
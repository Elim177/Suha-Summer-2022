[{"items": [{"tags": ["keras", "tensorflow2.0", "batchnorm"], "owner": {"account_id": 108161, "reputation": 5951, "user_id": 287238, "user_type": "registered", "accept_rate": 75, "profile_image": "https://i.stack.imgur.com/oQJH2.jpg?s=256&g=1", "display_name": "mathtick", "link": "https://stackoverflow.com/users/287238/mathtick"}, "is_answered": false, "view_count": 1276, "answer_count": 1, "score": 1, "last_activity_date": 1570098267, "creation_date": 1570015522, "last_edit_date": 1570098267, "question_id": 58200520, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58200520/tensorflow-2-0-keras-batchnorm-how-to-update-the-online-params-in-custom-traini", "title": "Tensorflow 2.0 Keras BatchNorm: how to update the online params in custom training?", "body": "<p>How to train the batch norm layer without using any keras.compile methods? Typically layers have losses that are accessible. Here the losses method is empty.</p>\n\n<p>UPDATE: </p>\n\n<p>It seems like there is a lot of confusion about this and even the way the BatchNorm is implemented is pretty confused. </p>\n\n<p>First, there is only on way to train the online parameters (use in training=False mode) to scale and shift the features: call the layer in training=True mode. And if you NEVER want to use the \"batch\" part of the batch normalization (i.e. you just want an online normalizer that trains itself with a Normal log-prob loss, you basically can't do this in a single call AFAIK.</p>\n\n<p>Calling the layer with training=False does not update the params. Calling it with training=True udpates the params but then you get the batch normed layer (does not use the online loc and scale). </p>\n\n<pre><code>import tensorflow as tf\n\nclass Model(tf.keras.models.Model):\n    def __init__(self):\n        super().__init__()\n        self.dense = tf.keras.layers.Dense(4)\n        self.bn = tf.keras.layers.BatchNormalization()\n    def call(self, x, training=False):\n        x = self.dense(x)\n        x = self.bn(x, training=training)\n        return x\n\nmodel = Model()    \nx = 10 * np.random.randn(30, 4).astype(np.float32)\n\nprint(tf.math.reduce_std(model(x)))\ntf.keras.backend.set_learning_phase(1)\nprint(tf.math.reduce_std(model(x)))\nprint(tf.math.reduce_std(model(x)))\ntf.keras.backend.set_learning_phase(0)\nprint(tf.math.reduce_std(model(x)))\nprint(tf.math.reduce_std(model(x)))\n\n\ntf.Tensor(9.504262, shape=(), dtype=float32)\ntf.Tensor(0.99999136, shape=(), dtype=float32)\ntf.Tensor(0.99999136, shape=(), dtype=float32)\ntf.Tensor(5.4472375, shape=(), dtype=float32)\ntf.Tensor(5.4472375, shape=(), dtype=float32)\n</code></pre>\n\n<p>UPDATE:</p>\n\n<p>Showing keras layers have losses sometimes (when subtasks exist like regulatization):</p>\n\n<pre><code>In [335]: l = tf.keras.layers.Dense(8, kernel_regularizer=tf.keras.regularizers.L1L2())\n\nIn [336]: l(np.random.randn(2, 4))\n\nOut[336]:\n&lt;tf.Tensor: id=2521999, shape=(2, 8), dtype=float32, numpy=\narray([[ 1.1332406 ,  0.32000083,  0.8104123 ,  0.5066328 ,  0.35904446, -1.4265257 ,  1.3057183 ,  0.34458983],\n       [-0.23246719, -0.46841025,  0.9706465 ,  0.42356712,  1.705613  , -0.08619405, -0.5261058 , -1.1696107 ]], dtype=float32)&gt;\n\nIn [337]: l.losses\nOut[337]: [&lt;tf.Tensor: id=2522000, shape=(), dtype=float32, numpy=0.0&gt;]\n\nIn [338]: l = tf.keras.layers.Dense(8)\n\nIn [339]: l(np.random.randn(2, 4))\n\nOut[339]:\n&lt;tf.Tensor: id=2522028, shape=(2, 8), dtype=float32, numpy=\narray([[ 1.0674231 , -0.13423748,  0.01775402,  2.5400681 , -0.53589094,  1.4460006 , -1.7197075 ,  0.3285858 ],\n       [ 2.2171447 , -1.7448915 ,  0.4758569 ,  0.58695656,  0.32054698,  0.7813705 , -2.3022552 ,  0.44061095]], dtype=float32)&gt;\n\nIn [340]: l.losses\nOut[340]: []\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 83}]
[{"items": [{"tags": ["python-3.x", "conv-neural-network", "tensorflow2.0", "pruning"], "owner": {"account_id": 4441934, "reputation": 1962, "user_id": 3616293, "user_type": "registered", "accept_rate": 35, "profile_image": "https://www.gravatar.com/avatar/cf7556b4227065cec9496375d64fea3d?s=256&d=identicon&r=PG&f=1", "display_name": "Arun", "link": "https://stackoverflow.com/users/3616293/arun"}, "is_answered": false, "view_count": 480, "answer_count": 0, "score": 2, "last_activity_date": 1638402904, "creation_date": 1621432196, "last_edit_date": 1621435772, "question_id": 67604580, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67604580/freeze-certain-weights-tensorflow-2", "title": "Freeze certain weights - TensorFlow 2", "body": "<p>I am using a Conv-6 CNN in TensorFlow 2.5 and Python3. The objective is to selectively set certain weights within any trainable layer. The Conv-6 CNN model definition is as follows:</p>\n<pre><code>def conv6_cnn():\n    &quot;&quot;&quot;\n    Function to define the architecture of a neural network model\n    following Conv-6 architecture for CIFAR-10 dataset and using\n    provided parameter which are used to prune the model.\n    \n    Conv-6 architecture-\n    64, 64, pool  -- convolutional layers\n    128, 128, pool -- convolutional layers\n    256, 256, pool -- convolutional layers\n    256, 256, 10  -- fully connected layers\n    \n    Output: Returns designed and compiled neural network model\n    &quot;&quot;&quot;\n    \n    l = tf.keras.layers\n    \n    model = Sequential()\n    \n    model.add(\n        Conv2D(\n            filters = 64, kernel_size = (3, 3),\n            activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),\n            strides = (1, 1), padding = 'same',\n            input_shape=(32, 32, 3)\n        )    \n    )\n        \n    model.add(\n        Conv2D(\n            filters = 64, kernel_size = (3, 3),\n            activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),\n            strides = (1, 1), padding = 'same'\n        )\n    )\n    \n    model.add(\n        MaxPooling2D(\n            pool_size = (2, 2),\n            strides = (2, 2)\n        )\n    )\n    \n    model.add(\n        Conv2D(\n            filters = 128, kernel_size = (3, 3),\n            activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),\n            strides = (1, 1), padding = 'same'\n        )\n    )\n\n    model.add(\n        Conv2D(\n            filters = 128, kernel_size = (3, 3),\n            activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),\n            strides = (1, 1), padding = 'same'\n        )\n    )\n\n    model.add(\n        MaxPooling2D(\n            pool_size = (2, 2),\n            strides = (2, 2)\n        )\n    )\n\n    model.add(\n        Conv2D(\n            filters = 256, kernel_size = (3, 3),\n            activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),\n            strides = (1, 1), padding = 'same'\n        )\n    )\n\n    model.add(\n        Conv2D(\n            filters = 256, kernel_size = (3, 3),\n            activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),\n            strides = (1, 1), padding = 'same'\n        )\n    )\n\n    model.add(\n        MaxPooling2D(\n            pool_size = (2, 2),\n            strides = (2, 2)\n        )\n    )\n    \n    model.add(Flatten())\n    \n    model.add(\n        Dense(\n            units = 256, activation='relu',\n            kernel_initializer = tf.initializers.GlorotNormal()\n        )\n    )\n    \n    model.add(\n        Dense(\n            units = 256, activation='relu',\n            kernel_initializer = tf.initializers.GlorotNormal()\n        )\n    )\n    \n    model.add(\n        Dense(\n            units = 10, activation='softmax'\n        )\n    )\n    \n\n    '''\n    # Compile CNN-\n    model.compile(\n        loss=tf.keras.losses.categorical_crossentropy,\n        # optimizer='adam',\n        optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0003),\n        metrics=['accuracy']\n    )\n    '''\n    \n    \n    return model\n\n\n# Load trained model from before-\nbest_model = conv6_cnn()\nbest_model.load_weights(&quot;best_weights.h5&quot;)\n</code></pre>\n<p>I came across <a href=\"https://github.com/tensorflow/tensorflow/issues/6264\" rel=\"nofollow noreferrer\">this</a> GitHub answer of freezing certain weights during training. On it's basis, I coded the following to freeze weights in the first and sixth conv layers:</p>\n<pre><code>conv1 = pruned_model.trainable_weights[0]\n\n# Find all weights less than a threshold (0.1) and set them to zero-\nconv1 = tf.where(conv1 &lt; 0.1, 0, conv1)\n\n# For all weights set to zero, stop training them-\nconv1 = tf.where(conv1 == 0, tf.stop_gradient(conv1), conv1)\n\n\n# Sanity check: number of parameters set at 0-\ntf.math.count_nonzero(conv1, axis = None).numpy()\n# 133\n\n# Original number of paramaters-\ntf.math.count_nonzero(best_model.trainable_weights[0], axis = None).numpy()\n# 1728\n\n# Assign conv layer1 back to pruned model-\npruned_model.trainable_weights[0].assign(conv1)\n\n# Sanity check-\ntf.math.count_nonzero(pruned_model.trainable_weights[0], axis = None).numpy()\n# 133\n\n# conv layer 6-\nconv6 = pruned_model.trainable_weights[10]\n\n# Find all weights less than a threshold (0.1) and set them to zero-\nconv6 = tf.where(conv6 &lt; 0.1, 0, conv6)\n\n# For all weights set to zero, stop training them-\nconv6 = tf.where(conv6 == 0, tf.stop_gradient(conv6), conv6)\n\n# Sanity check: number of parameters set at 0-\ntf.math.count_nonzero(conv6, axis = None).numpy()\n# 5369\n\n# Original number of paramaters-\ntf.math.count_nonzero(best_model.trainable_weights[10], axis = None).numpy()\n# 589824\n\n# Assign conv layer6 back to pruned model-\npruned_model.trainable_weights[10].assign(conv6)\n\n# Sanity check-\ntf.math.count_nonzero(pruned_model.trainable_weights[10], axis = None).numpy()\n# 5369\n\n\n# Train model for 10 epochs for testing:\n\n# Compile CNN-\npruned_model.compile(\n    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.01),\n    metrics=['accuracy']\n)\n\nhistory = pruned_model.fit(\n    x = X_train, y = y_train,\n    epochs = 10, validation_data = (X_test, y_test)\n)\n</code></pre>\n<p>However, after training when I check the number of non-zero weights:</p>\n<pre><code># first conv layer-\ntf.math.count_nonzero(pruned_model.trainable_weights[0], axis = None).numpy()\n\n# sixth conv layer-\ntf.math.count_nonzero(pruned_model.trainable_weights[10], axis = None).numpy()\n</code></pre>\n<p>The weights have increased in numbers again. They should have been 133 and 5369, but they are not.</p>\n<p>Help?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 141}]
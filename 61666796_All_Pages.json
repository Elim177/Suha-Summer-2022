[{"items": [{"tags": ["tensorflow", "moving-average"], "owner": {"account_id": 2579637, "reputation": 157, "user_id": 2236600, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8c003cc2d7f11c8bff67e5e5cc52e6d4?s=256&d=identicon&r=PG", "display_name": "user2236600", "link": "https://stackoverflow.com/users/2236600/user2236600"}, "is_answered": false, "view_count": 717, "answer_count": 1, "score": 5, "last_activity_date": 1620765119, "creation_date": 1588882001, "question_id": 61666796, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61666796/tensorflow-addon-moving-average", "title": "tensorflow addon moving average", "body": "<p>Does this implementation (<a href=\"https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/MovingAverage\" rel=\"noreferrer\">https://www.tensorflow.org/addons/api_docs/python/tfa/optimizers/MovingAverage</a>) is the same as ExponentialMovingAverage in tensorflow train module (<a href=\"https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\" rel=\"noreferrer\">https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage</a>)?</p>\n\n<pre><code>import tensorflow as tf\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\nloss_obj = tf.keras.losses.CategoricalCrossentropy()\n\n@tf.function\ndef train_step(inputs, outputs):\n\n    with tf.GradientTape() as tape:\n        start, end = model([inputs[0], inputs[1], inputs[2]], training=True)\n        start_truth, end_truth = tf.squeeze(outputs[0]), tf.squeeze(outputs[1])\n        start_loss = loss_obj(start_truth, start)\n        end_loss = loss_obj(end_truth, end)\n        total_loss = start_loss + end_loss\n\n    model_gradients = tape.gradient(total_loss, model.trainable_variables)\n    opt_op = optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))\n\n    ema = tf.train.ExponentialMovingAverage(decay=0.9999)\n    with tf.control_dependencies([opt_op]):\n        ema.apply(model.trainable_variables)\n\n    del tape\n    return total_loss, start_loss, end_loss\n</code></pre>\n\n<p>is the same as</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_addons as tfa\n\noptimizer = tfa.optimizers.MovingAverage(Adam(learning_rate=5e-5))\nloss_obj = tf.keras.losses.CategoricalCrossentropy()\n\n@tf.function\ndef train_step(inputs, outputs):\n\n    with tf.GradientTape() as tape:\n        start, end = model([inputs[0], inputs[1], inputs[2]], training=True)\n        start_truth, end_truth = tf.squeeze(outputs[0]), tf.squeeze(outputs[1])\n        start_loss = loss_obj(start_truth, start)\n        end_loss = loss_obj(end_truth, end)\n        total_loss = start_loss + end_loss\n\n    model_gradients = tape.gradient(total_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(model_gradients, model.trainable_variables))\n\n    del tape\n    return total_loss, start_loss, end_loss\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 84}]
[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "tensorflow-datasets"], "owner": {"user_type": "does_not_exist", "display_name": "user4911648"}, "is_answered": false, "view_count": 3747, "answer_count": 1, "score": 1, "last_activity_date": 1520430983, "creation_date": 1519988437, "last_edit_date": 1520430983, "question_id": 49067798, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49067798/understanding-the-tfrecorddataset-map-function-in-more-detail", "title": "Understanding the TfRecordDataset map function in more detail", "body": "<p>I am reading my <code>TfRecordData</code> like this:</p>\n\n<pre><code>dataset = tf.data.TFRecordDataset(filenames)\ndataset = dataset.map(_parse_function)\n\ndef _parse_function(example_proto):\n\n    features = {\n        'data':         tf.VarLenFeature(tf.float32),\n        'label':        tf.VarLenFeature(tf.float32),\n        'resolution':   tf.FixedLenFeature([], tf.int64)\n    }\n\n    parsed_features = tf.parse_single_example(example_proto, features)\n\n    resolution = tf.cast(parsed_features['resolution'], tf.int32)\n    tensor_feature1 = tf.sparse_tensor_to_dense(parsed_features['data'])\n    tensor_feature2 = tf.sparse_tensor_to_dense(parsed_features['label'])\n    ...\n\n    input = tf.reshape(tensor_feature1, [1, 256, 256])\n    output = tf.reshape(tensor_feature2, [1, 256, 256])\n\n    return input, output\n</code></pre>\n\n<p>Here I can only parse a single feature at once. Is it possible to concatenate my features in order to stack the input samples somehow like this:</p>\n\n<pre><code>for i in range(0,20)\n    parsed_features = tf.parse_single_example(example_proto, features)\n    tensor_feature1 = tf.sparse_tensor_to_dense(parsed_features['data'])\n\n    inputs = tf.stack(tensor_feature1, axis=0) # shape = [20, 256, 256]\nreturn inputs, output\n</code></pre>\n\n<p>** EDIT **</p>\n\n<p>I made progress:</p>\n\n<pre><code>datasets = []\n\nfor idx in range(20):\n    dataset = tf.data.TFRecordDataset(filenames, 'GZIP')\n    dataset = dataset.skip(idx)\n    dataset = dataset.map(_parse_function, num_parallel_calls=tf.constant(FLAGS.num_parallel_calls, dtype=tf.int32))\n    dataset = dataset.batch(20)\n    datasets.append(dataset)\n</code></pre>\n\n<p>Since batch only gives me consecutive batches I used <code>.skip(idx)</code> in hope to offset my starting point in order to sth like:</p>\n\n<pre><code>[1,2,3,4,5] --&gt; [1,2][2,3][3,4] rather than: [1,2][3,4]\n</code></pre>\n\n<p>I am not sure whether this is correct tho. The only problem now is: I also load 20 outputs rather than just one output. I was thinking of using <code>zip</code> but for now I could not get it done.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 285}]
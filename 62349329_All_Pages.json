[{"items": [{"tags": ["tensorflow", "keras", "deep-learning", "parallel-processing", "tensorflow2.0"], "owner": {"account_id": 7120877, "reputation": 337, "user_id": 5445548, "user_type": "registered", "accept_rate": 40, "profile_image": "https://i.stack.imgur.com/pmGVM.png?s=256&g=1", "display_name": "maracuja", "link": "https://stackoverflow.com/users/5445548/maracuja"}, "is_answered": false, "view_count": 1674, "answer_count": 3, "score": 4, "last_activity_date": 1659606604, "creation_date": 1591982420, "question_id": 62349329, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62349329/distributed-training-using-mirrorstrategy-in-tensorflow-2-2-with-custom-training", "title": "Distributed training using MirrorStrategy in tensorflow 2.2 with custom training loop not working - getting stuck when updating gradients", "body": "<p>I'm using tf.distribute.Strategy to train a model, based on unet, with MirrorStrategy over two (or more) gpus. Below is my code for the custom train loop I use for the forward and backward passes of the network. For some reason, the logits, loss and gradients of the first batch of the first epoch are calculated but then it gets stuck at optimizer.apply_gradients(zip(gradients, model.trainable_variables). I can't for the life of me what the problem is so any help would be much appreciated.</p>\n\n<pre><code>import os\nimport glob\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Progbar\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Activation, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.layers import UpSampling2D, concatenate\nfrom evaluation import diceCoef\n\ntf.config.experimental_run_functions_eagerly(True)\n\n\nclass Train():\n    def __init__(self, model, lossFunc, optimizer, strategy, epochs, batchSize):\n        self.epochs = epochs\n        self.batchSize = batchSize\n        self.strategy = strategy\n        #self.lossFunc = lossFunc\n        self.lossFunc = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n        self.optimizer = optimizer\n        self.model = model\n        self.history = {'trainloss': [], 'trainmetric':[], 'valmetric': []}\n\n\n    def computeLoss(self, yPred, yTrue):\n\n        #loss = tf.reduce_sum(self.lossFunc(yPred, yTrue)) * (1./self.batchSize)\n        loss = self.lossFunc(yPred, yTrue)\n        loss = loss * (1. / self.strategy.num_replicas_in_sync)\n        #print(loss)\n\n        return loss\n\n\n    @tf.function\n    def trainStep(self, x, y, i):\n        #x = batch[0]\n        #y = batch[1]\n        x = tf.cast(x, tf.float32)\n        y = tf.cast(y, tf.float32) \n        #print(self.model.trainable_variables)\n        with tf.GradientTape() as tape:\n            logits = self.model(x, training=True)\n            logits = tf.cast(logits, tf.float32) \n            loss = self.computeLoss(logits, y)\n            #loss = self.lossFunc(logits, y)\n            #print('loss', loss)\n        gradients = tape.gradient(loss, self.model.trainable_variables)\n        print(len(gradients))\n        print(len(self.model.trainable_variables))\n        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n        return loss, logits\n\n\n    @tf.function\n    def validStep(self, x, y):\n        logits = self.model(x, training=False)\n        loss = self.lossFunc(y, logits)\n\n        return loss, logits,\n\n\n    @tf.function\n    def distributedTrainEpoch(self, dataset, trainSteps):\n\n        totalDice = 0\n        totalLoss = 0\n        #prog = Progbar(trainSteps-1)\n\n        for i, batch in enumerate(dataset):\n            x = batch[0]\n            #y = tf.expand_dims(batch[1], axis=-1)\n            y = batch[1]\n            batchLoss, logits = self.strategy.run(self.trainStep, args=(x,y,i))\n            print('batchloss', batchLoss)\n            #pred = (logits.numpy() &gt; 0.5).astype('int16').astype(np.float16)\n            #batchDice = self.strategy.run(diceCoef, args=(pred, y))\n            totalLoss += self.strategy.reduce(tf.distribute.ReduceOp.SUM, batchLoss, axis=None)\n            #totalDice += self.strategy.reduce(tf.distribute.ReduceOp.SUM, batchDice, axis=None)\n            #prog.update(i)\n\n        return totalLoss, totalDice\n\n\n    @tf.function\n    def distributedValidEpoch(self, dataset):\n        totalLoss = 0\n        totalDice = 0\n        for d in dataset:\n            x = d[0]\n            y = tf.expand_dims(d[1], axis=-1)\n            loss, logits = self.strategy.run(self.validStep, args=(x, y))\n            pred = (logits.numpy() &gt; 0.5).astype('int16').astype(np.float16)\n            dice = self.strategy.run(diceCoef, args=(pred, y))\n            totalLoss += self.strategy.reduce(tf.distribute.ReduceOp.SUM, loss, axis=None)\n            totalDice += self.strategy.reduce(tf.distribute.ReduceOp.SUM, dice, axis=None)\n\n        return totalLoss, totalDice\n\n\n    def forward(self, trainDataset, validDataset, trainSteps, validSteps):\n\n        for e in range(self.epochs):\n\n            tf.print('Epoch: {}/{}...'.format(e+1, self.epochs), end=\"\")\n\n            trainLoss, trainDice = self.distributedTrainEpoch(trainDataset, trainSteps)\n            avgTrainDice = trainDice.numpy()[0] / trainSteps\n            avgTrainLoss = trainLoss.numpy() / trainSteps\n            print('train', avgTrainDice)\n            print('loss', avgTrainLoss)\n\n            tf.print(' Epoch: {}/{},  loss - {:.2f}, dice - {:.2f}'.format(e+1,\n                   self.epochs, avgTrainLoss, avgTrainDice), end=\"\")\n\n            valLoss, valDice = self.distributedValidEpoch(validDataset)\n\n            avgValidDice = valDice.numpy()[0] / validSteps\n            avgValidLoss = valLoss.numpy() / validSteps\n\n\n            self.history['trainmetric'].append(avgTrainDice)\n            self.history['trainloss'].append(avgTrainLoss)\n            self.history['valmetric'].append(avgValidDice)\n            self.history['valmetric'].append(avgValidLoss)\n\n            tf.print('  val_loss - {:.3f}, val_dice - {:.3f}'.format(avgValidLoss, avgValidDice))\n\n        return self.model, history\n</code></pre>\n\n<p>This is the part of the code from another script that sets up the strategy scope, builds the model and calls the train class.</p>\n\n<pre><code> with strategy.scope():\n\n        if model == 'fcn8':\n            print('Model: {}'.format(model))\n            with tf.device('/cpu:0'):\n                if api == 'functional':\n                    fcn = FCN()\n                    model = fcn.getFCN8()\n                elif api=='subclass':\n                    model = FCN()\n\n        elif model == 'unet':\n            print('Model: {}'.format(model))\n            with tf.device('/cpu:0'):\n                if api=='functional':\n                    unetModel = unet2.UnetFunc()\n                    model = unetModel.unet()\n                elif api=='subclass':\n                    model = unetsc.UnetSC(filters=filters)\n                    model.build((1, imgDims, imgDims, 3))\n\n        elif model == 'unetmini':\n            print('Model: {}'.format(model))\n            with tf.device('/cpu:0'):\n                if api == 'functional':\n                    unetminiModel = UnetMini(filters=filters)\n                    model = unetminiModel.unetmini()\n                elif api=='subclass':\n                    model = UnetMini(filters)\n\n        elif model == 'resunet':\n            print('Model: {}'.format(model))\n            with tf.device('/cpu:0'):\n                if api=='functional':\n                    resunetModel =  ResUnet(filters)\n                    model = resunetModel.ResUnetFunc()\n                elif api=='subclass':\n                    model = ResunetSc(filters)\n\n        elif model == 'resunet-a':\n            print('Model: {}'.format(model))\n            with tf.device('/cpu:0'):\n                if api=='functional':\n                    resunetModel =  ResUnetA(filters)\n                    model = resunetModel.ResUnetAFunc()\n                elif api=='subclass':\n                    model = ResunetASc(filters)\n\n        elif model == 'attention':\n            print('Model: {}'.format(model))\n            with tf.device('/cpu:0'):\n                if api == 'functional':\n                    attenModel = AttenUnetFunc(filters)\n                    model = attenModel.attenUnet()\n                elif api=='subclass':\n                    model = AttenUnetSC(filters)\n        else:\n            raise ValueError('No model requested, please update config file')\n\n#        print('trainable variables', str(model.trainable_variables))\n\n        trainer = train.Train(model, loss, optimizer, strategy, epoch, batchSize)\n\n        trainDistDataset = strategy.experimental_distribute_dataset(trainDataset)\n        validDistDataset = strategy.experimental_distribute_dataset(validDataset)\n\n        model, history = trainer.forward(trainDistDataset, validDistDataset, trainSteps, validSteps)\n</code></pre>\n\n<p>And the subclassed unet model as follows:</p>\n\n<pre><code>class UnetSC(Model):\n    def __init__(self, filters=[16,32,64,128, 256], finalActivation='sigmoid', activation='relu', \n                    nOutput=1, kSize=(3,3), pSize=(2,2), dropout=0, normalize=True, padding='same', dtype='float32'):\n        super(UnetSC, self).__init__(dtype=dtype)\n\n        self.normalize = normalize\n        self.conve1_1 = Conv2D(filters[0], kSize, activation='relu', padding='same', name='greg')\n        self.batchnorm1 = BatchNormalization(name='greggggggg')\n        self.conve1_2 = Conv2D(filters[0], kSize, activation='relu', padding='same')\n        self.batchnorm2 = BatchNormalization()\n        self.pool1 = MaxPooling2D((2, 2))\n\n        self.conve2_1 = Conv2D(filters[1], kSize, activation='relu', padding='same')\n        self.batchnorm3 = BatchNormalization()\n        self.conve2_2 = Conv2D(filters[1], kSize, activation='relu', padding='same')\n        self.batchnorm4 = BatchNormalization()\n        self.pool2 = MaxPooling2D((2, 2))\n\n        self.conve3_1 = Conv2D(filters[2], kSize, activation='relu', padding='same')\n        self.batchnorm5 = BatchNormalization()\n        self.conve3_2 = Conv2D(filters[2], kSize, activation='relu', padding='same')\n        self.batchnorm6 = BatchNormalization()\n        self.pool3 = MaxPooling2D((2, 2))\n\n        self.conve4_1 = Conv2D(filters[3], kSize, activation='relu', padding='same')\n        self.batchnorm7 = BatchNormalization()\n        self.conve4_2 = Conv2D(filters[3], kSize, activation='relu', padding='same', name='finalencoder')\n        self.batchnorm8 = BatchNormalization()\n        self.pool4 = MaxPooling2D((2, 2))\n\n        self.convb_1 = Conv2D(filters[4], kSize, activation='relu', padding='same')\n        self.batchnorm9 = BatchNormalization()\n        self.convb_2 = Conv2D(filters[4], kSize, activation='relu', padding='same')\n        self.batchnorm10 = BatchNormalization()\n\n        self.upsampling1 = UpSampling2D((2, 2))\n        self.conc1 = Concatenate()\n        self.convd1_1 = Conv2D(filters[3], kSize, activation='relu', padding='same')\n        self.batchnorm11 = BatchNormalization()\n        self.convd1_2 = Conv2D(filters[3], kSize, activation='relu', padding='same')\n        self.batchnorm12 = BatchNormalization()\n\n        self.upsampling2 = UpSampling2D((2, 2))\n        self.conc2 = Concatenate()\n        self.convd2_1 = Conv2D(filters[2], kSize, activation='relu', padding='same')\n        self.batchnorm13 = BatchNormalization()\n        self.convd2_2 = Conv2D(filters[2], kSize, activation='relu', padding='same')\n        self.batchnorm14 = BatchNormalization()\n\n        self.upsampling3 = UpSampling2D((2, 2))\n        self.conc3 = Concatenate()\n        self.convd3_1 = Conv2D(filters[1], kSize, activation='relu', padding='same')\n        self.batchnorm15 = BatchNormalization()\n        self.convd3_2 = Conv2D(filters[1], kSize, activation='relu', padding='same')\n        self.batchnorm16 = BatchNormalization()\n\n        self.upsampling4 = UpSampling2D((2, 2))\n        self.conc4 = Concatenate()\n        self.convd4_1 = Conv2D(filters[0], kSize, activation='relu', padding='same')\n        self.batchnorm17 = BatchNormalization()\n        self.convd4_2 = Conv2D(filters[0], kSize, activation='relu', padding='same')\n        self.batchnorm18 = BatchNormalization()\n\n        self.final = Conv2D(nOutput, kernel_size=(1, 1), strides=(1, 1), activation=finalActivation)\n\n\n    def call(self, x, training=True):\n\n        e1 = self.conve1_1(x)\n        e1 = self.batchnorm1(e1)\n        e1 = self.conve1_2(e1)\n        e1 = self.batchnorm2(e1)\n        p1 = self.pool1(e1)\n\n        e2 = self.conve2_1(p1)\n        e2 = self.batchnorm3(e2)\n        e2 = self.conve2_2(e2)\n        e2 = self.batchnorm4(e2)\n        p2 = self.pool2(e2)\n\n        e3 = self.conve3_1(p2)\n        e3 = self.batchnorm5(e3)\n        e3 = self.conve3_2(e3)\n        e3 = self.batchnorm6(e3)\n        p3 = self.pool3(e3)\n\n        e4 = self.conve4_1(p3)\n        e4 = self.batchnorm7(e4)\n        e4 = self.conve4_2(e4)\n        e4 = self.batchnorm8(e4)\n        p4 = self.pool4(e4)\n\n        b = self.convb_1(p4)\n        b = self.batchnorm9(b)\n        b = self.convb_2(b)\n        b = self.batchnorm10(b)\n\n        d1 = self.upsampling1(b)\n        d1 = self.conc1([e4, d1])\n        d1 = self.convd1_1(d1)\n        d1 = self.batchnorm11(d1)\n        d1 = self.convd1_2(d1)\n        d1 = self.batchnorm12(d1)\n\n        d2 = self.upsampling2(d1)\n        d2 = self.conc2([e3, d2])\n        d2 = self.convd2_1(d2)\n        d2 = self.batchnorm13(d2)\n        d2 = self.convd2_2(d2)\n        d2 = self.batchnorm14(d2)\n\n        d3 = self.upsampling3(d2)\n        d3 = self.conc3([e2, d3])\n        d3 = self.convd3_1(d3)\n        d3 = self.batchnorm15(d3)\n        d3 = self.convd3_2(d3)\n        d3 = self.batchnorm16(d3)\n\n        d4 = self.upsampling4(d3)\n        d4 = self.conc4([e1, d4])\n        d4 = self.convd4_1(d4)\n        d4 = self.batchnorm17(d4)\n        d4 = self.convd4_2(d4)\n        d4 = self.batchnorm18(d4)\n\n        x = self.final(d4)\n\n        return x\n\n\nu = UnetSC()\nu = u.build((1, 256,256,3))\n\nThe error output trace \n\nUsing TensorFlow backend.\n\nNow executing following model: unet_32_adam_diceloss_FR_0_2.5x_germ_32\n2020-06-12 18:14:00.672680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n2020-06-12 18:14:00.815119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:3f:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\ncoreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n2020-06-12 18:14:00.816539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \npciBusID: 0000:40:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\ncoreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n2020-06-12 18:14:00.817342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2020-06-12 18:14:00.820640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2020-06-12 18:14:00.823040: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n2020-06-12 18:14:00.823833: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n2020-06-12 18:14:00.826794: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n2020-06-12 18:14:00.829026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n2020-06-12 18:14:00.834643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2020-06-12 18:14:00.839962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\n2020-06-12 18:14:00.840532: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n2020-06-12 18:14:00.855173: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200000000 Hz\n2020-06-12 18:14:00.857769: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x58fdc10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n2020-06-12 18:14:00.857804: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n2020-06-12 18:14:01.277928: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59680f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n2020-06-12 18:14:01.278008: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-16GB, Compute Capability 7.0\n2020-06-12 18:14:01.278031: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-16GB, Compute Capability 7.0\n2020-06-12 18:14:01.284602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \npciBusID: 0000:3f:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\ncoreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n2020-06-12 18:14:01.291638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: \npciBusID: 0000:40:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\ncoreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.75GiB deviceMemoryBandwidth: 836.37GiB/s\n2020-06-12 18:14:01.291808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2020-06-12 18:14:01.291883: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n2020-06-12 18:14:01.291935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n2020-06-12 18:14:01.291988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n2020-06-12 18:14:01.292039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n2020-06-12 18:14:01.292086: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n2020-06-12 18:14:01.292151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2020-06-12 18:14:01.304148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1\n2020-06-12 18:14:01.304295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n2020-06-12 18:14:01.312107: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n2020-06-12 18:14:01.312143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 \n2020-06-12 18:14:01.312164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y \n2020-06-12 18:14:01.312180: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N \n2020-06-12 18:14:01.318105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14864 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:3f:00.0, compute capability: 7.0)\n2020-06-12 18:14:01.320434: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14864 MB memory) -&gt; physical GPU (device: 1, name: Tesla V100-PCIE-16GB, pci bus id: 0000:40:00.0, compute capability: 7.0)\n\nEpoch: 1/40...WARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `run` inside a tf.function to get the best performance.\n2020-06-12 18:14:16.135798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n2020-06-12 18:14:18.493751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n\n\n74\n74\n\n74\n74\n</code></pre>\n\n<p>Then it just sticks here. Please help!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 53}]
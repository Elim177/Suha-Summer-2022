[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 14386550, "reputation": 805, "user_id": 10392393, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/158e9cd5e0945cfb6ff4407c8c049768?s=256&d=identicon&r=PG&f=1", "display_name": "azerila", "link": "https://stackoverflow.com/users/10392393/azerila"}, "is_answered": false, "view_count": 59, "answer_count": 0, "score": 0, "last_activity_date": 1571389345, "creation_date": 1571388393, "last_edit_date": 1571389345, "question_id": 58447190, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58447190/how-to-optimize-layer-weights-in-tensorflow-2-0-with-random-starting-points", "title": "How to optimize layer weights in Tensorflow 2.0 with random starting points", "body": "<p>To avoid local optima, I want to randomly pick some sets of weights and choose the best one to run the optimizer on so that it is less likely to fall in a local optimum. </p>\n\n<p>Or another choice would be to run the optimizer on each set of weights and then pick the best. </p>\n\n<p>My model looks as follows:</p>\n\n<pre><code>class model(Model):\n    def __init__(self, input_dim=2, dtype=tf.float64):\n        super(model, self).__init__()\n\n        self.length_scales = Dense(1, kernel_initializer = 'random_uniform',\n                             bias_initializer='zeros',\n                             bias_constraint=constraints.max_norm(0.),\n                             activation='linear',\n                             name='hp')\n    @tf.function   \n    def call(self, x):\n        x_embedd      = self.length_scales(x)\n        return x_embedd\n</code></pre>\n\n<p>However, I'm not sure how to do it in a way that is more computationally efficient, for example without completely reinitializing the model.</p>\n\n<p>My application is also Gaussian processes and not necessarily deep learning.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 56}]
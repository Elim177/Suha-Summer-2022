[{"items": [{"tags": ["tensorflow"], "owner": {"account_id": 9760736, "reputation": 59, "user_id": 7235925, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/683f965b8d1f7f6690550e30d76b5a77?s=256&d=identicon&r=PG&f=1", "display_name": "Nick", "link": "https://stackoverflow.com/users/7235925/nick"}, "is_answered": false, "view_count": 110, "answer_count": 1, "score": 1, "last_activity_date": 1619716156, "creation_date": 1612790289, "question_id": 66102424, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66102424/tensorflow-gradient-with-repect-to-a-subset-of-trainable-variables", "title": "tensorflow gradient with repect to a subset of trainable variables", "body": "<p>I would like to take the gradient of the loss function just with respect to a single weight in a layer. For taking the derivative with respect to the entire first layer, the following works fine</p>\n<pre><code>with tf.GradientTape() as tape:\ng = tape.gradient(loss(y, model(X)), model.trainable_variables[0])\n</code></pre>\n<p>However I would like to do something like</p>\n<pre><code>with tf.GradientTape() as tape:\ng = tape.gradient(loss(y, y_pred), model.trainable_variables[0][:1])\n</code></pre>\n<p>but this returns None. I think accessing the variable sets it to a value and thus the derivative with respect to it is None. Is there some way to take the derivative with respect to a slice of a layers variables?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 213}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning", "eager-execution"], "owner": {"account_id": 26373, "reputation": 43816, "user_id": 68571, "user_type": "registered", "accept_rate": 79, "profile_image": "https://i.stack.imgur.com/Yw9Lg.png?s=256&g=1", "display_name": "VansFannel", "link": "https://stackoverflow.com/users/68571/vansfannel"}, "is_answered": true, "view_count": 762, "answer_count": 1, "score": 1, "last_activity_date": 1591871357, "creation_date": 1591810694, "last_edit_date": 1591871357, "question_id": 62309736, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62309736/use-hamming-distance-loss-function-with-tensorflow-gradienttape-no-gradients-i", "title": "Use Hamming Distance Loss Function with Tensorflow GradientTape: no gradients. Is it not differentiable?", "body": "<p>I'm using Tensorflow 2.1 and Python 3, creating my custom training model following the tutorial \"<a href=\"https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough\" rel=\"nofollow noreferrer\">Tensorflow - Custom training: walkthrough</a>\".</p>\n\n<p>I'm trying to use Hamming Distance on my loss function:</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_addons as tfa\n\ndef my_loss_hamming(model, x, y):\n  global output\n  output = model(x)\n\n  return tfa.metrics.hamming.hamming_loss_fn(y, output, threshold=0.5, mode='multilabel')\n\n\ndef grad(model, inputs, targets):\n  with tf.GradientTape() as tape:\n      tape.watch(model.trainable_variables)\n      loss_value = my_loss_hamming(model, inputs, targets)\n\n  return loss_value, tape.gradient(loss_value, model.trainable_variables)\n</code></pre>\n\n<p>When I call it:</p>\n\n<pre><code>loss_value, grads = grad(model, feature, label)\noptimizer.apply_gradients(zip(grads, model.trainable_variables))\n</code></pre>\n\n<p><code>grads</code> variable is a list with 38 None.</p>\n\n<p>And I get the error:</p>\n\n<pre><code>No gradients provided for any variable: ['conv1_1/kernel:0', ...]\n</code></pre>\n\n<p>Is there any way to use Hamming Distance without \"interrupts the gradient chain registered by the gradient tape\"?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 285}]
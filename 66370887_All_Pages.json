[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "deep-learning"], "owner": {"account_id": 7424649, "reputation": 13635, "user_id": 9215780, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-lqxossnzkSU/AAAAAAAAAAI/AAAAAAAAAJg/pHrYwIfRc-k/photo.jpg?sz=256", "display_name": "M.Innat", "link": "https://stackoverflow.com/users/9215780/m-innat"}, "is_answered": true, "view_count": 557, "accepted_answer_id": 66417795, "answer_count": 1, "score": 4, "last_activity_date": 1645700169, "creation_date": 1614264682, "last_edit_date": 1645700169, "question_id": 66370887, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66370887/understand-and-implement-element-wise-attention-module", "title": "Understand and Implement Element-Wise Attention Module", "body": "<p>Please add a minimum comment on your thoughts so that I can improve my query. Thank you.  -)</p>\n<hr />\n<p>I'm trying to understand and implement a research work on <a href=\"https://www.sciencedirect.com/science/article/abs/pii/S1361841520302103\" rel=\"nofollow noreferrer\">Triple Attention Learning</a>, which consists on</p>\n<pre><code>- channel-wise attention  (a)\n- element-wise attention  (b)\n- scale-wise attention    (c)\n</code></pre>\n<p>The mechanism is integrated experimentally inside the <code>DenseNet</code> model. The arch of the whole model's diagram is <a href=\"https://i.stack.imgur.com/Zu8Yy.png\" rel=\"nofollow noreferrer\">here</a>. The <strong>channel-wise</strong> attention module is simply nothing but the <strong>squeeze and excitation</strong> block. That gives a <code>sigmoid</code> output further to the <strong>element-wise</strong> attention module. Below is the more precise feature flow diagram of these modules (<code>a</code>, <code>b</code>, and <code>c</code>).</p>\n<img src=\"https://i.stack.imgur.com/nHnxL.png\" width=\"600\"/> \n<hr />\n<h3>Theory</h3>\n<p>For the most part, I was able to understand and implement it but was a bit lost in the <code>Element-Wise</code> attention section (part <code>b</code> from the above diagram). This is where I need your assistance. -)</p>\n<p>Here is a little theory on this topic to give you a rough idea of what all this is about. Please note, The paper is not openly accessible <strong>now</strong> but at its early stage of release on the publisher page <strong>it was</strong> free to get and I saved it at that time. And to be fair to all, I'm sharing it with you, <a href=\"https://drive.google.com/file/d/1BtH2qnAxWzGVScJ9tW4nGlZI0MI5IGGu/view?usp=sharing\" rel=\"nofollow noreferrer\">Link</a>. Anyway, from the paper (Section <strong>4.3</strong>) it shows:</p>\n<img src=\"https://i.stack.imgur.com/wjPTL.jpg\" width=\"400\"/> \n<p>So <strong>first</strong> of all, <code>f(att)</code> function (which is in the first inplace diagram, left-middle part or <code>b</code>) consists of three convolution layers with <strong>512</strong> kernels with <code>1 x 1</code>, <strong>512</strong> kernels with <code>3 x 3</code> and <code>C</code> kernels with <code>1 x 1</code>. Here <code>C</code> is the number of the classifier. And with <code>Softmax</code> activation!</p>\n<p>Next, it applies to the <code>Channel-Wise</code> attention module which we mentioned that simply a <code>SENet</code> module and gave a <code>sigmoid</code> probability score i.e <code>X(CA)</code>. So, from the function of <code>f(att)</code>, we're getting <code>C</code> times <code>softmax</code> probability scores and each of these scores get multiplied with <code>sigmoid</code> output and finally produces feature maps <code>A</code> (according to the equation <strong>4</strong> of the above diagram).</p>\n<p><strong>Second</strong>, there is a <code>C</code> linear classifier that implemented as a <code>1 x 1</code> - <code>C</code> kernels convolution layer. This layer also applied to the <code>SENet</code> module's output i.e. <code>X(CA)</code>, to each feature vector pixel-wise. And in the end, it gives an output of feature maps <code>S</code> (equation <strong>5</strong> shown below diagram).</p>\n<p>And <strong>Third</strong>, they element-wise multiply each confidence score (of <code>S</code>) with the corresponding attention element <code>A</code>. This multiplication is on purpose. They did it for preventing unnecessary attention on the feature maps.  To make it effective, they also use the <code>weighted cross-entropy</code> loss function to minimize it here between the classification <strong>ground truth</strong> and the <strong>score vector</strong>.</p>\n<img src=\"https://i.stack.imgur.com/upUs2.jpg\" width=\"400\"/> \n<p><strong>My Query</strong></p>\n<p>Mostly I don't get properly the minimization strategies in the middle of the network. I want someone who can give me a proper understanding and implementation of this `element-wise attention mechanism in detail that proposed in the mentioned paperwork (section <strong>4.3</strong>).</p>\n<hr />\n<h2>Implement</h2>\n<p>Here is a minimum code to get started. It should enough I guess. This is shallow implementation but too much away from the original element-wise module. I'm not sure how to implement it properly. For now, I want it as a layer that supposed to plug and play to any model. I was trying with MNIST and a simple <code>Conv</code> net.</p>\n<p>In a summary, for MNIST, we should have a network that contains both the <code>channel-wise</code> and <code>element-wise</code> attention model followed by the last <strong>10</strong> unit <code>softmax</code> layer. So for example:</p>\n<pre><code>Net: Conv2D - Attentions-Module - GAP - Softmax(10)\n</code></pre>\n<p>The <code>Attention-Module</code> consists of those two-part: <code>Channel-wise</code> and <code>Element-wise</code>, and the <code>Element-wise</code>supposed to have <code>Softmax</code> too that minimizes weighted <code>CE</code> loss function to <code>ground-truth</code> and <code>score vector</code> coming from this module (according to the paperwork, already described above too). The module also passes <strong>weighted feature maps</strong> to the consecutive layers. For more clarity here is a simple schematic diagram of what we're looking for</p>\n<img src=\"https://i.stack.imgur.com/2hP39.png\" width=\"500\"/> \n<p>Ok, for the <code>channel-wise</code> attention which should give us a single probability score (<code>sigmoid</code>), let's use a fake layer for now for simplicity:</p>\n<pre><code>class FakeSE(tf.keras.layers.Layer):\n    def __init__(self):\n        super(Block, self).__init__()\n        # conv layer\n        self.conv = tf.keras.layers.Conv2D(10, padding='same',\n                                           kernel_size=3)\n    def call(self, input_tensor, training=False):\n        x = self.conv(input_tensor)\n        return tf.math.sigmoid(x)\n</code></pre>\n<p>And for the <code>element-wise</code> attention part, following is the failed attempt so far:</p>\n<pre><code>class ElementWiseAttention(tf.keras.layers.Layer):\n    def __init__(self):\n        # for simplicity the f(attn) function here has 2 convolution instead of 3\n        # self.conv1, and self.conv2\n        self.conv1 = tf.keras.layers.Conv2D(16, \n                                            kernel_size=1, \n                                            strides=1, padding='same',\n                                            use_bias=True, activation=tf.nn.silu)\n\n        self.conv2 = tf.keras.layers.Conv2D(10, \n                                            kernel_size=1, \n                                            strides=1, padding='same',\n                                            use_bias=False, activation=tf.keras.activations.softmax)\n        \n        # fake SENet or channel-wise attention module \n        self.cam = FakeSE()\n        \n        # a linear layer \n        self.linear = tf.keras.layers.Conv2D(10,\n                                           kernel_size=1,\n                                           strides=1, padding='same',\n                                           use_bias=True, activation=None)\n        \n        super(ElementWiseAttention, self).__init__()\n    \n    def call(self, inputs):\n        # 2 stacked conv layer (in paper, it's 3. we set 2 for simplicity)\n        # this is the f(att)\n        x = self.conv1(inputs)\n        x = self.conv2(x)\n        \n        # this is the A = f(att)*X(CA)\n        camx = self.cam(x)*x\n        \n        # this is S = X(CA)*Linear_Classifier\n        linx = self.cam(self.linear(inputs))\n\n        # element-wise multiply to prevent unnecessary attention\n        # suppose to minimize with weighted cross entorpy loss \n        out = tf.multiply(camx, linx)\n        \n        return out\n</code></pre>\n<p>The above one is the <strong>Layer of Interest</strong>. If I understand the paper words correctly, this layer should not only minimize the weighted loss function to <code>gt</code> and <code>score_vector</code> but also produce some weighted feature maps (<code>2D</code>).</p>\n<h2>Run</h2>\n<p>Here is the toy data</p>\n<pre><code>\n(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\nx_train = np.expand_dims(x_train, axis=-1)\nx_train = x_train.astype('float32') / 255\nx_train = tf.image.resize(x_train, [32,32]) # if we want to resize \ny_train = tf.keras.utils.to_categorical(y_train , num_classes=10) \n\n# Model \ninput = tf.keras.Input(shape=(32,32,1))\nefnet = tf.keras.applications.DenseNet121(weights=None,\n                                             include_top = False, \n                                             input_tensor = input)\nem =  ElementWiseAttention()(efnet.output)\n# Now that we apply global max pooling.\ngap = tf.keras.layers.GlobalMaxPooling2D()(em)\n\n# classification layer.\noutput = tf.keras.layers.Dense(10, activation='softmax')(gap)\n\n# bind all\nfunc_model = tf.keras.Model(efnet.input, output)\nfunc_model.compile(\n          loss      = tf.keras.losses.CategoricalCrossentropy(),\n          metrics   = tf.keras.metrics.CategoricalAccuracy(),\n          optimizer = tf.keras.optimizers.Adam())\n# fit \nfunc_model.fit(x_train, y_train, batch_size=32, epochs=3, verbose = 1)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 199}]
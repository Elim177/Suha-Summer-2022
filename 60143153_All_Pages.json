[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0", "tensorflow-datasets"], "owner": {"account_id": 17725928, "reputation": 49, "user_id": 12869645, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-1Tvywxots08/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3reCpFRsgS-tpTV-87LchpLheSpFiA/photo.jpg?sz=256", "display_name": "jongsung park", "link": "https://stackoverflow.com/users/12869645/jongsung-park"}, "is_answered": false, "view_count": 727, "answer_count": 0, "score": 1, "last_activity_date": 1581364616, "creation_date": 1581299634, "last_edit_date": 1581364616, "question_id": 60143153, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60143153/is-there-a-way-in-tensorflow-to-load-batches-of-data-each-time", "title": "Is there a way in tensorflow to load batches of data each time?", "body": "<p>So I'm running tensorflow 2+ python in google colab.</p>\n\n<p>Each of my data file is a 3d image with shape [563, 563, 563, 1], so loading all of them throws a resource exhaustion error.</p>\n\n<p>I've spent days and hours searching for a way to load only a batch of my dataset as tensor and unloading/loading new batch each iteration. I'm guessing there might be a way using tf.data.Dataset.list_files, but I can't find the exact way.</p>\n\n<p>Is there any good suggestions on a way to do it or any documents I could try to read? I've read the tf.data document from tensorflow, but couldn't find the information I needed.</p>\n\n<p>Thank you!</p>\n\n<h1>Edit</h1>\n\n<p>so this is the function I want to use to load my image</p>\n\n<pre><code>def load_image(ind):\n    file_brain = \"/content/drive/My Drive/brain/\" + str(ind) + \".mgz\"\n    file_mask = \"/content/drive/My Drive/mask/\" + str(ind) + \".mgz\"\n    data_brain, affine = load_nifti(file_brain)\n    data_mask, affine = load_nifti(file_mask)\n    data_brain = affine_transform(data_brain, affine)\n    data_mask = affine_transform(data_mask, affine)\n    data_brain = normalize(data_brain)\n    data_brain = zoom(data_brain, (563/256, 563/256, 563/256))\n    data_brain = tf.expand_dims(data_brain, axis=-1)\n    data_mask = tf.expand_dims(data_mask, axis=-1)\n    return data_brain, data_mask\n</code></pre>\n\n<p>and this was the way I was loading the dataset before, which exhausted the resource;</p>\n\n<pre><code>def create_dataset():\n    train_data = []\n    train_label = []\n    test_data = []\n    test_label = []\n    test_n = np.random.randint(1, 10, 1)\n    for i in range(1, 10):\n        data_brain, data_mask = load_image(i)\n        if i in test_n:\n            test_data.append(data_brain)\n            test_label.append(data_mask)\n            continue\n        train_data.append(data_brain)\n        train_label.append(data_mask)\n        shifted_data = data_brain + tf.random.uniform(shape=(), minval=-0.05, maxval=0.05)\n        scaled_data = data_brain * tf.random.uniform(shape=(), minval=0.85, maxval=1.3)\n        train_data.append(shifted_data)\n        train_label.append(data_mask)\n        train_data.append(scaled_data)\n        train_label.append(data_mask)\n\"\"\"\ntrain_data = tf.data.Dataset.from_tensor_slices(train_data)\ntrain_label = tf.data.Dataset.from_tensor_slices(train_label)\ntest_data = tf.data.Dataset.from_tensor_slices(test_data)\ntest_label = tf.data.Dataset.from_tensor_slices(test_label)\nreturn train_data, train_label, test_data, test_label\n\"\"\"\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 10}]
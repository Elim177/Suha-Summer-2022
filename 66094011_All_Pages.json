[{"items": [{"tags": ["python", "numpy", "tensorflow"], "owner": {"account_id": 11241182, "reputation": 347, "user_id": 8245400, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/34ef59afbc82bc1e96bd802a6e45ce0e?s=256&d=identicon&r=PG&f=1", "display_name": "Raj", "link": "https://stackoverflow.com/users/8245400/raj"}, "is_answered": true, "view_count": 458, "accepted_answer_id": 66096450, "answer_count": 1, "score": 0, "last_activity_date": 1612769967, "creation_date": 1612738892, "question_id": 66094011, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66094011/convert-tf-argmax-results-to-numpy-array", "title": "Convert tf.argmax results to numpy array", "body": "<p>I am new to Tensorflow and wrote the following distributed training code. The code works fine.</p>\n<pre><code>import multiprocessing\nimport os\nimport portpicker\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow_hub as hub\nimport tensorflow.python.keras.backend as K\n#1. Define Workers\ndef create_in_process_cluster(num_workers, num_ps):\n  &quot;&quot;&quot;Creates and starts local servers and returns the cluster_resolver.&quot;&quot;&quot;\n  worker_ports = [portpicker.pick_unused_port() for _ in range(num_workers)]\n  ps_ports = [portpicker.pick_unused_port() for _ in range(num_ps)]\n\n  cluster_dict = {}\n  cluster_dict[&quot;worker&quot;] = [&quot;localhost:%s&quot; % port for port in worker_ports]\n  if num_ps &gt; 0:\n    cluster_dict[&quot;ps&quot;] = [&quot;localhost:%s&quot; % port for port in ps_ports]\n\n  cluster_spec = tf.train.ClusterSpec(cluster_dict)\n\n  # Workers need some inter_ops threads to work properly.\n  worker_config = tf.compat.v1.ConfigProto()\n  if multiprocessing.cpu_count() &lt; num_workers + 1:\n    worker_config.inter_op_parallelism_threads = num_workers + 1\n\n  for i in range(num_workers):\n    tf.distribute.Server(\n        cluster_spec, job_name=&quot;worker&quot;, task_index=i, config=worker_config,\n        protocol=&quot;grpc&quot;)\n\n  for i in range(num_ps):\n    tf.distribute.Server(\n        cluster_spec, job_name=&quot;ps&quot;, task_index=i, protocol=&quot;grpc&quot;)\n\n  cluster_resolver = tf.distribute.cluster_resolver.SimpleClusterResolver(\n      cluster_spec, task_id=0, task_type=&quot;worker&quot;,rpc_layer=&quot;grpc&quot;)\n  return cluster_resolver\n\nNUM_WORKERS = 3\nNUM_PS = 2\ncluster_resolver = create_in_process_cluster(NUM_WORKERS, NUM_PS)\n\n# Set the environment variable to allow reporting worker and ps failure to the\n# coordinator. This is a workaround and won't be necessary in the future.\nos.environ[&quot;GRPC_FAIL_FAST&quot;] = &quot;use_caller&quot;\n\nvariable_partitioner = (\n    tf.distribute.experimental.partitioners.FixedShardsPartitioner(\n        num_shards=NUM_PS))\n\nstrategy = tf.distribute.experimental.ParameterServerStrategy(cluster_resolver)\n\nword = &quot;Elephant&quot;\nsentence = &quot;I am a sentence for which I would like to get its embedding.&quot;\nparagraph = (\n    &quot;Universal Sentence Encoder embeddings also support short paragraphs. &quot;\n    &quot;There is no hard limit on how long the paragraph is. Roughly, the longer &quot;\n    &quot;the more 'diluted' the embedding will be.&quot;)\nmessages = [word, sentence, paragraph]\n#labels=[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;]\nreviews = [[1,0,0],[0,1,0],[0,0,1]]\n\n\nencoder=hub.load(&quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;)\n\nX_train=encoder(messages)\n\nBUFFER_SIZE = len(X_train)\nBATCH_SIZE_PER_REPLICA = 2\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nEPOCHS = 4\n\n\nwith strategy.scope():\n\n    model = keras.Sequential()\n\n    model.add(\n        keras.layers.Dense(\n            units=256,\n            input_shape=(X_train.shape[1],),\n            activation='relu'\n        )\n    )\n    model.add(\n        keras.layers.Dropout(rate=0.5)\n    )\n\n    model.add(\n        keras.layers.Dense(\n            units=128,\n            activation='relu'\n        )\n    )\n    model.add(\n        keras.layers.Dropout(rate=0.5)\n    )\n\n    model.add(keras.layers.Dense(3, activation='softmax'))\n    # model.compile(\n    #     loss='categorical_crossentropy',\n    #     optimizer=keras.optimizers.Adam(0.001),\n    #     metrics=['accuracy']\n    # )\n\n    # history = model.fit(\n    #     np.array(X_train), np.array(reviews),\n    #     epochs=10,\n    #     batch_size=16,\n    #     verbose=1,\n    #     shuffle=True\n    # )\n    optimizer=keras.optimizers.Adam(0.001)\n    accuracy = keras.metrics.Accuracy()\n\n\ndef step_fn(x_train_slice):\n\n    x_train, y_train = next(x_train_slice)\n    with tf.GradientTape() as tape:\n        pred=model(x_train,training=True)\n        # tf.print(x_train)\n        # tf.print(pred)\n        # tf.print(y_train)\n\n        per_example_loss = keras.losses.CategoricalCrossentropy(\n            reduction=tf.keras.losses.Reduction.NONE)(y_train, pred)\n        loss = tf.nn.compute_average_loss(per_example_loss)\n        gradients = tape.gradient(loss, model.trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    actual_pred = tf.cast(tf.greater(pred, 0.5), tf.int64)\n    tf.print(&quot;train values are&quot;,x_train)\n    tf.print(&quot; pred Values are : &quot;, pred)\n    tf.print(&quot; ArgMAx Values are &quot;,tf.math.argmax(pred,axis=0)) #problem\n    tf.print(&quot; actual_pred Values are : &quot;, actual_pred)\n    tf.print(&quot; Labels  are : &quot;, y_train)\n    tf.print(&quot; Labels Max Values are : &quot;, tf.argmax(y_train))\n    accuracy.update_state(y_train, actual_pred)\n    tf.print(&quot;Accuracy is : &quot;,accuracy.result())\n    return loss\n\n@tf.function\ndef distributed_train_step(x_train_slice):\n    losses = strategy.run(step_fn,args=(x_train_slice,))\n    return strategy.reduce(tf.distribute.ReduceOp.SUM, losses, axis=None)\n\n\n@tf.function\ndef per_worker_dataset_fn():\n    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, reviews)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n    # test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n    train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n    # test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n    return train_dist_dataset\n\n\ncoordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(strategy)\nper_worker_dataset = coordinator.create_per_worker_dataset(per_worker_dataset_fn)\nper_worker_iterator = iter(per_worker_dataset)\nnum_epoches = 5\nsteps_per_epoch = 1\nfor i in range(num_epoches):\n  accuracy.reset_states()\n  for _ in range(steps_per_epoch):\n    coordinator.schedule(distributed_train_step, args=(per_worker_iterator,))\n    # Wait at epoch boundaries.\n  coordinator.join()\n  print (&quot;Finished epoch %d, accuracy is %f.&quot;,(i,accuracy.result().numpy()))\n</code></pre>\n<p>The problem is, in the step_fn once I get the prediction values I would like to get the corresponding labels, for this I have used this line of code\n<code>tf.print(&quot; ArgMAx Values are &quot;,tf.math.argmax(pred,axis=0)) #problem</code></p>\n<p>The argmax gives the array of indices for max probabilities. I would like to extract this as numpy array and index it to reviews array (One-Hot encoded values) to get the confusion matrix.</p>\n<p>But I'm not able to convert <code>tf.math.argmax(pred,axis=0)</code> tensor to numpy array. I tried many approaches like eval(K.get_session()) and so on but nothing worked. Any help is appreciated.</p>\n<p>Thanks much</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 257}]
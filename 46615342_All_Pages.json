[{"items": [{"tags": ["python", "tensorflow", "distributed"], "owner": {"account_id": 5664091, "reputation": 1050, "user_id": 4480756, "user_type": "registered", "accept_rate": 38, "profile_image": "https://graph.facebook.com/1515949422/picture?type=large", "display_name": "Ruofan Kong", "link": "https://stackoverflow.com/users/4480756/ruofan-kong"}, "is_answered": false, "view_count": 205, "answer_count": 1, "score": 0, "last_activity_date": 1507403279, "creation_date": 1507334050, "last_edit_date": 1507334371, "question_id": 46615342, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/46615342/how-to-implement-distbelief-architecture-in-distributed-tensorflow", "title": "How to implement &quot;DistBelief&quot; architecture in Distributed Tensorflow", "body": "<p>The current architecture of Distributed Tensorflow is based on the \"Parameter-Server-like\" framework. Using <code>tf.device(tf.train.replica_device_setter())</code>, all tensor \"Variables\" are placed on the \"parameter server\" (\"PS\"), and other tensor ops are assigned to \"workers\". From what I understand, there would be much communication overhead between the \"worker\" and \"PS\". The reason is each worker does not have no local replicas of those \"Variables\" stored in \"PS\", which actually introduces much more communications during training, to retrieve variables from \"PS\", compute the intermediate results and send them back to \"PS\" to update those tensor \"Variables\"...</p>\n\n<p>Now if we don't follow the rule, instead, we adopt the \"DistBelief\" architecture: All shared parameters (like neural network weights) are still placed on \"PS\", but each worker now has the replicas of the shared tensor \"Variables\" which are stored in \"PS\". The benefit is during training each worker does not have to communicate with \"PS\", but just uses its local replicas to compute gradients, and the communication between \"worker\" and \"PS\" only happens when the shared parameters (neural network weights) are updated in \"PS\". In distributed Tensorflow, is there a way doing it?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 289}]
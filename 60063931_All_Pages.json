[{"items": [{"tags": ["tensorflow", "tensorflow-probability"], "owner": {"account_id": 17687571, "reputation": 93, "user_id": 12840171, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/40deff47c4178a9e4941e2ab8a580d6e?s=256&d=identicon&r=PG&f=1", "display_name": "cyberface", "link": "https://stackoverflow.com/users/12840171/cyberface"}, "is_answered": false, "view_count": 262, "answer_count": 1, "score": 0, "last_activity_date": 1580902946, "creation_date": 1580842732, "question_id": 60063931, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60063931/error-when-using-tensorflow-hmc-to-marginalise-gpr-hyperparameters", "title": "Error when using tensorflow HMC to marginalise GPR hyperparameters", "body": "<p>I would like to use tensorflow (version 2) to use gaussian process regression\nto fit some data and I found the google colab example online here [1].\nI have turned some of this notebook into a minimal example that is below.</p>\n\n<p>Sometimes the code fails with the following error when using MCMC to marginalize the hyperparameters: and I was wondering if anyone has seen this before or knows how to get around this?</p>\n\n<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError:  Input matrix is not invertible.\n     [[{{node mcmc_sample_chain/trace_scan/while/body/_168/smart_for_loop/while/body/_842/dual_averaging_step_size_adaptation___init__/_one_step/transformed_kernel_one_step/mh_one_step/hmc_kernel_one_step/leapfrog_integrate/while/body/_1244/leapfrog_integrate_one_step/maybe_call_fn_and_grads/value_and_gradients/value_and_gradient/gradients/leapfrog_integrate_one_step/maybe_call_fn_and_grads/value_and_gradients/value_and_gradient/PartitionedCall_grad/PartitionedCall/gradients/JointDistributionNamed/log_prob/JointDistributionNamed_log_prob_GaussianProcess/log_prob/JointDistributionNamed_log_prob_GaussianProcess/get_marginal_distribution/Cholesky_grad/MatrixTriangularSolve}}]] [Op:__inference_do_sampling_113645]\n\nFunction call stack:\ndo_sampling\n</code></pre>\n\n<p>[1] <a href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Process_Regression_In_TFP.ipynb#scrollTo=jw-_1yC50xaM\" rel=\"nofollow noreferrer\">https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Process_Regression_In_TFP.ipynb#scrollTo=jw-_1yC50xaM</a></p>\n\n<p>Note that some of code below is a bit redundant but it should\nin some sections but it should be able to reproduce the error.</p>\n\n<p>Thanks!</p>\n\n<pre><code>import time\n\nimport numpy as np\nimport tensorflow.compat.v2 as tf\nimport tensorflow_probability as tfp\ntfb = tfp.bijectors\ntfd = tfp.distributions\ntfk = tfp.math.psd_kernels\ntf.enable_v2_behavior()\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n#%pylab inline\n# Configure plot defaults\nplt.rcParams['axes.facecolor'] = 'white'\nplt.rcParams['grid.color'] = '#666666'\n#%config InlineBackend.figure_format = 'png'\n\ndef sinusoid(x):\n  return np.sin(3 * np.pi * x[..., 0])\n\ndef generate_1d_data(num_training_points, observation_noise_variance):\n  \"\"\"Generate noisy sinusoidal observations at a random set of points.\n\n  Returns:\n     observation_index_points, observations\n  \"\"\"\n  index_points_ = np.random.uniform(-1., 1., (num_training_points, 1))\n  index_points_ = index_points_.astype(np.float64)\n  # y = f(x) + noise\n  observations_ = (sinusoid(index_points_) +\n                   np.random.normal(loc=0,\n                                    scale=np.sqrt(observation_noise_variance),\n                                    size=(num_training_points)))\n  return index_points_, observations_\n\n# Generate training data with a known noise level (we'll later try to recover\n# this value from the data).\nNUM_TRAINING_POINTS = 100\nobservation_index_points_, observations_ = generate_1d_data(\n    num_training_points=NUM_TRAINING_POINTS,\n    observation_noise_variance=.1)\n\ndef build_gp(amplitude, length_scale, observation_noise_variance):\n  \"\"\"Defines the conditional dist. of GP outputs, given kernel parameters.\"\"\"\n\n  # Create the covariance kernel, which will be shared between the prior (which we\n  # use for maximum likelihood training) and the posterior (which we use for\n  # posterior predictive sampling)\n  kernel = tfk.ExponentiatedQuadratic(amplitude, length_scale)\n\n  # Create the GP prior distribution, which we will use to train the model\n  # parameters.\n  return tfd.GaussianProcess(\n      kernel=kernel,\n      index_points=observation_index_points_,\n      observation_noise_variance=observation_noise_variance)\n\ngp_joint_model = tfd.JointDistributionNamed({\n    'amplitude': tfd.LogNormal(loc=0., scale=np.float64(1.)),\n    'length_scale': tfd.LogNormal(loc=0., scale=np.float64(1.)),\n    'observation_noise_variance': tfd.LogNormal(loc=0., scale=np.float64(1.)),\n    'observations': build_gp,\n})\n\nx = gp_joint_model.sample()\nlp = gp_joint_model.log_prob(x)\n\nprint(\"sampled {}\".format(x))\nprint(\"log_prob of sample: {}\".format(lp))\n\n# Create the trainable model parameters, which we'll subsequently optimize.\n# Note that we constrain them to be strictly positive.\n\nconstrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())\n\namplitude_var = tfp.util.TransformedVariable(\n    initial_value=1.,\n    bijector=constrain_positive,\n    name='amplitude',\n    dtype=np.float64)\n\nlength_scale_var = tfp.util.TransformedVariable(\n    initial_value=1.,\n    bijector=constrain_positive,\n    name='length_scale',\n    dtype=np.float64)\n\nobservation_noise_variance_var = tfp.util.TransformedVariable(\n    initial_value=1.,\n    bijector=constrain_positive,\n    name='observation_noise_variance_var',\n    dtype=np.float64)\n\ntrainable_variables = [v.trainable_variables[0] for v in \n                       [amplitude_var,\n                       length_scale_var,\n                       observation_noise_variance_var]]\n# Use `tf.function` to trace the loss for more efficient evaluation.\n@tf.function(autograph=False, experimental_compile=False)\ndef target_log_prob(amplitude, length_scale, observation_noise_variance):\n  return gp_joint_model.log_prob({\n      'amplitude': amplitude,\n      'length_scale': length_scale,\n      'observation_noise_variance': observation_noise_variance,\n      'observations': observations_\n  })\n\n# Now we optimize the model parameters.\nnum_iters = 1000\noptimizer = tf.optimizers.Adam(learning_rate=.01)\n\n# Store the likelihood values during training, so we can plot the progress\nlls_ = np.zeros(num_iters, np.float64)\nfor i in range(num_iters):\n  with tf.GradientTape() as tape:\n    loss = -target_log_prob(amplitude_var, length_scale_var,\n                            observation_noise_variance_var)\n  grads = tape.gradient(loss, trainable_variables)\n  optimizer.apply_gradients(zip(grads, trainable_variables))\n  lls_[i] = loss\n\nprint('Trained parameters:')\nprint('amplitude: {}'.format(amplitude_var._value().numpy()))\nprint('length_scale: {}'.format(length_scale_var._value().numpy()))\nprint('observation_noise_variance: {}'.format(observation_noise_variance_var._value().numpy()))\n\n\nnum_results = 100\nnum_burnin_steps = 50\n\n\nsampler = tfp.mcmc.TransformedTransitionKernel(\n    tfp.mcmc.HamiltonianMonteCarlo(\n        target_log_prob_fn=target_log_prob,\n        step_size=tf.cast(0.1, tf.float64),\n        num_leapfrog_steps=8),\n    bijector=[constrain_positive, constrain_positive, constrain_positive])\n\nadaptive_sampler = tfp.mcmc.DualAveragingStepSizeAdaptation(\n    inner_kernel=sampler,\n    num_adaptation_steps=int(0.8 * num_burnin_steps),\n    target_accept_prob=tf.cast(0.75, tf.float64))\n\ninitial_state = [tf.cast(x, tf.float64) for x in [1., 1., 1.]]\n\n# Speed up sampling by tracing with `tf.function`.\n@tf.function(autograph=False, experimental_compile=False)\ndef do_sampling():\n    return tfp.mcmc.sample_chain(\n      kernel=adaptive_sampler,\n      current_state=initial_state,\n      num_results=num_results,\n      num_burnin_steps=num_burnin_steps,\n      trace_fn=lambda current_state, kernel_results: kernel_results)\n\nt0 = time.time()\nsamples, kernel_results = do_sampling()\nt1 = time.time()\nprint(\"Inference ran in {:.2f}s.\".format(t1-t0))\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 44}]
[{"items": [{"tags": ["tensorflow", "keras", "deep-learning"], "owner": {"account_id": 18533418, "reputation": 132, "user_id": 13503628, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4f81446e39ffc64772dbc6aa10f26f13?s=256&d=identicon&r=PG&f=1", "display_name": "The_Monk", "link": "https://stackoverflow.com/users/13503628/the-monk"}, "is_answered": false, "view_count": 4070, "answer_count": 0, "score": 6, "last_activity_date": 1623260851, "creation_date": 1623256737, "last_edit_date": 1623260851, "question_id": 67908331, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67908331/mismatch-in-the-calculated-and-the-actual-values-of-output-of-the-softmax-activa", "title": "Mismatch in the calculated and the actual values of Output of the Softmax Activation Function in the Output Layer", "body": "<p>In the below code, I am comparing the <strong><code>Predicted Output</code></strong> of the <strong><code>TF Keras Model</code></strong> with the respective value which is <strong><code>calculated Manually</code></strong> (<strong><code>Softmax Activation</code></strong> implemented using <strong><code>Numpy</code></strong>).</p>\n<p>Surprisingly, they are not same. Am I missing something?</p>\n<p>Also, there is a Warning,</p>\n<blockquote>\n<p>UserWarning: &quot;<code>sparse_categorical_crossentropy</code> received\n<code>from_logits=True</code>, but the <code>output</code> argument was produced by a\nsigmoid or softmax activation and thus does not represent logits. Was\nthis intended?&quot;   '&quot;<code>sparse_categorical_crossentropy</code> received\n<code>from_logits=True</code>, but '</p>\n</blockquote>\n<p>What does that warning mean? And is that warning the reason for the mismatch?</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nimport numpy as np\n\n\ninputs = tf.keras.Input(shape=(784,), name=&quot;digits&quot;)\nx1 = tf.keras.layers.Dense(64, activation=&quot;relu&quot;)(inputs)\nx2 = tf.keras.layers.Dense(64, activation=&quot;relu&quot;)(x1)\noutputs = tf.keras.layers.Dense(10, name=&quot;predictions&quot;, activation = 'softmax')(x2)\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n\n# Instantiate an optimizer.\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n# Instantiate a loss function.\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Prepare the training dataset.\nbatch_size = 64\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train = np.reshape(x_train, (-1, 784))\nx_test = np.reshape(x_test, (-1, 784))\n\n# Normalize the values of Pixels of Image. Else, Calculation of Softmax results in NaN\nx_train = x_train/255.0\nx_test = x_test/255.0\n\n# Reserve 10,000 samples for validation.\nx_val = x_train[-10000:]\ny_val = y_train[-10000:]\nx_train = x_train[:-10000]\ny_train = y_train[:-10000]\n\n# Prepare the training dataset.\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n\n# Prepare the validation dataset.\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nval_dataset = val_dataset.batch(batch_size)\n\nepochs = 2\nfor epoch in range(epochs):\n    print(&quot;\\nStart of epoch %d&quot; % (epoch,))\n\n    # Iterate over the batches of the dataset.\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        \n        x_batch_train = tf.cast(x_batch_train, tf.float32)               \n        \n        with tf.GradientTape() as tape:\n            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n\n            # Compute the loss value for this minibatch.\n            loss_value = loss_fn(y_batch_train, logits)\n        \n        grads = tape.gradient(loss_value, model.trainable_weights)\n        \n        Initial_Weights_1st_Hidden_Layer = model.trainable_weights[0]\n        \n        Initial_Weights_2nd_Hidden_Layer = model.trainable_weights[2]\n        \n        Initial_Weights_Output_Layer = model.trainable_weights[4]\n                \n        Initial_Bias_1st_Hidden_Layer = model.trainable_weights[1]\n        \n        Initial_Bias_2nd_Hidden_Layer = model.trainable_weights[3]\n        \n        Initial_Bias_Output_Layer = model.trainable_weights[5]\n        \n        # Implementing Relu Activation Function using Numpy\n        def Relu_Activation(Input):\n            return np.maximum(Input, 0)\n        \n        #Compute Softmax Activation Function using Numpy\n        def Softmax_Activation(Input):\n            return np.exp(Input) / np.sum(np.exp(Input), axis=0)\n        \n        # Calculations\n        Input_to_1st_Hidden_Layer = x_batch_train @ Initial_Weights_1st_Hidden_Layer + \\\n                                                                        Initial_Bias_1st_Hidden_Layer\n                     \n        Output_Of_1st_Hidden_Layer = Relu_Activation(Input_to_1st_Hidden_Layer)\n        \n        Input_to_2nd_Hidden_Layer = Output_Of_1st_Hidden_Layer @ Initial_Weights_2nd_Hidden_Layer + \\\n                                                                        Initial_Bias_2nd_Hidden_Layer\n                   \n        Output_Of_2nd_Hidden_Layer = Relu_Activation(Input_to_2nd_Hidden_Layer)\n      \n        Input_to_Final_Layer = Output_Of_2nd_Hidden_Layer @ Initial_Weights_Output_Layer + \\\n                                                                        Initial_Bias_Output_Layer\n        \n        # Softmax Activation Function has been used in the Output/Final Layer\n        Calculated_Y_Pred = Softmax_Activation(Input_to_Final_Layer)\n\n        # Log every 200 batches.\n        if step == 200:\n            print('\\n Y_Pred = ', logits[0:2])\n            print('\\n Calculated_Y_Pred = ', Calculated_Y_Pred[0:2])\n</code></pre>\n<p>The output of the above code is shown below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>Start of epoch 0\n/home/mothukuru/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4930: UserWarning: &quot;`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?&quot;\n  '&quot;`sparse_categorical_crossentropy` received `from_logits=True`, but '\n\n Y_Pred =  tf.Tensor(\n[[0.07784345 0.13746074 0.09005958 0.08652461 0.07746054 0.12440132\n  0.10698392 0.07508533 0.07116801 0.15301245]\n [0.0656803  0.08119027 0.09362638 0.10353054 0.12599334 0.10456354\n  0.1271341  0.08623642 0.08971243 0.12233265]], shape=(2, 10), dtype=float32)\n\n Calculated_Y_Pred =  [[0.01511016 0.02304603 0.01961761 0.01425961 0.01025286 0.02124614\n  0.01223315 0.01411171 0.01178642 0.01445299]\n [0.01271159 0.01357185 0.02033444 0.01701196 0.01662761 0.01780546\n  0.01449438 0.01615969 0.01481383 0.01152103]]\n\nStart of epoch 1\n\n Y_Pred =  tf.Tensor(\n[[0.12411885 0.08815324 0.05189805 0.07208851 0.11877609 0.06383732\n  0.13067529 0.08087374 0.09073243 0.17884655]\n [0.07584718 0.079349   0.06285123 0.1089478  0.09581042 0.09398626\n  0.12189291 0.10832074 0.08284932 0.17014521]], shape=(2, 10), dtype=float32)\n\n Calculated_Y_Pred =  [[0.02525741 0.01648222 0.01210153 0.012623   0.01642019 0.01224833\n  0.01583157 0.01587343 0.01606088 0.01728726]\n [0.01414648 0.01359805 0.01343262 0.01748529 0.01214003 0.01652816\n  0.01353526 0.01948644 0.01344168 0.01507382]]\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 213}]
[{"items": [{"tags": ["tensorflow2.0", "tensorflow-probability"], "owner": {"account_id": 108161, "reputation": 5951, "user_id": 287238, "user_type": "registered", "accept_rate": 75, "profile_image": "https://i.stack.imgur.com/oQJH2.jpg?s=256&g=1", "display_name": "mathtick", "link": "https://stackoverflow.com/users/287238/mathtick"}, "is_answered": false, "view_count": 367, "answer_count": 0, "score": 0, "last_activity_date": 1569147152, "creation_date": 1569103558, "last_edit_date": 1569147152, "question_id": 58044469, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58044469/gaussian-process-regression-in-tensorflow-2-0-leads-to-no-gradients", "title": "Gaussian Process Regression in Tensorflow 2.0 leads to no gradients?", "body": "<p>The following code is basically from the documentation, slightly converted to run in tensorflow 2.0. The gradients are all None. I'm not sure if this is a bug or just something I am missing:</p>\n\n<p>(corrected code)</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\n\ntfd = tfp.distributions\npsd_kernels = tfp.positive_semidefinite_kernels\n\ntf.keras.backend.set_floatx('float64')\n\nf = lambda x: np.sin(10*x[..., 0]) * np.exp(-x[..., 0]**2)\n\nobservation_index_points = np.random.uniform(-1., 1., 50)[..., np.newaxis]\nobservations = f(observation_index_points) + np.random.normal(0., .05, 50)\n\n\nclass Model(tf.keras.models.Model):\n    def __init__(self):\n        super().__init__()\n        self.amplitude_ = tf.Variable(np.float64(0), trainable=True)\n        self.amplitude = tf.exp(self.amplitude_, name='amplitude')\n        self.length_scale_ = tf.Variable(np.float64(0), trainable=True)\n        self.length_scale = tf.exp(self.length_scale_, name='length_scale')\n        self.kernel = psd_kernels.ExponentiatedQuadratic(self.amplitude, self.length_scale)\n        self.observation_noise_variance_ = tf.Variable(np.float64(-5), trainable=True)\n        self.observation_noise_variance = tf.exp(self.observation_noise_variance_, name='observation_noise_variance')\n\n\n    def gp(self, observation_index_points):\n        return tfd.GaussianProcess(\n            kernel=self.kernel,\n            index_points=observation_index_points,\n            observation_noise_variance=self.observation_noise_variance)\n\n    def call(self, observation_index_points, observations, index_points):\n        return tfd.GaussianProcessRegressionModel(\n        kernel=self.kernel,\n        index_points=index_points,\n        observation_index_points=observation_index_points,\n        observations=observations,\n        observation_noise_variance=self.observation_noise_variance)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=.05)\n\n# We can construct the posterior at a new set of `index_points` using the same\n# kernel (with the same parameters, which we'll optimize below).\nindex_points = np.linspace(-1., 1., 100)[..., np.newaxis]\n\nmodel = Model()\ngprm = model(observation_index_points, observations, index_points)\ngp = model.gp(observation_index_points)\ngp.log_prob(observations)\nsamples = gprm.sample(10)\n\ntrainable_variables = [model.amplitude_, model.length_scale_, model.observation_noise_variance_]\nwith tf.GradientTape() as tape:\n    loss = -gp.log_prob(observations)\nprint(loss)\ng = tape.gradient(loss, trainable_variables)\nprint(g)\n</code></pre>\n\n<p>UPDATE:</p>\n\n<p>The following example now works. Am wondering if there is a better pattern for organizing this flow in tf 2.0? </p>\n\n<pre><code> import numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\ntfb = tfp.bijectors\ntfd = tfp.distributions\npsd_kernels = tfp.positive_semidefinite_kernels\n\nm = 1000\nn = 3\nx = np.random.randn(m, n).astype(np.float32)\ny = np.random.randn(m).astype(np.float32)\nx_  = np.random.randn(100, n).astype(np.float32)\n\n\nclass GPRMatern(tf.keras.models.Model):\n    def __init__(self, feature_ndims=1):\n        super().__init__()\n        self.kernel = psd_kernels.MaternFiveHalves()\n        self.observation_noise_variance = tf.Variable(np.float32(.01), name='obs_noise_variance')\n\n    def gprm(self, x_obs, y_obs, x):\n        return tfd.GaussianProcessRegressionModel(\n            kernel=self.kernel,\n            index_points=x,\n            observation_index_points=x_obs,\n            observations=y_obs,\n            observation_noise_variance=self.observation_noise_variance)\n\n    def nll_for_train(self, x_obs, y_obs):\n        gp = tfd.GaussianProcess(\n            kernel=self.kernel,\n            index_points=x_obs,\n            observation_noise_variance=self.observation_noise_variance)\n        return -tf.reduce_mean(gp.log_prob(y_obs))\n\nclass GPRExpQuad(tf.keras.models.Model):\n    def __init__(self):\n        super().__init__()\n        self.amplitude = tf.Variable(np.float32(0.0), name='amplitude')\n        self.length_scale = tf.Variable(np.float32(0.0), name='length_scale')\n        self.observation_noise_variance = tf.Variable(np.float32(-5.0), name='obs_noise_variance')\n\n    @property\n    def kernel(self):\n        return psd_kernels.ExponentiatedQuadratic(tf.exp(self.amplitude), tf.exp(self.length_scale))\n\n    def nll_for_train(self, x_obs, y_obs):\n        gp = tfd.GaussianProcess(\n            kernel=self.kernel,\n            index_points=x_obs,\n            observation_noise_variance=tf.exp(self.observation_noise_variance))\n        return -tf.reduce_mean(gp.log_prob(y_obs))\n\n    def gprm(self, x_obs, y_obs, x):\n        return tfd.GaussianProcessRegressionModel(\n            kernel=self.kernel,\n            index_points=x,\n            observation_index_points=x_obs,\n            observations=y_obs,\n            observation_noise_variance=tf.exp(self.observation_noise_variance))\n\ndef test_model(model=GPRMatern):\n    model = model()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n    # model.fit(x, y, epochs=steps)\n    for i in range(10):\n        with tf.GradientTape() as tape:\n            l = model.nll_for_train(x, y)\n        g = tape.gradient(l, model.trainable_variables)\n        optimizer.apply_gradients(zip(g, model.trainable_variables))\n        print({x.name: x.numpy() for x in model.trainable_variables})\n\nmatern = GPRMatern()\nexpquad = GPRExpQuad()\n\ntest_matern = lambda : test_model(model=GPRMatern)\ntest_expquad = lambda : test_model(model=GPRExpQuad)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 87}]
[{"items": [{"tags": ["tensorflow", "keras", "pycharm"], "owner": {"account_id": 15062809, "reputation": 2258, "user_id": 10870968, "user_type": "registered", "profile_image": "https://graph.facebook.com/859956344339296/picture?type=large", "display_name": "Alex Deft", "link": "https://stackoverflow.com/users/10870968/alex-deft"}, "is_answered": false, "view_count": 64, "answer_count": 0, "score": 0, "last_activity_date": 1560629229, "creation_date": 1560599816, "last_edit_date": 1560629229, "question_id": 56610154, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/56610154/pycharm-changes-made-have-no-effect-how-to-explain-this-behaviour", "title": "Pycharm: Changes made have no effect, how to explain this behaviour?", "body": "<p>This problem only happens in Pycharm:</p>\n\n<p>I made a very simple NN based on TF2.0 website tutorial. The weird thing about it is when I change batch_size, it keeps going with the old one as if I did nothing. In fact, everything I do is irrelevant. </p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train = x_train.reshape(60000, 784).astype('float32') / 255\n\n\nclass Prototype(tf.keras.models.Model):\n    def __init__(self, **kwargs):\n        super(Prototype, self).__init__(**kwargs)\n        self.l1 = layers.Dense(64, activation='relu', name='dense_1')\n        self.l2 = layers.Dense(64, activation='relu', name='dense_2')\n        self.l3 = layers.Dense(10, activation='softmax', name='predictions')\n    def call(self, ip):\n        x = self.l1(ip)\n        x = self.l2(x)\n        return self.l3(x)\n\nmodel = Prototype()\nmodel.build(input_shape=(None, 784,))\noptimizer = keras.optimizers.SGD(learning_rate=1e-3)\nloss_fn = keras.losses.SparseCategoricalCrossentropy()\n</code></pre>\n\n<blockquote>\n<pre><code>batch_size = 250\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n</code></pre>\n</blockquote>\n\n<pre><code>def train_one_epoch():\n  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n    print(x_batch_train.shape)\n    with tf.GradientTape() as tape:\n      logits = model(x_batch_train)  # Logits for this minibatch\n      loss_value = loss_fn(y_batch_train, logits)\n    grads = tape.gradient(loss_value, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n</code></pre>\n\n<p>I run the train_one_epoch(), it trains for one epoch. Then I change batch size and consequently dataset object to give new chunk sizes, <strong>BUT</strong> when I run train_one_epoch() again, it keeps going with the old batch_size.</p>\n\n<p>Proof:\n<a href=\"https://i.stack.imgur.com/kXJh1.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/kXJh1.png\" alt=\"enter image description here\"></a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 115}]
[{"items": [{"tags": ["python", "tensorflow", "conv-neural-network"], "owner": {"user_type": "does_not_exist", "display_name": "user9324107"}, "is_answered": true, "view_count": 98, "accepted_answer_id": 54515468, "answer_count": 1, "score": 0, "last_activity_date": 1549280969, "creation_date": 1549128011, "last_edit_date": 1549134817, "question_id": 54495521, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/54495521/save-and-restore-for-a-cnn-based-denoising-network-tensorflow", "title": "Save and restore for a CNN based Denoising Network Tensorflow", "body": "<p>My question is about restoring the Denoised Trained Model.\nI have my network defined in the following way.</p>\n\n<p>Conv1->relu1->Conv2->relu2->Conv3->relu3->Deconv1</p>\n\n<p>The tf.variable_scope(name) is same as above.</p>\n\n<p>Now I have my <em>loss</em>, <em>optimizer</em> and <em>accuracy</em> defined with tf.name_scope.</p>\n\n<p>When I try to restore <em>loss</em> function, It will ask even for <em>labels</em> (which I don't have). </p>\n\n<pre><code>feed_dict={x:input, y:labels}\nsess.run('loss',feed_dict)\n</code></pre>\n\n<p>Can anyone please help me understand how to test this? Which operation should I restore ? </p>\n\n<p>Should I have to call all layers, pass the input and check the loss(MSE)?</p>\n\n<p>I checked many examples but it seems to be all Classification problem and defining softmax with logits at last works. </p>\n\n<p><strong>Edit:</strong>\nBelow is my code and now it is easily visible how tf.name_scope and tf.variable_scope is defined. I feel I may have to bring whole layer to test new Image. Is that right?</p>\n\n<pre><code>def new_conv_layer(input, num_input_channels, filter_size, num_filters, name):\n\nwith tf.variable_scope(name):\n    # Shape of the filter-weights for the convolution\nshape = [filter_size, filter_size, num_input_channels, num_filters]\n\n\n    # Create new weights (filters) with the given shape\n    weights = tf.Variable(tf.truncated_normal([filter_size, filter_size, num_input_channels, num_filters], stddev=0.5))\n\n\n    # Create new biases, one for each filter\n    biases = tf.Variable(tf.constant(0.05, shape=[num_filters]))\n\n    filters = tf.Variable(tf.truncated_normal([filter_size, filter_size, num_input_channels, num_filters], stddev=0.5))\n\n\n    # TensorFlow operation for convolution\n    layer = tf.nn.conv2d(input=input, filter=filters, strides=[1,1,1,1], padding='SAME')\n\n    # Add the biases to the results of the convolution.\n    layer += biases\n\n    return layer, weights\n\ndef new_relu_layer(input, name):\n\n with tf.variable_scope(name):\n    #TensorFlow operation for convolution\n    layer = tf.nn.relu(input)\n\n    return layer\ndef new_pool_layer(input, name):\n\n with tf.variable_scope(name):\n\n\n    # TensorFlow operation for convolution\n    layer = tf.nn.max_pool(value=input, ksize=[1, 1, 1, 1], strides=[1, 1, 1, 1], padding='SAME')\n    return layer \n\ndef new_layer(inputs, filters,kernel_size,strides,padding, name):\n\n with tf.variable_scope(name):\n\n    layer = tf.layers.conv2d_transpose(inputs=inputs, filters=filters , kernel_size=kernel_size, strides=strides, padding=padding,   data_format =  'channels_last')\n\n    return layer\n\n\n\n\n\n\n\nlayer_conv1, weights_conv1 = new_conv_layer(input=yTraininginput, num_input_channels=1, filter_size=5, num_filters=32, name =\"conv1\")\nlayer_relu1 = new_relu_layer(layer_conv1, name=\"relu1\")\n\nlayer_conv2, weights_conv2 = new_conv_layer(input=layer_relu1, num_input_channels=32, filter_size=5, num_filters=64, name =\"conv2\")\nlayer_relu2 = new_relu_layer(layer_conv2, name=\"relu2\")\n\n\nlayer_conv3, weights_conv3 = new_conv_layer(input=layer_relu2, num_input_channels=64, filter_size=5, num_filters=128, name =\"conv3\")\nlayer_relu3 = new_relu_layer(layer_conv3, name=\"relu3\")\n\n\nlayer_deconv1 = new_layer(inputs=layer_relu3, filters=1,  kernel_size=[5,5] ,strides=[1,1] ,padding='same',name = 'deconv1')\nlayer_relu4 = new_relu_layer(layer_deconv1, name=\"relu4\")\n\n\nlayer_conv4, weights_conv4 = new_conv_layer(input=layer_relu4, num_input_channels=1, filter_size=5, num_filters=128, name =\"conv4\")\nlayer_relu5 = new_relu_layer(layer_conv4, name=\"relu5\")\n\n\nlayer_deconv2 = new_layer(inputs=layer_relu5, filters=1,  kernel_size=[5,5] ,strides=[1,1] ,padding='same',name = 'deconv2')\nlayer_relu6 = new_relu_layer(layer_deconv2, name=\"relu6\")\n\n\n\n\n\n# Use Cross entropy cost function\nwith tf.name_scope(\"loss\"):\n    cross_entropy = tf.losses.mean_squared_error(labels = xTraininglabel,predictions = layer_relu6)\n\n\n# Use Adam Optimizer\nwith tf.name_scope(\"optimizer\"):\n    optimizer = tf.train.AdamOptimizer(learning_rate=1e-6).minimize(loss = cross_entropy)\n\n\n# Accuracy\nwith tf.name_scope(\"accuracy\"):\n    accuracy = tf.image.psnr(a=layer_relu6,b=xTraininglabel,max_val=1.0)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 270}]
[{"items": [{"tags": ["python", "tensorflow", "nlp"], "owner": {"account_id": 20346530, "reputation": 1, "user_id": 14924248, "user_type": "registered", "profile_image": "https://graph.facebook.com/716238369027495/picture?type=large", "display_name": "Bob John", "link": "https://stackoverflow.com/users/14924248/bob-john"}, "is_answered": false, "view_count": 83, "answer_count": 1, "score": 0, "last_activity_date": 1609538996, "creation_date": 1609535148, "question_id": 65533700, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65533700/tensorflow-network-didnt-fit", "title": "tensorflow network didn&#39;t fit", "body": "<p>I have a pandas <code>dataset</code> where <code>col1</code> -&gt; input text(text tokenize with pre-trained tokenizer),col2 -&gt; binary classification [0,1].\ntranslating it into tensorflow dataset</p>\n<pre><code>dataset = tf.data.Dataset.from_generator(lambda: dataset, output_types=(tf.string, tf.int32))\n</code></pre>\n<p>create model</p>\n<pre><code>def build_classifier_model():\n   text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n   preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n   encoder_inputs = preprocessing_layer(text_input)\n   encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n   outputs = encoder(encoder_inputs)\n   net = outputs['pooled_output']\n   net = tf.keras.layers.Dropout(0.2)(net)\n   net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n   return tf.keras.Model(text_input, net)\nclassifier_model = build_classifier_model()\n</code></pre>\n<p>fine_tune model(bert)</p>\n<pre><code>epochs = 5\nsteps_per_epoch = tf.data.experimental.cardinality(dataset).numpy()\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5 \noptimizer = optimization.create_optimizer(init_lr=init_lr,\n                                      num_train_steps=num_train_steps,\n                                      num_warmup_steps=num_warmup_steps,\n                                      optimizer_type='adamw')\n</code></pre>\n<p>model compile</p>\n<pre><code>classifier_model.compile(optimizer=optimizer,\n                     loss=loss,\n                     metrics=metrics)\n</code></pre>\n<p>and i begin fit model with loop</p>\n<pre><code>from tqdm import tqdm\nfor epoch in range(5):\n    for step, (x_batch_train, y_batch_train) in tqdm(enumerate(dataset)):\n        with tf.GradientTape() as tape:\n        logits = classifier_model(x_batch_train, training=True) \n        loss_value = loss_fn(y_batch_train, logits)\n        grads = tape.gradient(loss_value, classifier_model.trainable_weights)\n        optimizer.apply_gradients(zip(grads, classifier_model.trainable_weights))\n        print(step)\n        if step % 200 == 0:\n            print('loss_value %s: %s' % (step, float(loss_value)))\n</code></pre>\n<p>i run this in colab pro with gpu and the cell where this training is performed freezes and does not train the model.\nOutput:</p>\n<pre><code>0it [00:00, ?it/s]\n</code></pre>\n<p>Help me begin to train my model,please(when i try to fit method <code>.fit</code> (<code>model.fit()</code>)\nthe result was the same)</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 238}]
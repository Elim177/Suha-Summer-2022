[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning"], "owner": {"account_id": 22661694, "reputation": 11, "user_id": 16834692, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJw5AabORmZS55WhnX0EKE3OAEazNN6Gjhp_ESGv=k-s256", "display_name": "monk_ktr", "link": "https://stackoverflow.com/users/16834692/monk-ktr"}, "is_answered": true, "view_count": 43, "answer_count": 1, "score": 1, "last_activity_date": 1630840503, "creation_date": 1630837878, "question_id": 69062546, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69062546/custom-dynamic-loss-function-no-gradients-provided-for-any-variable", "title": "Custom Dynamic Loss function: No gradients provided for any variable", "body": "<p>I am using an RGB dataset for my  x train and the loss is calculated in a dynamic loss function that gets the distances of pairs  and compares them against the ideal distance dist_train. Here is the model:</p>\n<pre><code>class MyModel(Model):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.d1 = Dense(3, activation='relu')\n    self.flatten = Flatten()\n    self.d2 = Dense(3, activation='relu')\n    self.d3 = Dense(2)\n\n  def call(self, x):\n    x = self.d1(x)\n    x = self.flatten(x)\n    x = self.d2(x)\n    return self.d3(x)\n\n# Create an instance of the model\nmodel = MyModel()\n\noptimizer = tf.keras.optimizers.Adam()\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\n\n@tf.function\ndef train_step(rgb):\n    with tf.GradientTape() as tape:\n        predictions = model(rgb, training=True)\n        loss = tf_function(predictions)\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n    train_loss(loss)\n</code></pre>\n<p>Here is the loss function and the tf.function wrapping it:</p>\n<pre><code>def mahal_loss(output):\n  \n    mahal = sp.spatial.distance.pdist(output, metric='mahalanobis')\n    mahal = sp.spatial.distance.squareform(mahal, force='no', checks=True)\n\n    new_distance = []\n\n    mahal =  np.ma.masked_array(mahal, mask=mahal==0)\n    for i in range(len(mahal)):\n        pw_dist = mahal[i, indices_train[i]]\n        new_distance.append(pw_dist)\n\n    mahal_loss = np.mean((dist_train - new_distance)**2)\n\n\n    \n    return mahal_loss\n\n@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])\ndef tf_function(pred):\n    y = tf.numpy_function(mahal_loss, [pred], tf.float32) \n    return y\n</code></pre>\n<p>Running the model:</p>\n<pre><code>\nfor epoch in range(EPOCHS):\n    train_loss.reset_states()\n\n    test_loss.reset_states()\n    \n    for i in x_train:\n        train_step(i)\n\n    print(\n        f'Epoch {epoch + 1}, '\n        f'Loss: {train_loss.result()}, '\n        f'Test Loss: {test_loss.result()}, '\n     )\n</code></pre>\n<p>I believe the reason I am running into problems lies in the dynamic loss function, as I need to calculate the distance between certain pairs to get the results I expect. This means that inside the loss function I have to calculate the mahalanobis distance of each pair to get the ones I will compare against the correct distances. The error I get is the following:</p>\n<pre><code>\n    &lt;ipython-input-23-0e975da5cbc2&gt;:15 train_step  *\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    C:\\Anaconda3\\envs\\colour_env\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:622 apply_gradients  **\n        grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n    C:\\Anaconda3\\envs\\colour_env\\lib\\site-packages\\keras\\optimizer_v2\\utils.py:72 filter_empty_gradients\n        raise ValueError(&quot;No gradients provided for any variable: %s.&quot; %\n\n    ValueError: No gradients provided for any variable: ['my_model/dense/kernel:0', 'my_model/dense/bias:0', 'my_model/dense_1/kernel:0', 'my_model/dense_1/bias:0', 'my_model/dense_2/kernel:0', 'my_model/dense_2/bias:0'].```\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 22}]
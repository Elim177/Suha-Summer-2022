[{"items": [{"tags": ["tensorflow", "keras", "tensorflow2.0"], "owner": {"account_id": 12178633, "reputation": 722, "user_id": 9347417, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b4eade3369e9b12786525484f752e6ed?s=256&d=identicon&r=PG&f=1", "display_name": "TheEngineer", "link": "https://stackoverflow.com/users/9347417/theengineer"}, "is_answered": true, "view_count": 586, "accepted_answer_id": 62456109, "answer_count": 1, "score": 3, "last_activity_date": 1592525324, "creation_date": 1592183017, "last_edit_date": 1592525324, "question_id": 62379895, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62379895/different-forward-and-backward-propagation-in-tensroflow2-keras", "title": "Different Forward and Backward Propagation in TensroFlow2 &amp; Keras", "body": "<p>I am training a neural network when in the forward pass, randomly half of the time a non-differentiable activation is used which rounds activation to either 0 or 1 (binary), and the other half it uses a differentiable function similar to Sigmoid (saturated-Sigmoid to be exact) In the backward pass however, we use the gradient with regard to the differentiable function, even when we have used the non-differentiable discrete one for forward pass. The code I have so far is:</p>\n\n<pre><code>diff_active = tf.math.maximum(sat_sigmoid_term1(feature), sat_sigmoid_term2(feature))\nbinary_masks = diff_active \nrand_cond = tf.random.uniform([1,])\ncond = tf.constant(rand_cond, shape=[1,])\nif cond &lt;0.5:\n        with tf.GradientTape() as tape:\n            non_diff_active = tf.grad_pass_through(tf.keras.layers.Lambda(lambda x: tf.where(tf.math.greater(x,0), x, tf.zeros_like(x))))(feature)\n            grads = tape.gradient(non_diff_active , feature)\n            binary_masks = non_diff_active  \ntf.math.multiply(binary_masks, feature)  \n</code></pre>\n\n<p>My intuition is that this way, the differentiable activation is always applied (and hopefully gradient of it is always included in bacl-prop) and with <code>tf.grad_pass_through()</code> I can apply the non-differentiable activation while replacing it's back-propagation with identity matrix. However, I am not sure if my use of <code>tf.grad_pass_through()</code> or the way I condition of my random variable is correct aand if the behaviour is as intended?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 8}]
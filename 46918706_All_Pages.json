[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "nlp", "tensorlayer"], "owner": {"account_id": 7696984, "reputation": 103, "user_id": 5831117, "user_type": "registered", "accept_rate": 40, "profile_image": "https://www.gravatar.com/avatar/0d50919174b290f0f6143e96d2d43c6b?s=256&d=identicon&r=PG&f=1", "display_name": "N. Chalifour", "link": "https://stackoverflow.com/users/5831117/n-chalifour"}, "is_answered": false, "view_count": 468, "answer_count": 1, "score": 0, "last_activity_date": 1532951522, "creation_date": 1508873798, "last_edit_date": 1532951522, "question_id": 46918706, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/46918706/seq2seq-in-python-using-tensorflow-and-tensorlayer", "title": "Seq2Seq in Python using tensorflow and tensorlayer", "body": "<p>I am trying to build a chatbot in python using tensorflow and tensorlayer. My code is the following:</p>\n\n<pre><code>from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import word_tokenize\nimport os\nimport time\nimport re\nimport tensorflow as tf\nimport tensorlayer as tl\nimport cPickle as pickle\n\nFILE_DIR = os.path.dirname(os.path.realpath(__file__))\n\nPAD_TOKEN = '&lt;PAD&gt;'\nSTART_TOKEN = '&lt;START&gt;'\nEND_TOKEN = '&lt;END&gt;'\nUNK_TOKEN = '&lt;UNK&gt;'\n\nPAD_ID = 0\nSTART_ID = 1\nEND_ID = 2\nUNK_ID = 3\n\nSTARTING_VOCAB = {PAD_TOKEN: PAD_ID, START_TOKEN: START_ID, END_TOKEN: END_ID, UNK_TOKEN: UNK_ID}\n\n_DIGIT_RE = re.compile(br\"\\d\")\n\n\nclass Chatbot(object):\n\ndef __init__(self, embedding_dim, n_layers=3):\n\n    self.embedding_dim = embedding_dim\n    self.n_layers = n_layers\n    self.w2idx = STARTING_VOCAB\n    self.idx2w = {}\n    self.encode_seqs = None\n    self.decode_seqs = None\n    self.net = None\n    self.net_rnn = None\n    self.y = None\n\n@staticmethod\ndef load():\n    with open(os.path.join(location, 'object.pkl'), 'rb') as pickle_file:\n        obj = pickle.load(pickle_file)\n    obj.encode_seqs = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n    obj.decode_seqs = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n    obj.net, obj.net_rnn = Chatbot.model(obj.encode_seqs, obj.decode_seqs, obj.idx2w, obj.embedding_dim, obj.n_layers, is_train=False, reuse=True)\n    obj.y = tf.nn.softmax(obj.net.outputs)\n    new_saver = tf.train.import_meta_graph(os.path.join(location, 'my-model.meta'))\n    new_saver.restore(sess, tf.train.latest_checkpoint(location))\n    return obj\n\n@staticmethod\ndef model(encode_seqs, decode_seqs, idx2w, embedding_dim, n_layers, is_train=True, reuse=False):\n\n    with tf.variable_scope(\"model\", reuse=reuse):\n        # for chatbot, you can use the same embedding layer,\n        # for translation, you may want to use 2 seperated embedding layers\n        with tf.variable_scope(\"embedding\") as vs:\n            net_encode = tl.layers.EmbeddingInputlayer(inputs=encode_seqs,\n                                                       vocabulary_size=len(idx2w),\n                                                       embedding_size=embedding_dim,\n                                                       name='seq_embedding')\n            vs.reuse_variables()\n            tl.layers.set_name_reuse(True)\n            net_decode = tl.layers.EmbeddingInputlayer(inputs=decode_seqs,\n                                                       vocabulary_size=len(idx2w),\n                                                       embedding_size=embedding_dim,\n                                                       name='seq_embedding')\n        net_rnn = tl.layers.Seq2Seq(net_encode,\n                                    net_decode,\n                                    cell_fn=tf.contrib.rnn.BasicLSTMCell,\n                                    n_hidden=embedding_dim,\n                                    initializer=tf.random_uniform_initializer(-0.1, 0.1),\n                                    encode_sequence_length=tl.layers.retrieve_seq_length_op2(encode_seqs),\n                                    decode_sequence_length=tl.layers.retrieve_seq_length_op2(decode_seqs),\n                                    initial_state_encode=None,\n                                    dropout=(0.5 if is_train else None),\n                                    n_layer=n_layers,\n                                    return_seq_2d=True, name='seq2seq')\n        net_out = tl.layers.DenseLayer(net_rnn, n_units=len(idx2w), act=tf.identity, name='output')\n    return net_out, net_rnn\n\ndef train(self, X_train, y_train, sess, batch_size, n_epochs):\n\n    n_step = int(len(X_train) / batch_size)\n\n    # Create vocabulary\n    X_train = [re.sub(_DIGIT_RE, UNK_TOKEN, x.decode('utf-8')) for x in X_train]\n    y_train = [re.sub(_DIGIT_RE, UNK_TOKEN, x.decode('utf-8')) for x in y_train]\n    vectorizer = CountVectorizer(tokenizer=word_tokenize)\n    all_sentences = X_train + y_train\n    vectorizer.fit_transform(all_sentences)\n    for k, v in vectorizer.vocabulary_.iteritems():\n        vectorizer.vocabulary_[k] = v + len(self.w2idx)\n    self.w2idx.update(vectorizer.vocabulary_)\n    self.idx2w = dict((v, k) for k, v in self.w2idx.iteritems())\n\n    # Transform data from sentences to sequences of ids\n    for i in range(len(X_train)):\n        X_train_id_seq, y_train_id_seq = [], []\n        for w in word_tokenize(X_train[i]):\n            if w.lower() in self.w2idx:\n                X_train_id_seq.append(self.w2idx[w.lower()])\n            else:\n                X_train_id_seq.append(self.w2idx[UNK_TOKEN])\n        X_train[i] = X_train_id_seq + [PAD_ID]\n        for w in word_tokenize(y_train[i]):\n            if w.lower() in self.w2idx:\n                y_train_id_seq.append(self.w2idx[w.lower()])\n            else:\n                y_train_id_seq.append(self.w2idx[UNK_TOKEN])\n        y_train[i] = y_train_id_seq + [PAD_ID]\n\n    training_encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\n    training_decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\n    training_target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\n    training_target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\")\n    training_net_out, _ = Chatbot.model(training_encode_seqs, training_decode_seqs, self.idx2w, self.embedding_dim, self.n_layers, is_train=True, reuse=False)\n\n    # model for inferencing\n    self.encode_seqs = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n    self.decode_seqs = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n    self.net, self.net_rnn = Chatbot.model(self.encode_seqs, self.decode_seqs, self.idx2w, self.embedding_dim, self.n_layers, is_train=False, reuse=True)\n    self.y = tf.nn.softmax(self.net.outputs)\n\n    loss = tl.cost.cross_entropy_seq_with_mask(logits=training_net_out.outputs, target_seqs=training_target_seqs,\n                                               input_mask=training_target_mask, return_details=False, name='cost')\n    lr = 0.0001\n    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n\n    tl.layers.initialize_global_variables(sess)\n\n    for epoch in range(n_epochs):\n        epoch_time = time.time()\n\n        # shuffle training data\n        from sklearn.utils import shuffle\n        X_train, y_train = shuffle(X_train, y_train, random_state=0)\n\n        # train an epoch\n        total_err, n_iter = 0, 0\n        for X, Y in tl.iterate.minibatches(inputs=X_train, targets=y_train, batch_size=batch_size, shuffle=False):\n            step_time = time.time()\n\n            X = tl.prepro.pad_sequences(X)\n            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=END_ID)\n            _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n\n            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=START_ID, remove_last=False)\n            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\n            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n\n            _, err = sess.run([train_op, loss], \n                {training_encode_seqs: X, \n                training_decode_seqs: _decode_seqs,\n                training_target_seqs: _target_seqs,\n                training_target_mask: _target_mask})\n\n            print(\"Epoch[%d/%d] step:[%d/%d] loss:%f took:%.5fs\" % (\n            epoch, n_epochs, n_iter, n_step, err, time.time() - step_time))\n\n            total_err += err;\n            n_iter += 1\n\n        print(\"Epoch[%d/%d] averaged loss:%f took:%.5fs\" % (epoch, n_epochs, total_err / n_iter,\n                                                            time.time() - epoch_time))\n\ndef save(self):\n    if not os.path.exists(location):\n        os.makedirs(location)\n    saver = tf.train.Saver()\n    saver.save(sess, os.path.join(location, 'my-model'))\n    tf.train.write_graph(sess.graph, location, 'my-graph.pbtxt')\n    self.net = None\n    self.net_rnn = None\n    self.y = None\n    self.encode_seqs = None\n    self.decode_seqs = None\n    with open(os.path.join(location, 'object.pkl'), 'wb') as pickle_file:\n        pickle.dump(self, pickle_file)\n</code></pre>\n\n<p>I can do training perfectly fine, I pass a list of sentences as X_train and another as y_train. What I need help with is saving the model and then reloading it later for training or testing. I tried just using pickle but it gives an error. How can I save and load seq2seq models in python using tensorflow and tensorlayer?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 93}]
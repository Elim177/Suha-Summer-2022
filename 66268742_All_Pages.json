[{"items": [{"tags": ["python", "tensorflow", "keras", "reinforcement-learning"], "owner": {"account_id": 11735602, "reputation": 177, "user_id": 8589444, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6136cd02b236b91a58068d0e3c240529?s=256&d=identicon&r=PG&f=1", "display_name": "St&#233;phane", "link": "https://stackoverflow.com/users/8589444/st%c3%a9phane"}, "is_answered": true, "view_count": 780, "accepted_answer_id": 66366863, "answer_count": 1, "score": 2, "last_activity_date": 1614249379, "creation_date": 1613685492, "last_edit_date": 1613866273, "question_id": 66268742, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66268742/how-do-you-prevent-memory-usage-to-explode-when-using-keras-in-a-loop", "title": "How do you prevent memory usage to explode when using Keras in a loop", "body": "<p>My problem seems to be very common.</p>\n<p>I am doing some reinforcement learning using a vanilla policy gradient method. The environment is just a simple one period game where the state and action spaces are the real line. The agent is a neural network with two output heads that I build manually using dense layers from Keras, e.g. my first hidden layer would be</p>\n<pre><code>layers.Dense(NH[0], activation =&quot;relu&quot;, \\\n             kernel_initializer=initializers.GlorotNormal())(inputs)\n</code></pre>\n<p>where NH contains a list of number of neurons for hidden layers. The outputs are the mean and standard deviation for my gaussian policy. I don't if this part matters, but I included it nonetheless.</p>\n<p>The environment is simple: the state is a normal variable, the action is some real scalar, and there is just one period. I run the policy a bunch of times, collect the resulting batch and use the tools from tf.GradientTape() to update the network on the basis of a custom loss function. I have no problem running that code thousands of times to see the algorithm learn.</p>\n<p>The real problem is that I'd like to run the learning process multiple times, each time re-initializing the network weights randomly to have distributions for the history of rewards, but if I run all of this in a loop the computer freezes rapidly. Apparently, this is a very common problem with Keras and Tensorflow, one that people have been complaining about for years and it is still a problem... Now, I have tried the usual solutions. <a href=\"https://stackoverflow.com/questions/58137677/keras-model-training-memory-leak\">Here</a>, people suggested adding something like the following at the end of the loop so that before I reinitialize the network I get a clean slate.</p>\n<pre><code>keras.backend.clear_session()\ngc.collect()\ndel actor\n</code></pre>\n<p>This doesn't solve the problem. Then, I saw someone gave a function that went a little further</p>\n<pre><code>def reset_keras(model):\n\n# Clear model, if possible\ntry:\n    del model\nexcept:\n    pass\n\n# Garbage collection\ngc.collect()\n\n# Clear and close tensorflow session\nsession = K.get_session() # Get session\nK.clear_session()         # Clear session\nsession.close()           # Close session\n\n# Reset all tensorflow graphs\ntf.compat.v1.reset_default_graph()\n</code></pre>\n<p>And that doesn't work either. I also tried moving around the order of the first three commands and it doesn't work either...</p>\n<p>Anyone has any idea how to solve the problem? It would also be useful to know <strong>why</strong> this happens. I'd also like to know how to profile memory usage here so that I don't have to wait 4 hours to learn the computer is freezing again with the new solution.</p>\n<p>In fact, if you have a minimal working example where you can demonstrate the code doesn't lead to exploding memory use, I would be very much disposed to re-code the whole damn thing from scratch to stop the problem. As a side note, why haven't the developers solve this issue? It's the only package on both R and Python where this has ever happened to me...</p>\n<p>EDIT\nAs asked, I provide a minimal working example of the issue. I made up a quick game: it's a moving target where the optimal action is to play some multiple of the state value which yields a reward of 0.</p>\n<p>I wrote down an actor class and used a simple linear regression as a critic which may be turned off. If you look at the memory usage, it is climbing... That game won't crash my computer unless I play it a lot more, but it shows that memory usage increases.</p>\n<pre><code>import numpy      as np\nimport psutil\n\nimport tensorflow                    as tf\nimport tensorflow.keras              as keras\nimport tensorflow.keras.layers       as layers\nimport tensorflow.keras.initializers as initializers\n\nimport tensorflow.python.keras.backend as kb\nimport matplotlib.pyplot as plt\n\nBATCH    = 10\nMC_DRAWS = 2000\nM        = 10\n\n# Training options\nLR = 0.01\ndef display_memory():\n    print( f'{round(psutil.virtual_memory().used/2**30, 2)} GB' )\n\nclass Actor:\n\n    def __init__(self):\n        self.nn    = self.make_actor()\n        self.batch = BATCH\n        self.opt   = keras.optimizers.Adam( learning_rate = LR )\n\n    def make_actor(self):\n        inputs = layers.Input( shape=(1) )\n        hidden = layers.Dense(5, activation='relu', \n                              kernel_initializer=initializers.GlorotNormal() )(inputs)\n        mu     = layers.Dense(1, activation='linear',\n                          kernel_initializer=initializers.GlorotNormal() )(hidden)\n        sigma  = layers.Dense(1, activation='softplus',\n                          kernel_initializer=initializers.GlorotNormal() )(hidden)\n    \n        nn  = keras.Model(inputs=inputs, outputs=[mu, sigma])\n    \n        return nn\n\n    def update_weights(self, state, action, reward):\n\n        # Get proper format\n        state  = tf.constant(state,  dtype='float32', shape=(self.batch,1))\n        action = tf.constant(action, dtype='float32', shape=(self.batch,1))\n        reward = tf.constant(reward, dtype='float32', shape=(self.batch,1))\n    \n        # Update Policy Network Parameters\n        with tf.GradientTape() as tape:   \n            # Compute Gaussian loss\n            loss_value = self.custom_loss(state, action, reward)\n            loss_value = tf.math.reduce_mean( loss_value, keepdims=True )\n        \n            # Compute gradients\n            grads = tape.gradient(loss_value, self.nn.trainable_variables)\n \n            # Apply gradients to update network weights\n            self.opt.apply_gradients(zip(grads, self.nn.trainable_variables))\n        \n    def custom_loss(self, state, action, reward):\n        # Obtain mean and standard deviation\n        nn_mu, nn_sigma = self.nn(state)\n    \n        # Gaussian pdf\n        pdf_value = tf.exp(-0.5 *((action - nn_mu) / (nn_sigma))**2) *\\\n                    1/(nn_sigma*tf.sqrt(2 *np.pi))\n                    \n        # Log probabilities\n        log_prob  = tf.math.log( pdf_value + 1e-5 )\n    \n        # Compute loss\n        loss_actor = -reward * log_prob\n    \n        return loss_actor\n\nclass moving_target_game:\n\n    def __init__(self):\n        self.action_range = [-np.inf, np.inf]\n        self.state_range  = [1, 2]\n        self.reward_range = [-np.inf, 0]\n\n    def draw(self):\n        return np.random.ranint(low  = self.state_range[0],\n                            high = self.state_range[1])\n\n    def get_reward(self, action, state):\n        return -(5*state - action)**2\n\nclass Critic:  \n    def __init__(self):\n    \n        self.order      = 3\n        self.projection = None\n\n    def predict(self, state, reward):\n    \n        # Enforce proper format\n        x = np.array( state ).reshape(-1,1)\n        y = np.array( reward ).reshape(-1,1)\n    \n        # Make regression matrix\n        X = np.ones( shape = x.shape )\n        for i in range( self.order ):\n            X = np.hstack( (X, x**(i+1)) )\n        \n        # Prediction\n        xt = x.transpose()\n        P  = x @ np.linalg.inv( xt @ x  ) @ xt\n        Py = P @ y\n    \n        self.projection = P\n    \n        return Py\n\n#%% Moving Target Game with Actor and Actor-Critic\n\ndo_actor_critic = True\n\ndisplay_memory()\n\nhistory    = np.zeros( shape=(MC_DRAWS, M) )\nenv        = moving_target_game()\n\nfor m in range(M):\n\n    # New Actor Network\n    actor  = Actor()\n\n    if do_actor_critic:\n        critic = Critic()\n\n    for i in range(MC_DRAWS):\n    \n        state_tape  = []\n        action_tape = []\n        reward_tape = []\n    \n        for j in range(BATCH):\n        \n            # Draw state\n            state = env.draw()\n            s     = tf.constant([state], dtype='float32')\n        \n            # Take action\n            mu, sigma = actor.nn( s )\n            a         = tf.random.normal([1], mean=mu, stddev=sigma)\n        \n            # Reward\n            r = env.get_reward( state, a )\n        \n            # Collect results\n            action_tape.append( float(a)     )\n            reward_tape.append( float(r)     )\n            state_tape.append(  float(state) )\n        \n            del (s, a, mu, sigma)\n    \n        # Update network weights\n        history[i,m] = np.mean( reward_tape )\n    \n        if do_actor_critic:\n            # Update critic\n            value = critic.predict(state_tape, reward_tape)\n            # Benchmark reward\n            mod = np.array(reward_tape).reshape(-1,1) - value\n            # Update actor\n            actor.update_weights(state_tape, action_tape, mod)\n        else:\n            actor.update_weights(state_tape, action_tape, reward_tape)\n\n    del actor\n    kb.clear_session()\n\n    if do_actor_critic:\n        del critic\n    \n    print( f'Average Reward on last: {np.mean(reward_tape)} ' )\n    display_memory()\n\nplt.plot( history )\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 202}]
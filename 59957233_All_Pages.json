[{"items": [{"tags": ["python", "machine-learning", "logistic-regression", "tensorflow2.0", "gradient-descent"], "owner": {"account_id": 4595147, "reputation": 33, "user_id": 3727439, "user_type": "registered", "profile_image": "https://graph.facebook.com/100000824796228/picture?type=large", "display_name": "Nirban", "link": "https://stackoverflow.com/users/3727439/nirban"}, "is_answered": true, "view_count": 611, "accepted_answer_id": 59996156, "answer_count": 1, "score": 0, "last_activity_date": 1580427986, "creation_date": 1580246539, "question_id": 59957233, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59957233/tensorflow-2-0-gradienttape-returne-none-as-gradients-for-manual-models", "title": "TensorFlow 2.0 GradientTape returne None as gradients for Manual Models", "body": "<p>I am Trying to create a Logistic regression Model manually but GradientTape returns NoneType gradients</p>\n\n<pre><code>class LogisticRegressionTF:\n    def __init__(self,dim):\n        #dim = X_train.shape[0]\n        tf.random.set_seed(1)\n        weight_init = tf.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=1)\n        zeros_init = tf.zeros_initializer()\n        self.W = tf.Variable(zeros_init([dim,1]), trainable=True, name=\"W\")\n        self.b = tf.Variable(zeros_init([1]), trainable=True, name=\"b\")\n\n    def sigmoid(self,z):\n        x = tf.Variable(z, trainable=True,dtype=tf.float32, name='x')\n        sigmoid = tf.sigmoid(x)\n        result = sigmoid\n        return result\n\n    def predict(self, x):\n        x = tf.cast(x, dtype=tf.float32)\n        h = tf.sigmoid(tf.add(tf.matmul(tf.transpose(self.W), x), self.b))\n        return h\n\n    def loss(self,logits, labels):\n        z = tf.Variable(logits, trainable=False,dtype=tf.float32, name='z')\n        y = tf.Variable(labels, trainable=False,dtype=tf.float32, name='y')\n        m = tf.cast(tf.size(z), dtype=tf.float32)\n        cost = tf.divide(tf.reduce_sum(y*tf.math.log(z) + (1-y)*tf.math.log(1-z)),-m)\n        return cost\n\n    def fit(self,X_train, Y_train, lr_rate = 0.01, epochs = 1000):\n        costs=[]\n        optimizer = tf.optimizers.SGD(learning_rate=lr_rate)\n\n        for i in range(epochs):\n            current_loss = self.loss(self.predict(X_train), Y_train)\n            print(current_loss)\n            with tf.GradientTape() as t:\n                t.watch([self.W, self.b])\n                currt_loss = self.loss(self.predict(X_train), Y_train)\n                print(currt_loss)\n            grads = t.gradient(currt_loss, [self.W, self.b])\n            print(grads)\n            #optimizer.apply_gradients(zip(grads,[self.W, self.b]))\n            self.W.assign_sub(lr_rate * grads[0])\n            self.b.assign_sub(lr_rate * grads[1])\n            if(i %100 == 0):\n                print('Epoch %2d: , loss=%2.5f' %(i, current_loss))\n            costs.append(current_loss)\n\n        plt.plot(costs)\n        plt.ylim(0,50)\n        plt.ylabel('Cost J')\n        plt.xlabel('Iterations')\n\nlog_reg = LogisticRegressionTF(train_set_x.shape[0])\nlog_reg.fit(train_set_x, train_set_y)\n\n</code></pre>\n\n<p>This gives a TypeError which is due to gradients returning <code>None</code></p>\n\n<pre><code>tf.Tensor(0.6931474, shape=(), dtype=float32)\ntf.Tensor(0.6931474, shape=(), dtype=float32)\n[None, None]\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-192-024668d532b0&gt; in &lt;module&gt;()\n      1 log_reg = LogisticRegressionTF(train_set_x.shape[0])\n----&gt; 2 log_reg.fit(train_set_x, train_set_y)\n\n&lt;ipython-input-191-4fef932eb231&gt; in fit(self, X_train, Y_train, lr_rate, epochs)\n     40             print(grads)\n     41             #optimizer.apply_gradients(zip(grads,[self.W, self.b]))\n---&gt; 42             self.W.assign_sub(lr_rate * grads[0])\n     43             self.b.assign_sub(lr_rate * grads[1])\n     44             if(i %100 == 0):\n\nTypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n\n</code></pre>\n\n<p>My Hypothesis Function is tf.sigmoid(tf.add(tf.matmul(tf.transpose(self.W), x), self.b))</p>\n\n<p>I have defined the cost function manually as tf.divide(tf.reduce_sum(y*tf.math.log(z) + (1-y)*tf.math.log(1-z)),-m), where m is the number of training examples</p>\n\n<p>to verify it is returning the loss as tf.Tensor(0.6931474, shape=(), dtype=float32)</p>\n\n<p>I also did a <code>t.watch()</code> but nothing happened it is still returning [None, None] </p>\n\n<p><code>train_set_y.dtype is dtype('int64')</code></p>\n\n<p><code>train_set_x.dtype is dtype('float64')</code></p>\n\n<p><code>train_set_x.shape is (12288, 209)</code></p>\n\n<p><code>train_set_y.shape is (1, 209)</code></p>\n\n<p><code>type(train_set_x) is numpy.ndarray</code></p>\n\n<p>Where did I go wrong??</p>\n\n<p>Thanks</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 38}]
[{"items": [{"tags": ["tensorflow", "tensorflow-datasets"], "owner": {"account_id": 10716056, "reputation": 2138, "user_id": 7886651, "user_type": "registered", "accept_rate": 76, "profile_image": "https://i.stack.imgur.com/zfb59.jpg?s=256&g=1", "display_name": "I. A", "link": "https://stackoverflow.com/users/7886651/i-a"}, "is_answered": true, "view_count": 536, "accepted_answer_id": 54082831, "answer_count": 1, "score": 5, "last_activity_date": 1546902242, "creation_date": 1546528706, "last_edit_date": 1546529811, "question_id": 54025104, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/54025104/how-to-retrieve-examples-from-multiple-tfrecords-in-tensorflow-while-using-initi", "title": "How to retrieve examples from multiple tfrecords in tensorflow while using initializable iterator", "body": "<p>I have multiple tfrecord files named: <code>Train_DE_01.tfrecords</code> through <code>Train_DE_34.tfrecords</code>; and <code>Devel_DE_01.tfrecords</code> through <code>Devel_DE_14.tfrecords</code>. Hence, I have a training and a validation dataset. And My aim was to iterator over the examples of the tfrecords such that I retrieve 2 examples from <code>Train_DE_01.tfrecords</code>, 2 from <code>Train_DE_02.tfrecords</code> ... and 2 <code>Train_DE_34.tfrecords</code>. In other words, when the batch size is 68, I need 2 examples from each <code>tfrecord</code> file. I my code, I have used an <code>initializable</code> Iterator as follows:</p>\n\n<pre><code># file_name: This is a place_holder that will contain the name of the files of the tfrecords.\ndef load_sewa_data(file_name, batch_size):\n\n    with tf.name_scope('sewa_tf_records'):\n        dataset = tf.data.TFRecordDataset(file_name).map(_parse_sewa_example).batch(batch_size)\n        iterator = dataset.make_initializable_iterator(shared_name='sewa_iterator')\n\n        next_batch = iterator.get_next()\n\n        names, detected, arousal, valence, liking, istalkings, images = next_batch\n\n        print(names, detected, arousal, valence, liking, istalkings, images)\n\n        return names, detected, arousal, valence, liking, istalkings, images, iterator\n</code></pre>\n\n<p>After running the names through a session using sess.run(); I figured out that the first 68 example are being fetched from <code>Train_DE_01.tfrecords</code>; then, subsequent examples are fetched from the same tfrecord until all the examples in the <code>Train_DE_01.tfrecords</code> are being consumed.</p>\n\n<p>I have tried using the zip() function of Dataset api with the reinitializable iterator as follows:</p>\n\n<pre><code>def load_devel_sewa_tfrecords(filenames_dev, test_batch_size):\n\n    datasets_dev_iterators = []\n\n    with tf.name_scope('TFRecordsDevel'):\n        for file_name in filenames_dev:\n            dataset_dev = tf.data.TFRecordDataset(file_name).map(_parse_devel_function).batch(test_batch_size)\n            datasets_dev_iterators.append(dataset_dev)\n\n        dataset_dev_all = tf.data.Dataset.zip(tuple(datasets_dev_iterators))\n        return dataset_dev_all\n\n\ndef load_train_sewa_tfrecords(filenames_train, train_batch_size):\n    datasets_train_iterators = []\n\n    with tf.name_scope('TFRecordsTrain'):\n        for file_name in filenames_train:\n            dataset_train = tf.data.TFRecordDataset(file_name).map(_parse_train_function).batch(train_batch_size)\n            datasets_train_iterators.append(dataset_train)\n\n        dataset_train_all = tf.data.Dataset.zip(tuple(datasets_train_iterators))\n\n        return dataset_train_all\n\n\ndef load_sewa_dataset(filenames_train, train_batch_size, filenames_dev, test_batch_size):\n    dataset_train_all = load_train_sewa_tfrecords(filenames_train, train_batch_size)\n    dataset_dev_all = load_devel_sewa_tfrecords(filenames_dev, test_batch_size)\n\n    iterator = tf.data.Iterator.from_structure(dataset_train_all.output_types,\n                                               dataset_train_all.output_shapes)\n\n    training_init_op = iterator.make_initializer(dataset_train_all)\n    validation_init_op = iterator.make_initializer(dataset_dev_all)\n\n    with tf.name_scope('inputs'):\n        next_batch = iterator.get_next(name='next_batch')\n        names = []\n        detected = []\n        arousal = []\n        valence = []\n        liking = []\n        istalkings = []\n        images = []\n\n        # len(next_batch) is 34.\n        # len(n) is 7. Since we are extracting: name, detected, arousal, valence, liking, istalking and images...\n        # len(n[0 or 1 or 2 or ... or 6]) = is batch size.\n        for n in next_batch:\n\n            names.append(n[0])\n            detected.append(n[1])\n            arousal.append(n[2])\n            valence.append(n[3])\n            liking.append(n[4])\n            istalkings.append(n[5])\n            images.append(n[6])\n\n        names = tf.concat(names, axis=0, name='names')\n        detected = tf.concat(detected, axis=0, name='detected')\n        arousal = tf.concat(arousal, axis=0, name='arousal')\n        valence = tf.concat(valence, axis=0, name='valence')\n        liking = tf.concat(liking, axis=0, name='liking')\n        istalkings = tf.concat(istalkings, axis=0, name='istalkings')\n        images = tf.concat(images, axis=0, name='images')\n\n        return names, detected, arousal, valence, liking, istalkings, images, training_init_op, validation_init_op\n</code></pre>\n\n<p>Now if I try the following:</p>\n\n<pre><code>sess = tf.Session()\nsess.run(training_init_op)\nprint(sess.run(names))\n</code></pre>\n\n<p>I got the following error:</p>\n\n<pre><code>ValueError: The two structures don't have the same number of elements.\n</code></pre>\n\n<p>which makes sense because the number of training files is 34 while that for validation dataset is 14. </p>\n\n<p>I would like to know how can I achieve the goal in mind?</p>\n\n<p>Any help is much appreciated!!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 67}]
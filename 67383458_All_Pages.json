[{"items": [{"tags": ["python", "performance", "tensorflow", "deep-learning", "pytorch"], "owner": {"account_id": 113960, "reputation": 13642, "user_id": 299897, "user_type": "registered", "accept_rate": 84, "profile_image": "https://www.gravatar.com/avatar/ef66ffcc88f26e3542a3ee13af0a991f?s=256&d=identicon&r=PG", "display_name": "Ivan", "link": "https://stackoverflow.com/users/299897/ivan"}, "is_answered": true, "view_count": 789, "accepted_answer_id": 67420239, "answer_count": 1, "score": 9, "last_activity_date": 1620909764, "creation_date": 1620125992, "last_edit_date": 1620310190, "question_id": 67383458, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67383458/why-is-this-tensorflow-training-taking-so-long", "title": "Why is this tensorflow training taking so long?", "body": "<p>I'm learning DRL with the book <strong>Deep Reinforcement Learning in Action</strong>. In chapter 3, they present the simple game Gridworld (<a href=\"https://towardsdatascience.com/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff#4283\" rel=\"noreferrer\">instructions here</a>, in the rules section) with the corresponding code in <strong>PyTorch</strong>.</p>\n<p>I've experimented with the code and it takes less than 3 minutes to train the network with 89% of wins (won 89 of 100 games after training).</p>\n<p><a href=\"https://i.stack.imgur.com/s5lAj.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/s5lAj.png\" alt=\"Training loss with pytorch\" /></a></p>\n<p>As an exercise, I have migrated the code to <strong>tensorflow</strong>. All the code is <a href=\"https://github.com/navi2000/drl_test\" rel=\"noreferrer\">here</a>.</p>\n<p>The problem is that with my tensorflow port it takes near 2 hours to train the network with a win rate of 84%. Both versions are using the only CPU to train (I don't have GPU)</p>\n<p><a href=\"https://i.stack.imgur.com/EtGIZ.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/EtGIZ.png\" alt=\"Training loss with tensorflow\" /></a></p>\n<p>Training loss figures seem correct and also the rate of a win (we have to take into consideration that the game is random and can have impossible states). The problem is the performance of the overall process.</p>\n<p>I'm doing something terribly wrong, but what?</p>\n<p>The main differences are in the training loop, in torch is this:</p>\n<pre><code>        loss_fn = torch.nn.MSELoss()\n        learning_rate = 1e-3\n        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n        ....\n        Q1 = model(state1_batch) \n        with torch.no_grad():\n            Q2 = model2(state2_batch) #B\n        \n        Y = reward_batch + gamma * ((1-done_batch) * torch.max(Q2,dim=1)[0])\n        X = Q1.gather(dim=1,index=action_batch.long().unsqueeze(dim=1)).squeeze()\n        loss = loss_fn(X, Y.detach())\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n</code></pre>\n<p>and in the tensorflow version:</p>\n<pre><code>        loss_fn = tf.keras.losses.MSE\n        learning_rate = 1e-3\n        optimizer = tf.keras.optimizers.Adam(learning_rate)\n        ...\n        Q2 = model2(state2_batch) #B\n        with tf.GradientTape() as tape:\n            Q1 = model(state1_batch)\n            Y = reward_batch + gamma * ((1-done_batch) * tf.math.reduce_max(Q2, axis=1))\n            X = [Q1[i][action_batch[i]] for i in range(len(action_batch))]\n            loss = loss_fn(X, Y)\n        grads = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n</code></pre>\n<p>Why is the training taking so long?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 151}]
[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 3948330, "reputation": 5043, "user_id": 3259896, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/641c30a7b383022f22b53c8cedb04e3f?s=256&d=identicon&r=PG&f=1", "display_name": "SantoshGupta7", "link": "https://stackoverflow.com/users/3259896/santoshgupta7"}, "is_answered": true, "view_count": 135, "accepted_answer_id": 50995660, "answer_count": 1, "score": 0, "last_activity_date": 1529701868, "creation_date": 1529617593, "question_id": 50977844, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/50977844/tensorflow-when-re-running-a-training-session-get-variable-runs-the-initialize", "title": "Tensorflow: When re-running a training session, get_variable runs the initializer and recreate variables, instead of retrieving the existing variables", "body": "<p>Since I am using get_variable to create and retrieve the variables, I would imagine that the 2nd time the training session is run, it would retrieve the variables and start where the last training session had left off, but this does not seem to be the case. </p>\n\n<p>My code below</p>\n\n<p>Code for the graph definition:</p>\n\n<pre><code>graph = tf.Graph()\n\nwith graph.as_default(): \n\n  valid_examples = np.array(random.sample(range(1, valid_window), valid_size)) \n\n  train_dataset = tf.placeholder(tf.int32, shape=[batch_size, cbow_window*2 ])\n  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n  valid_datasetSM = tf.constant(valid_examples, dtype=tf.int32)\n\n  embeddings = tf.get_variable( 'embeddings', \n    initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n\n  softmax_weights = tf.get_variable( 'softmax_weights',\n    initializer= tf.truncated_normal([vocabulary_size, embedding_size],\n                         stddev=1.0 / math.sqrt(embedding_size)))\n\n  softmax_biases = tf.get_variable('softmax_biases', \n    initializer= tf.zeros([vocabulary_size]),  trainable=False )\n  embed = tf.nn.embedding_lookup(embeddings, train_dataset) \n  embed_reshaped = tf.reshape( embed, [batch_size*cbow_window*2, embedding_size] )\n\n\n  segments= np.arange(batch_size).repeat(cbow_window*2)\n\n  averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)\n\n  loss = tf.reduce_mean(\n    tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,\n                               labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n\n  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n  normSM = tf.sqrt(tf.reduce_sum(tf.square(softmax_weights), 1, keepdims=True))\n\n  normalized_embeddings = embeddings / norm\n  normalized_embeddingsSM = softmax_weights / normSM\n\n  valid_embeddings = tf.nn.embedding_lookup(\n    normalized_embeddings, valid_dataset)\n  valid_embeddingsSM = tf.nn.embedding_lookup(\n    normalized_embeddingsSM, valid_datasetSM)\n\n  similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))\n  similaritySM = tf.matmul(valid_embeddingsSM, tf.transpose(normalized_embeddingsSM))\n</code></pre>\n\n<p>Code for Tensorflow session to train variables in the graph ( the embeddings and softmax weights)</p>\n\n<pre><code>num_steps = 1000001\n\nwith tf.Session(graph=graph) as session:\n  tf.global_variables_initializer().run()\n  print('Initialized')\n  average_loss = 0\n  saveIteration = 1\n  for step in range(num_steps):\n\n    batch_data, batch_labels = generate_batch(\n      batch_size, cbow_window)\n    feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n    _, l = session.run([optimizer, loss], feed_dict=feed_dict) \n\n    average_loss += l\n    if step % 2000 == 0:\n      if step &gt; 0:\n        average_loss = average_loss / 2000\n      print('Average loss at step %d: %f' % (step, average_loss))\n      average_loss = 0\n</code></pre>\n\n<p>After this session is done running, I run only the code section above (I do NOT run the graph definition section or any other code section again) and the loss is around where it is at the beginning of the last training session, so it looks like get_variable is initializing<code>embeddings</code> and <code>softmax_weights</code> again, instead of retrieving the already created variables by their names in tensorflow ( <code>embeddings</code> and <code>softmax_weights</code> respectively). </p>\n\n<p>I set the graph to default in the graph definition, so when I run the session it should be the same graph, and so it should still have the <code>embeddings</code> and <code>softmax_weights</code> variables for  get_variable to find and retrieve from Tensorflow's stored variables. </p>\n\n<p>So why it is using the initializers again ( <code>initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)</code> and <code>initializer= tf.truncated_normal([vocabulary_size, embedding_size],\n                             stddev=1.0 / math.sqrt(embedding_size))</code> ) ?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 108}]
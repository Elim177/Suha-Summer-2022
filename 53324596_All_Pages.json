[{"items": [{"tags": ["python", "python-3.x", "tensorflow", "keras", "neural-network"], "owner": {"account_id": 7254915, "reputation": 3462, "user_id": 5533928, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/517d5a0e08d76cd446ac7e6b156093a4?s=256&d=identicon&r=PG&f=1", "display_name": "Susmit Agrawal", "link": "https://stackoverflow.com/users/5533928/susmit-agrawal"}, "is_answered": true, "view_count": 51, "accepted_answer_id": 53325468, "answer_count": 1, "score": 1, "last_activity_date": 1542305177, "creation_date": 1542301619, "last_edit_date": 1542302486, "question_id": 53324596, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/53324596/problems-when-implementing-keras-model-in-tensorflow", "title": "Problems when implementing Keras model in Tensorflow", "body": "<p>I'm just starting off with <code>Tensorflow</code>.</p>\n\n<p>I tried implementing a model to classify digits in the MNSIT dataset.</p>\n\n<p>I am familiar with <code>Keras</code>, so I first used it to create the model.</p>\n\n<p>Keras code:</p>\n\n<pre><code>from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.datasets import mnist\nfrom os import path\n\nimport numpy as np\n\nnetwork = Sequential()\nnetwork.add(Dense(700, input_dim=784, activation='tanh'))\nnetwork.add(Dense(500, activation='tanh'))\nnetwork.add(Dense(500, activation='tanh'))\nnetwork.add(Dense(500, activation='tanh'))\nnetwork.add(Dense(10, activation='softmax'))\n\nnetwork.compile(loss='categorical_crossentropy', optimizer='adam')\n\n(x_train, y_temp), (x_test, y_test) = mnist.load_data()\ny_train = vectorize(y_temp)  # I defined this function to create vectors of the labels. It works without issues.\n\nx_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n\nnetwork.fit(x_train, y_train, batch_size=100, epochs=3)\n\nx_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])\n\n\nscores = network.predict(x_test)\n\ncorrect_pred = 0\nfor i in range(len(scores)):\n    if np.argmax(scores[i]) == y_test[i]:\n        correct_pred += 1\n\nprint((correct_pred/len(scores))*100)\n</code></pre>\n\n<p>The above code gives me an accuracy of around 92%.</p>\n\n<p>I tried implementing the same model in Tensorflow:</p>\n\n<pre><code>import sys\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\ndata = input_data.read_data_sets('.', one_hot=True)\n\nsess = tf.InteractiveSession()\n\nx = tf.placeholder(tf.float32, [None, 784])\ny = tf.placeholder(tf.float32, [None, 10])\n\nw = tf.Variable(tf.zeros([784, 700]))\nw2 = tf.Variable(tf.zeros([700, 500]))\nw3 = tf.Variable(tf.zeros([500, 500]))\nw4 = tf.Variable(tf.zeros([500, 500]))\nw5 = tf.Variable(tf.zeros([500, 10]))\n\nh1 = tf.nn.tanh(tf.matmul(x, w))\nh2 = tf.nn.tanh(tf.matmul(h1, w2))\nh3 = tf.nn.tanh(tf.matmul(h2, w3))\nh4 = tf.nn.tanh(tf.matmul(h3, w4))\nh = tf.matmul(h4, w5)\n\nloss = tf.math.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=h, labels=y))\ngradient_descent = tf.train.AdamOptimizer().minimize(loss)\n\ncorrect_mask = tf.equal(tf.argmax(h, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n\nsess.run(tf.global_variables_initializer())\n\nfor i in range(3):\n    batch_x, batch_y = data.train.next_batch(100)\n    loss_print = tf.print(loss, output_stream=sys.stdout)\n    sess.run([gradient_descent, loss_print], feed_dict={x: batch_x, y: batch_y})\n\nans = sess.run(accuracy, feed_dict={x: data.test.images, y: data.test.labels})\n\nprint(ans)\n</code></pre>\n\n<p>However, this code only gave me an accuracy of around 11%.\nI tried increasing the number of epochs to 1000, but the result didn't change. Furthermore, the loss in every epoch was the same (2.30).</p>\n\n<p>Am I missing something in the Tensorflow code?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 134}]
[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 4179598, "reputation": 326, "user_id": 7636462, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/KfjtH.jpg?s=256&g=1", "display_name": "S. P", "link": "https://stackoverflow.com/users/7636462/s-p"}, "is_answered": false, "view_count": 1260, "answer_count": 1, "score": 2, "last_activity_date": 1620824617, "creation_date": 1602256673, "question_id": 64282812, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64282812/custom-callbacks-with-warm-up-and-cosine-decay-in-tensorflow", "title": "Custom callbacks with warm up and cosine decay in TensorFlow", "body": "<p>I referred to <a href=\"https://www.tensorflow.org/tutorials/text/transformer\" rel=\"nofollow noreferrer\">this example</a> and especially the callback presented here. So, I decided to write out a callback inspired by <a href=\"https://github.com/facebookresearch/swav/blob/master/main_swav.py#L175\" rel=\"nofollow noreferrer\">this one</a>. Basically, it combines warm-ups and cosine decays.</p>\n<p>Here's how I coded it up -</p>\n<pre class=\"lang-py prettyprint-override\"><code>class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, base_lr=0.1, end_lr=0.001, warmup_steps=390*5):\n        super(CustomSchedule, self).__init__()\n\n        self.base_lr = base_lr\n        self.end_lr = end_lr\n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step=390*35):\n        warmup_lr_schedule = tf.linspace(0., self.base_lr, self.warmup_steps)\n        iters = tf.range(step, dtype=tf.float32) \n        cosine_lr_schedule = tf.convert_to_tensor([self.end_lr + 0.5 * (self.base_lr - self.end_lr) * (1 + \\\n                        tf.math.cos(tf.constant(math.pi) * t / (step))) for t in iters])\n        lr_schedule = tf.concat([warmup_lr_schedule, cosine_lr_schedule], axis=0)\n        \n        return lr_schedule\n</code></pre>\n<p>I verified if this is the one I wanted and indeed it is -</p>\n<p><a href=\"https://i.stack.imgur.com/Mes2q.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Mes2q.png\" alt=\"enter image description here\" /></a></p>\n<p>But when I pass this callback inside an optimizer I run into weird stuff -</p>\n<pre><code>OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n</code></pre>\n<p>Here's <a href=\"https://colab.research.google.com/gist/sayakpaul/0f75d0177cf6824f80fcc62cd49dc78f/scratchpad.ipynb\" rel=\"nofollow noreferrer\">my Colab Notebook</a> for full reproducibility. I am aware that the <code>LearningRateSchedule.__call__</code> method accepts a <code>step</code> argument and outputs the learning rate to use for that particular step. But the class I presented is outputting an entire schedule every step rather than the learning rate for that particular step. But I could not figure out how could achieve this effectively.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 101}]
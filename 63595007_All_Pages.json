[{"items": [{"tags": ["tensorflow", "huggingface-transformers"], "owner": {"account_id": 7297168, "reputation": 5375, "user_id": 5561472, "user_type": "registered", "accept_rate": 25, "profile_image": "https://i.stack.imgur.com/kf45N.jpg?s=256&g=1", "display_name": "Andrey", "link": "https://stackoverflow.com/users/5561472/andrey"}, "is_answered": false, "view_count": 373, "answer_count": 1, "score": 1, "last_activity_date": 1606653157, "creation_date": 1598435443, "question_id": 63595007, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63595007/how-to-train-huggingface-tft5forconditionalgeneration-model", "title": "How to train Huggingface TFT5ForConditionalGeneration model?", "body": "<p>My code is as follows:</p>\n<pre><code>batch_size=8\nsequence_length=25\nvocab_size=100\nimport tensorflow as tf\nfrom transformers import T5Config, TFT5ForConditionalGeneration\nconfigT5 = T5Config(\n    vocab_size=vocab_size,\n    d_ff =512, \n)  \nmodel = TFT5ForConditionalGeneration(configT5)\n\nmodel.compile(\n    optimizer = tf.keras.optimizers.Adam(),\n    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n)\ninput = tf.random.uniform([batch_size,sequence_length],0,vocab_size,dtype=tf.int32)\nlabels = tf.random.uniform([batch_size,sequence_length],0,vocab_size,dtype=tf.int32)\ninput = {'inputs': input, 'decoder_input_ids': input}\nmodel.fit(input, labels)\n</code></pre>\n<p>It generates an error:</p>\n<blockquote>\n<p>logits and labels must have the same first dimension, got logits shape\n[1600,64] and labels shape [200]    [[node\nsparse_categorical_crossentropy_3/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n(defined at C:\\Users\\FA.PROJECTOR-MSK\\Google \u0414\u0438\u0441\u043a\\Colab\nNotebooks\\PoetryTransformer\\experiments\\TFT5.py:30) ]]\n[Op:__inference_train_function_25173]  Function call stack:\ntrain_function</p>\n</blockquote>\n<p>I dont understand - why the model returns a tensor of [1600, 64]. According to <a href=\"https://huggingface.co/transformers/model_doc/t5.html#tft5forconditionalgeneration\" rel=\"nofollow noreferrer\">https://huggingface.co/transformers/model_doc/t5.html#tft5forconditionalgeneration</a> model returns [batch_size, sequence_len, vocab_size].</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 15}]
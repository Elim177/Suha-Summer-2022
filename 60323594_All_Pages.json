[{"items": [{"tags": ["python", "tensorflow", "keras", "regression", "gradienttape"], "owner": {"account_id": 8494439, "reputation": 113, "user_id": 6369451, "user_type": "registered", "profile_image": "https://graph.facebook.com/1127806810594007/picture?type=large", "display_name": "Vignesh Gopakumar", "link": "https://stackoverflow.com/users/6369451/vignesh-gopakumar"}, "is_answered": false, "view_count": 448, "answer_count": 0, "score": 1, "last_activity_date": 1582213282, "creation_date": 1582213282, "question_id": 60323594, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60323594/how-different-is-model-fit-from-using-explicit-gradienttape-on-the-model-trainab", "title": "How different is model.fit from using explicit GradientTape on the model.trainable variables?", "body": "<p>So I have been experimenting with both Keras' Model.fit() and the low-level TF GradientTape for optimising the trainable parameters of a neural network and noticed that the Keras version is significantly better. </p>\n\n<p>Code for the Keras Optimised Version where the final MSE is  : </p>\n\n<pre><code>from tensorflow import keras\nimport tensorflow as tf\n\nfrom sklearn.datasets import load_boston\nX,y = load_boston(return_X_y=True)\nX_tf = tf.cast(X, dtype=tf.float32)\n\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(100, activation = 'relu', input_shape = (13,)), )\nmodel.add(keras.layers.Dense(100, activation = 'relu'))\nmodel.add(keras.layers.Dense(100, activation = 'relu'))\nmodel.add(keras.layers.Dense(1, activation = 'linear'))\n\nmodel.compile(optimizer = tf.keras.optimizers.Adam(0.01),\n             loss = tf.keras.losses.MSE\n             )\n\nmodel.fit(X, y, epochs=1000)enter code here\n</code></pre>\n\n<p>which gives the graph : <a href=\"https://i.stack.imgur.com/0r6mJ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/0r6mJ.png\" alt=\"Keras Fit Deviation from Actual Values\"></a></p>\n\n<p>However, when I optimise the Keras model with tf.GradientTape as shown in the following code: </p>\n\n<pre><code>    from tensorflow import keras\n    import tensorflow as tf\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    from sklearn.datasets import load_boston\n    X,y = load_boston(return_X_y=True)\n    X_tf = tf.cast(X, dtype=tf.float32)\n\n\n\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(100, activation = 'relu', input_shape = (np.shape(X)[1],)), \n     )\n    model.add(keras.layers.Dense(100, activation = 'relu'))\n    model.add(keras.layers.Dense(100, activation = 'relu'))\n    model.add(keras.layers.Dense(1, activation = 'linear'))\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01)\n\n    def loss_func(pred, target):\n        return tf.reduce_mean(tf.square(pred - target))\n\n    trainable_params = model.trainable_variables\n\n     def train_step():\n         with tf.GradientTape() as tape:\n             y_tild = model(X_tf)\n             loss = loss_func(y_tild, y)\n     grads = tape.gradient(loss, trainable_params)\n     optimizer.apply_gradients(zip(grads, trainable_params))\n     print(\"Loss : \" + str(loss.numpy()))\n\n     epochs = 1000\n\n     for ii in range(epochs):\n        train_step()\n</code></pre>\n\n<p>And obtain the following graph for the deviation values. <a href=\"https://i.stack.imgur.com/MEzpZ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/MEzpZ.png\" alt=\"Deviation Values for GradeintTape fit \"></a></p>\n\n<p>You can notice that the values in the Keras fit version are more closer to the actual values than the ones obtained using GradientTape. Also, Gradient Tape values also ended up not varying much for different inputs and worked around a mean, while the Keras one exhibited a lot more diversity. </p>\n\n<p>So how do I use GradientTape low level API to have comparable performances with that of the Keras High Level API ? What is it that Model.fit is doing that outperforms my implementation so much ? I tried going through the source code and couldn't essentially pin it down. </p>\n\n<p>Thanks in advance. </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 185}]
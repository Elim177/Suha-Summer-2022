[{"items": [{"tags": ["tensorflow", "tensor2tensor"], "owner": {"account_id": 438612, "reputation": 22254, "user_id": 826983, "user_type": "registered", "accept_rate": 69, "profile_image": "https://i.stack.imgur.com/B9PSD.jpg?s=256&g=1", "display_name": "Stefan Falk", "link": "https://stackoverflow.com/users/826983/stefan-falk"}, "is_answered": true, "view_count": 363, "accepted_answer_id": 60967271, "answer_count": 2, "score": 2, "last_activity_date": 1585732023, "creation_date": 1585297947, "last_edit_date": 1585663228, "question_id": 60882387, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60882387/how-to-get-current-global-step-in-data-pipeline", "title": "How to get current global_step in data pipeline", "body": "<p>I am trying to create a filter which depends on the current <code>global_step</code> of the training but I am failing to do so properly.</p>\n\n<p>First, I cannot use <code>tf.train.get_or_create_global_step()</code> in the code below because it will throw</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>ValueError: Variable global_step already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n</code></pre>\n\n<p>This is why I tried fetching the scope with <code>tf.get_default_graph().get_name_scope()</code> and within that context I was able to \"<em>get</em>\" the global step:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def filter_examples(example):\n    scope = tf.get_default_graph().get_name_scope()\n\n    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        current_step = tf.train.get_or_create_global_step()\n\n    subtokens_by_step = tf.floor(current_step / curriculum_step_update)\n    max_subtokens = min_subtokens + curriculum_step_size * tf.cast(subtokens_by_step, dtype=tf.int32)\n\n    return tf.size(example['targets']) &lt;= max_subtokens\n\n\ndataset = dataset.filter(filter_examples)\n</code></pre>\n\n<p>The problem with this is that it does not seem to work as I expected. From what I am observing, the <code>current_step</code> in the code above seems to be 0 all the time (I don't know that, just based on my observations I assume that).</p>\n\n<p>The only thing that seems to make a difference, and it sounds weird, is restarting the training. I think, also based on observations, in that case <code>current_step</code> will be the actual current step of the training at this point. But the value itself won't update as the training continues.</p>\n\n<p>If there a way to get the <em>actual</em> value of the current step and use it in my filter like above?</p>\n\n<hr>\n\n<h3>Environment</h3>\n\n<p>Tensorflow 1.12.1</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": -1}]
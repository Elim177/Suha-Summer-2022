[{"items": [{"tags": ["python", "csv", "tensorflow"], "owner": {"account_id": 12829196, "reputation": 3190, "user_id": 9280994, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/378f51f8fdf92e078e0fd52507fed62f?s=256&d=identicon&r=PG&f=1", "display_name": "Jonathan R", "link": "https://stackoverflow.com/users/9280994/jonathan-r"}, "is_answered": true, "view_count": 163, "accepted_answer_id": 56278489, "answer_count": 1, "score": 1, "last_activity_date": 1558625103, "creation_date": 1558382429, "last_edit_date": 1558625103, "question_id": 56227420, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/56227420/tensorflow-convert-tf-csvdataset-map-to-bert-input-format", "title": "Tensorflow convert tf.CsvDataset.map() to Bert input format", "body": "<p>Note: this question started differently, but I deleted all previous (now unnecessary) information.</p>\n\n<p>I have a CsvDataset which consists of a label (float) and a text (string). I want to transform every line so that I can feed it into a pretrained Bert model. Unfortunately, I cannot get past the .map function</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>files = glob.glob(\"example*.tsv\")\nd = tf.data.experimental.CsvDataset(files, \n    [tf.float32, tf.string], \n    select_cols=[3,4], \n    field_delim=\"\\t\", \n    header=True)\nparsed_dataset = d.map(lambda label, text: tf.py_func(_decode_record, [label, text], [tf.float32, tf.string]))\n\ndef _decode_record(label, text):\n    \"\"\"Decodes a row to a TensorFlow example.\"\"\"\n    label_list = [1, 2, 3, 4, 5]\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n    tokens_a = tokenizer.tokenize(text)\n    # Account for [CLS] and [SEP] with \"- 2\"\n    if len(tokens_a) &gt; max_seq_length - 2:\n        tokens_a = tokens_a[0: (max_seq_length - 2)]\n    tokens = []\n    segment_ids = []\n    tokens.append(\"[CLS]\")\n    segment_ids.append(0)\n    for token in tokens_a:\n        tokens.append(token)\n        segment_ids.append(0)\n    tokens.append(\"[SEP]\")\n    segment_ids.append(0)\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n    # tokens are attended to.\n    input_mask = [1] * len(input_ids)\n\n    # Zero-pad up to the sequence length.\n    while len(input_ids) &lt; max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n\n    label_id = label_map[label]\n    features = collections.OrderedDict()\n    features[\"input_ids\"] = create_int_feature(input_ids)\n    features[\"input_mask\"] = create_int_feature(input_mask)\n    features[\"segment_ids\"] = create_int_feature(segment_ids)\n    features[\"label_ids\"] = create_int_feature([label_id])\n    features[\"is_real_example\"] = create_int_feature(\n        [int(True)])\n    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n    return tf_example\n</code></pre>\n\n<p>This breaks with: <code>tensorflow.python.framework.errors_impl.UnimplementedError: Unsupported object type Example [[{{node PyFunc}}]] [Op:IteratorGetNextSync]</code></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 268}]
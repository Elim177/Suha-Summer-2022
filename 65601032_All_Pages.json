[{"items": [{"tags": ["tensorflow", "keras", "neural-network"], "owner": {"account_id": 20030521, "reputation": 13, "user_id": 14683406, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d63dc483fb053388ea564c25125adb14?s=256&d=identicon&r=PG&f=1", "display_name": "deemar", "link": "https://stackoverflow.com/users/14683406/deemar"}, "is_answered": true, "view_count": 266, "accepted_answer_id": 65602424, "answer_count": 1, "score": 1, "last_activity_date": 1619602936, "creation_date": 1609956177, "last_edit_date": 1609964869, "question_id": 65601032, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65601032/variational-autoencoder-loss-not-displayed-right", "title": "Variational Autoencoder loss not displayed right?", "body": "<p>I have implemented a variational autoencoder with the Keras implementation as an example (<a href=\"https://keras.io/examples/generative/vae/\" rel=\"nofollow noreferrer\">https://keras.io/examples/generative/vae/</a>). When plotting the training loss I noticed that these were not the same as displayed in the console. I also saw that the displayed loss in the console in the Keras example was not right considering total_loss = reconstruction_loss + kl_loss.</p>\n<p>Is the displayed loss in the console not the total_loss?</p>\n<p>My VAE code:</p>\n<pre><code>class Sampling(layers.Layer):\n    &quot;&quot;&quot;Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.&quot;&quot;&quot;\n\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n\nlatent_dim = 100 \n\nencoder_inputs = keras.Input(shape=(64, 64, 3)) #eigentlich 160\nx = layers.Conv2D(32, 4, strides=2, padding=&quot;same&quot;)(encoder_inputs)   \nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2D(32, 3, strides=1, padding=&quot;same&quot;)(x)   \nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2D(64, 4,strides=2, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2D(64, 3,strides=1, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2D(128, 4,strides=2, padding=&quot;same&quot;)(x)  \nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2D(64, 3,strides=1, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2D(32, 3,strides=1, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2D(100, 8,strides=1, padding=&quot;valid&quot;)(x)        \nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Flatten()(x)\nz_mean = layers.Dense(latent_dim, name=&quot;z_mean&quot;)(x)\nz_log_var = layers.Dense(latent_dim, name=&quot;z_log_var&quot;)(x)\nz = Sampling()([z_mean, z_log_var])\nencoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=&quot;encoder&quot;)\nencoder.summary()\n\nlatent_inputs = keras.Input(shape=(latent_dim,))\nx = layers.Reshape((1, 1, 100))(latent_inputs)\nx = layers.Conv2DTranspose(100, 8, strides=1, padding=&quot;valid&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2DTranspose(32, 3, strides=1, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2DTranspose(64, 3, strides=1, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2DTranspose(128, 4, strides=2, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2DTranspose(64, 3, strides=1, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2DTranspose(64, 4, strides=2, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2DTranspose(32, 3, strides=1, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\nx = layers.Conv2DTranspose(32, 4, strides=2, padding=&quot;same&quot;)(x)\nx = layers.LeakyReLU(alpha=0.2)(x)\n\ndecoder_outputs = layers.Conv2DTranspose(3, 3, activation=&quot;sigmoid&quot;, padding=&quot;same&quot;)(x)\ndecoder = keras.Model(latent_inputs, decoder_outputs, name=&quot;decoder&quot;)\ndecoder.summary()\n\nclass VAE(keras.Model):\n    def __init__(self, encoder, decoder, encoder_t1, encoder_t2, encoder_t3, encoder_t4, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def train_step(self, data):\n        if isinstance(data, tuple):\n            data = data[0]\n        with tf.GradientTape() as tape:\n            z_mean, z_log_var, z = encoder(data)\n            reconstruction = decoder(z)\n            reconstruction_loss = tf.reduce_mean(    #mean\n                keras.losses.mse(data, reconstruction)      #binary_crossentropy\n            )\n            reconstruction_loss *= 64 * 64                                    #entspricht bildgr\u00c3\u00b6\u00c3\u0178e\n            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n            kl_loss = tf.reduce_mean(kl_loss)      #mean\n            kl_loss *= -0.5 \n            total_loss = reconstruction_loss + kl_loss\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        return {\n            &quot;loss&quot;: total_loss,\n            &quot;reconstruction_loss&quot;: reconstruction_loss,\n            &quot;kl_loss&quot;: kl_loss,\n        }\n    def call(self, inputs):\n      z_mean, z_log_var, z = encoder(inputs)\n      reconstruction = decoder(z)\n      reconstruction_loss = tf.reduce_mean(\n          keras.losses.mse(inputs, reconstruction)\n      )\n      reconstruction_loss *= 64 * 64\n      kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n      kl_loss = tf.reduce_mean(kl_loss)\n      kl_loss *= -0.5 \n      total_loss = reconstruction_loss + kl_loss\n      self.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n      self.add_metric(total_loss, name='total_loss', aggregation='mean')\n      self.add_metric(reconstruction_loss, name='reconstruction_loss', aggregation='mean')\n      return reconstruction\n</code></pre>\n<p>When I plot my loss with the following code:</p>\n<pre><code>vae_train = vae.fit(\n        train_generator,\n        steps_per_epoch=nb_train_samples,\n        epochs=nb_epoch,\n        validation_data=val_generator,\n        validation_steps=nb_validation_samples, #141 #3963\n        callbacks=[es_callback]\n        )\n\nloss = vae_train.history['loss']\nval_loss = vae_train.history['val_total_loss']\nplt.figure()\nplt.plot(range(len(loss)), loss, 'b', label = 'Training loss')\nplt.plot(range(len(val_loss)), val_loss, 'm', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n</code></pre>\n<p>The resulting plot displays the loss differently than the displayed loss in the console. As the displayed loss in the console is not reconstruction_loss + kl_loss but the plotted loss is.</p>\n<p>For example the displayed loss here is not correct, but it is plotted right: (interestingly the val_total_loss is displayed correctly)</p>\n<pre><code>Epoch 20/100\n1266/1266 [==============================] - 82s 65ms/step - loss: 45.2503 - reconstruction_loss: 49.9395 - kl_loss: 0.5695 - val_loss: 0.0000e+00 - val_kl_loss: 0.5888 - val_total_loss: 48.9094 - val_reconstruction_loss: 48.3206\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 267}]
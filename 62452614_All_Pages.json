[{"items": [{"tags": ["python", "python-3.x", "tensorflow", "machine-learning", "eager-execution"], "owner": {"account_id": 4883126, "reputation": 799, "user_id": 3936294, "user_type": "registered", "accept_rate": 100, "profile_image": "https://i.stack.imgur.com/pQAM5.jpg?s=256&g=1", "display_name": "KyleL", "link": "https://stackoverflow.com/users/3936294/kylel"}, "is_answered": true, "view_count": 475, "accepted_answer_id": 62471608, "answer_count": 1, "score": 1, "last_activity_date": 1592575321, "creation_date": 1592491293, "question_id": 62452614, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62452614/how-to-reuse-the-inner-gradient-in-nested-gradient-tapes", "title": "How to reuse the inner gradient in nested gradient tapes?", "body": "<p>I am working on a routine in tensorflow 1.15 that evaluates several hessian-vector products for different vectors</p>\n\n<pre><code>def hessian_v_prod(self, v):\n    with tf.GradientTape() as t1:\n        with tf.GradientTape() as t2:\n            # evaluate loss which uses self.variables\n            loss_val = self.loss()\n        grad = t2.gradient(loss_val, self.variables)\n        v_hat = tf.reduce(tf.multiply(v, grad))\n\n    return t1.gradient(v_hat, self.variables)\n</code></pre>\n\n<p>Each time I call this function it must evaluate the inner loop and calculate the gradient but this is the same regardless of the value of <code>v</code>. How can I reuse the <code>grad</code> value each time I call this function?</p>\n\n<p>I see there is an option to create a tape as <code>tf.GradientTape(persist=True)</code> which keeps the resources for the tape, but can't work out how to incorporate this into my function.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 9}]
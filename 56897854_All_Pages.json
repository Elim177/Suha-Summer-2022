[{"items": [{"tags": ["tensorflow", "keras", "deep-learning"], "owner": {"account_id": 15062809, "reputation": 2258, "user_id": 10870968, "user_type": "registered", "profile_image": "https://graph.facebook.com/859956344339296/picture?type=large", "display_name": "Alex Deft", "link": "https://stackoverflow.com/users/10870968/alex-deft"}, "is_answered": true, "view_count": 75, "accepted_answer_id": 56900679, "answer_count": 1, "score": 0, "last_activity_date": 1562320359, "creation_date": 1562309138, "last_edit_date": 1562319730, "question_id": 56897854, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/56897854/how-to-handle-bn-and-do-behavioural-changes-in-subclassed-models", "title": "How to handle BN and DO behavioural changes in subclassed models?", "body": "<p>So, batch normalization and dropout are layers that change behaviour depending on whether you're in training or inferencing phase. Usually, Keras takes care of that on behalf of me. But, if I'm doing custom training, how can I handle that?</p>\n\n<p>What I've done: added if statement to bypass dropout layer while in inference mode</p>\n\n<pre><code>class mymodel(tf.keras.models.Model):\n    def __init__(self, **kwargs):\n        super(mymodel, self).__init__(**kwargs)\n        self.l1 = tf.keras.layers.Dense(3, input_shape=(2,))\n        self.l2 = tf.keras.layers.Dropout(0.9)\n    def call(self, x, training=None):\n        x = self.l1(x)\n        if training:\n            x = self.l2(x)\n        return x\n</code></pre>\n\n<p>I'm not sure if that's all? And what about Batch normalization?</p>\n\n<p>EDIT: my 'custom training loop' for the toy example above is:</p>\n\n<pre><code>def train_one_ste(model, batch)\n    with tf.GradientTape() as tape:\n        output = model(batch)\n    grad = tape.gradient(output, model.trainable_weights)\n    optimizer.apply_gradients(zip(grad, model.trainable_weight)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 115}]
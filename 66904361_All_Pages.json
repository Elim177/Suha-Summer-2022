[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "activation-function"], "owner": {"account_id": 7773738, "reputation": 29, "user_id": 15450208, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/68102ef812f46c290af79b7ba72aa108?s=256&d=identicon&r=PG&f=1", "display_name": "Wasonic", "link": "https://stackoverflow.com/users/15450208/wasonic"}, "is_answered": false, "view_count": 110, "answer_count": 1, "score": 0, "last_activity_date": 1617694901, "creation_date": 1617279574, "last_edit_date": 1617694901, "question_id": 66904361, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66904361/keras-artificial-neural-networks-error-when-using-a-custom-activation-functi", "title": "Keras \u2013 Artificial Neural Networks - Error when using a custom activation function", "body": "<p>I\u2019m creating an Artificial Neural Network (ANN) using Kera\u2019s Functional API. Link to the data csv file: <a href=\"https://github.com/dpintof/SPX_Options_ANN/blob/master/MLP3/call_df.csv\" rel=\"nofollow noreferrer\">https://github.com/dpintof/SPX_Options_ANN/blob/master/MLP3/call_df.csv</a>. Relevant part of the code that reproduces problem:</p>\n<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom tensorflow.keras import layers\n\n\n# Data \ncall_df = pd.read_csv(&quot;call_df.csv&quot;)\n\ncall_X_train, call_X_test, call_y_train, call_y_test = train_test_split(call_df.drop([&quot;Option_Average_Price&quot;],\n                    axis = 1), call_df.Option_Average_Price, test_size = 0.01)\n\n\n# Hyperparameters\nn_hidden_layers = 2 # Number of hidden layers.\nn_units = 128 # Number of neurons of the hidden layers.\n\n# Create input layer\ninputs = keras.Input(shape = (call_X_train.shape[1],))\nx = layers.LeakyReLU(alpha = 1)(inputs)\n\n&quot;&quot;&quot;\nFunction that creates a hidden layer by taking a tensor as input and applying a\nmodified ELU (MELU) activation function.\n&quot;&quot;&quot;\ndef hl(tensor):\n    # Create custom MELU activation function\n    def melu(z):\n        return tf.cond(z &gt; 0, lambda: ((z**2)/2 + 0.02*z) / (z - 2 + 1/0.49), \n                        lambda: 0.49*(keras.activations.exponential(z)-1))\n    \n    y = layers.Dense(n_units, activation = melu)(tensor)\n    return y\n\n# Create hidden layers\nfor _ in range(n_hidden_layers):\n    x = hl(x)\n\n# Create output layer\noutputs = layers.Dense(1, activation = keras.activations.softplus)(x)\n\n# Actually create the model\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n\n# QUICK TEST\nmodel.compile(loss = &quot;mse&quot;, optimizer = keras.optimizers.Adam())\nhistory = model.fit(call_X_train, call_y_train, \n                    batch_size = 4096, epochs = 1,\n                    validation_split = 0.01, verbose = 1)\n</code></pre>\n<p>This is the error I get when I do model.fit(\u2026) (notice that 4096 is my batch size and 128 is the number of neurons of the hidden layers):</p>\n<pre><code>InvalidArgumentError:  The second input must be a scalar, but it has shape [4096,128]\n     [[{{node dense/cond/dense/BiasAdd/_5}}]] [Op:__inference_keras_scratch_graph_1074]\n\nFunction call stack:\nkeras_scratch_graph\n</code></pre>\n<p>I know the problem has to do with the custom activation function because the program runs fine if I use the following hl function instead:</p>\n<pre><code>def hl(tensor):\n    lr = layers.Dense(n_units, activation = layers.LeakyReLU())(tensor)\n    return lr\n</code></pre>\n<p>I got the same error when trying to define melu(z) like this:</p>\n<pre><code>@tf.function\ndef melu(z):\n    if z &gt; 0:\n        return ((z**2)/2 + 0.02*z) / (z - 2 + 1/0.49)\n    else:\n        return 0.49*(keras.activations.exponential(z)-1)\n</code></pre>\n<p>From <a href=\"https://stackoverflow.com/questions/43915482/how-do-you-create-a-custom-activation-function-with-keras\">How do you create a custom activation function with Keras?</a> I also tried the following, but without success:</p>\n<pre><code>def hl(tensor):\n    # Create custom MELU activation function\n    def melu(z):\n        return tf.cond(z &gt; 0, lambda: ((z**2)/2 + 0.02*z) / (z - 2 + 1/0.49), \n                        lambda: 0.49*(keras.activations.exponential(z)-1))\n    \n    from keras.utils.generic_utils import get_custom_objects\n    get_custom_objects().update({'melu': layers.Activation(melu)})\n \n    x = layers.Dense(n_units)(tensor)\n    y = layers.Activation(melu)(x)\n    return y\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 36}]
[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "lstm"], "owner": {"account_id": 13492482, "reputation": 437, "user_id": 9734248, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c8fd8584be5278402a0034f62976afbe?s=256&d=identicon&r=PG&f=1", "display_name": "DY92", "link": "https://stackoverflow.com/users/9734248/dy92"}, "is_answered": true, "view_count": 728, "accepted_answer_id": 58404309, "answer_count": 1, "score": 0, "last_activity_date": 1571187015, "creation_date": 1571085078, "last_edit_date": 1571178593, "question_id": 58383691, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58383691/lstm-input-0-of-layer-lstm-1-is-incompatible-with-the-layer-expected-ndim-3-f", "title": "LSTM: Input 0 of layer lstm_1 is incompatible with the layer: expected ndim=3, found ndim=2 (reshaping input)", "body": "<p>I want to generate poems based on  Robert Frost's Poems.\nI have preprocessed my dataset:</p>\n\n<pre><code>max_sentence_len = max(len(l) for l in corpus_int)\n\ninput_seq = np.array(tf.keras.preprocessing.sequence.pad_sequences(corpus_int,padding = 'pre',truncating = 'pre',maxlen = max_sentence_len))\npredictors, label = input_seq[:,:-1],input_seq[:,-1]#predictors everything except last, label only last\nlabel = ku.to_categorical(label, num_classes=total_words,dtype='int32')\n\npredictors\n\narray([[   0,    0,    0, ...,   10,    5,  544],\n       [   0,    0,    0, ...,   64,    8,  854],\n       [   0,    0,    0, ...,  855,  174,    2],\n       ...,\n       [   0,    0,    0, ...,  129,   49,   94],\n       [   0,    0,    0, ...,  183,  159,   60],\n       [   0,    0,    3, ...,    3, 2157,    4]], dtype=int32)\n\nlabel\n\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 1]], dtype=int32)\n</code></pre>\n\n<p>After that i have built my model using encoder - decoder arhcitecture:</p>\n\n<pre><code>class seq2seq(tf.keras.Model):\n  def __init__(self,max_sequence_len,total_words):\n    super(seq2seq,self).__init__()\n    self.max_sequence_len = max_sequence_len\n    self.total_words = total_words\n\n    self.input_len = self.max_sequence_len - 1\n    self.total_words = self.total_words\n\n    #Encoder\n    self.enc_embedding = tf.keras.layers.Embedding(input_dim = total_words,output_dim = 300,input_length = max_sentence_len - 1)\n    self.enc_lstm_1 = tf.keras.layers.LSTM(units = 300, activation = 'tanh')\n    self.enc_lstm_2 = tf.keras.layers.LSTM(units = 300, activation = 'tanh', return_state = True)\n\n    #decoder\n    self.dec_embedding = tf.keras.layers.Embedding(input_dim = total_words,output_dim = 300,input_length = max_sentence_len - 1)\n    self.dec_lstm_1 = tf.keras.layers.LSTM(units = 300, activation = 'tanh')\n    self.dec_lstm_2 = tf.keras.layers.LSTM(units = 300, activation = 'tanh', return_state = True,return_sequences = True)\n\n    #Dense layer and output:\n    self.dense = tf.keras.layers.Dense(total_words, activation='softmax')\n\n  def call(self,inputs):\n    #Encoding\n    enc_x = self.enc_embedding(inputs)\n    enc_x = self.enc_lstm_1(enc_x)\n    enc_outputs, state_h, state_c = self.enc_lstm_2(enc_x)\n\n    #Decoding:\n    dec_x = self.dec_embedding(enc_outputs)\n    dec_x = self.dec_lstm_1(dec_x,initial_state = [state_h, state_c])\n    dec_outputs, _, _ = self.enc_lstm_2(dec_x)\n    output_dense = self.dense(dec_outputs)\n\n    return output_dense\n\nmodel = seq2seq(max_sequence_len = max_sentence_len,total_words = total_words)   \nmodel.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001),loss='categorical_crossentropy', metrics=['accuracy']) \nmodel.fit(predictors,label,epochs=5, batch_size=128)\n</code></pre>\n\n<p>But at the end I get the following error:</p>\n\n<pre><code>ValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-1c349573302d&gt; in &lt;module&gt;()\n     37 model = seq2seq(max_sequence_len = max_sentence_len,total_words = total_words)\n     38 model.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.0001),loss='categorical_crossentropy', metrics=['accuracy'])\n---&gt; 39 model.fit(predictors,label,epochs=5, batch_size=128)\n\n8 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)\n    235       except Exception as e:  # pylint:disable=broad-except\n    236         if hasattr(e, 'ag_error_metadata'):\n--&gt; 237           raise e.ag_error_metadata.to_exception(e)\n    238         else:\n    239           raise\n\nValueError: in converted code:\n\n    &lt;ipython-input-4-1c349573302d&gt;:27 call  *\n        enc_outputs, state_h, state_c = self.enc_lstm_2(enc_x)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/layers/recurrent.py:623 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py:812 __call__\n        self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/input_spec.py:177 assert_input_compatibility\n        str(x.shape.as_list()))\n\n    ValueError: Input 0 of layer lstm_1 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: [None, 300]\n</code></pre>\n\n<p>I understand, that the problem is in the input shape(as it was answered in the post <a href=\"https://stackoverflow.com/questions/54416322/expected-ndim-3-found-ndim-2\">expected ndim=3, found ndim=2</a>).</p>\n\n<p>But I don't know how should I reshape my data for the tensorflow 2.0.\nCan you help me with the problem?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 5}]
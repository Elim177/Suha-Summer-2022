[{"items": [{"tags": ["tensorflow", "machine-learning", "keras"], "owner": {"account_id": 16319390, "reputation": 13, "user_id": 11785620, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/eff4d5b75f9a5fe0b0f7964aa01f0926?s=256&d=identicon&r=PG&f=1", "display_name": "Franz", "link": "https://stackoverflow.com/users/11785620/franz"}, "is_answered": false, "view_count": 200, "answer_count": 0, "score": 1, "last_activity_date": 1611593406, "creation_date": 1611059726, "last_edit_date": 1611593406, "question_id": 65791633, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65791633/custom-optimizer-with-multiple-loss-function-evaluations", "title": "Custom optimizer with multiple loss function evaluations", "body": "<p>I want to implement a custom optimization algorithm for TF models.\nI have read the following sources</p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer#creating_a_custom_optimizer_2\" rel=\"nofollow noreferrer\">tf documentation on custom optimizers</a></li>\n<li><a href=\"https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/optimizer_v2/gradient_descent.py\" rel=\"nofollow noreferrer\">tf SGD implementation</a></li>\n<li><a href=\"https://keras.io/guides/customizing_what_happens_in_fit/\" rel=\"nofollow noreferrer\">keras documentation on custom models</a></li>\n<li><a href=\"https://towardsdatascience.com/custom-optimizer-in-tensorflow-d5b41f75644a\" rel=\"nofollow noreferrer\">towardsdatascience guide on custom optimizers</a></li>\n</ul>\n<p>However lot of questions remain.</p>\n<ol>\n<li><p>It seems like it is not possible to evaluate the loss function multiple times (for different weight settings) before applying a gradient step, when using the custom optimizer API. For example in a line-search type of algorithm this is necessary.</p>\n</li>\n<li><p>I tried to do all steps manually.</p>\n</li>\n</ol>\n<p>Assume I have setup my model and my optimization problem like this</p>\n<pre class=\"lang-py prettyprint-override\"><code>from tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import models\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(15, input_dim=10))\nmodel.add(layers.Dense(20))\nmodel.add(layers.Dense(1))\n\nx_train, y_train = get_train_data()\nloss = losses.MeanSquaredError()\n\ndef val_and_grads(weights):\n    model.set_weights(weights)\n\n    with tf.GradientTape() as tape:\n        val = loss(y_train, model(x_train))\n\n    grads = tape.gradient(val, model.trainable_variables)\n\n    return val, grads\n\ninitial_weights = model.get_weights()\noptimal_weigths = my_fancy_optimization_algorithm(val_and_grads, initial_weights)\n</code></pre>\n<p>However my function <code>val_and_grads</code> needs a <code>list</code> of weights and returns a <code>list</code> of gradients from <code>my_fancy_optimization_algorithm</code>s point of view that seems unnatural.</p>\n<p>I could warp <code>val_and_grads</code> to &quot;stack&quot; the returned gradients and &quot;split&quot; the passed weights like this</p>\n<pre class=\"lang-py prettyprint-override\"><code>def wrapped_val_and_grad(weights):\n    grads = val_and_grads(split_weights(weights))\n    return stack_grads(grads)\n</code></pre>\n<p>however that seems very inefficient.</p>\n<p>Anyway, I do not like this approach since it seems that I would loose out out on a lot of the surrounding tensorflow infrastructure (printing of current loss function values and metrics during learning, tensorboard stuff, ...).</p>\n<ol start=\"3\">\n<li>I could also pack the above in a custom model with a tailored <code>train_step</code> like this</li>\n</ol>\n<pre class=\"lang-py prettyprint-override\"><code>def CustomModel(keras.Model):\n    def train_step(self, data):\n    \n        x_train, y_train = data\n\n        def val_and_grads(weights):\n            self.set_weights(weights)\n        \n            with tf.GradientTape() as tape:\n                val = loss(y_train, self(x_train))\n        \n            grads = tape.gradient(val, self.trainable_variables)\n            return val, grads\n\n        trainable_vars = self.trainable_variables\n\n        old_weights = self.get_weights()\n        update = my_fancy_update_finding_algorithm(val_and_grads, self.get_weights()) # this can do multiple evaluations of the model\n        self.set_weights(old_weights) # restore the weights\n\n        self.optimizer.apply_gradients(zip(update, trainable_vars))\n</code></pre>\n<p>Here I would need a accompanying custom optimizer that does nothing else than updating the current weights by adding the <code>update</code> (new_weigths = current_weights + update).</p>\n<p>I am still unsure if this is the best way to go.</p>\n<p>If someone can comment on the snippets and ideas above, guide me to any other resource that I should consider or provide new approaches and other feedback I would be very glad.</p>\n<p>Thanks all.</p>\n<p>Franz</p>\n<hr />\n<p>EDIT:\nSadly I did not get any response here so far. Maybe my question is not concrete enough. As a first smaller question:\nGiven the <code>model</code> and <code>val_and_grads</code> in the first listing. How would I efficiently calculate the norm of the WHOLE gradient? What I do so far is</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\n_, grads = val_and_grad(model.get_weights())\nnorm_grads = np.linalg.norm(np.concatenate([grad.numpy().flatten() for grad in grad]))\n</code></pre>\n<p>This surely cannot be the &quot;right&quot; way.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 263}]
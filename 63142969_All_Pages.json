[{"items": [{"tags": ["tensorflow", "google-compute-engine", "tpu", "google-cloud-tpu"], "owner": {"account_id": 3948330, "reputation": 5043, "user_id": 3259896, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/641c30a7b383022f22b53c8cedb04e3f?s=256&d=identicon&r=PG&f=1", "display_name": "SantoshGupta7", "link": "https://stackoverflow.com/users/3259896/santoshgupta7"}, "is_answered": true, "view_count": 100, "answer_count": 1, "score": 0, "last_activity_date": 1596477643, "creation_date": 1595971411, "last_edit_date": 1595971830, "question_id": 63142969, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63142969/how-to-solve-data-fetch-bottle-neck-for-tpu-inference", "title": "How to solve data fetch bottle neck for TPU inference?", "body": "<p>This is what my inference setup looks like</p>\n<pre><code>autotune = tf.data.experimental.AUTOTUNE\n\nwith strategy.scope():\n    model = LoadModel()\n    raw_dataset = tf.data.TFRecordDataset(tfRecordAddress)\n    train_dataset = raw_dataset.map(_parse_example, num_parallel_calls=autotune)\n    train_dataset = train_dataset.padded_batch(batch_size, padding_values=(1, 1, b'-'), padded_shapes=(512, 512, 1))\n    # train_dataset = train_dataset.repeat()\n    train_dataset = train_dataset.prefetch(autotune)\n    train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n\ndef per_core_inference_fn(inputIds,attnIds ):\n    return model.inference((inputIds, attnIds))\n\n@tf.function\ndef inference_fn(inputIds, attnIds):\n    return strategy.run(per_core_inference_fn, args=(inputIds,attnIds))\n\nresults = []\nfor x in train_dataset:\n    t0 = time.time()\n    results.append(inference_fn(x[0], x[1]))\n    t1 = time.time()\n    print('time is :', t1-t0)\n</code></pre>\n<p>With huge batch_sizes, the inference is blazing fast, something like .0003 seconds. However, the fetching of the next batch takes a long time, <code>for x in train_dataset:</code>, like 60-80 seconds.</p>\n<p>As far as I can tell, I am doing the inference correctly, but somehow the TPU's CPU is running into a huge bottleneck with the batch retrieval.</p>\n<p>I did Not see this bottleneck during training. So it looks like <code>model.fit</code> is doing something I'm not.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 204}]
[{"items": [{"tags": ["python", "neural-network", "data-science", "tensorflow2.0", "gradient-descent"], "owner": {"account_id": 20099401, "reputation": 1, "user_id": 14738203, "user_type": "registered", "profile_image": "https://lh4.googleusercontent.com/-jwstnVAR1dg/AAAAAAAAAAI/AAAAAAAAAAA/AMZuuck4qlUwFMGHMAz8v2jZBc22FMScZA/s96-c/photo.jpg?sz=256", "display_name": "Maciej \u0106wir", "link": "https://stackoverflow.com/users/14738203/maciej-%c4%86wir"}, "is_answered": false, "view_count": 73, "answer_count": 1, "score": 0, "last_activity_date": 1606873506, "creation_date": 1606775783, "last_edit_date": 1606873480, "question_id": 65082154, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65082154/how-to-properly-apply-stochastic-gradient-descent-to-simple-neural-network", "title": "How to properly apply stochastic gradient descent to simple neural network?", "body": "<p>Im trying to write NN from scratch to better understand what is going on underneath Keras API. And now I have got problem with applying gradinets to my loss. I believe there is some problem with architecture and my understanding of TF. Basically line with grads returns Nones and hence code returns:</p>\n<pre><code>ValueError: No gradients provided for any variable: ['Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0', 'Variable:0'].\n</code></pre>\n<p>Here is my code. Its very straightforward but I want to do it simply before warping it into model class.</p>\n<pre class=\"lang-py prettyprint-override\"><code>input_shape = x_train.shape[1]\nn_hidden_1 = 32\nn_hidden_2 = 8\noutput_shape = 1\nepochs = 1\n\nW_1 = tf.Variable(tf.random.normal([input_shape,n_hidden_1]))\nW_2 = tf.Variable(tf.random.normal([n_hidden_1,n_hidden_2]))\nW_output = tf.Variable(tf.random.normal([n_hidden_2,output_shape]))\n    \nB_1 = tf.Variable(tf.random.normal([n_hidden_1]))\nB_2 = tf.Variable(tf.random.normal([n_hidden_2]))\nB_output = tf.Variable(tf.random.normal([output_shape]))\n\nvar_list= [W_1, W_2, W_output, B_1, B_2, B_output]\n\nopt = tf.keras.optimizers.SGD(learning_rate=0.1)\n\n\nfor epoch in range(epochs):\n    \n    input_tensor = tf.convert_to_tensor(x_train, dtype=tf.float32)\n    size = input_tensor.shape[0]\n    \n    labels = tf.convert_to_tensor(y_train, dtype=tf.float32)\n    labels = tf.reshape(labels, (size,1))\n    \n    #1_layer\n    layer_1 = tf.matmul(input_tensor, W_1)\n    layer_1 = tf.add(layer_1, B_1)\n    layer_1 = tf.nn.relu(layer_1)\n\n    #2_layer\n    layer_2 = tf.matmul(layer_1, W_2)\n    layer_2 = tf.add(layer_2, B_2)\n    layer_2 = tf.nn.relu(layer_2)\n\n    #output layer\n    output = tf.matmul(layer_2, W_output)\n    output = tf.add(output, B_output)\n    \n    with tf.GradientTape() as tape:\n        _loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels, output))\n        \n    grads = tape.gradient(_loss, var_list)\n    grads_and_vars = zip(grads, var_list)\n    optimizer.apply_gradients(grads_and_vars)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 128}]
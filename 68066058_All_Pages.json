[{"items": [{"tags": ["tensorflow"], "owner": {"account_id": 21094651, "reputation": 1, "user_id": 15505884, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GhFxPwi1ihOApxsoeNSxSPgxytlmDyZfkzbYAVnLA=k-s256", "display_name": "FODHA Ghassen", "link": "https://stackoverflow.com/users/15505884/fodha-ghassen"}, "is_answered": false, "view_count": 42, "answer_count": 0, "score": 0, "last_activity_date": 1624624018, "creation_date": 1624270317, "last_edit_date": 1624624018, "question_id": 68066058, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68066058/neural-network-predicts-the-same-number-for-every-handwritten-digit-or-false-num", "title": "neural network predicts the same number for every handwritten digit or false number", "body": "<p>I've tried to built a handwritten digit recognition neural network based on the mnist dataset and when I test it with the test images provided by the data set itself it seems to work pretty well (that's what the function test_predict is for). Now I would like to step it up and have the network recognise some actual handwritten digits that I've taken photos of. I provide some photos of individual digits and they all look something like this:\n<a href=\"https://i.stack.imgur.com/HbXOl.png\" rel=\"nofollow noreferrer\">view image here</a>\n<em><strong>The problem is: My neural network's prediction for this one of my test image is &quot;1&quot;.</strong></em>\nDo you have any idea why this is happening? Any advise is welcome. Thank you in advance.\nHere's my code :</p>\n<pre><code>     from __future__ import absolute_import, division, print_function\n\n# Import TensorFlow v2.\n     import tensorflow as tf\n     from tensorflow.keras import Model, layers\n     import numpy as np\n\n# MNIST dataset parameters.\n     num_classes = 10 # total classes (0-9 digits).\n     num_features = 784 # data features (img shape: 28*28).\n\n# Training Parameters\n     learning_rate = 0.001\n     training_steps = 1000\n     batch_size = 32\n     display_step = 100\n\n# Network Parameters\n# MNIST image shape is 28*28px, we will then handle 28 sequences of 28 timesteps for every sample.\n     num_input = 28 # number of sequences.\n     timesteps = 28 # timesteps.\n     num_units = 32 # number of neurons for the LSTM layer.\n\n# Prepare MNIST data.\n     from tensorflow.keras.datasets import mnist\n     (x_train, y_train), (x_test, y_test) = mnist.load_data()\n# Convert to float32.\n     x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n# Flatten images to 1-D vector of 784 features (28*28).\n     x_train, x_test = x_train.reshape([-1, 28, 28]), x_test.reshape([-1, num_features])\n# Normalize images value from [0, 255] to [0, 1].\n     x_train, x_test = x_train / 255., x_test / 255.\n\n# Use tf.data API to shuffle and batch data.\n    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n    train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n\n# Create LSTM Model.\n     class LSTM(Model):\n         # Set layers.\n         def __init__(self):\n             super(LSTM, self).__init__()\n             # RNN (LSTM) hidden layer.\n             self.lstm_layer = layers.LSTM(units=num_units)\n             self.out = layers.Dense(num_classes)\n\n         # Set forward pass.\n         def call(self, x, is_training=False):\n             # LSTM layer.\n             x = self.lstm_layer(x)\n             # Output layer (num_classes).\n             x = self.out(x)\n             if not is_training:\n                # tf cross entropy expect logits without softmax, so only\n                # apply softmax when not training.\n                x = tf.nn.softmax(x)\n             return x\n\n# Build LSTM model.\n     lstm_net = LSTM()\n\n# Cross-Entropy Loss.\n# Note that this will apply 'softmax' to the logits.\n     def cross_entropy_loss(x, y):\n         # Convert labels to int 64 for tf cross-entropy function.\n         y = tf.cast(y, tf.int64)\n         # Apply softmax to logits and compute cross-entropy.\n         loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n         # Average loss across the batch.\n         return tf.reduce_mean(loss)\n\n# Accuracy metric.\n     def accuracy(y_pred, y_true):\n         # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n         correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n         return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis=-1)\n\n# Adam optimizer.\n     optimizer = tf.optimizers.Adam(learning_rate)\n\n# Optimization process. \ndef run_optimization(x, y):\n    # Wrap computation inside a GradientTape for automatic differentiation.\n    with tf.GradientTape() as g:\n        # Forward pass.\n        pred = lstm_net(x, is_training=True)\n        # Compute loss.\n        loss = cross_entropy_loss(pred, y)\n        \n    # Variables to update, i.e. trainable variables.\n    trainable_variables = lstm_net.trainable_variables\n\n    # Compute gradients.\n    gradients = g.gradient(loss, trainable_variables)\n    \n    # Update weights following gradients.\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n# Run training for the given number of steps.\nfor step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n    # Run the optimization to update W and b values.\n    run_optimization(batch_x, batch_y)\n    \n    if step % display_step == 0:\n        pred = lstm_net(batch_x, is_training=True)\n        loss = cross_entropy_loss(pred, batch_y)\n        acc = accuracy(pred, batch_y)\n        print(&quot;step: %i, loss: %f, accuracy: %f&quot; % (step, loss, acc))\n\n# select an image from the list Ensemble_Chiffre_extf.\n     taszwira = Ensemble_Chiffre_extf[0]\n     cv2_imshow(taszwira)\n\n# Convert to float32.\n     taszwira = np.array(taszwira, np.float32)\n# Flatten images to 1-D vector of 784 features (28*28).\n     resized = cv2.resize(taszwira,(28,28),interpolation = cv2.INTER_CUBIC)\n     newing = tf.keras.utils.normalize(resized,axis=1)\n     newing = np.array(newing).reshape(-1,28,28)\n\n# Normalize images value from [0, 255] to [0, 1].\n     newing = newing / 255.\n# predection\n     pred = lstm_net(newing, is_training=False)\n     print(np.argmax(pred))\n</code></pre>\n<p>the result of 'print(np.argmax(pred))' = 1 but the image have such number 3 . i test other image that is 5 but the prediction is 7 : <a href=\"https://i.stack.imgur.com/dAuvr.png\" rel=\"nofollow noreferrer\">View image 5</a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 119}]
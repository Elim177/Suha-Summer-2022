[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0"], "owner": {"account_id": 108161, "reputation": 5951, "user_id": 287238, "user_type": "registered", "accept_rate": 75, "profile_image": "https://i.stack.imgur.com/oQJH2.jpg?s=256&g=1", "display_name": "mathtick", "link": "https://stackoverflow.com/users/287238/mathtick"}, "is_answered": true, "view_count": 2592, "accepted_answer_id": 68954464, "answer_count": 2, "score": 10, "last_activity_date": 1630072277, "creation_date": 1564512803, "last_edit_date": 1571450494, "question_id": 57278335, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57278335/tensorflow-2-0-an-op-outside-of-the-function-building-code-is-being-passed", "title": "tensorflow 2.0: An op outside of the function building code is being passed", "body": "<p>I'm getting an error: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>TypeError: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\n</code></pre>\n\n<p>Using an NVP layer like follows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow_probability as tfp\ntfb = tfp.bijectors\ntfd = tfp.distributions\nclass NVPLayer(tf.keras.models.Model):\n\n    def __init__(self, *, output_dim, num_masked, **kwargs):\n        super().__init__(**kwargs)\n        self.output_dim = output_dim\n        self.num_masked = num_masked\n        self.shift_and_log_scale_fn = tfb.real_nvp_default_template(\n            hidden_layers=[2], # HERE HERE ADJUST THIS\n            activation=None, # linear\n            )\n        self.loss = None\n\n    def get_nvp(self):\n        nvp = tfd.TransformedDistribution(\n            distribution=tfd.MultivariateNormalDiag(loc=[0.] * self.output_dim),\n            bijector=tfb.RealNVP(\n                num_masked=self.num_masked,\n                shift_and_log_scale_fn=self.shift_and_log_scale_fn)\n            )\n        return nvp\n\n    def call(self, *inputs):\n        nvp = self.get_nvp()\n        self.loss = tf.reduce_mean(nvp.log_prob(*inputs)) # how else to do this?\n        # return nvp.bijector.forward(*inputs)\n        return nvp.bijector.inverse(*inputs)\n</code></pre>\n\n<p>I'm not calling <code>tf.init_scope</code> anywhere. A simple version training a layer like seems to work. </p>\n\n<p>I will try to get more more granular trace but I suspect this is something to do with non-eager mode stuff. </p>\n\n<p><strong>UPDATE:</strong> so this is definitely coming from the <code>self.loss</code> inclusion in some gradient tape layer. What is the correct way of doing this?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 82}]
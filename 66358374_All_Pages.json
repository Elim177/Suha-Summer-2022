[{"items": [{"tags": ["python", "tensorflow", "migration", "tensorflow2.x", "tensorflow1.15"], "owner": {"account_id": 16705366, "reputation": 1, "user_id": 14693976, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/LHvRF.png?s=256&g=1", "display_name": "schissmantics", "link": "https://stackoverflow.com/users/14693976/schissmantics"}, "is_answered": true, "view_count": 359, "answer_count": 1, "score": 0, "last_activity_date": 1614666207, "creation_date": 1614198556, "last_edit_date": 1614646707, "question_id": 66358374, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66358374/conversion-from-tf-gradients-to-tf-gradienttape-returns-none", "title": "Conversion from tf.gradients() to tf.GradientTape() returns None", "body": "<p>I'm migrating some TF1 code to TF2. For full code, you may check <a href=\"https://github.com/openai/baselines/blob/master/baselines/acer/acer.py\" rel=\"nofollow noreferrer\">here</a> lines [155-176]. There is a line in TF1 that gets gradients given a loss (float value) and a (m, n) tensor</p>\n<p><strong>Edit:</strong> the problem persists</p>\n<p><strong>Note:</strong> the TF2 code should be compatible and should work inside a <code>tf.function</code></p>\n<pre><code>g = tf.gradients(-loss, f)  # loss being a float and f being a (m, n) tensor\nk = -f_pol / (f + eps)  # f_pol another (m, n) tensor and eps a float\nk_dot_g = tf.reduce_sum(k * g, axis=-1)\nadj = tf.maximum(\n    0.0,\n    (tf.reduce_sum(k * g, axis=-1) - delta)\n    / (tf.reduce_sum(tf.square(k), axis=-1) + eps),\n)\ng = g - tf.reshape(adj, [nenvs * nsteps, 1]) * k\ngrads_f = -g / (nenvs * nsteps)\ngrads_policy = tf.gradients(f, params, grads_f)  # params being the model parameters\n</code></pre>\n<p>In TF2 code I'm trying:</p>\n<pre><code>with tf.GradientTape() as tape:\n    f = calculate_f()\n    f_pol = calculate_f_pol()\n    others = do_further_calculations()\n    loss = calculate_loss()\ng = tape.gradient(-loss, f)\n</code></pre>\n<p>However I keep getting <code>g = [None]</code> whether I use <code>tape.watch(f)</code> or create a <code>tf.Variable</code> with the value of <code>f</code> or even use <code>tf.gradients()</code> inside a <code>tf.function</code> because otherwise, it will complain.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 249}]
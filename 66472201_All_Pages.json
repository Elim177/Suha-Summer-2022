[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "deep-learning"], "owner": {"account_id": 7424649, "reputation": 13635, "user_id": 9215780, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-lqxossnzkSU/AAAAAAAAAAI/AAAAAAAAAJg/pHrYwIfRc-k/photo.jpg?sz=256", "display_name": "M.Innat", "link": "https://stackoverflow.com/users/9215780/m-innat"}, "is_answered": true, "view_count": 2808, "accepted_answer_id": 66524901, "answer_count": 2, "score": 7, "last_activity_date": 1642190607, "creation_date": 1614850275, "last_edit_date": 1642190607, "question_id": 66472201, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66472201/gradient-accumulation-with-custom-model-fit-in-tf-keras", "title": "Gradient Accumulation with Custom model.fit in TF.Keras?", "body": "<p>Please add a minimum comment on your thoughts so that I can improve my query. Thank you. -)</p>\n<hr />\n<p>I'm trying to train a <code>tf.keras</code> model with <strong>Gradient Accumulation</strong> (GA). But I don't want to use it in the custom training loop (<a href=\"https://stackoverflow.com/questions/59893850/how-to-accumulate-gradients-in-tensorflow-2-0\">like</a>) but customize the <code>.fit()</code> method by overriding the <code>train_step</code>.Is it possible? How to accomplish this?  The reason is if we want to get the benefit of <code>keras</code> built-in functionality like <code>fit</code>, <code>callbacks</code>, we don't want to use the custom training loop but at the same time if we want to override <code>train_step</code> for some reason (like GA or else) we can customize the <code>fit</code> method and still get the leverage of using those built-in functions.</p>\n<p>And also, I know the pros of using <strong>GA</strong> but what are the major cons of using it? Why does it's not come as a default but an optional feature with the framework?</p>\n<pre><code># overriding train step \n# my attempt \n# it's not appropriately implemented \n# and need to fix \nclass CustomTrainStep(tf.keras.Model):\n    def __init__(self, n_gradients, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.n_gradients = n_gradients\n        self.gradient_accumulation = [tf.zeros_like(this_var) for this_var in \\\n                                           self.trainable_variables]\n\n    def train_step(self, data):\n        x, y = data\n        batch_size = tf.cast(tf.shape(x)[0], tf.float32)  \n        # Gradient Tape\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        # Calculate batch gradients\n        gradients = tape.gradient(loss, self.trainable_variables)\n        # Accumulate batch gradients\n        accum_gradient = [(acum_grad+grad) for acum_grad, grad in \\\n               zip(self.gradient_accumulation, gradients)]\n        accum_gradient = [this_grad/batch_size for this_grad in accum_gradient]\n        # apply accumulated gradients\n        self.optimizer.apply_gradients(zip(accum_gradient, self.trainable_variables))\n        # TODO: reset self.gradient_accumulation \n        # update metrics\n        self.compiled_metrics.update_state(y, y_pred)\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>\n<p>Please, run and check with the following toy setup.</p>\n<pre><code># Model \nsize = 32\ninput = tf.keras.Input(shape=(size,size,3))\nefnet = tf.keras.applications.DenseNet121(weights=None,\n                                          include_top = False, \n                                          input_tensor = input)\nbase_maps = tf.keras.layers.GlobalAveragePooling2D()(efnet.output) \nbase_maps = tf.keras.layers.Dense(units=10, activation='softmax', \n                                             name='primary')(base_maps) \ncustom_model = CustomTrainStep(n_gradients=10, inputs=[input], outputs=[base_maps])\n\n# bind all\ncustom_model.compile(\n    loss = tf.keras.losses.CategoricalCrossentropy(),\n    metrics = ['accuracy'],\n    optimizer = tf.keras.optimizers.Adam() )\n</code></pre>\n<pre><code># data \n(x_train, y_train), (_, _) = tf.keras.datasets.mnist.load_data()\nx_train = tf.expand_dims(x_train, -1)\nx_train = tf.repeat(x_train, 3, axis=-1)\nx_train = tf.divide(x_train, 255)\nx_train = tf.image.resize(x_train, [size,size]) # if we want to resize \ny_train = tf.one_hot(y_train , depth=10) \n\n# customized fit \ncustom_model.fit(x_train, y_train, batch_size=64, epochs=3, verbose = 1)\n</code></pre>\n<hr />\n<h3>Update</h3>\n<p>I've found that some others also tried to achieve this and ended up with the same issue. One has got some workaround, <a href=\"https://github.com/keras-team/keras/issues/14483\" rel=\"nofollow noreferrer\">here</a>, but it's too messy and I think there should be some better approach.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 247}]
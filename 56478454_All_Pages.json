[{"items": [{"tags": ["python", "tensorflow", "gradient", "tensorflow2.0", "tf.keras"], "owner": {"account_id": 2520819, "reputation": 5843, "user_id": 2191236, "user_type": "registered", "accept_rate": 92, "profile_image": "https://i.stack.imgur.com/2AlZn.jpg?s=256&g=1", "display_name": "Vahid Mirjalili", "link": "https://stackoverflow.com/users/2191236/vahid-mirjalili"}, "is_answered": true, "view_count": 16010, "answer_count": 4, "score": 9, "last_activity_date": 1599657084, "creation_date": 1559827071, "last_edit_date": 1559831561, "question_id": 56478454, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/56478454/in-tensorflow-2-0-with-eager-execution-how-to-compute-the-gradients-of-a-networ", "title": "In TensorFlow 2.0 with eager-execution, how to compute the gradients of a network output wrt a specific layer?", "body": "<p>I have a network made with InceptionNet, and for an input sample <code>bx</code>, I want to compute the gradients of the model output w.r.t. the hidden layer. I have the following code: </p>\n\n<pre class=\"lang-py prettyprint-override\"><code>bx = tf.reshape(x_batch[0, :, :, :], (1, 299, 299, 3))\n\n\nwith tf.GradientTape() as gtape:\n    #gtape.watch(x)\n    preds = model(bx)\n    print(preds.shape, end='  ')\n\n    class_idx = np.argmax(preds[0])\n    print(class_idx, end='   ')\n\n    class_output = model.output[:, class_idx]\n    print(class_output, end='   ')\n\n    last_conv_layer = model.get_layer('inception_v3').get_layer('mixed10')\n    #gtape.watch(last_conv_layer)\n    print(last_conv_layer)\n\n\ngrads = gtape.gradient(class_output, last_conv_layer.output)#[0]\nprint(grads)\n\n</code></pre>\n\n<p>But, this will give <code>None</code>. I tried <code>gtape.watch(bx)</code> as well, but it still gives <code>None</code>.</p>\n\n<p>Before trying GradientTape, I tried using <code>tf.keras.backend.gradient</code> but that gave an error as follows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.\n</code></pre>\n\n<p>My model is as follows:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninception_v3 (Model)         (None, 1000)              23851784  \n_________________________________________________________________\ndense_5 (Dense)              (None, 2)                 2002      \n=================================================================\nTotal params: 23,853,786\nTrainable params: 23,819,354\nNon-trainable params: 34,432\n_________________________________________________________________\n</code></pre>\n\n<p>Any solution is appreciated. It doesn't have to be GradientTape, if there is any other way to compute these gradients.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 102}]
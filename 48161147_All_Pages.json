[{"items": [{"tags": ["optimization", "tensorflow"], "owner": {"account_id": 10716056, "reputation": 2138, "user_id": 7886651, "user_type": "registered", "accept_rate": 76, "profile_image": "https://i.stack.imgur.com/zfb59.jpg?s=256&g=1", "display_name": "I. A", "link": "https://stackoverflow.com/users/7886651/i-a"}, "is_answered": true, "view_count": 3844, "accepted_answer_id": 48212514, "answer_count": 1, "score": 4, "last_activity_date": 1596599124, "creation_date": 1515469560, "question_id": 48161147, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/48161147/error-restoring-model-in-tensorflow-after-changing-the-optimizer-paramter", "title": "Error Restoring Model in Tensorflow After Changing the Optimizer Paramter", "body": "<p>I have trained a model in Tensorflow. During the training process, I was setting a var_list in the optimizer, in other words, I was training a GRU on top of a CNN. Here is the code for the optimizer: </p>\n\n<pre><code>with tf.name_scope('optimizer'):\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        optimizer = tf.train.AdamOptimizer(0.0001).minimize(MSE, var_list=gru_output_var_list)\n</code></pre>\n\n<p>Then, after training, and saving the variables in checkpoints, I was trying to remove the <code>var_list</code> from the optimizer in order to be able to fine tune the whole network, conv layers with GRU. But, that is raising an error:</p>\n\n<pre><code>Key weight_fc_sig/Adam_1 not found in checkpoint\n</code></pre>\n\n<p>where <code>weight_fc_sig</code> is the name of one of the variables in the model. </p>\n\n<p>I have read through github, and I found that the state of the optimizer is saved as well as variables in the checkpoint. So, I would like to know how to solve this issue, in other words, I need to know how to reset the state of the optimizer.</p>\n\n<p>Any help is much appreciated!!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 66}]
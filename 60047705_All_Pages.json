[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "neural-network", "automatic-differentiation"], "owner": {"account_id": 15100437, "reputation": 23, "user_id": 11737392, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/5cb4324092637dc634e7b979a8a12f13?s=256&d=identicon&r=PG&f=1", "display_name": "SemiPolish", "link": "https://stackoverflow.com/users/11737392/semipolish"}, "is_answered": true, "view_count": 668, "accepted_answer_id": 61826667, "answer_count": 2, "score": 2, "last_activity_date": 1589595744, "creation_date": 1580766877, "question_id": 60047705, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60047705/repeated-use-of-gradienttape-for-multiple-jacobian-calculations", "title": "Repeated use of GradientTape for multiple Jacobian calculations", "body": "<p>I am attempting to compute the Jacobian of a TensorFlow neural network's outputs with respect to its inputs. This is easily achieved with the <code>tf.GradientTape.jacobian</code> method. The trivial example provided in the TensorFlow documentation is as follows:</p>\n\n<pre><code>with tf.GradientTape() as g:\n  x  = tf.constant([1.0, 2.0])\n  g.watch(x)\n  y = x * x\njacobian = g.jacobian(y, x)\n</code></pre>\n\n<p>This is fine if I want only want to compute the Jacobian of a single instance of the input tensor <code>x</code>. However, I need to repeatedly evaluate this Jacobian many, many times for various instances of <code>x</code>. For a non-trivial Jacobian calculation (e.g. for a deep convolutional neural network with non-linear activation functions), this is incredibly expensive to repeatedly rerun the GradientTape calculation and evaluate the <code>jacobian</code> method. I know from the <a href=\"https://www.tensorflow.org/tutorials/customization/autodiff\" rel=\"nofollow noreferrer\">TensorFlow documentation</a> that the gradients (and hence the Jacobian) are computed via automatic differentiation.  I have to imagine there is some internal storage of the analytical gradient of the network (computed by automatic differentiation) which is evaluated at the given inputs. </p>\n\n<p>My question: am I correct in assuming that TensorFlow builds and stores (at least parts of) the analytical gradients needed to compute the Jacobian? And if so, is there a way to save this analytical gradient and re-evaluate the Jacobian with new inputs without having to reconstruct it via the GradientTape method?</p>\n\n<p>A \"persistent\" GradientTape does not seem to solve this issue: it only allows for the repeated evaluation of a single GradientTape instance with respect to multiple internal arguments of the computation.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 11}]
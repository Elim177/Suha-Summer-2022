[{"items": [{"tags": ["machine-learning", "tensorflow"], "migrated_from": {"other_site": {"aliases": ["https://statistics.stackexchange.com", "https://crossvalidated.com"], "styling": {"tag_background_color": "#edefed", "tag_foreground_color": "#5D5D5D", "link_color": "#0077CC"}, "related_sites": [{"relation": "meta", "api_site_parameter": "stats.meta", "site_url": "https://stats.meta.stackexchange.com", "name": "Cross Validated Meta"}, {"relation": "chat", "site_url": "https://chat.stackexchange.com?tab=site&host=stats.stackexchange.com", "name": "Chat Stack Exchange"}], "markdown_extensions": ["MathJax", "Prettify"], "launch_date": 1288900046, "open_beta_date": 1280170800, "closed_beta_date": 1279566000, "site_state": "normal", "high_resolution_icon_url": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon@2.png", "twitter_account": "StackStats", "favicon_url": "https://cdn.sstatic.net/Sites/stats/Img/favicon.ico", "icon_url": "https://cdn.sstatic.net/Sites/stats/Img/apple-touch-icon.png", "audience": "people interested in statistics, machine learning, data analysis, data mining, and data visualization", "site_url": "https://stats.stackexchange.com", "api_site_parameter": "stats", "logo_url": "https://cdn.sstatic.net/Sites/stats/Img/logo.png", "name": "Cross Validated", "site_type": "main_site"}, "on_date": 1595537908, "question_id": 478429}, "owner": {"account_id": 19134367, "reputation": 43, "user_id": 13975623, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/tnWGl.jpg?s=256&g=1", "display_name": "Puro", "link": "https://stackoverflow.com/users/13975623/puro"}, "is_answered": false, "view_count": 1350, "answer_count": 0, "score": 2, "last_activity_date": 1595537908, "creation_date": 1595435344, "question_id": 63063022, "link": "https://stackoverflow.com/questions/63063022/training-loss-increases-over-time-despite-of-a-tiny-learning-rate", "title": "training loss increases over time despite of a tiny learning rate", "body": "<p>I'm playing with CIFAR-10 dataset using ResNet-50 on Keras with Tensorflow backend, but I ran into a very strange training pattern, where the model loss decreased first, and then started to increase until it plateaued/stuck at a single value due to almost 0 learning rate. Correspondingly, the model accuracy increased first, and then started to decrease until it plateaued at 10% (aka random guess). I wonder what is going wrong?</p>\n<p>Typically this U shaped pattern happens with a learning rate that is too large (like <a href=\"https://stats.stackexchange.com/questions/324896/training-loss-increases-with-time?newreg=3d838414b18e47d2b4639cf771b1d92c\">this post</a>), but it is not the case here. This pattern also doesn't' look like a classic &quot;over-fitting&quot; as both the training and validation loss increase over time. In the answer to the above linked <a href=\"https://stats.stackexchange.com/questions/324896/training-loss-increases-with-time\">post</a>, someone mentioned that if Adam optimizer is used, loss may explode under small learning rate when local minimum is exceeded, I'm not sure I can follow what is said there, and also I'm using SGD with weight decay instead of Adam.</p>\n<p>Specifically for the training set up, I used resent50 with random initialization, SGD optimizer with 0.9 momentum and a weight decay of 0.0001 using <a href=\"https://www.tensorflow.org/addons/api_docs/python/tfa/opti\" rel=\"nofollow noreferrer\">decoupled weight decay regularization</a>, batch-size 64, initial learning_rate 0.01 which declined by a factor of 0.5 with each 10 epochs of non-decreasing validation loss.</p>\n<pre><code>base_model = tf.keras.applications.ResNet50(include_top=False,\n                                        weights=None,pooling='avg', \n                                        input_shape=(32,32,3))\nprediction_layer = tf.keras.layers.Dense(10)\nmodel = tf.keras.Sequential([base_model,\n                         prediction_layer])\nSGDW = tfa.optimizers.extend_with_decoupled_weight_decay(tf.keras.optimizers.SGD)\noptimizer = SGDW(weight_decay=0.0001, learning_rate=0.01, momentum=0.9)\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n          metrics=[&quot;accuracy&quot;])\nreduce_lr= tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.5, patience=10)\nmodel.compile(optimizer=optimizer, \n          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n          metrics=[&quot;accuracy&quot;])\nmodel.fit(train_batches, epochs=250, \n      validation_data=validation_batches,\n      callbacks=[reduce_lr])\n</code></pre>\n<p><a href=\"https://i.stack.imgur.com/pXrLP.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/pXrLP.png\" alt=\"enter image description here\" /></a><a href=\"https://i.stack.imgur.com/tic8e.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/tic8e.png\" alt=\"enter image description here\" /></a><a href=\"https://i.stack.imgur.com/UEx3X.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/UEx3X.png\" alt=\"enter image description here\" /></a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 207}]
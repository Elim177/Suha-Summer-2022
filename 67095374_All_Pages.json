[{"items": [{"tags": ["tensorflow2.0", "tf.keras"], "owner": {"account_id": 1987773, "reputation": 1216, "user_id": 1782553, "user_type": "registered", "accept_rate": 70, "profile_image": "https://www.gravatar.com/avatar/bc4d4fbae28ecd225bd84e7e8cb6fc5e?s=256&d=identicon&r=PG", "display_name": "Jav", "link": "https://stackoverflow.com/users/1782553/jav"}, "is_answered": false, "view_count": 157, "answer_count": 0, "score": 0, "last_activity_date": 1618417691, "creation_date": 1618416793, "last_edit_date": 1618417691, "question_id": 67095374, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67095374/how-should-i-loop-over-the-batch-items-in-a-tensorflow-tf-keras-layers-layer", "title": "How should I loop over the batch items in a tensorflow tf.keras.layers.Layer?", "body": "<p>I have a computation requiring to sum a high number of 2D maps. Unfortunately, this cannot hold on a GPU memory so I have to break down the computation.</p>\n<pre class=\"lang-py prettyprint-override\"><code>class Decode(tf.keras.layers.Layer):\n    def __init__(self, scale_factor, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scale_factor = scale_factor\n    def build(self, input_shape):\n        _, self.height, self.width, _ = input_shape\n        # high resolution map of positons\n        self.positions = tf.stack(tf.meshgrid(\n                                tf.range(0,self.width*self.scale_factor, dtype=tf.float32),\n                                tf.range(0,self.height*self.scale_factor, dtype=tf.float32)), axis=-1)\n    def call(self, mean, variance, confidence):\n        # Doesn't fit in memory\n        return tf.reduce_sum(\n            tf.exp(\n                -tf.reduce_sum(\n                    (self.positions[tf.newaxis,:,:,tf.newaxis,:] - tf.reshape(mean, [-1, 1, 1, self.width*self.height, 2]))**2, axis=-1\n                )/(2*tf.reshape(variance, [-1, 1, 1, self.width*self.height]))\n            )*tf.reshape(confidence, [-1, 1, 1, self.width*self.height]),\n            axis=-1\n        )/16\n</code></pre>\n<p>I would like to loop over all the items in the batch with something like</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def call(self, mean, variance, confidence):\n        batch_size = confidence.get_shape().as_list()[0]\n        acc = tf.zeros((batch_size, self.height*self.scale_factor, self.width*self.scale_factor))\n        for b in range(batch_size):\n            acc[b] = tf.reduce_sum(\n                tf.exp(\n                    -tf.reduce_sum(\n                        (self.positions[:,:,tf.newaxis,:] - tf.reshape(mean[b], [1, 1, self.width*self.height, 2]))**2,\n                        axis=-1\n                    )/(2*tf.reshape(variance[b], [1, 1, self.width*self.height]))\n                )*tf.reshape(confidence[b], [1, 1, self.width*self.height]),\n                axis=-1\n            )/16\n        return acc\n</code></pre>\n<p>However, this loop cannot be defined since the batch size is not known (<code>None</code>).</p>\n<p>Is there a way to loop in the batch dimension in an eager way?\nIs there another implementation that you think would work better?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 232}]
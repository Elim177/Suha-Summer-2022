[{"items": [{"tags": ["python", "deep-learning", "neural-network", "tensorflow2.0"], "owner": {"account_id": 7408811, "reputation": 187, "user_id": 6204891, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ZMT7u.jpg?s=256&g=1", "display_name": "Robert L.", "link": "https://stackoverflow.com/users/6204891/robert-l"}, "is_answered": true, "view_count": 2065, "accepted_answer_id": 60744419, "answer_count": 1, "score": 3, "last_activity_date": 1584552043, "creation_date": 1584075001, "question_id": 60665006, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60665006/conceptual-understanding-of-gradienttape-gradient", "title": "Conceptual understanding of GradientTape.gradient", "body": "<h2>Background</h2>\n\n<p>In Tensorflow 2, there exists a class called <code>GradientTape</code> which is used to record operations on tensors, the result of which can then be differentiated and fed to some minimization algorithm.  For example, <a href=\"https://www.tensorflow.org/api_docs/python/tf/GradientTape\" rel=\"nofollow noreferrer\">from the documentation</a> we have this example:</p>\n\n<pre><code>x = tf.constant(3.0)\nwith tf.GradientTape() as g:\n  g.watch(x)\n  y = x * x\ndy_dx = g.gradient(y, x) # Will compute to 6.0\n</code></pre>\n\n<p>The <a href=\"https://raw.githubusercontent.com/tensorflow/tensorflow/v2.1.0/tensorflow/python/eager/backprop.py\" rel=\"nofollow noreferrer\">docstring</a> for the <code>gradient</code> method implies that the first argument can be not just a tensor, but a list of tensors:</p>\n\n<pre><code> def gradient(self,\n               target,\n               sources,\n               output_gradients=None,\n               unconnected_gradients=UnconnectedGradients.NONE):\n    \"\"\"Computes the gradient using operations recorded in context of this tape.\n\n    Args:\n      target: a list or nested structure of Tensors or Variables to be\n        differentiated.\n      sources: a list or nested structure of Tensors or Variables. `target`\n        will be differentiated against elements in `sources`.\n      output_gradients: a list of gradients, one for each element of\n        target. Defaults to None.\n      unconnected_gradients: a value which can either hold 'none' or 'zero' and\n        alters the value which will be returned if the target and sources are\n        unconnected. The possible values and effects are detailed in\n        'UnconnectedGradients' and it defaults to 'none'.\n\n    Returns:\n      a list or nested structure of Tensors (or IndexedSlices, or None),\n      one for each element in `sources`. Returned structure is the same as\n      the structure of `sources`.\n\n    Raises:\n      RuntimeError: if called inside the context of the tape, or if called more\n       than once on a non-persistent tape.\n      ValueError: if the target is a variable or if unconnected gradients is\n       called with an unknown value.\n    \"\"\"\n</code></pre>\n\n<p>In the above example, it is easy to see that <code>y</code>, the <code>target</code>, is the function to be differentiated, and <code>x</code> is the dependent variable the \"gradient\" is taken with respect to.</p>\n\n<p>From my limited experience, it appears that the <code>gradient</code> method returns a list of tensors, one per each element of <code>sources</code>, and each of these gradients is a tensor that is the same shape as the corresponding member of <code>sources</code>.</p>\n\n<h2>Question</h2>\n\n<p>The above description of the behavior of <code>gradients</code> makes sense if <code>target</code> contains a single 1x1 \"tensor\" to be differentiated, because mathematically a gradient vector should be the same dimension as the domain of the function. </p>\n\n<p>However, if <code>target</code> is a list of tensors, the output of <code>gradients</code> is still the same shape. Why is this the case? If <code>target</code> is thought of as a list of functions, shouldn't the output resemble something like a Jacobian?  How am I to interpret this behavior conceptually?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 67}]
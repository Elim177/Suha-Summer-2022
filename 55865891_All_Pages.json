[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0", "tensorflow-probability"], "owner": {"account_id": 474979, "reputation": 2169, "user_id": 1447953, "user_type": "registered", "accept_rate": 58, "profile_image": "https://www.gravatar.com/avatar/7950906f179a27d0b6f71d84841374cb?s=256&d=identicon&r=PG", "display_name": "Ben Farmer", "link": "https://stackoverflow.com/users/1447953/ben-farmer"}, "is_answered": true, "view_count": 301, "accepted_answer_id": 55923875, "answer_count": 1, "score": 3, "last_activity_date": 1556641804, "creation_date": 1556275254, "last_edit_date": 1556641804, "question_id": 55865891, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/55865891/optimise-function-for-many-pseudodata-realisations-in-tensorflow-2", "title": "Optimise function for many pseudodata realisations in TensorFlow 2", "body": "<p>My end goal is to simulate likelihood ratio test statistics, however, the core problem I am having is that I do not understand how to get TensorFlow 2 to perform many optimizations for different data inputs. Here is my attempt, hopefully, it gives you the idea of what I am trying:</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow_probability import distributions as tfd\nimport numpy as np\n\n# Bunch of independent Poisson distributions that we want to combine\npoises0 = [tfp.distributions.Poisson(rate = 10) for i in range(5)]\n\n# Construct joint distributions\njoint0 = tfd.JointDistributionSequential(poises0)\n\n# Generate samples\nN = int(1e3)\nsamples0 = joint0.sample(N)\n\n# Now we need the same distributions but with floating parameters,\n# and need to define the function to be minimised\nmus = [tf.Variable(np.random.randn(), name='mu{0}'.format(i)) for i in range(5)]\n\n#@tf.function\ndef loss():\n    poises_free = [tfp.distributions.Poisson(rate = mus[i]) for i in range(5)]\n    joint_free = tfd.JointDistributionSequential(poises_free)\n    # Construct (half of) test statistic\n    return -2*(joint_free.log_prob(samples0))\n\n# Minimise (for all samples? Apparently not?)\nopt = tf.optimizers.SGD(0.1).minimize(loss,var_list=mus)\n\nprint(mus)\nprint(loss())\nprint(opt)\nquit()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>[&lt;tf.Variable 'mu0:0' shape=() dtype=float32, numpy=53387.016&gt;, &lt;tf.Variable 'mu1:0' shape=() dtype=float32, numpy=2540.568&gt;, &lt;tf.Variable 'mu2:0' shape=() dtype=float32, numpy=-5136.6226&gt;, &lt;tf.Variable 'mu3:0' shape=() dtype=float32, numpy=-3714.5227&gt;, &lt;tf.Variable 'mu4:0' shape=() dtype=float32, numpy=1062.9396&gt;]\ntf.Tensor(\n[nan nan nan nan ... nan nan nan], shape=(1000,), dtype=float32)\n&lt;tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1&gt;\n</code></pre>\n\n<p>In the end I want to compute the test statistic</p>\n\n<pre><code>q = -2*joint0.log_prob(samples0) - loss()\n</code></pre>\n\n<p>and show that it has a chi-squared distribution with 5 degrees of freedom.</p>\n\n<p>I am new to TensorFlow so perhaps I am doing this entirely wrong, but I hope you get the idea of what I want.</p>\n\n<p><strong>Edit:</strong></p>\n\n<p>So I played around a bit more, and I suppose that TensorFlow simply doesn't perform optimizations over the input tensors in parallel like I assumed. Or perhaps it can, but I need to set things up differently, i.e. perhaps give it a tensor of input parameters and a gigantic joint loss function for all the minimizations at once?</p>\n\n<p>I also tried doing things with a simple loop just to see what happens. As predicted it is pathetically slow, but I also don't even get the right answer:</p>\n\n<pre><code>poises0 = [tfp.distributions.Poisson(rate = 10) for i in range(5)]\njoint0 = tfd.JointDistributionSequential(poises0)\n\nN = int(5e2)\nsamples0 = joint0.sample(N)\n\nmus = [tf.Variable(10., name='mu{0}'.format(i)) for i in range(5)]\n\n#@tf.function\ndef loss(xi):\n    def loss_inner():\n        poises_free = [tfp.distributions.Poisson(rate = mus[i]) for i in range(5)]\n        joint_free = tfd.JointDistributionSequential(poises_free)\n        # Construct (half of) test statistic\n        return -2*(joint_free.log_prob(xi))\n    return loss_inner\n\n# Minimise\n# I think I have to loop over the samples... bit lame. Can perhaps parallelise though.\nq = []\nfor i in range(N):\n   xi = [x[i] for x in samples0]\n   opt = tf.optimizers.SGD(0.1).minimize(loss=loss(xi),var_list=mus)\n   q += [-2*joint0.log_prob(xi) - loss(xi)()]\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nsns.distplot(q, kde=False, ax=ax, norm_hist=True)\nqx = np.linspace(np.min(q),np.max(q),1000)\nqy = np.exp(tfd.Chi2(df=5).log_prob(qx))\nsns.lineplot(qx,qy)\nplt.show()\n</code></pre>\n\n<p>The output is not a chi-squared distribution with DOF=5. Indeed the test statistic often has negative values, which means that the optimized result is often a worse fit than the null hypothesis, which should be impossible.</p>\n\n<p><a href=\"https://i.stack.imgur.com/ZzdC1.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ZzdC1.png\" alt=\"Not a chi-squared distribution with DOF=5\"></a></p>\n\n<p><strong>Edit 2:</strong></p>\n\n<p>Here is an attempt at the \"monster\" solution where I minimize a giant network of different input variables for each pseudodata realization all at once. This feels more like something that TensorFlow might be good at doing, though I feel like I will run out of RAM once I go to large sets of pseudo-data. Still, I can probably loop over batches of pseudo-data.</p>\n\n<pre><code>poises0 = [tfp.distributions.Poisson(rate = 10) for i in range(5)]\njoint0 = tfd.JointDistributionSequential(poises0)\n\nN = int(5e3)\nsamples0 = joint0.sample(N)\n\nmus = [tf.Variable(10*np.ones(N, dtype='float32'), name='mu{0}'.format(i)) for i in range(5)]\n\npoises_free = [tfp.distributions.Poisson(rate = mus[i]) for i in range(5)]\njoint_free = tfd.JointDistributionSequential(poises_free)\nqM = -2*(joint_free.log_prob(samples0))\n\n@tf.function\ndef loss():\n    return tf.math.reduce_sum(qM,axis=0)\n\n# Minimise\nopt = tf.optimizers.SGD(0.1).minimize(loss,var_list=mus)\nprint(\"parameters:\", mus)\nprint(\"loss:\", loss())\nq0 =-2*joint0.log_prob(samples0)\nprint(\"q0:\", q0)\nprint(\"qM:\", qM)\nq = q0 - qM\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nsns.distplot(q, kde=False, ax=ax, norm_hist=True)\nqx = np.linspace(np.min(q),np.max(q),1000)\nqy = np.exp(tfd.Chi2(df=5).log_prob(qx))\nsns.lineplot(qx,qy)\nplt.show()\n</code></pre>\n\n<p>Unfortunately I now get the error:</p>\n\n<pre><code>Traceback (most recent call last):\n  File \"testing3.py\", line 35, in &lt;module&gt;\n    opt = tf.optimizers.SGD(0.1).minimize(loss,var_list=mus)   \n  File \"/home/farmer/anaconda3/envs/general/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 298, in minimize\n    return self.apply_gradients(grads_and_vars, name=name)\n  File \"/home/farmer/anaconda3/envs/general/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 396, in apply_gradients\n    grads_and_vars = _filter_grads(grads_and_vars)\n  File \"/home/farmer/anaconda3/envs/general/lib/python3.6/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\", line 924, in _filter_grads\n    ([v.name for _, v in grads_and_vars],))\nValueError: No gradients provided for any variable: ['mu0:0', 'mu1:0', 'mu2:0', 'mu3:0', 'mu4:0'].\n</code></pre>\n\n<p>which I suppose is a basic sort of error. I think I just don't understand how TensorFlow keeps track of the derivatives it needs to compute. It seems like things work if I define variables inside the loss function rather than outside, but I need them outside in order to access their values later. So I guess I don't understand something here.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 178}]
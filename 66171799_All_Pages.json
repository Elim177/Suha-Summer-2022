[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 17713573, "reputation": 101, "user_id": 12860541, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/37647418ced93b688fb7ef561c889fae?s=256&d=identicon&r=PG&f=1", "display_name": "gilligan", "link": "https://stackoverflow.com/users/12860541/gilligan"}, "is_answered": false, "view_count": 148, "answer_count": 1, "score": 0, "last_activity_date": 1613138123, "creation_date": 1613132531, "question_id": 66171799, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66171799/tensorflow-incompatible-matrix-size-when-using-gradienttape", "title": "Tensorflow incompatible matrix size when using GradientTape", "body": "<p>I am trying to run code that previously worked on tensorflow 2.2.0 on version 2.4.0-rc0 for apple silicon (using python 3.8), but it is now generating the following error regarding the matrix dimensions:</p>\n<p><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: GetOutputShape: Matrix size-incompatible: In[0]: [256,4], In[1]: [4,400]</code></p>\n<p>I am using nested gradient tapes to compute the gradient of my MLP model wrt the inputs (which form part of the loss), after which I compute the gradient of the loss wrt the trainable variables as below:</p>\n<pre><code>    def get_grad_and_loss(self, x, y):\n        with tf.GradientTape(persistent=True) as gl_tape:\n            gl_tape.watch(x)\n\n            with tf.GradientTape(persistent=True) as l_tape:\n                l_tape.watch(x)\n                y_pred = self.call(x)\n\n            grad_mat = l_tape.gradient(y_pred, x)\n            loss = tf.reduce_mean(tf.math.square(y_pred - y[:, tf.newaxis])) + tf.reduce_mean(tf.maximum(0, -1 * (grad_mat[:, 0])))\n\n        g = gl_tape.gradient(loss, self.trainable_weights)\n\n        return g, loss\n</code></pre>\n<p>In words I am computing the MSE and trying to force the sign of the gradient to be positive (as a soft constraint). I have read through the documentation on <a href=\"https://www.tensorflow.org/api_docs/python/tf/GradientTape\" rel=\"nofollow noreferrer\">gradient tape</a> and as I understand it, setting <code>persistent=True</code> should allow me to recompute gradients freely. As a side note my code works fine if I omit the nested gradient tape and simply use the MSE metric, so I don't think the issue lies anywhere else in the code. Any pointers would be much appreciated, thanks in advance :)</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 255}]
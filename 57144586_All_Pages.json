[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 10967328, "reputation": 2032, "user_id": 8058705, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/146fa54b2324332097a1b6cda3420328?s=256&d=identicon&r=PG&f=1", "display_name": "D.Griffiths", "link": "https://stackoverflow.com/users/8058705/d-griffiths"}, "is_answered": true, "view_count": 25309, "answer_count": 7, "score": 27, "last_activity_date": 1656925108, "creation_date": 1563793190, "question_id": 57144586, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57144586/tensorflow-gradienttape-gradients-does-not-exist-for-variables-intermittently", "title": "Tensorflow GradientTape &quot;Gradients does not exist for variables&quot; intermittently", "body": "<p>When training my network I am occasionally met with the warning: </p>\n\n<p><code>W0722 11:47:35.101842 140641577297728 optimizer_v2.py:928] Gradients does not exist for variables ['model/conv1d_x/Variable:0'] when minimizing the loss.\n</code></p>\n\n<p>This happens sporadically at infrequent intervals (maybe once in every 20 successful steps). My model basically has two paths which join together with concatenations at various positions in the network. To illustrate this, here is a simplified example of what I mean.</p>\n\n<pre><code>class myModel(tf.keras.Model):\n\n  def __init__(self):\n\n    self.conv1 = Conv2D(32)\n    self.conv2 = Conv2D(32)\n    self.conv3 = Conv2D(16)\n\n  def call(self, inputs):\n\n    net1 = self.conv1(inputs)\n    net2 = self.conv2(inputs)\n    net = tf.concat([net1, net2], axis=2)\n    net = self.conv3(net)\n    end_points = tf.nn.softmax(net)\n\nmodel = myModel()\n\nwith tf.GradientTape() as tape:\n\n  predicition = model(image)\n  loss = myloss(labels, prediction)\n\ngradients = tape.gradient(loss, model.trainable_variables)\noptimizer.apply_gradients(zip(gradients, model.trainable_variables))\n</code></pre>\n\n<p>In reality my network is much larger, but the variables that generally don't have gradients tend to be the ones at the top of the network. Before each <code>Conv2D</code> layer I also have a custom gradient. Sometimes when I the error appears I can notice that the gradient function for that layer has not been called.</p>\n\n<p>My question is how can the gradient tape sometimes take what appears to be different paths when propagating backwards through my network. My secondary question, is this caused by having two separate routes through my network (i.e. conv1 AND conv2). Is there a fundamental flaw in this network architecture?</p>\n\n<p>Ideally, could I define to the <code>GradientTape()</code> that it must find the gradients for each of the top layers? </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 43}]
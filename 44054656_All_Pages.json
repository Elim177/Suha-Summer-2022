[{"items": [{"tags": ["python", "tensorflow", "binary", "batch-processing", "string-decoding"], "owner": {"account_id": 10716056, "reputation": 2138, "user_id": 7886651, "user_type": "registered", "accept_rate": 76, "profile_image": "https://i.stack.imgur.com/zfb59.jpg?s=256&g=1", "display_name": "I. A", "link": "https://stackoverflow.com/users/7886651/i-a"}, "is_answered": true, "view_count": 7372, "accepted_answer_id": 44099995, "answer_count": 1, "score": 5, "last_activity_date": 1501729112, "creation_date": 1495129586, "question_id": 44054656, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/44054656/creating-tfrecords-from-a-list-of-strings-and-feeding-a-graph-in-tensorflow-afte", "title": "Creating TfRecords from a list of strings and feeding a Graph in tensorflow after decoding", "body": "<p>The aim was to create a database of TfRecords. \nGiven: I have 23 folders each contain 7500 image, and 23 text file, each with 7500 line describing features for the 7500 images in separate folders. </p>\n\n<p>I created the database through this code: </p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom PIL import Image\n\ndef _Float_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef create_image_annotation_data():\n    # Code to read images and features.\n    # images represent a list of numpy array of images, and features_labels represent a list of strings\n    # where each string represent the whole set of features for each image. \n    return images, features_labels\n\n# This is the starting point of the program.\n# Now I have the images stored as list of numpy array, and the features as list of strings.\nimages, annotations = create_image_annotation_data()\n\ntfrecords_filename = \"database.tfrecords\"\nwriter = tf.python_io.TFRecordWriter(tfrecords_filename)\n\nfor img, ann in zip(images, annotations):\n\n    # Note that the height and width are needed to reconstruct the original image.\n    height = img.shape[0]\n    width = img.shape[1]\n\n    # This is how data is converted into binary\n    img_raw = img.tostring()\n    example = tf.train.Example(features=tf.train.Features(feature={\n        'height': _int64_feature(height),\n        'width': _int64_feature(width),\n        'image_raw': _bytes_feature(img_raw),\n        'annotation_raw': _bytes_feature(tf.compat.as_bytes(ann))\n    }))\n\n    writer.write(example.SerializeToString())\n\nwriter.close()\n\nreconstructed_images = []\n\nrecord_iterator = tf.python_io.tf_record_iterator(path=tfrecords_filename)\n\nfor string_record in record_iterator:\n    example = tf.train.Example()\n    example.ParseFromString(string_record)\n\n    height = int(example.features.feature['height']\n                 .int64_list\n                 .value[0])\n\n    width = int(example.features.feature['width']\n                .int64_list\n                .value[0])\n\n    img_string = (example.features.feature['image_raw']\n                  .bytes_list\n                  .value[0])\n\n    annotation_string = (example.features.feature['annotation_raw']\n                         .bytes_list\n                         .value[0])\n\n    img_1d = np.fromstring(img_string, dtype=np.uint8)\n    reconstructed_img = img_1d.reshape((height, width, -1))\n    annotation_reconstructed = annotation_string.decode('utf-8')\n</code></pre>\n\n<p>Therefore, after converting images and text into tfRecords and after being able to read them and convert images into numpy and the (binary text) into string in python, I tried to go the extra mile by using a filename_queue with a reader (The purpose was to provide the graph with batch of data rather one peace of data at a time. Additionally, the aim was to enqueue and dequeue the queue of examples through different threads, therefore, making training the network faster)</p>\n\n<p>Therefore, I used the following code:</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport time\n\nimage_file_list = [\"database.tfrecords\"]\nbatch_size = 16\n\n# Make a queue of file names including all the JPEG images files in the relative\n# image directory.\nfilename_queue = tf.train.string_input_producer(image_file_list, num_epochs=1, shuffle=False)\n\nreader = tf.TFRecordReader()\n\n# Read a whole file from the queue, the first returned value in the tuple is the\n# filename which we are ignoring.\n_, serialized_example = reader.read(filename_queue)\n\nfeatures = tf.parse_single_example(\n      serialized_example,\n      # Defaults are not specified since both keys are required.\n      features={\n          'height': tf.FixedLenFeature([], tf.int64),\n          'width': tf.FixedLenFeature([], tf.int64),\n          'image_raw': tf.FixedLenFeature([], tf.string),\n          'annotation_raw': tf.FixedLenFeature([], tf.string)\n      })\n\nimage = tf.decode_raw(features['image_raw'], tf.uint8)\nannotation = tf.decode_raw(features['annotation_raw'], tf.float32)\n\nheight = tf.cast(features['height'], tf.int32)\nwidth = tf.cast(features['width'], tf.int32)\n\nimage = tf.reshape(image, [height, width, 3])\n\n# Note that the minimum after dequeue is needed to make sure that the queue is not empty after dequeuing so that\n# we don't run into errors\n'''\nmin_after_dequeue = 100\ncapacity = min_after_dequeue + 3 * batch_size\nann, images_batch = tf.train.batch([annotation, image],\n                                   shapes=[[1], [112, 112, 3]],\n                                   batch_size=batch_size,\n                                   capacity=capacity,\n                                   num_threads=1)\n'''\n\n# Start a new session to show example output.\nwith tf.Session() as sess:\n    merged = tf.summary.merge_all()\n    train_writer = tf.summary.FileWriter('C:/Users/user/Documents/tensorboard_logs/New_Runs', sess.graph)\n\n    # Required to get the filename matching to run.\n    tf.global_variables_initializer().run()\n\n    # Coordinate the loading of image files.\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n\n    for steps in range(16):\n        t1 = time.time()\n        annotation_string, batch, summary = sess.run([annotation, image, merged])\n        t2 = time.time()\n        print('time to fetch 16 faces:', (t2 - t1))\n        print(annotation_string)\n        tf.summary.image(\"image_batch\", image)\n        train_writer.add_summary(summary, steps)\n\n    # Finish off the filename queue coordinator.\n    coord.request_stop()\n    coord.join(threads)\n</code></pre>\n\n<p>Finally, after running the above code, I got the following error:\n<strong>OutOfRangeError (see above for traceback): FIFOQueue '_0_input_producer' is closed and has insufficient elements (requested 1, current size 0)\n     [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](TFRecordReaderV2, input_producer)]]</strong></p>\n\n<p>Another Question:</p>\n\n<ol>\n<li>How to decode binary database (tfrecords) to retrieve back the features stored \"as python string data structure\".</li>\n<li>How to use the <strong>tf.train.batch</strong> to create a batch of examples to feed the network. </li>\n</ol>\n\n<p>Thank you!!\nAny help is much appreciated.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 68}]
[{"items": [{"tags": ["tensorflow", "machine-learning", "tf.keras", "encoder-decoder", "gradienttape"], "owner": {"account_id": 17912389, "reputation": 476, "user_id": 13014194, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ceTVa.png?s=256&g=1", "display_name": "Darien Schettler", "link": "https://stackoverflow.com/users/13014194/darien-schettler"}, "is_answered": true, "view_count": 331, "accepted_answer_id": 61189095, "answer_count": 1, "score": 1, "last_activity_date": 1586785026, "creation_date": 1584119221, "question_id": 60674600, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60674600/implementing-the-learning-to-read-with-tensorflow-talk-from-tf-summit-2020-e", "title": "Implementing The &#39;Learning To Read With Tensorflow&#39; Talk From TF Summit 2020 - EncoderDecoder Seq2Seq Model In Tensorflow 2.1/2.2 - Custom Train Step", "body": "<hr>\n\n<h2>Background Info</h2>\n\n<hr>\n\n<p>I am creating <strong>Google Colabs</strong> for each talk I found interesting from the Tensorflow 2020 Summit. As a note, I am using Tensorflow 2.1.</p>\n\n<hr>\n\n<p><strong><em>I have encountered a problem when attempting to implement the <code>'Learning To Read With Tensorflow'</code> talk.</em></strong></p>\n\n<hr>\n\n<p>Everything is peachy up until we get to the <strong><code>EncoderDecoder</code></strong> class definition. When I implement the fit method on my custom <strong><code>Model</code></strong> subclass I get an error which will be detailed below. </p>\n\n<p>The last salient error is <strong><code>AttributeError: 'NoneType' object has no attribute 'dtype'</code></strong>. </p>\n\n<p>However, I believe this is due to a problem within the <strong><code>GradientTape</code></strong> scope code and/or problems with the definition of the <strong><code>Decoder Layers</code></strong> (including the <strong><code>Attention Layers</code></strong>)</p>\n\n<hr>\n\n<p><br></p>\n\n<h2><strong>Main Code</strong></h2>\n\n<pre class=\"lang-py prettyprint-override\"><code># Not normally defined here... but doing so for clarity\nMAX_VOCAB_SIZE = 5000\nWINDOW_LENGTH = 11\n\nclass EncoderDecoder(tf.keras.Model):\n    def __init__(self, \n                 max_features=MAX_VOCAB_SIZE, \n                 output_seq_len=WINDOW_LENGTH-1,\n                 embedding_dims=200,\n                 rnn_units=512):\n\n        super().__init__()\n\n        self.max_features = max_features\n        self.output_seq_len = output_seq_len\n        self.embedding_dims = embedding_dims\n        self.rnn_units = rnn_units\n\n        self.vectorize_layer = \\\n            tf.keras.layers.experimental.preprocessing.TextVectorization(\n                max_tokens=self.max_features,\n                standardize='lower_and_strip_punctuation',\n                split='whitespace', \n                ngrams=None, \n                output_mode='int',\n                output_sequence_length=self.output_seq_len, \n                pad_to_max_tokens=True)\n\n        # --- &lt;ENCODER STUFF&gt; ---\n        # Embedding\n        self.encoder_embedding = \\\n            tf.keras.layers.Embedding(input_dim=self.max_features+1,\n                                      output_dim=self.embedding_dims)\n\n        # ENCODER\n        self.lstm_layer = \\\n            tf.keras.layers.LSTM(units=self.rnn_units, \n                                 return_state=True)\n        # --- &lt;/ENCODER STUFF&gt; ---        \n\n        # --- &lt;DECODER STUFF&gt; ---\n        # Embedding\n        self.decoder_embedding = \\\n            tf.keras.layers.Embedding(input_dim=self.max_features+1,\n                                      output_dim=self.embedding_dims)\n\n        # ---------------- MAYBE NOT NECESSARY ----------------\n        # Sampler (for use during training)\n        # This was not shown during the talk but it is pretty obvious\n        sampler = tfa.seq2seq.sampler.TrainingSampler()\n\n        # This was not shown during the talk but is required... \n        # This is my best guess\n        decoder_cell = tf.keras.layers.LSTMCell(units=self.rnn_units)\n        # ---------------- MAYBE NOT NECESSARY ----------------\n\n        # Output Layer For Decoder\n        self.projection_layer = \\\n            tf.keras.layers.Dense(self.max_features)\n\n        # DECODER\n        self.decoder = \\\n            tfa.seq2seq.BasicDecoder(cell=decoder_cell,\n                                     sampler=sampler,\n                                     output_layer=self.projection_layer)\n        # --- &lt;/DECODER STUFF&gt; ---\n\n        # --- &lt;ATTN STUFF&gt; ---\n        # Basic dense attention layer to connect Encoder &amp; Decoder\n        self.attention = tf.keras.layers.Attention()\n        # --- &lt;/ATTN STUFF&gt; ---\n\n    def train_step(self, data):\n        \"\"\" Overwrite built-in train_step method\n\n        Args:\n            data (tuple): The example (ten `words`), and the label (one `word`)\n\n        Returns:\n            Metric results for all passed metrics\n        \"\"\"\n\n        # Split data into example (x) and label (y)\n        x, y = data[0], data[1]\n\n        # Vectorize the example words (x)\n        x = self.vectorize_layer(x)\n\n        # Vectorize the labels\n        # This will by default pad the output to 10 ... but we only need the\n        # first entry (the true label not the useless padding)\n        y = self.vectorize_layer(y)[:, 0]\n\n        # Convert our label into a one-hot encoding based on the max number of\n        # features that we will be using for our model\n        y_one_hot = tf.one_hot(y, self.max_features)\n\n        # Everything within GradientTape is recorded \n        # for later automatic differentiation\n        with tf.GradientTape() as tape:\n\n            # --- &lt;ENCODER STUFF&gt; ---\n\n            # Transform the example utilizing the encoder embedding\n            inputs = self.encoder_embedding(x)\n\n            # Get the encoder outputs and state by \n            # utilizing the encoder (lstm_layer)\n            #   - encoder_outputs : [max_time, batch_size, num_units]\n            #   - encoder_state   : [state_h, state_c]\n            #       * state_h --- The Hidden State\n            #       * state_c --- The Cell   State\n            encoder_outputs, state_h, state_c = self.lstm_layer(inputs)\n\n            # --- &lt;/ENCODER STUFF&gt; ---\n\n            # --- &lt;ATTN STUFF&gt; ---\n\n            # Pass the encoder outputs and hidden state allowing us\n            # to track the intermediate state coming out of the encoder layers\n            attn_output = self.attention([encoder_outputs, state_h])\n            attn_output = tf.expand_dims(attn_output, axis=1)\n\n            # --- &lt;/ATTN STUFF&gt; ---\n\n            # --- &lt;DECODER STUFF&gt; ---\n\n            # ??? Create an empty embedding ???\n            targets = self.decoder_embedding(tf.zeros_like(y))\n\n            # Concat the output of the attention layer to the last axis\n            # of the empty targets embedding\n            concat_output = tf.concat([targets, attn_output], axis=-1)\n\n            # Predict the targets using the state from the encoder\n            outputs, _, _ = \\\n                self.decoder(concat_output, initial_state=[state_h, state_c])\n\n            # --- &lt;/DECODER STUFF&gt; ---\n\n        # Automatically differeniate utilizing the loss and trainable variables\n        gradients = tape.gradient(loss, trainable_variables)\n\n        # Collect the outputs so that they can be optimized\n        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n        # Update the metric state prior to return\n        self.compiled_metrics.update_state(y_one_hot, y_pred)\n\n        return {m.name: m.result() for m in self.metrics}\n\n\nmodel = EncoderDecoder()\n\nmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n              optimizer=\"adam\",\n              metrics=[\"accuracy\"])\n\nmodel.vectorize_layer.adapt(lines.batch(256))\n\n# ERROR OCCURS ON THIS LINE\nmodel.fit(data.batch(256),\n          epochs=45,\n          callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath='text_gen')])\n\n</code></pre>\n\n<hr>\n\n<p><br></p>\n\n<h2><strong>Detailed Error Message</strong></h2>\n\n<pre class=\"lang-py prettyprint-override\"><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-40-779906f7f617&gt; in &lt;module&gt;()\n      1 model.fit(data.batch(256),\n      2           epochs=45,\n----&gt; 3           callbacks=[tf.keras.callbacks.ModelCheckpoint(filepath='text_gen')])\n\n8 frames\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\n    817         max_queue_size=max_queue_size,\n    818         workers=workers,\n--&gt; 819         use_multiprocessing=use_multiprocessing)\n    820 \n    821   def evaluate(self,\n\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\n    233           max_queue_size=max_queue_size,\n    234           workers=workers,\n--&gt; 235           use_multiprocessing=use_multiprocessing)\n    236 \n    237       total_samples = _get_total_number_of_samples(training_data_adapter)\n\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in _process_training_inputs(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\n    591         max_queue_size=max_queue_size,\n    592         workers=workers,\n--&gt; 593         use_multiprocessing=use_multiprocessing)\n    594     val_adapter = None\n    595     if validation_data:\n\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in _process_inputs(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\n    704       max_queue_size=max_queue_size,\n    705       workers=workers,\n--&gt; 706       use_multiprocessing=use_multiprocessing)\n    707 \n    708   return adapter\n\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, standardize_function, **kwargs)\n    700 \n    701     if standardize_function is not None:\n--&gt; 702       x = standardize_function(x)\n    703 \n    704     # Note that the dataset instance is immutable, its fine to reusing the user\n\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py in standardize_function(dataset)\n    658         model.sample_weight_mode = getattr(model, 'sample_weight_mode', None)\n    659 \n--&gt; 660       standardize(dataset, extract_tensors_from_dataset=False)\n    661 \n    662       # Then we map using only the tensor standardization portion.\n\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\n   2358     is_compile_called = False\n   2359     if not self._is_compiled and self.optimizer:\n-&gt; 2360       self._compile_from_inputs(all_inputs, y_input, x, y)\n   2361       is_compile_called = True\n   2362 \n\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py in _compile_from_inputs(self, all_inputs, target, orig_inputs, orig_target)\n   2578       if training_utils.has_tensors(target):\n   2579         target = training_utils.cast_if_floating_dtype_and_mismatch(\n-&gt; 2580             target, self.outputs)\n   2581       training_utils.validate_input_types(target, orig_target,\n   2582                                           allow_dict=False, field_name='target')\n\n/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_utils.py in cast_if_floating_dtype_and_mismatch(targets, outputs)\n   1334   if tensor_util.is_tensor(targets):\n   1335     # There is one target, so output[0] should be the only output.\n-&gt; 1336     return cast_single_tensor(targets, dtype=outputs[0].dtype)\n   1337   new_targets = []\n   1338   for target, out in zip(targets, outputs):\n\nAttributeError: 'NoneType' object has no attribute 'dtype'\n</code></pre>\n\n<hr>\n\n<p><br></p>\n\n<h2><strong>How To Get <code>data</code> &amp; <code>lines</code> Variables If Wishing To Replicate</strong></h2>\n\n<p><strong>Get the Data</strong></p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>&gt;&gt;&gt; wget http://www.thespermwhale.com/jaseweston/babi/CBTest.tgz\n&gt;&gt;&gt; tar zxvf CBTest.tgz\n&gt;&gt;&gt; rm -rf CBTest.tgz\n</code></pre>\n\n<p><strong>Preprocess The Data</strong></p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Load data from a dataset comprising lines \n# from one or more text files.\nlines = tf.data.TextLineDataset(\"&lt;path-to&gt;/cbt_train.txt\")\n\n# Filter Out Title Lines First \n# This simple fn not included in this stackoverflow code\nlines = lines.filter(lambda x: not is_title(x))\n\n# Then We Remove All Punctuation \n# This simple fn not included in this stackoverflow code\nlines = lines.map(lambda x: remove_punc(x))\n\n# Then We Remove All Extra Spaces Created By The Previous FN\n# This simple fn not included in this stackoverflow code\nlines = lines.map(lambda x: remove_extra_spaces(x))\n\n# Then We Turn All The Uppercase Letters into Lowercase Letters\n# This simple fn not included in this stackoverflow code\nlines = lines.map(lambda x: make_lower(x))\n\n# Get words from lines\nwords = lines.map(tf.strings.split)\nwords = words.unbatch()\n\n# Get wordsets\nwordsets = words.batch(11)\n\n# get_example_label is a simple fn to split wordsets into examples and labels\n# First ten words are the example and last word is the label\ndata = wordsets.map(get_example_label)\n\n# Shuffle\ndata = data.shuffle(1024)\n</code></pre>\n\n<strong>References</strong>\n\n<hr>\n\n<ul>\n<li><a href=\"https://github.com/tensorflow/addons/blob/master/tensorflow_addons/seq2seq/README.md\" rel=\"nofollow noreferrer\">Tensorflow Seq2Seq ReadMe</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=aNrqaOAt5P4&amp;list=PLQY2H8rRoyvzuJw20FG82Lgm2SZjTdIXU&amp;index=3&amp;t=0s\" rel=\"nofollow noreferrer\">Youtube Video For the Tensorflow Summit Talk</a></li>\n</ul>\n\n<hr>\n\n<p>Thanks in advance!!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 127}]
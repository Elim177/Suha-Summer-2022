[{"items": [{"tags": ["python", "tensorflow", "openai-gym"], "owner": {"user_type": "does_not_exist", "display_name": "user9553863"}, "is_answered": true, "view_count": 83, "answer_count": 1, "score": 0, "last_activity_date": 1531366054, "creation_date": 1531348120, "question_id": 51295015, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51295015/machine-learning-reward-artificially-capping", "title": "Machine Learning reward artificially capping", "body": "<p>So when I run this, it works perfectly, however, for some reason the reward caps at 200. I'm not sure what could be causing this. I'm new to machine learning and this is my first project, so sorry if I am missing something stupid.I hypothesize that <code>done</code> is triggering before I want it too, but playing with that hasn't led to anything. Thanks so much.</p>\n\n<pre><code>import gym\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport sys\n\nenv = gym.make('CartPole-v0')\ndiscount_rate=.95\n\n# TODO Build the policy gradient neural network\nclass Agent:\n     def __init__(self, num_actions, state_size):\n\n        initializer = tf.contrib.layers.xavier_initializer()\n\n        self.input_layer = tf.placeholder(dtype=tf.float32, shape=[None, state_size])\n\n        # Neural net starts here\n\n        hidden_layer = tf.layers.dense(self.input_layer, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n        hidden_layer_2 = tf.layers.dense(hidden_layer, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n\n        # Output of neural net\n        out = tf.layers.dense(hidden_layer_2, num_actions, activation=None)\n\n        self.outputs = tf.nn.softmax(out)\n        self.choice = tf.argmax(self.outputs, axis=1)\n\n        # Training Procedure\n        self.rewards = tf.placeholder(shape=[None, ], dtype=tf.float32)\n        self.actions = tf.placeholder(shape=[None, ], dtype=tf.int32)\n\n        one_hot_actions = tf.one_hot(self.actions, num_actions)\n\n        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=one_hot_actions)\n\n        self.loss = tf.reduce_mean(cross_entropy * self.rewards)\n\n        self.gradients = tf.gradients(self.loss, tf.trainable_variables())\n\n        # Create a placeholder list for gradients\n        self.gradients_to_apply = []\n        for index, variable in enumerate(tf.trainable_variables()):\n            gradient_placeholder = tf.placeholder(tf.float32)\n            self.gradients_to_apply.append(gradient_placeholder)\n\n        # Create the operation to update gradients with the gradients placeholder.\n        optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n        self.update_gradients = \noptimizer.apply_gradients(zip(self.gradients_to_apply, tf.trainable_variables()))\n\n\n\ndef discount_normalize_rewards(rewards):\n    discounted_rewards = np.zeros_like(rewards)\n    total_rewards = 0\n\n    for i in reversed(range(len(rewards))):\n        total_rewards = total_rewards * discount_rate + rewards[i]\n        discounted_rewards[i] = total_rewards\n\n    discounted_rewards -= np.mean(discounted_rewards)\n    discounted_rewards /= np.std(discounted_rewards)\n\n    return discounted_rewards\n\n\n#initialize the training loop\ntf.reset_default_graph()\n\n# Modify these to match shape of actions and states in your environment\nnum_actions = 2\nstate_size = 4\n\npath = \"./cartpole-pg/\"\n\ntraining_episodes = 1000\nmax_steps_per_episode = 20000\nepisode_batch_size = 5\n\nagent = Agent(num_actions, state_size)\n\ninit = tf.global_variables_initializer()\n\nsaver = tf.train.Saver(max_to_keep=2)\n\nif not os.path.exists(path):\n   os.makedirs(path)\n\nwith tf.Session() as sess:\n    sess.run(init)\n\ntotal_episode_rewards = []\n\n# Create a buffer of 0'd gradients\ngradient_buffer = sess.run(tf.trainable_variables())\nfor index, gradient in enumerate(gradient_buffer):\n    gradient_buffer[index] = gradient * 0\n\nfor episode in range(training_episodes):\n\n    state = env.reset()\n\n    episode_history = []\n    episode_rewards = 0\n\n    for step in range(max_steps_per_episode):\n\n        if episode % 100 == 0:\n            env.render()\n\n        # Get weights for each action\n        action_probabilities = sess.run(agent.outputs, feed_dict={agent.input_layer: [state]})\n        action_choice = np.random.choice(range(num_actions), p=action_probabilities[0])\n\n        state_next, reward, done, _ = env.step(action_choice)\n        episode_history.append([state, action_choice, reward, state_next])\n        state = state_next\n\n        episode_rewards += reward\n\n        if done:\n            total_episode_rewards.append(episode_rewards)\n            episode_history = np.array(episode_history)\n            episode_history[:,2] = discount_normalize_rewards(episode_history[:,2])\n\n            ep_gradients = sess.run(agent.gradients, feed_dict={agent.input_layer: np.vstack(episode_history[:, 0]),\n                                                                agent.actions: episode_history[:, 1],\n                                                                agent.rewards: episode_history[:, 2]})\n            # add the gradients to the grad buffer:\n            for index, gradient in enumerate(ep_gradients):\n                gradient_buffer[index] += gradient\n\n            break\n\n    if episode % episode_batch_size == 0:\n\n        feed_dict_gradients = dict(zip(agent.gradients_to_apply, gradient_buffer))\n\n        sess.run(agent.update_gradients, feed_dict=feed_dict_gradients)\n\n        for index, gradient in enumerate(gradient_buffer):\n            gradient_buffer[index] = gradient * 0\n\n    if episode % 1 == 0:\n        saver.save(sess, path + \"pg-checkpoint\", episode)\n        print(\"Reward: \" + str(total_episode_rewards[-1:]))\n\n\nenv.close()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 281}]
[{"items": [{"tags": ["tensorflow", "keras", "deep-learning"], "owner": {"account_id": 20919846, "reputation": 65, "user_id": 15368739, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bb2a8d65aacd135f242682086255673c?s=256&d=identicon&r=PG&f=1", "display_name": "Agnosie", "link": "https://stackoverflow.com/users/15368739/agnosie"}, "is_answered": true, "view_count": 661, "accepted_answer_id": 68981398, "answer_count": 2, "score": 0, "last_activity_date": 1630313867, "creation_date": 1629725230, "question_id": 68893554, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68893554/how-to-freeze-unfreeze-a-pretrained-model-as-part-of-a-subclassed-model-in-tenso", "title": "How to freeze/unfreeze a pretrained Model as part of a subclassed Model in Tensorflow?", "body": "<p>I am trying to build a subclassed Model which consists of a pretrained convolutional Base and some Dense Layers on top, using Tensorflow &gt;= 2.4.\nHowever freezing/unfreezing of the subclassed Model has no effect once it was trained before. When I do the same with the Functional API everything works as expected. I would really appreciate some Hint to what im missing here: Following Code should specify my problem further. Pardon me the amount of Code:</p>\n<pre><code>#Setup\n\n\nimport tensorflow as tf\ntf.config.run_functions_eagerly(False)\n \nimport numpy as np\nfrom tensorflow.keras.regularizers import l1 \nimport matplotlib.pyplot as plt\n\n\n@tf.function\ndef create_images_and_labels(img,label, height = 70, width = 70): #Image augmentation\n\n    label = tf.cast(label, 'float32')\n    label = tf.squeeze(label)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, (height, width))\n  #  img = preprocess_input(img)\n    \n    return img, label\n\n\n\ncifar = tf.keras.datasets.cifar10\n(x_train, y_train), (x_test, y_test)  = cifar.load_data()\nnum_classes = len(np.unique(y_train))\n\nds_train = tf.data.Dataset.from_tensor_slices((x_train, tf.one_hot(y_train, depth = len(np.unique(y_train)))))\nds_train = ds_train.map(lambda img, label: create_images_and_labels(img, label, height = 70, width = 70))\n\nds_train = ds_train.shuffle(50000)\nds_train = ds_train.batch(50, drop_remainder = True)\n\n\nds_val = tf.data.Dataset.from_tensor_slices((x_test, tf.one_hot(y_test, depth = len(np.unique(y_train)))))\nds_val = ds_val.map(lambda img, label: create_images_and_labels(img, label, height = 70, width = 70))\nds_val = ds_val.batch(50, drop_remainder=True)\n\n\n\n# for i in ds_train.take(1):\n#     x, y = i\n#     for ind in range(x.shape[0]):\n#         plt.imshow(x[ind,:,:])\n#         plt.show()\n#         print(y[ind])\n\n\n'''\nDefining simple subclassed Model consisting of \nVGG16\nFlatten\nDense Layers\n\ncustomized what happens in model.fit and model.evaluate (Actually its the standard Keras procedure with custom Metrics)\ncustomized metrics: Loss and Accuracy for Training and Validation Step\n\nadded unfreezing Method \n'set_trainable_layers'\nArguments: \n    num_head (How many dense Layers)\n    num_base (How many VGG Layers)\n'''\n\n\n\nclass Test_Model(tf.keras.models.Model):\n    \n    def __init__(\n            self,\n            num_unfrozen_head_layers, \n            num_unfrozen_base_layers,\n            num_classes,\n            conv_base = tf.keras.applications.VGG16(include_top = False, weights = 'imagenet', input_shape = (70,70,3)),\n            \n            \n            \n            ):\n                super(Test_Model, self).__init__(name = &quot;Test_Model&quot;)\n                \n                self.base = conv_base\n                self.flatten = tf.keras.layers.Flatten()\n                self.dense1 = tf.keras.layers.Dense(2048, activation = 'relu')\n                self.dense2 = tf.keras.layers.Dense(1024, activation = 'relu')\n                self.dense3 = tf.keras.layers.Dense(128, activation = 'relu')\n                self.out = tf.keras.layers.Dense(num_classes, activation = 'softmax')\n                self.out._name = 'out'\n\n\n\n                    \n                self.train_loss_metric = tf.keras.metrics.Mean('Supervised Training Loss')\n                self.train_acc_metric = tf.keras.metrics.CategoricalAccuracy('Supervised Training Accuracy')\n                self.val_loss_metric = tf.keras.metrics.Mean('Supervised Validation Loss')\n                self.val_acc_metric = tf.keras.metrics.CategoricalAccuracy('Supervised Validation Accuracy')\n                self.loss_fn = tf.keras.losses.categorical_crossentropy\n                self.learning_rate = 1e-4\n                \n              #  self.build((None, 32,32,3))\n                self.set_trainable_layers(num_unfrozen_head_layers, num_unfrozen_base_layers)\n        \n                \n    @tf.function\n    def call(self, inputs, training = False):\n        x = self.base(inputs)\n        x = self.flatten(x)\n        x = self.dense1(x)\n        x = self.dense2(x)\n        x = self.dense3(x)\n        x = self.out(x)\n        return x\n    @tf.function\n    def train_step(self, input_data):\n         x_batch, y_batch = input_data\n         with tf.GradientTape() as tape: \n                tape.watch(x_batch)\n                y_pred = self(x_batch, training = True)\n                loss = self.loss_fn(y_batch, y_pred)\n                \n         trainable_vars = self.trainable_weights\n         gradients = tape.gradient(loss, trainable_vars)\n            \n         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n         self.train_loss_metric.update_state(loss)\n         self.train_acc_metric.update_state(y_batch, y_pred)\n            \n         return {&quot;Supervised Loss&quot;: self.train_loss_metric.result(),\n                 &quot;Supervised Accuracy&quot;:self.train_acc_metric.result()}\n     \n    @tf.function\n    def test_step(self, input_data):\n        x_batch,y_batch = input_data\n        y_pred = self(x_batch, training = False)\n        loss = self.loss_fn(y_batch, y_pred)\n        \n        self.val_loss_metric.update_state(loss)\n        self.val_acc_metric.update_state(y_batch, y_pred)\n            \n        return {&quot;Val Supervised Loss&quot;: self.val_loss_metric.result(),\n                &quot;Val Supervised Accuracy&quot;:self.val_acc_metric.result()}\n    \n    @property\n    def metrics(self):\n        # We list our `Metric` objects here so that `reset_states()` can be\n        # called automatically at the start of each epoch\n        # or at the start of `evaluate()`.\n        # If you don't implement this property, you have to call\n        # `reset_states()` yourself at the time of your choosing.\n        return [self.train_loss_metric,\n                self.train_acc_metric,\n                self.val_loss_metric,\n                self.val_acc_metric]  \n    \n    def set_trainable_layers(self, num_head, num_base):\n        \n        for layer in [lay for lay in self.layers if not isinstance(lay , tf.keras.models.Model)]: \n            layer.trainable = False\n            print(layer.name, layer.trainable)\n        for block in self.layers:\n            \n            if isinstance(block, tf.keras.models.Model):\n                print('Found Submodel', block.name)\n                for layer in block.layers: \n                    layer.trainable = False\n                    print(layer.name, layer.trainable)\n                if num_base &gt; 0:    \n                    for layer in block.layers[-num_base:]:\n                        layer.trainable = True\n                        print(layer.name, layer.trainable)\n        if num_head &gt; 0: \n            for layer in [lay for lay in self.layers if not isinstance(lay, tf.keras.models.Model)][-num_head:]:\n                layer.trainable = True \n                print(layer.name, layer.trainable)\n        \n        \n    \n'''\nShowcase1: First training completely frozen Model, then unfreezing: \n    unfreezed model doesnt learn\n'''    \n    \nmodel = Test_Model(num_unfrozen_head_layers= 0, num_unfrozen_base_layers = 0, num_classes = num_classes)    # Should NOT learn -&gt; doesnt learn\nmodel.build((None, 70,70,3))\nmodel.summary()\nmodel.compile(optimizer = tf.keras.optimizers.Adam(1e-5))\nmodel.fit(ds_train, validation_data = ds_val)\n\n\nmodel.set_trainable_layers(10,20) # SHOULD LEARN -&gt; Doesnt learn\nmodel.summary()\nmodel.compile(optimizer = tf.keras.optimizers.Adam(1e-5))\nmodel.fit(ds_train, validation_data = ds_val)\n#DOESNT LEARN\n    \n'''\nShowcase2: when first training the Model with more trainable Layers than in the second step:\n    AssertionError occurs\n'''\nmodel = Test_Model(num_unfrozen_head_layers= 10, num_unfrozen_base_layers = 2, num_classes = num_classes)    # SHOULD LEARN -&gt; learns\nmodel.build((None, 70,70,3))\nmodel.summary()\nmodel.compile(optimizer = tf.keras.optimizers.Adam(1e-5))\nmodel.fit(ds_train, validation_data = ds_val)\n\n\nmodel.set_trainable_layers(1,1) # SHOULD NOT LEARN -&gt; AssertionError\nmodel.summary()\nmodel.compile(optimizer = tf.keras.optimizers.Adam(1e-5))\nmodel.fit(ds_train, validation_data = ds_val)\n\n'''\nShowcase3: same Procedure as in Showcase2 but optimizer State is transferred to recompiled Model:\n    Cant set Weigthts because optimizer expects List of Length 0\n    \n'''\n\nmodel = Test_Model(num_unfrozen_head_layers= 10, num_unfrozen_base_layers = 20, num_classes = num_classes)    # SHOULD LEARN -&gt; learns\nmodel.build((None, 70,70,3))\nmodel.summary()\nmodel.compile(optimizer = tf.keras.optimizers.Adam(1e-5))\nmodel.fit(ds_train, validation_data = ds_val)\n\nopti_state = model.optimizer.get_weights()\nmodel.set_trainable_layers(0,0) # SHOULD NOT LEARN -&gt; Learns\nmodel.summary()\nmodel.compile(optimizer = tf.keras.optimizers.Adam(1e-5))\nmodel.optimizer.set_weights(opti_state)\nmodel.fit(ds_train, validation_data = ds_val)\n    \n\n\n#%%%    \n'''\nConstructing same Architecture with Functional API and running Experiments\n'''\n\nimport tensorflow as tf    \nconv_base = tf.keras.applications.VGG16(include_top = False, weights = 'imagenet', input_shape = (70,70,3))\n    \ninputs = tf.keras.layers.Input((70,70,3))\nx = conv_base(inputs)\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(2048, activation = 'relu') (x)\nx = tf.keras.layers.Dense(1024,activation = 'relu') (x)\nx = tf.keras.layers.Dense(128,activation = 'relu') (x)\nout = tf.keras.layers.Dense(num_classes,activation = 'softmax') (x)\n    \n\n\nisinstance(tf.keras.layers.Flatten(), tf.keras.models.Model)\nisinstance(conv_base, tf.keras.models.Model)\n\n\ndef set_trainable_layers(mod, num_head, num_base):\n    import time\n    for layer in [lay for lay in mod.layers if not isinstance(lay , tf.keras.models.Model)]: \n        layer.trainable = False\n        print(layer.name, layer.trainable)\n    for block in mod.layers:\n        \n        if isinstance(block, tf.keras.models.Model):\n            print('Found Submodel')\n            for layer in block.layers: \n                layer.trainable = False\n                print(layer.name, layer.trainable)\n            if num_base &gt; 0:    \n                for layer in block.layers[-num_base:]:\n                    layer.trainable = True\n                    print(layer.name, layer.trainable)\n    if num_head &gt; 0: \n        for layer in [lay for lay in mod.layers if not isinstance(lay, tf.keras.models.Model)][-num_head:]:\n            layer.trainable = True \n            print(layer.name, layer.trainable)\n       \n    \n    \n'''\nShowcase1: First training frozen Model, then unfreezing, recomiling and retraining:\n    model behaves as expected\n'''    \nmod = tf.keras.models.Model(inputs,out, name = 'TestModel')    \nset_trainable_layers(mod, 0 ,0)    \nmod.summary()\nmod.compile(optimizer = tf.keras.optimizers.Adam(1e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmod.fit(ds_train, validation_data = ds_val) # Model should NOT learn\n\n\nset_trainable_layers(mod, 10,20)\nmod.summary()\nmod.compile(optimizer = tf.keras.optimizers.Adam(1e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmod.fit(ds_train, validation_data = ds_val) #Model SHOULD learn\n\n\n'''\nShowcase2: First training unfrozen Model, then reducing number of trainable Layers:\n    Model behaves as Expected\n'''\n\n\nmod = tf.keras.models.Model(inputs,out, name = 'TestModel')    \nset_trainable_layers(mod, 10 ,20)    \nmod.summary()\nmod.compile(optimizer = tf.keras.optimizers.Adam(1e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmod.fit(ds_train, validation_data = ds_val) # Model SHOULD learn\n\n\nset_trainable_layers(mod, 0,0)\nmod.summary()\nmod.compile(optimizer = tf.keras.optimizers.Adam(1e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmod.fit(ds_train, validation_data = ds_val) #Model should NOT learn\n\n\n\n\n'''\nShowcase3: First training unfrozen Model, then reducing number of trainable Layers but also trying to trasnfer Optimizer States:\n    Behaves as subclassed Model: New Optimizer shouldnt have Weights\n'''\n\n\nmod = tf.keras.models.Model(inputs,out, name = 'TestModel')    \nset_trainable_layers(mod, 1 ,3)    \nmod.summary()\nmod.compile(optimizer = tf.keras.optimizers.Adam(1e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmod.fit(ds_train, validation_data = ds_val) # Model SHOULD learn\n\nopti_state = mod.optimizer.get_weights()\nset_trainable_layers(mod, 4,8)\nmod.summary()\nmod.compile(optimizer = tf.keras.optimizers.Adam(1e-5), loss = 'categorical_crossentropy', metrics = ['accuracy'])\nmod.optimizer.set_weights(opti_state)\nmod.fit(ds_train, validation_data = ds_val) #Model should NOT learn\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 12}]
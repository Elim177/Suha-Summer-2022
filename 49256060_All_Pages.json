[{"items": [{"tags": ["tensorflow"], "owner": {"user_type": "does_not_exist", "display_name": "user4805479"}, "is_answered": true, "view_count": 257, "accepted_answer_id": 49256546, "answer_count": 1, "score": -1, "last_activity_date": 1520945006, "creation_date": 1520943613, "last_edit_date": 1520943932, "question_id": 49256060, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49256060/casting-object-returned-by-tf-trainable-variables-as-tensor", "title": "Casting object returned by tf.trainable_variables() as Tensor", "body": "<p><code>tf.trainable_variables()</code> returns a list of all trainable variable objects. When an object from the list is passed to an op, such as <code>tf.nn.l2_loss</code>, TensorFlow is able to cast the object as a Tensor and perform the necessary calculations. However, passing the same object to a user defined function throws an error.</p>\n\n<p>Consider the following two layer network to work with:</p>\n\n<pre><code># Generate random data\nx_train = np.random.rand(64, 16, 16, 8)\ny_train = np.random.randint(0, 5, 64)\none_hot = np.zeros((len(y_train), 5))\none_hot[list(np.indices((len(y_train),))) + [y_train]] = 1\ny_train = one_hot\n\n# Model definition\nclass FeedForward(object):\n    def __init__(self, l2_lambda=0.01):\n        self.x = tf.placeholder(tf.float32, shape=[None, 16, 16, 4], name=\"input_x\")\n        self.y = tf.placeholder(tf.float32, [None, 5], name=\"input_y\")\n\n        l2_loss = tf.constant(0.0)\n\n        with tf.name_scope(\"conv1\"):\n            kernel_shape=[1, 1, 4, 4]\n            w = tf.Variable(tf.truncated_normal(kernel_shape, stddev=0.1), name=\"weight\")\n            conv1 = tf.nn.conv2d(self.x, w, strides=[1, 1, 1, 1], padding=\"SAME\", name=\"conv\")\n\n        with tf.name_scope(\"conv2\"):\n            kernel_shape=[1, 1, 4, 2]\n            w = tf.Variable(tf.truncated_normal(kernel_shape, stddev=0.1), name=\"weight\")\n            conv2 = tf.nn.conv2d(conv1, w, strides=[1, 1, 1, 1], padding=\"SAME\", name=\"conv\")\n\n        out = tf.contrib.layers.flatten(conv2)\n\n        with tf.name_scope(\"output\"):\n            kernel_shape=[out.get_shape()[1].value, 5]\n            w = tf.Variable(tf.truncated_normal(kernel_shape, stddev=0.1), name=\"weight\")\n            self.scores = tf.matmul(out, w, name=\"scores\")\n            predictions = tf.argmax(self.scores, axis=1, name=\"predictions\")\n\n        # L2 Regularizer\n        if l2_reg_lambda &gt; 0.:\n            l2_loss = tf.add_n([self.some_norm(var) for var in tf.trainable_variables() if (\"weight\" in var.name)])\n\n        losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.y)\n        self.loss = tf.reduce_mean(losses) + (l2_lambda * l2_loss)\n\n        correct_predictions = tf.equal(predictions, tf.argmax(self.y, axis=1))\n        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n\n    def some_norm(w):\n        # operate on w and return scalar\n        # (only) for example\n        return (1 / tf.nn.l2_loss(w))\n\nwith tf.Graph().as_default():\n    sess = tf.Session()      \n\n    with sess.as_default():\n        ffn = FeedForward()\n\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2)\n        grads_and_vars = optimizer.compute_gradients(ffn.loss)\n        sess.run(tf.global_variables_initializer())\n\n        def train_step(x_batch, y_batch):\n            feed_dict = {\n                ffn.x: x_batch, \n                ffn.y: y_batch,                \n            }\n            _, step, loss, accuracy = sess.run([train_op, global_step, ffn.loss, ffn.accuracy], feed_dict)\n            print(\"step {}, loss {:g}, acc {:g}\".format(step, loss, accuracy))\n\n        batch_size = 32\n        n_epochs = 4\n        s_idx = - batch_size\n\n        for batch_index in range(n_epochs):\n            s_idx += batch_size\n            e_idx = s_idx + batch_size\n            x_batch = x_train[s_idx:e_idx]\n            y_batch = y_train[s_idx:e_idx]\n\n            train_step(x_batch, y_batch)\n            current_step = tf.train.global_step(sess, global_step)\n</code></pre>\n\n<p>The problem here is that on passing the trainable variable to <code>some_norm()</code>, it is passed as an object and can not be operated on. The related error message encountered at the first line inside <code>some_norm()</code> is:</p>\n\n<pre><code>Failed to convert object of type &lt;class '__main__.FeedForward'&gt; to Tensor.\nContents: &lt;__main__.FeedForward object at 0x7fefde7e97b8&gt;. \nConsider casting elements to a supported type.\n</code></pre>\n\n<p>Is there a way to cast the object returned by <code>tf.trainable_variables()</code> as a tensor or is there a possible workaround such as passing a reference?</p>\n\n<p>How is using the above different from using <code>l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables()...])</code> which works just fine?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 298}]
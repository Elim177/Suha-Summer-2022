[{"items": [{"tags": ["python", "tensorflow", "differential-equations", "gradienttape"], "owner": {"account_id": 22351536, "reputation": 21, "user_id": 16569549, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/1ec98e875e19d9e0917e0f4802ea7f10?s=256&d=identicon&r=PG&f=1", "display_name": "rihimetebmahk", "link": "https://stackoverflow.com/users/16569549/rihimetebmahk"}, "is_answered": false, "view_count": 153, "answer_count": 0, "score": 1, "last_activity_date": 1627776262, "creation_date": 1627776262, "question_id": 68606608, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68606608/zerodivisionerror-when-getting-gradients-with-tensorflow-ode-solver", "title": "ZeroDivisionError when getting gradients with Tensorflow ODE Solver?", "body": "<p>I'm trying to implement a mechanistic model using TensorFlow which will be used as part of a GAN, based on the approach shown in this paper: <a href=\"https://arxiv.org/abs/2009.08267\" rel=\"nofollow noreferrer\">https://arxiv.org/abs/2009.08267</a>. I am using tensorflow 2.5.0 and tensorflow-probability 0.13.0.</p>\n<p>The mechanistic model uses a TF Dormand-Prince solver to solve a set of differential equations which yield pressure waveforms for different regions of the cardiovascular system. I want to get gradients of the waveforms with respect to parameters of the mechanistic model for training the generator of the GAN.</p>\n<p>A couple of my differential equations incorporate a variable which is time-varying (piecewise but continuous, no &quot;sharp corners&quot;) and which is computed from a subset of the parameters to the mechanistic model. If I set this variable to a constant, I can get gradients of the waveforms wrt model parameters. However, if I keep this variable as time-varying, then I get a ZeroDivisionError when I try to compute the gradients.</p>\n<p>Any idea why this error might appear? I have included a stack trace below.</p>\n<p>Thanks a lot for your help!</p>\n<pre><code>---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\n&lt;ipython-input-4-462599885903&gt; in &lt;module&gt;\n----&gt; 1 dy6_dX = tape.gradient(y6, X)\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\n   1078         output_gradients=output_gradients,\n   1079         sources_raw=flat_sources_raw,\n-&gt; 1080         unconnected_gradients=unconnected_gradients)\n   1081 \n   1082     if not self._persistent:\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\n     75       output_gradients,\n     76       sources_raw,\n---&gt; 77       compat.as_str(unconnected_gradients.value))\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow/python/ops/custom_gradient.py in actual_grad_fn(*result_grads)\n    472                          &quot;@custom_gradient grad_fn.&quot;)\n    473     else:\n--&gt; 474       input_grads = grad_fn(*result_grads)\n    475       variable_grads = []\n    476     flat_grads = nest.flatten(input_grads)\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow_probability/python/math/ode/base.py in grad_fn(*dresults, **kwargs)\n    454               initial_time=result_time_array.read(initial_n),\n    455               initial_state=make_augmented_state(initial_n,\n--&gt; 456                                                  terminal_augmented_state),\n    457           )\n    458 \n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow_probability/python/math/ode/dormand_prince.py in _initialize_solver_internal_state(self, ode_fn, initial_time, initial_state)\n    307     p = self._prepare_common_params(initial_state, initial_time)\n    308 \n--&gt; 309     initial_derivative = ode_fn(p.initial_time, p.initial_state)\n    310     initial_derivative = tf.nest.map_structure(tf.convert_to_tensor,\n    311                                                initial_derivative)\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow_probability/python/math/ode/base.py in augmented_ode_fn(backward_time, augmented_state)\n    388              adjoint_constants_ode) = tape.gradient(\n    389                  adjoint_dot_derivatives, (state, tuple(variables), constants),\n--&gt; 390                  unconnected_gradients=tf.UnconnectedGradients.ZERO)\n    391             return (negative_derivatives, adjoint_ode, adjoint_variables_ode,\n    392                     adjoint_constants_ode)\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)\n   1078         output_gradients=output_gradients,\n   1079         sources_raw=flat_sources_raw,\n-&gt; 1080         unconnected_gradients=unconnected_gradients)\n   1081 \n   1082     if not self._persistent:\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\n     75       output_gradients,\n     76       sources_raw,\n---&gt; 77       compat.as_str(unconnected_gradients.value))\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\n    157       gradient_name_scope += forward_pass_name_scope + &quot;/&quot;\n    158     with ops.name_scope(gradient_name_scope):\n--&gt; 159       return grad_fn(mock_op, *out_grads)\n    160   else:\n    161     return grad_fn(mock_op, *out_grads)\n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py in _ConcatGradV2(op, grad)\n    228 def _ConcatGradV2(op, grad):\n    229   return _ConcatGradHelper(\n--&gt; 230       op, grad, start_value_index=0, end_value_index=-1, dim_index=-1)\n    231 \n    232 \n\n~/.conda/envs/mpk/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py in _ConcatGradHelper(op, grad, start_value_index, end_value_index, dim_index)\n    117       # in concat implementation to be within the allowed [-rank, rank) range.\n    118       non_neg_concat_dim = (\n--&gt; 119           concat_dim._numpy().item(0) % input_values[0]._rank())  # pylint: disable=protected-access\n    120       # All inputs are guaranteed to be EagerTensors in eager mode\n    121       sizes = pywrap_tfe.TFE_Py_TensorShapeSlice(input_values,\n\nZeroDivisionError: integer division or modulo by zero\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 8}]
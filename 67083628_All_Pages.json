[{"items": [{"tags": ["deep-learning", "neural-network", "tensorflow2.0", "tf.keras", "generative-adversarial-network"], "owner": {"account_id": 21223774, "reputation": 11, "user_id": 15609804, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/692739d164e260b26a564c201f04619a?s=256&d=identicon&r=PG&f=1", "display_name": "moooo112", "link": "https://stackoverflow.com/users/15609804/moooo112"}, "is_answered": false, "view_count": 59, "answer_count": 0, "score": 1, "last_activity_date": 1618356757, "creation_date": 1618356757, "question_id": 67083628, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67083628/valueerror-no-gradients-provided-for-any-variable-no-gradients-in-gans-genera", "title": "ValueError: No gradients provided for any variable - No gradients in GANs generator model", "body": "<p>I'm currently coding a GAN to generate sequences. Both the generator and the discriminator are working, when trained standalone. As soon as I combine both to the complete GAN model (to train the generator with discriminators weights frozen) the following error occurs and the graph seems to be not connected between generator and discriminator.</p>\n<blockquote>\n<p>ValueError: No gradients provided for any variable: ['generator_lstm/kernel:0', 'generator_lstm/recurrent_kernel:0', 'generator_lstm/bias:0', 'generator_softmax/kernel:0', 'generator_softmax/bias:0'].</p>\n</blockquote>\n<p>At first i thought my custom activation function was causing the issue. But since it works standalone i think the both &quot;submodels&quot; are not connected correctly. Im not sure if its important, but in the tensorboard graph there is no connection between both models.</p>\n<p><a href=\"https://i.stack.imgur.com/VjJyL.png\" rel=\"nofollow noreferrer\">Tensorboard Graph to my model</a></p>\n<p>The error occurs on exactly the last line of the train() function.\nI already tried TF versions 2.1 and 2.4.1, it makes no difference.</p>\n<pre><code># softargmax and build[...]() functions are located in my &quot;gan&quot; python module\n# custom softargmax implementation \n@tf.function\ndef softargmax(values, beta = 1000000.0):\n\n    # tf.range over all possible indices\n    range_tensor = tf.range(54, dtype=tf.float32)\n    range_tensor = tf.reshape(range_tensor, [-1, 1])\n\n    # softmax activation of (input*beta)\n    values = tf.cast(values, dtype=tf.float32)\n    beta = tf.cast(beta, dtype=tf.float32)\n    softmax = tf.nn.softmax(((values*beta) - tf.reduce_max(values*beta)))\n    return softmax @ range_tensor\n\ncallable_softargmax = tf.function(softargmax)\nget_custom_objects().update({'custom_activation': Activation(callable_softargmax)})\n\ndef build_generator(z_dim, seq_len, num_of_words):\n\n    gen_input = Input(shape=(z_dim,), name=&quot;generator_input&quot;)\n    gen_repeat = RepeatVector(seq_len, name=&quot;generator_repeat&quot;)(gen_input)\n    gen_lstm = LSTM(128, activation=&quot;relu&quot;, return_sequences=True, name=&quot;generator_lstm&quot;)(gen_repeat)\n    gen_softmax = Dense(num_of_words, name=&quot;generator_softmax&quot;)(gen_lstm)\n    #gen_activation = tf.keras.layers.Activation(callable_softargmax)(gen_softmax)\n    gen_soft_argmax = Lambda(callable_softargmax, name=&quot;generator_soft_argmax&quot;)(gen_softmax)\n    generator = Model(gen_input, gen_soft_argmax, name=&quot;generator_model&quot;)\n    generator.summary()\n    return generator\n\n\ndef build_discriminator(seq_len, num_of_words, embedding_len):\n\n    embedding = np.load(PATH + MODELS + &quot;embedding_ae.npy&quot;)\n    discriminator = Sequential(name=&quot;gan_discriminator&quot;)\n    discriminator.add(tf.keras.layers.InputLayer(input_shape=(seq_len,1), name=&quot;discriminator_input&quot;))\n    discriminator.add(Reshape(target_shape=[18,], dtype=tf.float32, name=&quot;discriminator_reshape&quot;))\n    discriminator.add(Embedding(input_dim=num_of_words, output_dim=embedding_len, input_length=seq_len, mask_zero=False,\n                         embeddings_initializer=tf.keras.initializers.Constant(embedding), trainable=False, name=&quot;discriminator_emb&quot;))\n    discriminator.add(Bidirectional(LSTM(128, activation=&quot;tanh&quot;, recurrent_activation=&quot;sigmoid&quot;, recurrent_dropout=0, unroll=False, use_bias=True,\n                                         return_sequences=True), name=&quot;discriminator_lstm&quot;))\n    discriminator.add(Dropout(0.2, name=&quot;discriminator_dropout&quot;))\n    discriminator.add(LSTM(128, activation=&quot;tanh&quot;, recurrent_activation=&quot;sigmoid&quot;, recurrent_dropout=0, unroll=False, use_bias=True,\n                           name=&quot;discriminator_lstm2&quot;))\n    discriminator.add(Dropout(0.2, name=&quot;discriminator_dropout2&quot;))\n    discriminator.add(Dense(1, activation=&quot;sigmoid&quot;, name=&quot;discriminator_output&quot;))\n    discriminator.summary()\n\n    return discriminator\n\n\ndef build_gan(generator, discriminator):\n    gan = Sequential(name=&quot;gan&quot;)\n    gan.add(generator)\n    gan.add(discriminator)\n\n    return gan\n\ndef train(train_data, generator, discriminator, gan, iterations, batch_size, z_dim):\n   \n    real = np.ones((batch_size, 1))\n    fake = np.zeros((batch_size, 1))\n\n    for iteration in range(iterations):\n        idx = np.random.randint(0, train_data.shape[0], batch_size)\n        train_samples = train_data[idx]\n        train_samples = np.reshape(train_samples, [batch_size, 18, 1])\n\n        # train discriminator\n        z = np.random.normal(0, 1, (batch_size, z_dim))\n        z = np.reshape(z, [batch_size, z_dim])\n        gen_samples = generator.predict(z)\n\n        d_loss_real = discriminator.train_on_batch(train_samples, real)\n        d_loss_fake = discriminator.train_on_batch(gen_samples, fake)\n        d_loss, accuracy, = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # train generator\n        z = np.random.normal(0, 1, (batch_size, z_dim))\n        gen_samples = generator.predict(z)\n        g_loss = gan.train_on_batch(z, real)\n\n# compiling and running models in main.py\ndiscriminator = gan.build_discriminator(seq_len=18, num_of_words=54, embedding_len=200)\ndiscriminator.compile(loss=&quot;binary_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=[&quot;accuracy&quot;])\ndiscriminator.trainable = False\n    \ngenerator = gan.build_generator(z_dim, seq_len=18, num_of_words=54)\n\n\ngan_model = gan.build_gan(generator, discriminator)\ngan_model.compile(loss=&quot;binary_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(lr=0.001))\n\ngan.train(train_data=train, generator=generator, discriminator=discriminator,\n          gan=gan_model, iterations=iterations, batch_size=batch_size, z_dim=z_dim)\n\n\nModel: &quot;gan_discriminator&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndiscriminator_reshape (Resha (None, 18)                0         \n_________________________________________________________________\ndiscriminator_emb (Embedding (None, 18, 200)           10800     \n_________________________________________________________________\ndiscriminator_lstm (Bidirect (None, 18, 256)           336896    \n_________________________________________________________________\ndiscriminator_dropout (Dropo (None, 18, 256)           0         \n_________________________________________________________________\ndiscriminator_lstm2 (LSTM)   (None, 128)               197120    \n_________________________________________________________________\ndiscriminator_dropout2 (Drop (None, 128)               0         \n_________________________________________________________________\ndiscriminator_output (Dense) (None, 1)                 129       \n=================================================================\nTotal params: 544,945\nTrainable params: 534,145\nNon-trainable params: 10,800\n_________________________________________________________________\nModel: &quot;generator_model&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngenerator_input (InputLayer) [(None, 128)]             0         \n_________________________________________________________________\ngenerator_repeat (RepeatVect (None, 18, 128)           0         \n_________________________________________________________________\ngenerator_lstm (LSTM)        (None, 18, 128)           131584    \n_________________________________________________________________\ngenerator_softmax (Dense)    (None, 18, 54)            6966      \n_________________________________________________________________\ngenerator_soft_argmax (Lambd (None, 18, 1)             0         \n=================================================================\nTotal params: 138,550\nTrainable params: 138,550\nNon-trainable params: 0\n\n_________________________________________________________________\nModel: &quot;gan&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ngenerator_model (Model)      (None, 18, 1)             138550    \n_________________________________________________________________\ngan_discriminator (Sequentia (None, 1)                 544945    \n=================================================================\nTotal params: 683,495\nTrainable params: 138,550\nNon-trainable params: 544,945\n_________________________________________________________________\n\n\n</code></pre>\n<p>Do you have any suggestions on the model and what might possibly be wrong?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 169}]
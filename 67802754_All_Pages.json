[{"items": [{"tags": ["tensorflow", "keras", "tf.keras"], "owner": {"account_id": 21805961, "reputation": 11, "user_id": 16104165, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a/AATXAJzIRKDpiRfMCLamczVuicvqtbyIPLBcBiyIGkcV=k-s256", "display_name": "Nidhi Arora", "link": "https://stackoverflow.com/users/16104165/nidhi-arora"}, "is_answered": false, "view_count": 163, "answer_count": 0, "score": 1, "last_activity_date": 1622638753, "creation_date": 1622627298, "last_edit_date": 1622638753, "question_id": 67802754, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67802754/tensorflow-error-function-optimizer-failed-and-layout-failed-while-training-sub", "title": "Tensorflow error: function_optimizer failed and layout failed while training subclassed keras.Model", "body": "<p>Creating a Seq2Seq keras model for summary generation by subclassing keras.Model. The model works fine on smaller toy samples but I get tensorflow errors when I start training on real dataset.\nCode and Model arch are as follows:</p>\n<pre><code>class Summary_Generator(keras.Model):\n    def __init__(self, vs_text = 4280,\n               \n    def call(self, inputs):\n       en_outputs = self.encoder(source_seq,initial_states)\n       en_states = en_outputs[1:]\n       de_states = en_states\n       print(&quot;enc call over, dec call begins&quot;)\n       dec_outputs = self.decoder(dec_in, de_states, en_outputs[0])\n       print(&quot;dec execution over ...&quot;) \n       return dec_outputs\n\nclass Encoder_LSTM(keras.Model):\n     def __init__():\n     def call():\n\nclass Decoder_LSTM_Attn(keras.Model):\n     def __init__():\n     def call():\n         print(&quot;lstm_out shape: &quot;,lstm_out.shape)\n         logits = self.ws(lstm_out)\n         print(&quot;logits shape: &quot;,logits.shape)\n\ndef train_one_step(text, summary, in_states):\n    summary_input = summary[:, :-1]\n    summary_gt = summary[:, 1:]\n    with tf.GradientTape() as tape:\n        predictions = None\n        for i in range((summary.shape[1])-1): \n            dec_in = tf.expand_dims(summary[:, i], 1)\n            print(i,  end=&quot;, &quot;)\n         \n            de_outputs = sg([text, dec_in, in_states[0], in_states[1]])\n        \n            print(&quot;back&quot;, end = &quot; &quot;)\n</code></pre>\n<p>The code words as expected with toy samples, but fails to run even for one complete batch with training on real data. Tried de-bugging with print statements to find out where the flow stops. train_step() begins execution and i get i=0 printed on the screen, it then executes object of Summary_Generator class, encoder executes, i get &quot;enc ends decoder begins&quot; printed and then I get shape of logits also printed.\nHere is where my confusion starts, instead of getting &quot;back&quot; printed as expected from the code, there is second round of execution, then too it executes fine till the end, as shape of logits is printed on the screen. Which return statement is not working and why?\nWhy is the code block executed twice?\nThis is a snapshot of the errors:</p>\n<pre><code>0,\nenc call over, dec call begins\nlstm out shape :  (None, 1000)\nlogits shape:  (None, 6764)\ndec execution over ...\nenc call over, dec call begins\nlstm out shape :  (None, 1000)\nlogits shape:  (None, 6764)\ndec execution over ... \n</code></pre>\n<p>Error:</p>\n<pre><code>2021-06-02 05:28:27.130200: E \ntensorflow/core/grappler/optimizers/meta_optimizer.cc:563] function_optimizer \nfailed: Invalid argument: Input 0 of node \nencoder_lstm_lstm_statefulpartitionedcall_10_RetVal was passed float from \nencoder_lstm/lstm/StatefulPartitionedCall:13 incompatible with expected \nvariant.\n2021-06-02 05:28:27.154121: E \ntensorflow/core/grappler/optimizers/meta_optimizer.cc:563] layout failed: Out \nof range: src_output = 30, but num_outputs is only 30\n2021-06-02 05:28:27.187119: E \ntensorflow/core/grappler/optimizers/meta_optimizer.cc:563] function_optimizer \nfailed: Invalid argument: Input 0 of node \nencoder_lstm_lstm_statefulpartitionedcall_10_RetVal was passed float from \nencoder_lstm/lstm/StatefulPartitionedCall:13 incompatible with expected \nvariant.\n&lt;br&gt;stack of other messages that ends with &lt;br&gt;\nFile &quot;/home/narora/anaconda3/envs/tf_env/lib/python3.6/site- \npackages/tensorflow/python/framework/ops.py&quot;, line 6653, in \nraise_from_not_ok_status\nsix.raise_from(core._status_to_exception(e.code, message), None)\nFile &quot;&lt;string&gt;&quot;, line 3, in raise_from\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when \nallocating tensor with shape[500,2000] and type float on \n/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc \n[Op:ZerosLike]\n</code></pre>\n<p>When all neural computations have been succesfully completed, why will there be an OOM error and if it has not returned back (as I did not get &quot;back&quot; printed even once, which function optimizer failed?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 130}]
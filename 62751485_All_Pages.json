[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0"], "owner": {"account_id": 438612, "reputation": 22264, "user_id": 826983, "user_type": "registered", "accept_rate": 69, "profile_image": "https://i.stack.imgur.com/B9PSD.jpg?s=256&g=1", "display_name": "Stefan Falk", "link": "https://stackoverflow.com/users/826983/stefan-falk"}, "is_answered": true, "view_count": 318, "accepted_answer_id": 62771771, "answer_count": 1, "score": 1, "last_activity_date": 1594113743, "creation_date": 1594021639, "last_edit_date": 1594113743, "question_id": 62751485, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62751485/modelfeatures-does-not-return-eagertensor", "title": "model(features) does not return EagerTensor", "body": "<p>I have built a model which I can only train with a custom loss which I am trying to debug.</p>\n<p>For this I have this simple loop here:</p>\n<pre class=\"lang-py prettyprint-override\"><code>for (mel_specs, pred_inp), labels in train_dataset:\n    enc_predictions = model((mel_specs, pred_inp))  # &lt;--- Returns a Tensor, not an EagerTensor\n    input_lengths = get_padded_length(mel_specs[:, :, 0])\n    label_lengths = get_padded_length(labels)\n    print(enc_predictions)\n    loss_value = rnnt_loss(enc_predictions, labels, input_lengths, label_lengths)\n    print(loss_value)\n</code></pre>\n<p>The <code>model</code> is just:</p>\n<pre class=\"lang-py prettyprint-override\"><code>model = tf.keras.Model(\n    inputs=[mel_specs, pred_inp],\n    outputs=[outputs]\n)\n</code></pre>\n<p>The problem is that <code>model((mel_specs, pred_inp))</code> just gives me a regular <code>Tensor</code> but not a <code>EagerTensor</code> and I don't understand why. <code>mel_specs</code> and <code>pred_inpu</code> are <code>EagerTensor</code>s coming from <code>train_dataset</code> which is a <code>tf.data.Dataset</code>.</p>\n<p>What am I missing here?</p>\n<h3>Environment</h3>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip freeze | grep tensorflow\ntensorflow==2.2.0\ntensorflow-addons==0.10.0\ntensorflow-datasets==3.1.0\ntensorflow-estimator==2.2.0\ntensorflow-metadata==0.22.2\nwarprnnt-tensorflow==0.1\n</code></pre>\n<hr />\n<h2>Update: MVCE</h2>\n<p>I was able to boil it down to the encoder part of the model. If I run this it will fail and print:</p>\n<pre class=\"lang-none prettyprint-override\"><code>Calling model(x) didn't return EagerTensor\nTraceback (most recent call last):\n    ...\n    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n  File &quot;/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py&quot;, line 1042, in gradient\n    flat_grad = imperative_grad.imperative_grad(\n  File &quot;/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py&quot;, line 71, in imperative_grad\n    return pywrap_tfe.TFE_Py_TapeGradient(\n  File &quot;/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py&quot;, line 157, in _gradient_function\n    return grad_fn(mock_op, *out_grads)\n  File &quot;/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py&quot;, line 252, in _MeanGrad\n    sum_grad = _SumGrad(op, grad)[0]\n  File &quot;/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py&quot;, line 211, in _SumGrad\n    output_shape_kept_dims = math_ops.reduced_shape(input_shape,\n  File &quot;/home/sfalk/miniconda3/envs/asr2/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py&quot;, line 3735, in reduced_shape\n    input_shape = input_shape.numpy()\nAttributeError: 'Tensor' object has no attribute 'numpy'\n</code></pre>\n<p>The code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework.ops import EagerTensor\n\n\nclass TimeReduction(tf.keras.layers.Layer):\n\n    def __init__(self,\n                 reduction_factor,\n                 batch_size=None,\n                 **kwargs):\n        super(TimeReduction, self).__init__(**kwargs)\n        self.reduction_factor = reduction_factor\n        self.batch_size = batch_size\n\n    def call(self, inputs):\n        input_shape = tf.shape(inputs)\n        batch_size = self.batch_size\n        if batch_size is None:\n            batch_size = input_shape[0]\n        max_time = input_shape[1]\n        num_units = inputs.get_shape().as_list()[-1]\n        outputs = inputs\n        paddings = [[0, 0], [0, tf.math.floormod(max_time, self.reduction_factor)], [0, 0]]\n        outputs = tf.pad(outputs, paddings)\n        return tf.reshape(outputs, (batch_size, -1, num_units * self.reduction_factor))\n\n\ndef make_encoder_model(\n    input_shape: tuple,\n    out_dim: int,\n    num_layers: int,\n    d_model: int,\n    proj_size,\n    initializer=None,\n    dtype=tf.float32,\n    stateful: bool = False,\n    dropout=0.5,\n    reduction_index=1,\n    reduction_factor=2,\n):\n    def lstm_cell():\n        return tf.compat.v1.nn.rnn_cell.LSTMCell(\n            d_model,\n            num_proj=proj_size,\n            initializer=initializer,\n            dtype=dtype\n        )\n\n    batch_size = None if not stateful else 1\n\n    inputs = tf.keras.Input(\n        shape=input_shape,\n        batch_size=batch_size,\n        dtype=tf.float32\n    )\n\n    x = tf.keras.layers.BatchNormalization()(inputs)\n\n    for i in range(num_layers):\n        rnn_layer = tf.keras.layers.RNN(lstm_cell(), return_sequences=True, stateful=stateful)\n        x = rnn_layer(x)\n        x = tf.keras.layers.Dropout(dropout)(x)\n        x = tf.keras.layers.LayerNormalization(dtype=dtype)(x)\n        if i == reduction_index:\n            x = TimeReduction(reduction_factor, batch_size=batch_size)(x)\n\n    outputs = tf.keras.layers.Dense(out_dim)(x)\n\n    return tf.keras.Model(\n        inputs=[inputs],\n        outputs=[outputs],\n        name='encoder'\n    )\n\n\ndef gradient(model, loss, inputs, y_true):\n    y_true = tf.transpose(y_true, perm=(0, 2, 1))\n    with tf.GradientTape() as tape:\n        y_pred = model(inputs, training=True)\n        loss_value = loss(y_true=y_true, y_pred=y_pred)\n        return loss_value, tape.gradient(loss_value, model.trainable_variables)\n\n\ndef main():\n    X, Y = [\n        np.random.rand(100, 512),\n        np.random.rand(100, 512)\n    ], [[[0]*50], [[1]*50]]\n    # assert len(X) == len(Y)\n\n    encoder_model = make_encoder_model(\n        input_shape=(None, 512),\n        out_dim=1,\n        num_layers=2,\n        d_model=10,\n        proj_size=23,\n        dropout=0.5,\n        reduction_index=1,\n        reduction_factor=2\n    )\n\n    enc_dataset = tf.data.Dataset.from_generator(\n        lambda: zip(X, Y),\n        output_types=(tf.float32, tf.int32),\n        output_shapes=([None, 512], [None, None]),\n    ).batch(2)\n\n    loss = tf.keras.losses.MeanSquaredError()\n\n    for x, y in enc_dataset:\n        from_predict = encoder_model.predict(x)\n        from_call = encoder_model(x)\n        if not isinstance(from_predict, np.ndarray):\n            print(&quot;Calling model.predict(x) didn't return np.ndarray&quot;)\n        if not isinstance(from_call, EagerTensor):\n            print(&quot;Calling model(x) didn't return EagerTensor&quot;)\n        loss_value, gradients = gradient(encoder_model, loss, x, y)\n        print(loss_value)\n        print(gradients)\n\n    print('All done.')\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 215}]
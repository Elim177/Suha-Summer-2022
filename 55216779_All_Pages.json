[{"items": [{"tags": ["python", "tensorflow", "deep-learning"], "owner": {"account_id": 4548208, "reputation": 3430, "user_id": 8089695, "user_type": "registered", "accept_rate": 100, "profile_image": "https://www.gravatar.com/avatar/92107cb1b109e2b8281e9cb76b5e3f16?s=256&d=identicon&r=PG&f=1", "display_name": "MiloMinderbinder", "link": "https://stackoverflow.com/users/8089695/milominderbinder"}, "is_answered": true, "view_count": 113, "answer_count": 1, "score": 2, "last_activity_date": 1552943852, "creation_date": 1552895726, "last_edit_date": 1552901517, "question_id": 55216779, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/55216779/tf-cond-on-a-variable-failedpreconditionerror-in-tf-global-variables-initialize", "title": "tf.cond on a variable. FailedPreconditionError in tf.global_variables_initializer()", "body": "<p>I am running into  FailedPreconditionError error in <code>tf.global_variables_initializer()</code>. I have zeroed-in on the following part of the code to be the culprit:</p>\n\n<pre><code>def __init__(...):\n    ...\n    self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')\n    ...\n    step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)\n\ndef step_rampup(self, global_step, rampup_length):\n    result = tf.cond(global_step &lt; rampup_length,\n                     lambda: tf.constant(0.0),\n                     lambda: tf.constant(1.0))\n    return tf.identity(result, name=\"step_rampup\")\nsession.run(tf.global_variables_initilizer())\n</code></pre>\n\n<p><code>self.global_step</code> is to be incremented by <code>1</code> by optimizer at each iteration. It's value has to change. So, that is the behavior I want.</p>\n\n<p>Error message:</p>\n\n<pre><code>FailedPreconditionError ...\n506         with tf.Session(graph=highgraph) as session:\n--&gt; 507             session.run(tf.global_variables_initializer())\n...\nFailedPreconditionError: Attempting to use uninitialized value global_step\n [[node global_step/read (defined at NML_U/sNeural.py:103)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](global_step)]]\n</code></pre>\n\n<p>Why is that part of the code is culprit?\nBecause, The following code works</p>\n\n<pre><code>def __init__(...):\n    ...\n    self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')\n    ...\n    step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)\n\ndef step_rampup(self, global_step, rampup_length):\n    result = tf.cond(global_step.initialized_value() &lt; rampup_length,\n                     lambda: tf.constant(0.0),\n                     lambda: tf.constant(1.0))\n    return tf.identity(result, name=\"step_rampup\")\nsession.run(tf.global_variables_initilizer())\n</code></pre>\n\n<p>but that will evaluate the conditional with the initialized value of <code>self.global_step(=0)</code> each time which is not the intended behavior</p>\n\n<p>Also,</p>\n\n<p>This code works as well:</p>\n\n<pre><code>def __init__(...):\n    ...\n    self.global_step = tf.get_variable(initializer=tf.zeros_initializer(), trainable=False, shape=(), name='global_step')\n    self.global_step = tf.assign(self.global_step,0.)\n    ...\n    step_rampup_value = self.step_rampup(self.global_step, self.rampup_length)\n\ndef step_rampup(self, global_step, rampup_length):\n    result = tf.cond(global_step &lt; rampup_length,\n                     lambda: tf.constant(0.0),\n                     lambda: tf.constant(1.0))\n    return tf.identity(result, name=\"step_rampup\")\nsession.run(tf.global_variables_initilizer())\n</code></pre>\n\n<p>But (maybe) this will again not lead to the dependency on <code>global_step</code> but  instead on assign op which will keep assigning <code>0</code> to <code>self.global_step</code></p>\n\n<p>How do I go about achieving the behavior?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 167}]
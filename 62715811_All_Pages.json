[{"items": [{"tags": ["tensorflow", "deep-learning", "pytorch"], "owner": {"account_id": 6956681, "reputation": 1056, "user_id": 5337505, "user_type": "registered", "accept_rate": 67, "profile_image": "https://lh5.googleusercontent.com/-F4WMV67MG-o/AAAAAAAAAAI/AAAAAAAABGA/b-vMtZ0piXI/photo.jpg?sz=256", "display_name": "Siladittya", "link": "https://stackoverflow.com/users/5337505/siladittya"}, "is_answered": true, "view_count": 983, "accepted_answer_id": 62718194, "answer_count": 1, "score": 2, "last_activity_date": 1593788894, "creation_date": 1593780609, "last_edit_date": 1593781497, "question_id": 62715811, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62715811/does-tf-math-reduce-max-allows-gradient-flow-like-torch-max", "title": "Does tf.math.reduce_max allows gradient flow like torch.max?", "body": "<p>I am trying to build a multi-label binary classification model in Tensorflow. The model has a <code>tf.math.reduce_max</code> operator between two layers (It is not Max Pooling, it's for a different purpose).</p>\n<p>And the number of classes is 3.</p>\n<p>I am using Binary Cross Entropy loss and using Adam optimizer.</p>\n<p>Even after hours of training, when I check the predictions, all the predictions are in the range 0.49 to 0.51.</p>\n<p>It seems that the model is not learning anything and is making random predictions, which is making me think that using a <code>tf.math.reduce_max</code> function may be causing the problems.</p>\n<p>However, I read on the web that the <code>torch.max</code> function allows back propagation of gradients through it.</p>\n<p>When I checked the Graph in Tensorboard, I saw that the graph is showing unconnected at the <code>tf.math.reduce_max</code> operator.\nSO, does this operator allows gradients ot back propagate through it?</p>\n<p>EDIT :\nAddin the code</p>\n<pre><code>input_tensor = Input(shape=(256, 256, 3))\nbase_model_toc = VGG16(input_tensor=input_tensor,weights='imagenet',pooling=None, include_top=False)\n\nx = base_model.output\n\nx = GlobalAveragePooling2D()(x)\n\nx = tf.math.reduce_max(x,axis=0,keepdims=True)\n\nx = Dense(1024,activation='relu')(x)\n\noutput_1 = Dense(3, activation='sigmoid')(x)\n\nmodel_a = Model(inputs=base_model_toc.input, outputs=output_1)\n\nfor layer in base_model.layers:\n    layer.trainable = True\n</code></pre>\n<p>THe <code>tf.math.reduce_max</code> is done along <code>axis = 0</code> becasue that is what needs to be done in this model</p>\n<p>Optimizer that I am using is Adam with initial learning rate 0.00001</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 21}]
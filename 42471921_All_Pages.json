[{"items": [{"tags": ["python", "tensorflow", "neural-network"], "owner": {"user_type": "does_not_exist", "display_name": "user1218191"}, "is_answered": true, "view_count": 4846, "accepted_answer_id": 42472053, "answer_count": 2, "score": 1, "last_activity_date": 1624818014, "creation_date": 1488130920, "last_edit_date": 1488324888, "question_id": 42471921, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/42471921/executing-function-in-tensorflow", "title": "executing function in TensorFlow", "body": "<p>I have some questions regarding this <a href=\"https://github.com/nlintz/TensorFlow-Tutorials/blob/master/03_net.py\" rel=\"nofollow noreferrer\">Code</a> - neural networks in TensorFlow. </p>\n\n<pre><code>#!/usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\n\ndef model(X, w_h, w_o):\n    h = tf.nn.sigmoid(tf.matmul(X, w_h)) # this is a basic mlp, think 2 stacked logistic regressions\n    return tf.matmul(h, w_o) # note that we dont take the softmax at the end because our cost fn does that for us\n\n\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n\nX = tf.placeholder(\"float\", [None, 784])\nY = tf.placeholder(\"float\", [None, 10])\n\nw_h = init_weights([784, 625]) # create symbolic variables\nw_o = init_weights([625, 10])\n\npy_x = model(X, w_h, w_o)\n\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y)) # compute costs\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(cost) # construct an optimizer\npredict_op = tf.argmax(py_x, 1)\n\n# Launch the graph in a session\nwith tf.Session() as sess:\n    # you need to initialize all variables\n    tf.global_variables_initializer().run()\n\n    for i in range(100):\n        for start, end in zip(range(0, len(trX), 128), range(128, len(trX)+1, 128)):\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n        print(i, np.mean(np.argmax(teY, axis=1) ==\n                         sess.run(predict_op, feed_dict={X: teX})))\n</code></pre>\n\n<ul>\n<li><p>After a single run of the loop at line 37, how do i call <strong>model()</strong> with <strong>X[0]</strong> and the newly learnt <strong>w_h</strong> and <strong>w_o</strong> , so that I can see the function returns</p></li>\n<li><p>Similarly, how to print the values of <strong>h</strong> inside the <strong>model()</strong> function ? </p></li>\n</ul>\n\n<p>Thanks in advance. I am new to tensorFlow :)</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 109}]
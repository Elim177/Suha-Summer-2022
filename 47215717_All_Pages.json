[{"items": [{"tags": ["parallel-processing", "deep-learning", "tensorflow-gpu", "multi-gpu", "parallelism-amdahl"], "owner": {"account_id": 9288576, "reputation": 21, "user_id": 6897635, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-kffu7k7dHQE/AAAAAAAAAAI/AAAAAAAAAGo/M5HyCjM7Urg/photo.jpg?sz=256", "display_name": "Max Shek-wai Chu", "link": "https://stackoverflow.com/users/6897635/max-shek-wai-chu"}, "is_answered": false, "view_count": 566, "answer_count": 1, "score": 0, "last_activity_date": 1550581560, "creation_date": 1510288326, "last_edit_date": 1510296193, "question_id": 47215717, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/47215717/tensorflow-multi-gpu-training-cannot-make-all-gpu-running-at-the-same-time", "title": "Tensorflow: Multi-GPU training cannot make all GPU running at the same time", "body": "<p>I have a machine that has 3x 1080 GPU. Below are the code of the training:</p>\n\n<pre><code>    dynamic_learning_rate = tf.placeholder(tf.float32, shape=[])\n    model_version = tf.constant(1, tf.int32)\n\n    with tf.device('/cpu:0'):\n        with tf.name_scope('Input'):\n            # Input images and labels.\n            batch_images,\\\n                batch_input_vectors,\\\n                batch_one_hot_labels,\\\n                batch_file_paths,\\\n                batch_labels = self.get_batch()\n\n    grads = []\n    pred = []\n    cost = []\n\n    # Define optimizer\n    optimizer = tf.train.MomentumOptimizer(learning_rate=dynamic_learning_rate / self.batch_size,\n                                           momentum=0.9,\n                                           use_nesterov=True)\n\n    split_input_image = tf.split(batch_images, self.num_gpus)\n    split_input_vector = tf.split(batch_input_vectors, self.num_gpus)\n    split_input_one_hot_label = tf.split(batch_one_hot_labels, self.num_gpus)\n    for i in range(self.num_gpus):\n        with tf.device(tf.DeviceSpec(device_type=\"GPU\", device_index=i)):\n            with tf.variable_scope(tf.get_variable_scope(), reuse=i &gt; 0):\n                with tf.name_scope('Model'):\n                    # Construct model\n                    with tf.variable_scope(\"inference\"):\n                        tower_pred = self.model(split_input_image[i], split_input_vector[i], is_training=True)\n\n                    pred.append(tower_pred)\n\n                with tf.name_scope('Loss'):\n                    # Define loss and optimizer\n                    softmax_cross_entropy_cost = tf.reduce_mean(\n                        tf.nn.softmax_cross_entropy_with_logits(logits=tower_pred, labels=split_input_one_hot_label[i]))\n\n                    cost.append(softmax_cross_entropy_cost)\n\n    # Concat variables\n    pred = tf.concat(pred, 0)\n    cost = tf.reduce_mean(cost)\n\n    # L2 regularization\n    trainable_vars = tf.trainable_variables()\n    l2_regularization = tf.add_n(\n        [tf.nn.l2_loss(v) for v in trainable_vars if any(x in v.name for x in ['weights', 'biases'])])\n    for v in trainable_vars:\n        if any(x in v.name for x in ['weights', 'biases']):\n            print(v.name + ' - included for L2 regularization!')\n        else:\n            print(v.name)\n\n    cost = cost + self.l2_regularization_strength*l2_regularization\n\n    with tf.name_scope('Accuracy'):\n        # Evaluate model\n        correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(batch_one_hot_labels, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        prediction = tf.nn.softmax(pred, name='softmax')\n\n    # Creates a variable to hold the global_step.\n    global_step = tf.Variable(0, trainable=False, name='global_step')\n\n    # Minimization\n    update = optimizer.minimize(cost, global_step=global_step, colocate_gradients_with_ops=True)\n</code></pre>\n\n<p>After I run the training:</p>\n\n<pre><code>Fri Nov 10 12:28:00 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1080    Off  | 00000000:03:00.0 Off |                  N/A |\n| 42%   65C    P2    62W / 198W |   7993MiB /  8114MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX 1080    Off  | 00000000:04:00.0 Off |                  N/A |\n| 33%   53C    P2   150W / 198W |   7886MiB /  8114MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX 1080    Off  | 00000000:05:00.0  On |                  N/A |\n| 26%   54C    P2   170W / 198W |   7883MiB /  8108MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0     23228      C   python                                      7982MiB |\n|    1     23228      C   python                                      7875MiB |\n|    2      4793      G   /usr/lib/xorg/Xorg                            40MiB |\n|    2     23228      C   python                                      7831MiB |\n+-----------------------------------------------------------------------------+\n\nFri Nov 10 12:28:36 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 1080    Off  | 00000000:03:00.0 Off |                  N/A |\n| 42%   59C    P2    54W / 198W |   7993MiB /  8114MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  GeForce GTX 1080    Off  | 00000000:04:00.0 Off |                  N/A |\n| 33%   57C    P2   154W / 198W |   7886MiB /  8114MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n|   2  GeForce GTX 1080    Off  | 00000000:05:00.0  On |                  N/A |\n| 27%   55C    P2   155W / 198W |   7883MiB /  8108MiB |    100%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0     23228      C   python                                      7982MiB |\n|    1     23228      C   python                                      7875MiB |\n|    2      4793      G   /usr/lib/xorg/Xorg                            40MiB |\n|    2     23228      C   python                                      7831MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n\n<p>You see that the whenever the first GPU is running, the other two GPUs will be idle and vice versa. The alternate frequency is about 0.5 second.</p>\n\n<p>For a single GPU, the training speed is around <strong><code>650 [images/second]</code></strong>,<br> with all the 3 GPUs I got <strong>only <code>1050 [images/second]</code></strong>.</p>\n\n<p>Any idea of the problem?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 49}]
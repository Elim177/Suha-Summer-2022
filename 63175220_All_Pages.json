[{"items": [{"tags": ["python", "tensorflow", "reinforcement-learning", "random-seed", "dqn"], "owner": {"account_id": 11743208, "reputation": 135, "user_id": 8794630, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/c2f2787a461ebb8547c8bc07dba0bf4a?s=256&d=identicon&r=PG&f=1", "display_name": "AleB", "link": "https://stackoverflow.com/users/8794630/aleb"}, "is_answered": false, "view_count": 107, "answer_count": 0, "score": 1, "last_activity_date": 1596118957, "creation_date": 1596118957, "question_id": 63175220, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63175220/understanding-the-behavior-of-tf-random-seed", "title": "Understanding the behavior of tf.random.seed", "body": "<p>I usually set a seed for experiment reproducibility when working with tensorflow. In this case my code is as follows:</p>\n<pre><code>class DeepNetworkModel(tf.keras.Model):\n    \n    def __init__(self, \n                 seed: int, \n                 input_shape: int, \n                 hidden_units: list, \n                 num_actions: int, \n                 batch_norm_input: bool, \n                 batch_norm_hidden: bool,\n                 activation: str, \n                 kernel_initializer: str, \n                 modelname: str = 'Deep Q Network'):\n        # call the parent constructor\n        super(DeepNetworkModel, self).__init__(name=modelname)\n\n        # set dimensionality of input/output depending on the model\n        inp_shape = input_shape\n        out_shape = num_actions\n        # set random seed \n        tf.random.set_seed(seed)\n        # set flag for batch norm as attribute\n        self.bnflag_input = batch_norm_input\n        self.batch_norm_hidden = batch_norm_hidden\n        # In setting input_shape, the batch dimension is not included.\n        # input layer\n        self.input_layer = InputLayer(input_shape=inp_shape)\n        # batch norm layer for inputs\n        if self.bnflag_input:\n            self.bnorm_layer = BatchNormalization(center=False,scale=False)\n        \n        # set of hidden layers\n        self.hids = []\n        \n        for i in hidden_units:\n            self.hids.append(Dense(i, kernel_initializer=kernel_initializer))\n            # check what type of activation is set\n            if activation == 'leaky_relu':\n                leaky_relu = tf.nn.leaky_relu\n                self.hids.append(Activation(leaky_relu))\n            elif activation == 'relu6':\n                relu6 = tf.nn.relu6\n                self.hids.append(Activation(relu6))\n            elif activation == 'elu':\n                elu = tf.nn.elu\n                self.hids.append(Activation(elu))\n            else:\n                self.hids.append(Activation(activation))\n                \n            if self.batch_norm_hidden:\n                self.hids.append(BatchNormalization())\n        # output layer with linear activation by default\n        self.output_layer = Dense(out_shape)\n\n\n    def call(self, \n             inputs: Union[np.ndarray or tf.Tensor], \n             training: bool = True, \n             store_intermediate_outputs: bool = False):\n        \n        if store_intermediate_outputs:\n            # build the input layer\n            if self.bnflag_input:\n                z = self.input_layer(inputs)\n                self.inputs = z\n                z = self.bnorm_layer(z, training)\n                self.bninputs = z\n            else:\n                z = self.input_layer(inputs)\n                self.inputs = z\n            # build the hidden layer\n            for layer in self.hids:\n                if 'batch' in layer.name:\n                    z = layer(z, training)\n                else:\n                    z = layer(z)\n                layer.out = z\n                \n            # build the output layer\n            z = self.output_layer(z)\n            self.output_layer.out = z \n \n        else:\n            # build the input layer\n            if self.bnflag_input:\n                z = self.input_layer(inputs)\n                z = self.bnorm_layer(z, training)\n            else:\n                z = self.input_layer(inputs)\n            # build the hidden layer\n            for layer in self.hids:\n                if 'batch' in layer.name:\n                    z = layer(z, training)\n                else:\n                    z = layer(z)\n            # build the output layer\n            z = self.output_layer(z)\n        return z\n</code></pre>\n<p>Setting a seed for this simple DNN model, which operations should impact in the optimization? The only step I have in mind are the initialization of the parameters of the network. The randomness involved in selecting the minibatch to perform SGD updated (or whatever else choice of the optimizer) does not matter here, because I am implementing DQN, so that I pass as input a randomly selected batch from the buffer (without setting a seed).</p>\n<p>Are there any other way in which I am fixing randomness working with this code? My question is relevant to understand the results of different experiment when I pass different seed as input. For now I suppose that changing the seed will just vary the initialization of the weights, but I want to be sure of that.</p>\n<p>I have already read the <a href=\"https://www.tensorflow.org/api_docs/python/tf/random/set_seed\" rel=\"nofollow noreferrer\">doc</a> of tensorflow abound random seeds, but it doesn't help much.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 12}]
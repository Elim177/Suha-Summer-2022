[{"items": [{"tags": ["tensorflow", "rnn", "gradient"], "owner": {"account_id": 10716056, "reputation": 2138, "user_id": 7886651, "user_type": "registered", "accept_rate": 76, "profile_image": "https://i.stack.imgur.com/zfb59.jpg?s=256&g=1", "display_name": "I. A", "link": "https://stackoverflow.com/users/7886651/i-a"}, "is_answered": true, "view_count": 299, "accepted_answer_id": 51539767, "answer_count": 2, "score": 5, "last_activity_date": 1533069891, "creation_date": 1532114064, "last_edit_date": 1533069891, "question_id": 51449027, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51449027/carrying-gradients-from-previous-time-steps-to-current-time-steps-with-gru-in-te", "title": "Carrying gradients from previous time steps to current time steps with GRU in Tensorflow", "body": "<p>I have the following model in tensorflow:</p>\n\n<pre><code>def output_layer(input_layer, num_labels):\n    '''\n    :param input_layer: 2D tensor\n    :param num_labels: int. How many output labels in total? (10 for cifar10 and 100 for cifar100)\n    :return: output layer Y = WX + B\n    '''\n    input_dim = input_layer.get_shape().as_list()[-1]\n    fc_w = create_variables(name='fc_weights', shape=[input_dim, num_labels],\n                            initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n    fc_b = create_variables(name='fc_bias', shape=[num_labels], initializer=tf.zeros_initializer())\n\n    fc_h = tf.matmul(input_layer, fc_w) + fc_b\n    return fc_h\n\ndef model(input_features):\n\n    with tf.variable_scope(\"GRU\"):\n        cell1 = tf.nn.rnn_cell.GRUCell(gru1_cell_size)\n\n        cell2 = tf.nn.rnn_cell.GRUCell(gru2_cell_size)\n\n        mcell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2], state_is_tuple=False)\n\n        # shape=(?, 64 + 32) \n        initial_state = tf.placeholder(shape=[None, gru1_cell_size + gru2_cell_size], dtype=tf.float32, name=\"initial_state\")\n        output, new_state = tf.nn.dynamic_rnn(mcell, input_features, dtype=tf.float32, initial_state=initial_state)\n\n    with tf.variable_scope(\"output_reshaped\"):\n        # before, shape: (34, 1768, 32), after, shape: (34 * 1768, 32)\n        output = tf.reshape(output, shape=[-1, gru2_cell_size])\n\n    with tf.variable_scope(\"output_layer\"):\n        # shape: (34 * 1768, 3)\n        predictions = output_layer(output, num_labels)\n        predictions = tf.reshape(predictions, shape=[-1, 100, 3])\n    return predictions, initial_state, new_state, output\n</code></pre>\n\n<p>So as we can see from the code that the cell size of the first GRU is 64, the cell size of the second GRU is 32. And the batch size is 34 (but this is not important for me now). And the size of input features is 200. I have tried computing the gradients of the loss with respect to the trainable variables through:</p>\n\n<pre><code>local_grads_and_vars = optimizer.compute_gradients(loss, tf.trainable_variables())\n# only the gradients are taken to add them later with the back propagated gradients from previous batch.\nlocal_grads = [grad for grad, var in local_grads_and_vars]\n\nfor v in local_grads:\n    print(\"v\", v)\n</code></pre>\n\n<p>After printing out the grads I got the following:</p>\n\n<pre><code>v Tensor(\"Optimizer/gradients/GRU_Layer1/rnn/while/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(264, 128), dtype=float32)\nv Tensor(\"Optimizer/gradients/GRU_Layer1/rnn/while/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(128,), dtype=float32)\nv Tensor(\"Optimizer/gradients/GRU_Layer1/rnn/while/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(264, 64), dtype=float32)\nv Tensor(\"Optimizer/gradients/GRU_Layer1/rnn/while/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\nv Tensor(\"Optimizer/gradients/GRU_Layer2/rnn/while/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(96, 64), dtype=float32)\nv Tensor(\"Optimizer/gradients/GRU_Layer2/rnn/while/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\nv Tensor(\"Optimizer/gradients/GRU_Layer2/rnn/while/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(96, 32), dtype=float32)\nv Tensor(\"Optimizer/gradients/GRU_Layer2/rnn/while/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(32,), dtype=float32)\nv Tensor(\"Optimizer/gradients/output_layer/MatMul_grad/tuple/control_dependency_1:0\", shape=(32, 3), dtype=float32)\nv Tensor(\"Optimizer/gradients/output_layer/add_grad/tuple/control_dependency_1:0\", shape=(3,), dtype=float32)\n</code></pre>\n\n<p>Assume that I saved the gradients after training the model on the first batch, that is, after feeding a tensor of shape: <code>(34, 100, 200)</code> as <code>input_features</code> \"In the model function argument\", and output of shape <code>(34 * 100, 3)</code>, how to back propagate these gradients on the second mini-batch?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 59}]
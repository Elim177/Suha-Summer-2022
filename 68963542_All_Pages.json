[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 7846906, "reputation": 2947, "user_id": 5931672, "user_type": "registered", "accept_rate": 86, "profile_image": "https://lh5.googleusercontent.com/-Ljkm-NVRzOc/AAAAAAAAAAI/AAAAAAAAAFE/EelBBzc8ji0/photo.jpg?sz=256", "display_name": "Agustin Barrachina", "link": "https://stackoverflow.com/users/5931672/agustin-barrachina"}, "is_answered": true, "view_count": 25, "accepted_answer_id": 68963602, "answer_count": 1, "score": 0, "last_activity_date": 1630325858, "creation_date": 1630147165, "question_id": 68963542, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68963542/implementing-z-relu", "title": "Implementing z_relu", "body": "<p>I am trying to implement zReLU presented in &quot;On Complex Valued Convolutional Neural Networks&quot; from Nitzan Guberman (2016).</p>\n<p>This activation functions let's the output as the input if both real and imaginary parts are positive. There are several ways I imagine how to implement it but they all use <code>tf.keras.backend.switch</code> which is just a way of doing <code>else if</code> statements. Here one example.</p>\n<pre><code>def zrelu(z: Tensor) -&gt; Tensor:\n    angle = tf.math.angle(z)\n    return tf.keras.backend.switch(0 &lt;= angle,\n                                   tf.keras.backend.switch(angle &lt;= pi / 2,\n                                                           z,\n                                                           tf.cast(0., dtype=z.dtype)),\n                                   tf.cast(0., dtype=z.dtype))\n</code></pre>\n<p>This gives me the desired output, and when testing the activation function with data it works correctly, however, I have a problem when using it on models like this:</p>\n<pre><code>model = tf.keras.Sequential([\n    cvnn.layers.ComplexInput((4)),\n    cvnn.layers.ComplexDense(1, activation=tf.keras.layers.Activation(zrelu)),\n    cvnn.layers.ComplexDense(1, activation='linear')\n])\n</code></pre>\n<p>It gives <code>TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'</code> on the initializer line: <code>return tf.math.sqrt(6. / (fan_in + fan_out))</code>. I believe, as there is a switch, tf ignores the size of the activation function output and therefore outputs <code>None</code> shape, which then conflicts with the next layer. This is strange because the output shape is actually forced by <code>tf.keras.layers.Activation</code> as there is the function <code>compute_output_shape</code>, which to my understanding tells tf that the output will have that shape.</p>\n<p>My problem can be solved by either of these two options:</p>\n<ol>\n<li>Understand why <code>compute_output_shape</code> and how to tell tf not to worry</li>\n<li>An alternative way of implementing the activation function in which tensorflow can understand the output shape.</li>\n</ol>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 190}]
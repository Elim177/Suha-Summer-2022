[{"items": [{"tags": ["tensorflow", "keras"], "owner": {"account_id": 19848241, "reputation": 11, "user_id": 14539318, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0964e40e211d3b9986f1554d322044c2?s=256&d=identicon&r=PG&f=1", "display_name": "rsabe", "link": "https://stackoverflow.com/users/14539318/rsabe"}, "is_answered": false, "view_count": 136, "answer_count": 0, "score": 1, "last_activity_date": 1603934184, "creation_date": 1603934184, "question_id": 64583564, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64583564/issue-with-gradient-tape-in-tensorflow", "title": "Issue with Gradient Tape in tensorflow", "body": "<p>I am attempting to implement a U-Net with region proposal network in tensorflow. I have a keras model that compiles, but when I try to write a training loop with GradientTape() I get the &quot;ValueError: No gradients provided for any variable:&quot; error. This appears to be a fairly common issue with a variety of causes, and I'm struggling to figure out what the cause is in this case.</p>\n<p>Here is the function where I define my model (with various blocks defined outside the function):</p>\n<pre><code>def get_UNet_RPN(params): \n    n_filters=params['n_filters']\n    batchnorm=params['batchnorm']\n    attention=params['attention']\n    imh=params['imh']\n    imw=params['imw']\n    input_img=Input(shape=(imh,imw,3))\n    c1 = conv2d_block(input_img, n_filters=n_filters*1, c1=True, kernel_size=3, batchnorm=batchnorm)\n    p1 = MaxPooling2D((2, 2)) (c1)\n    #p1 = Dropout(dropout*0.5)(p1)\n    \n    c2 = conv2d_block(p1, n_filters=n_filters*2, c1=False, kernel_size=3, batchnorm=batchnorm)\n    p2 = MaxPooling2D((2, 2)) (c2)\n    #p2 = Dropout(dropout)(p2)\n    \n    c3 = conv2d_block(p2, n_filters=n_filters*4, c1=False, kernel_size=3, batchnorm=batchnorm)\n    p3 = MaxPooling2D((2, 2)) (c3)\n    #p3 = Dropout(dropout)(p3)\n    \n    c4 = conv2d_block(p3, n_filters=n_filters*8, c1=False, kernel_size=3, batchnorm=batchnorm)\n    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n    #p4 = Dropout(dropout)(p4)\n    \n    c5 = conv2d_block(p4, n_filters=n_filters*16, c1=False, kernel_size=3, batchnorm=batchnorm)\n    \n    # expansive path\n    u6 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c5)\n    if attention:\n        c4 = att_block(c4, c5, n_filters=n_filters*8, kernel_size=2, batchnorm=batchnorm)\n    u6 = concatenate([u6, c4])\n    #u6 = Dropout(dropout)(u6)\n    c6 = conv2d_block(u6, n_filters=n_filters*8, c1=False, kernel_size=3, batchnorm=batchnorm)\n    \n    u7 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c6)\n    if attention:\n        c3 = att_block(c3, c6, n_filters=n_filters*4, kernel_size=2, batchnorm=batchnorm)\n    u7 = concatenate([u7, c3])\n    #u7 = Dropout(dropout)(u7)\n    c7 = conv2d_block(u7, n_filters=n_filters*4, c1=False, kernel_size=3, batchnorm=batchnorm)\n    \n    u8 = Conv2DTranspose(n_filters*2, (3, 3), strides=(2, 2), padding='same') (c7)\n    if attention:\n        c2 = att_block(c2, c7, n_filters=n_filters*2, kernel_size=2, batchnorm=batchnorm)\n    u8 = concatenate([u8, c2])\n    #u8 = Dropout(dropout)(u8)\n    c8 = conv2d_block(u8, n_filters=n_filters*2, c1=False, kernel_size=3, batchnorm=batchnorm)\n    \n    u9 = Conv2DTranspose(n_filters*1, (3, 3), strides=(2, 2), padding='same') (c8)\n    if attention:\n        c1 = att_block(c1, c8, n_filters=n_filters*1, kernel_size=2, batchnorm=batchnorm)\n    u9 = concatenate([u9, c1], axis=3)\n    #u9 = Dropout(dropout)(u9)\n    c9 = conv2d_block(u9, n_filters=n_filters*1, c1=False,  kernel_size=3, batchnorm=batchnorm)\n    \n    UNet_out = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n    # Define RPN\n    scales=params['scales']\n    ratios=params['ratios']\n    input_height=params['imh']\n    input_width=params['imw']\n    feat_map=p4 #change this to alter where RPN input gets pulled \n    feat_map_shape=tf.shape(feat_map)\n    feat_map_shape=tf.cast(feat_map_shape,tf.float64)\n    num_ref_anchors = scales.shape[0] * ratios.shape[0]\n    rpn = Conv2D(512, (3, 3),activation=&quot;relu&quot;, padding=&quot;same&quot;, name=&quot;rpn_conv/3x3&quot;)(feat_map)\n    # This is the score to determine an anchor is foreground vs background\n    rpn_cls_score = Conv2D(num_ref_anchors, [1, 1], activation='sigmoid',\n                            padding='valid', name='rpn_cls_score')(rpn)\n\n    # change it so that the score has 2 as its channel size\n    rpn_cls_score_reshape = tf.reshape(rpn_cls_score, [-1, 2])\n    # Now the shape of rpn_cls_score_reshape is (H * W * num_anchors, 4)\n    rpn_cls_prob = Softmax()(rpn_cls_score_reshape)\n    # rpn_bbox_pred has shape (?, H, W, num_anchors * 4)\n    rpn_bbox_pred = Conv2D(num_ref_anchors * 4, [1, 1],\n                            (1, 1), activation=&quot;linear&quot;, name='rpn_bbox_pred')(rpn)\n\n    # Generate segmentation mask\n    pred_mask = tf.argmax(UNet_out, axis=3)\n    pred_mask = tf.reshape(pred_mask,[input_height,input_width])\n    pred_mask = tf.cast(pred_mask,dtype=tf.float32)\n    model=Model(input_img,[pred_mask,rpn_bbox_pred,rpn_cls_score,rpn_cls_prob,feat_map_shape],name='unet_rpn')\n    return model\n</code></pre>\n<p>And here is a skeleton of the training loop:</p>\n<pre><code>unet_rpn=get_UNet_RPN(params)\nunet_rpn.summary()\n\noptimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n\nfor epoch in range(0,epochs):\n    #Shuffle training data\n    np.random.shuffle(train_i)\n    #Iterate through each image \n    for i in train_i:\n        with tf.GradientTape(watch_accessed_variables=False) as tape:\n            tape.watch(unet_rpn.trainable_variables)\n            # forward pass through the unet/rpn\n            y_pred, bbox_pred, cls_pred, cls_prob, feat_map_shape = unet_rpn(i)\n\n            # Generate region proposals\n            ...\n            # Calculate loss\n            loss=...\n                                             \n        grads=tape.gradient(loss,unet_rpn.trainable_weights)\n        optimizer.apply_gradients(zip(grads,unet_rpn.trainable_variables))\n</code></pre>\n<p>My perusal of the docs have unearthed many reasons why all the gradients might return &quot;None&quot; (<a href=\"https://www.tensorflow.org/guide/autodiff\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/guide/autodiff</a>) but none of them make sense to me, as my loop basically follows the structure of the one in the docs (<a href=\"https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch</a>). Any assistance would be greatly appreciated!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 43}]
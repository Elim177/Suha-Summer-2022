[{"items": [{"tags": ["python", "tensorflow2.0", "reinforcement-learning", "custom-training"], "owner": {"account_id": 16705366, "reputation": 1, "user_id": 14693976, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/LHvRF.png?s=256&g=1", "display_name": "schissmantics", "link": "https://stackoverflow.com/users/14693976/schissmantics"}, "is_answered": true, "view_count": 622, "answer_count": 1, "score": 1, "last_activity_date": 1607691995, "creation_date": 1607422381, "last_edit_date": 1607535010, "question_id": 65196943, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65196943/slow-training-on-cpu-and-gpu-in-a-small-network-tensorflow", "title": "Slow training on CPU and GPU in a small network (tensorflow)", "body": "<p>Here is the original <a href=\"https://github.com/marload/DeepRL-TensorFlow2/blob/master/DQN/DQN_Discrete.py\" rel=\"nofollow noreferrer\">script</a> I'm trying to run on both CPU and GPU, I'm expecting a much faster training on GPU however it's taking almost the same time. I made the following modification to <code>main()</code>(the first 4 lines) because the original script does not activate / use the GPU. Suggestions ... ?</p>\n<pre><code>def main():\n    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n    if len(physical_devices) &gt; 0:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n        print('GPU activated')\n    env = gym.make('CartPole-v1')\n    agent = Agent(env)\n    agent.train(max_episodes=1000)\n</code></pre>\n<p><strong>Update:</strong></p>\n<p>wandb's <a href=\"https://drive.google.com/file/d/1Fgn7tlm6HBUyZIcPelVjxNz_RWvY7QU_/view?usp=sharing\" rel=\"nofollow noreferrer\">report</a> shows 0% GPU utilization which confirms that there is a problem</p>\n<p>Full code in question which is not mine and belongs to this <a href=\"https://github.com/marload/DeepRL-TensorFlow2\" rel=\"nofollow noreferrer\">repository</a>:</p>\n<pre><code>import wandb\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.optimizers import Adam\n\nimport gym\nimport argparse\nimport numpy as np\nfrom collections import deque\nimport random\n\ntf.keras.backend.set_floatx('float64')\nwandb.init(name='DQN', project=&quot;deep-rl-tf2&quot;)\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--gamma', type=float, default=0.95)\nparser.add_argument('--lr', type=float, default=0.005)\nparser.add_argument('--batch_size', type=int, default=32)\nparser.add_argument('--eps', type=float, default=1.0)\nparser.add_argument('--eps_decay', type=float, default=0.995)\nparser.add_argument('--eps_min', type=float, default=0.01)\n\nargs = parser.parse_args()\n\nclass ReplayBuffer:\n    def __init__(self, capacity=10000):\n        self.buffer = deque(maxlen=capacity)\n    \n    def put(self, state, action, reward, next_state, done):\n        self.buffer.append([state, action, reward, next_state, done])\n    \n    def sample(self):\n        sample = random.sample(self.buffer, args.batch_size)\n        states, actions, rewards, next_states, done = map(np.asarray, zip(*sample))\n        states = np.array(states).reshape(args.batch_size, -1)\n        next_states = np.array(next_states).reshape(args.batch_size, -1)\n        return states, actions, rewards, next_states, done\n    \n    def size(self):\n        return len(self.buffer)\n\nclass ActionStateModel:\n    def __init__(self, state_dim, aciton_dim):\n        self.state_dim  = state_dim\n        self.action_dim = aciton_dim\n        self.epsilon = args.eps\n        \n        self.model = self.create_model()\n    \n    def create_model(self):\n        model = tf.keras.Sequential([\n            Input((self.state_dim,)),\n            Dense(32, activation='relu'),\n            Dense(16, activation='relu'),\n            Dense(self.action_dim)\n        ])\n        model.compile(loss='mse', optimizer=Adam(args.lr))\n        return model\n    \n    def predict(self, state):\n        return self.model.predict(state)\n    \n    def get_action(self, state):\n        state = np.reshape(state, [1, self.state_dim])\n        self.epsilon *= args.eps_decay\n        self.epsilon = max(self.epsilon, args.eps_min)\n        q_value = self.predict(state)[0]\n        if np.random.random() &lt; self.epsilon:\n            return random.randint(0, self.action_dim-1)\n        return np.argmax(q_value)\n\n    def train(self, states, targets):\n        self.model.fit(states, targets, epochs=1, verbose=0)\n    \n\nclass Agent:\n    def __init__(self, env):\n        self.env = env\n        self.state_dim = self.env.observation_space.shape[0]\n        self.action_dim = self.env.action_space.n\n\n        self.model = ActionStateModel(self.state_dim, self.action_dim)\n        self.target_model = ActionStateModel(self.state_dim, self.action_dim)\n        self.target_update()\n\n        self.buffer = ReplayBuffer()\n\n    def target_update(self):\n        weights = self.model.model.get_weights()\n        self.target_model.model.set_weights(weights)\n    \n    def replay(self):\n        for _ in range(10):\n            states, actions, rewards, next_states, done = self.buffer.sample()\n            targets = self.target_model.predict(states)\n            next_q_values = self.target_model.predict(next_states).max(axis=1)\n            targets[range(args.batch_size), actions] = rewards + (1-done) * next_q_values * args.gamma\n            self.model.train(states, targets)\n    \n    def train(self, max_episodes=1000):\n        for ep in range(max_episodes):\n            done, total_reward = False, 0\n            state = self.env.reset()\n            while not done:\n                action = self.model.get_action(state)\n                next_state, reward, done, _ = self.env.step(action)\n                self.buffer.put(state, action, reward*0.01, next_state, done)\n                total_reward += reward\n                state = next_state\n            if self.buffer.size() &gt;= args.batch_size:\n                self.replay()\n            self.target_update()\n            print('EP{} EpisodeReward={}'.format(ep, total_reward))\n            wandb.log({'Reward': total_reward})\n\n\ndef main():\n    env = gym.make('CartPole-v1')\n    agent = Agent(env)\n    agent.train(max_episodes=1000)\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 255}]
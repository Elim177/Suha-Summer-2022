[{"items": [{"tags": ["tensorflow", "matrix", "keras", "deep-learning", "tensorflow2.0"], "owner": {"account_id": 10939129, "reputation": 87, "user_id": 8039629, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/1feb74879bf7849888687912bfad044a?s=256&d=identicon&r=PG&f=1", "display_name": "Shanthan K", "link": "https://stackoverflow.com/users/8039629/shanthan-k"}, "is_answered": false, "view_count": 55, "answer_count": 0, "score": 2, "last_activity_date": 1598173994, "creation_date": 1598083848, "last_edit_date": 1598173994, "question_id": 63533971, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63533971/what-is-the-efficient-way-to-write-this-custom-operation-in-tensorflow", "title": "What is the efficient way to write this custom operation in tensorflow?", "body": "<p>I want to implement a new operation similar to matrix multiplication in tensorflow. For example, If there are two matrices <code>input, W</code> and <code> C = Op(input,W)</code>, then, <code>C[i,j] = tf.math.reduce_prod(input[i,:] - W[:,j])</code>. For implementing this operation I am planning to use two &quot;for-loops&quot; iterating over rows &amp; columns, however, I believe that  the for-loop implementation will decrease the training performance. I tried looking into the implementation of <code>tf.lingalg.matmul()</code> but the function seems to call C++ binaries which are not editable. Is there any way to speed up this computation? The following is the function I have written to implement Op(input,w):</p>\n<pre><code>def difference(input,w):\n    result = tf.Variable(tf.zeros((input.shape[0],w.shape[1]),dtype=tf.float16))\n    for i in tf.range(0,input.shape[0]):\n        for j in tf.range(0,w.shape[1]):\n            result[i,j].assign(tf.math.reduce_prod(input[i,:] - W[:,j]))\n    return result\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 193}]
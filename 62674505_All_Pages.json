[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "reinforcement-learning", "tflearn"], "owner": {"account_id": 3728998, "reputation": 246, "user_id": 3701747, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0b2e3e0cb01e1de3ae5e8028b38db798?s=256&d=identicon&r=PG&f=1", "display_name": "kosa", "link": "https://stackoverflow.com/users/3701747/kosa"}, "is_answered": true, "view_count": 1826, "accepted_answer_id": 62676198, "answer_count": 1, "score": 1, "last_activity_date": 1593664753, "creation_date": 1593598277, "last_edit_date": 1593664753, "question_id": 62674505, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62674505/tensorflow-1-x-typeerror-unsupported-operand-types-for-nonetype-and-in", "title": "TensorFlow 1.x: TypeError: unsupported operand type(s) for /: &#39;NoneType&#39; and &#39;int&#39;", "body": "<p>I am new to TensorFlow. I made the following Neural Network in TensorFlow 1.x</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nimport tflearn\n\nclass ActorNetwork(object):\n    &quot;&quot;&quot;\n    Input to the network is the state, output is the action\n    under a deterministic policy.\n    The output layer activation is a tanh to keep the action\n    between -action_bound and action_bound\n    &quot;&quot;&quot;\n\n    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n        self.sess = sess\n        self.s_dim = state_dim\n        self.a_dim = action_dim\n        self.action_bound = action_bound\n        self.learning_rate = learning_rate\n        self.tau = tau\n        self.batch_size = batch_size\n\n        # Actor Network\n        self.inputs, self.out, self.scaled_out = self.create_actor_network()\n\n        self.network_params = tf.trainable_variables()\n\n        # Target Network\n        self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n\n        self.target_network_params = tf.trainable_variables()[\n            len(self.network_params):]\n\n        # Op for periodically updating target network with online network\n        # weights\n        self.update_target_network_params = \\\n            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) +\n                                                  tf.multiply(self.target_network_params[i], 1. - self.tau))\n                for i in range(len(self.target_network_params))]\n\n        # This gradient will be provided by the critic network\n        self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n\n        # Combine the gradients here\n        self.unnormalized_actor_gradients = tf.gradients(\n            self.scaled_out, self.network_params, -self.action_gradient)\n        self.actor_gradients = list(map(lambda x: tf.math.divide(x, self.batch_size), self.unnormalized_actor_gradients))\n\n        # Optimization Op\n        self.optimize = tf.train.AdamOptimizer(self.learning_rate).\\\n            apply_gradients(zip(self.actor_gradients, self.network_params))\n\n        self.num_trainable_vars = len(\n            self.network_params) + len(self.target_network_params)\n\n    def create_actor_network(self):\n        inputs = tflearn.input_data(shape=[None, self.s_dim])\n        net = tflearn.fully_connected(inputs, 400)\n        net = tflearn.layers.normalization.batch_normalization(net)\n        net = tflearn.activations.relu(net)\n        net = tflearn.fully_connected(net, 300)\n        net = tflearn.layers.normalization.batch_normalization(net)\n        net = tflearn.activations.relu(net)\n        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n        out = tflearn.fully_connected(\n            net, self.a_dim, activation='tanh', weights_init=w_init)\n        # Scale output to -action_bound to action_bound\n        scaled_out = tf.multiply(out, self.action_bound)\n        return inputs, out, scaled_out\n\n    def train(self, inputs, a_gradient):\n        self.sess.run(self.optimize, feed_dict={\n            self.inputs: inputs,\n            self.action_gradient: a_gradient\n        })\n\n    def predict(self, inputs):\n        return self.sess.run(self.scaled_out, feed_dict={\n            self.inputs: inputs\n        })\n\n    def predict_target(self, inputs):\n        return self.sess.run(self.target_scaled_out, feed_dict={\n            self.target_inputs: inputs\n        })\n\n    def update_target_network(self):\n        self.sess.run(self.update_target_network_params)\n\n    def get_num_trainable_vars(self):\n        return self.num_trainable_vars\n</code></pre>\n<p>When I call it once it does not give any error, however in the second time it gives an error. For example</p>\n<pre><code>with tf.Session() as sess:\n    actor1 = ActorNetwork(sess, 1, 2, 1, 0.01, 0.003, 200)\n    actor2 = ActorNetwork(sess, 1, 2, 1, 0.01, 0.003, 200)\n</code></pre>\n<p>I get the following error for only actor2:</p>\n<p>TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'</p>\n<p>It has something to do with None value in the lambda function. But, why does it not provide an error for the first time?</p>\n<p>Edit: Stack trace:</p>\n<pre><code>---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-2-2323bc1d5028&gt; in &lt;module&gt;()\n      1 with tf.Session() as sess:\n      2     actor1 = ActorNetwork(sess, 1, 2, 1, 0.01, 0.003, 200)\n----&gt; 3     actor2 = ActorNetwork(sess, 1, 2, 1, 0.01, 0.003, 200)\n\n3 frames\n&lt;ipython-input-1-895268594a81&gt; in __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size)\n     48         self.unnormalized_actor_gradients = tf.gradients(\n     49             self.scaled_out, self.network_params, -self.action_gradient)\n---&gt; 50         self.actor_gradients = list(map(lambda x: tf.math.divide(x, self.batch_size), self.unnormalized_actor_gradients))\n     51 \n     52         # Optimization Op\n\n&lt;ipython-input-1-895268594a81&gt; in &lt;lambda&gt;(x)\n     48         self.unnormalized_actor_gradients = tf.gradients(\n     49             self.scaled_out, self.network_params, -self.action_gradient)\n---&gt; 50         self.actor_gradients = list(map(lambda x: tf.math.divide(x, self.batch_size), self.unnormalized_actor_gradients))\n     51 \n     52         # Optimization Op\n\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)\n    178     &quot;&quot;&quot;Call target, and fall back on dispatchers if there is a TypeError.&quot;&quot;&quot;\n    179     try:\n--&gt; 180       return target(*args, **kwargs)\n    181     except (TypeError, ValueError):\n    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a\n\n/tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_ops.py in divide(x, y, name)\n    323     return DivideDelegateWithName(x, name) / y\n    324   else:\n--&gt; 325     return x / y\n    326 \n    327 \n\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\n\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'int'\n</code></pre>\n<p>EDIT-2: From the suggestion, I wrote in TF 2.x. This actually eliminated the error. But are these two networks the same?</p>\n<pre><code>class ActorNetwork(object):\n  def __init__(self, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n    self.state_dim = state_dim\n    self.action_dim = action_dim\n    self.action_bound = action_bound\n    self.learning_rate = learning_rate\n    self.tau  = tau\n    self.batch_size = batch_size\n    self.optimizer = tf.keras.optimizers.Adam(self.learning_rate)\n\n    #actor network\n    self.inputs, self.out, self.scaled_out = self.create_actor_network()\n    self.actor_model = keras.Model(inputs=self.inputs, outputs=self.scaled_out, name='actor_network')\n    self.network_params = self.actor_model.trainable_variables\n\n    #target actor network\n    self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n    self.target_actor_model = keras.Model(inputs=self.target_inputs, outputs=self.target_scaled_out, name='target_actor_network')\n    self.target_network_params = self.target_actor_model.trainable_variables\n\n\n  def create_actor_network(self):\n    inputs = Input(shape = (self.state_dim,), batch_size = None, name = &quot;actor_input_state&quot;)\n\n    net = layers.Dense(400, name = 'actor_dense_1a')(inputs)\n    net = layers.BatchNormalization()(net)\n    net = layers.Activation(activation=tf.nn.relu)(net)\n\n    net = layers.Dense(300, name = 'actor_dense_1b')(net)\n    net = layers.BatchNormalization()(net)\n    net = layers.Activation(activation=tf.nn.relu)(net)\n\n    # net = layers.Dense(20, name = 'actor_dense_1c')(net)\n    # net = layers.BatchNormalization()(net)\n    # net = layers.Activation(activation=tf.nn.relu)(net)\n\n    # net = layers.Dense(10, name = 'actor_dense_1d')(net)\n    # net = layers.BatchNormalization()(net)\n    # net = layers.Activation(activation=tf.nn.relu)(net)\n    \n    w_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003, seed=None)\n    out = layers.Dense(self.action_dim, activation='tanh', name = 'actor_dense_2', kernel_initializer = w_init)(net)\n    scaled_out = tf.multiply(out, self.action_bound, name = &quot;actions_scaling&quot;)\n    return inputs, out, scaled_out\n  \n  def update_target_network(self):\n    self.update_target_network_params = [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) + tf.multiply(self.target_network_params[i], 1-self.tau)) for i in range(len(self.target_network_params))]\n  \n  def train(self, inputs, a_gradient):\n    with tf.GradientTape() as self.tape:\n      self.prediction = self.actor_model(inputs)\n    self.unnormalized_actor_gradients = self.tape.gradient(self.prediction, self.network_params, output_gradients = -a_gradient)\n    self.actor_gradients = list(map(lambda x: tf.math.divide(x, self.batch_size), self.unnormalized_actor_gradients))\n    self.optimizer.apply_gradients(zip(self.actor_gradients, self.network_params))\n    \n  def predict(self, inputs):\n    return self.actor_model(inputs)\n\n  def predict_target(self, inputs):\n    return self.target_actor_model(inputs)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 42}]
[{"items": [{"tags": ["python", "tensorflow", "neural-network", "tensorflow2.0"], "owner": {"account_id": 5775921, "reputation": 21, "user_id": 4557973, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-eNrLUzSOg5g/AAAAAAAAAAI/AAAAAAAAA3I/dh7ZFfKGgSI/photo.jpg?sz=256", "display_name": "galaxy_m31", "link": "https://stackoverflow.com/users/4557973/galaxy-m31"}, "is_answered": false, "view_count": 374, "answer_count": 0, "score": 2, "last_activity_date": 1598178687, "creation_date": 1581723172, "last_edit_date": 1598178687, "question_id": 60234725, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60234725/how-to-use-gradient-override-map-with-tf-gradienttape-in-tf2-0", "title": "How to use gradient_override_map with tf.GradientTape in TF2.0", "body": "<p>I'd like to override default gradient calculations in Tensorflow 2.0.</p>\n<p>In TF1, each time we run something, we create a Session and a Graph explicitly. Then we can use the <code>with graph.gradient_override_map({'GradName': 'CustGradName'})</code> context apply the custom-registered gradient. In TF2, graphs and sessions can be created using the <code>tf.compat.v1</code> API, so it's done in the same way as follows</p>\n<pre><code># tf 2.0.0 with compatible API\n\n# register a custom gradient\n@tf.RegisterGradient('CustZero')\n    def _custom_grad(op, grad):\n        return tf.zeros_like(op.inputs[0])\n\nwith tf.compat.v1.Session() as sess:\n    with sess.graph.as_default() as g:\n        x = tf.convert_to_tensor([-1.0, 0.0, 1.0, 2.0])\n        with g.gradient_override_map({'Relu': 'CustZero'}):\n            y = tf.nn.relu(x)\n        dy = tf.gradients(y , x)\n    print(sess.run(dy)[0])\n\n# output: [0. 0. 0. 0.]\n</code></pre>\n<p>Now in TF2, <code>tf.function</code> and <code>tf.GradientTape</code> are recommended and is the default way to get gradients. So I'd like to use TF2 native API to do similar things instead of using <code>tf.compat.v1</code>. Any functions decorated with <code>tf.function</code> will create an <code>AutoGraph</code> once it's called. Different graphs will be created and called for different kind of input signatures. If an <code>input_signature</code> is past to <code>tf.function</code>, only one graph will be created, and therefore it takes tensors matching the specified input signature only. This is called a <strong><a href=\"https://www.tensorflow.org/guide/concrete_function\" rel=\"nofollow noreferrer\">concrete function</a></strong> in TF2. From the concrete function, we can get access to its associated graph and do gradient override. So I did as follows</p>\n<pre><code># tf 2.0.0 with tf.function and tf.GradientTape\n\n# register custom gradient\n@tf.RegisterGradient('CustZero')\ndef _custom_grad(op, grad):\n    return tf.zeros_like(op.inputs[0])\n\n# create a concrete function with the specified input signatures\n@tf.function(input_signature=(tf.TensorSpec(shape=(None,), dtype=tf.float32),))\ndef my_relu(x):\n    return tf.nn.relu(x)\nmy_relu_conc = my_relu.get_concrete_function()\n    \nx = tf.constant([-1.0, 0.0, 1.0, 2.0], dtype=tf.float32)\nwith tf.GradientTape() as tape:\n    with my_relu_conc.graph.gradient_override_map({'Relu': 'CustZero'}):\n        tape.watch(x)\n        y = my_relu_conc(x)\ndy = tape.gradient(y, x).numpy()\nprint(dy)\n\n# output: array([0., 0., 1., 1.], dtype=float32)\n</code></pre>\n<p>As we see, if seems that the gradient override map doesn't work somehow in the second method. I checked <code>my_relu_conc.graph._gradient_override_map</code> and got <code>{'Relu': 'CustZero'}</code>, so the context works as expected. However, the custom gradient is not used.</p>\n<p>Does anyone know how to use <code>gradient_override_map</code> with <code>tf.function</code> and <code>tf.GradientTape</code> in a correct manner? Thanks!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 247}]
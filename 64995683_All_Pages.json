[{"items": [{"tags": ["tensorflow", "neural-network"], "owner": {"account_id": 2111877, "reputation": 1829, "user_id": 1877002, "user_type": "registered", "accept_rate": 73, "profile_image": "https://www.gravatar.com/avatar/8a1c3de36e3fb239c77f4439d1866894?s=256&d=identicon&r=PG", "display_name": "Benny K", "link": "https://stackoverflow.com/users/1877002/benny-k"}, "is_answered": true, "view_count": 640, "accepted_answer_id": 65006330, "answer_count": 1, "score": 2, "last_activity_date": 1620215102, "creation_date": 1606256353, "last_edit_date": 1620215102, "question_id": 64995683, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64995683/loss-function-with-derivative-in-tensorflow-2", "title": "Loss function with derivative in TensorFlow 2", "body": "<p>I am using TF2 (2.3.0) NN to approximate the function y which solves the ODE:  y'+3y=0</p>\n<p>I have defined cutsom loss class and function in which I am trying to differentiate the single output with respect to the single input so the equation holds, provided that <code>y_true</code> is zero:</p>\n<pre><code>from tensorflow.keras.losses import Loss\nimport tensorflow as tf\n\nclass CustomLossOde(Loss):\n    def __init__(self, x, model, name='ode_loss'):\n        super().__init__(name=name)\n        self.x = x\n        self.model = model\n\n    def call(self, y_true, y_pred):\n\n        with tf.GradientTape() as tape:\n            tape.watch(self.x)\n            y_p = self.model(self.x)\n\n\n        dy_dx = tape.gradient(y_p, self.x)\n        loss = tf.math.reduce_mean(tf.square(dy_dx + 3 * y_pred - y_true))\n        return loss\n</code></pre>\n<p>but running the following NN:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import Input\nfrom custom_loss_ode import CustomLossOde\n\n\nnum_samples = 1024\nx_train = 4 * (tf.random.uniform((num_samples, )) - 0.5)\ny_train = tf.zeros((num_samples, ))\ninputs = Input(shape=(1,))\nx = Dense(16, 'tanh')(inputs)\nx = Dense(8, 'tanh')(x)\nx = Dense(4)(x)\ny = Dense(1)(x)\nmodel = Model(inputs=inputs, outputs=y)\nloss = CustomLossOde(model.input, model)\nmodel.compile(optimizer=Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.99),loss=loss)\nmodel.run_eagerly = True\nmodel.fit(x_train, y_train, batch_size=16, epochs=30)\n</code></pre>\n<p>for now I am getting 0 loss from the fisrt epoch, which doesn't make any sense.</p>\n<p>I have printed both <code>y_true</code> and <code>y_test</code> from within the function and they seem OK so I suspect that the problem is in the gradien which I didn't succeed to print.\nApprecitate any help</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 32}]
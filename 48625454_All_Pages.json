[{"items": [{"tags": ["tensorflow", "embedding", "word-embedding", "gradient"], "owner": {"account_id": 2249163, "reputation": 820, "user_id": 1981791, "user_type": "registered", "accept_rate": 22, "profile_image": "https://www.gravatar.com/avatar/595118ac793182b2dc50f367f280b9d2?s=256&d=identicon&r=PG", "display_name": "Want", "link": "https://stackoverflow.com/users/1981791/want"}, "is_answered": true, "view_count": 599, "answer_count": 1, "score": 4, "last_activity_date": 1525697912, "creation_date": 1517843054, "last_edit_date": 1517854100, "question_id": 48625454, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/48625454/tensorflow-gradients-getting-unnecessary-0-0-gradients-by-tf-gradients", "title": "TensorFlow gradients: Getting unnecessary 0.0 gradients by tf.gradients", "body": "<p>Let's suppose I have the following variable</p>\n\n<blockquote>\n  <p>embeddings = tf.Variable(tf.random_uniform(dtype=tf.float32,shape =\n  [self.vocab_size, self.embedding_dim], minval=-0.001, maxval=0.001))</p>\n  \n  <p>sent_1 = construct_sentence(word_ids_1)</p>\n  \n  <p>sent_2 = construct_sentence(word_ids_2)</p>\n</blockquote>\n\n<p>Where <code>construct_sentence</code> is a method of obtaining a sentence representation based on the placeholders <code>word_ids_1</code> and <code>word_ids_2</code></p>\n\n<p>Let's suppose I have some loss:</p>\n\n<blockquote>\n  <p>loss = construct_loss(sent_1, sent_2, label)</p>\n</blockquote>\n\n<p>Now, when I try to get the gradients using:</p>\n\n<blockquote>\n  <p>gradients_wrt_w = tf.gradients(loss, embeddings)</p>\n</blockquote>\n\n<p>Instead of getting only the gradients with respect to the specific variables involved in <code>construct_sentence</code> and <code>construct_loss</code>, I get the gradients of every embedding in the variable <code>embeddings</code> (the gradients being 0 for those embeddings that are not involved in the loss &amp; sentence representations). </p>\n\n<p><strong>How can I get the gradients wrt variables I am only interested in?</strong></p>\n\n<p>Moreover, I get repetitions of some variables (with the same value) because of the partial derivatives involved.  Since embeddings is a 2D Variable I can't do a simple lookup like this:</p>\n\n<blockquote>\n  <p>tf.gradients(loss, tf.nn.embedding_lookup(embeddings, word_ids))</p>\n</blockquote>\n\n<p>This introduces a huge performance slow-down, since I am working with large number of word embeddings and I want to take the derivative only wrt some word embeddings per time.</p>\n\n<p>Moreover, I am getting a lot of duplicate gradients (because of the partial derivatives) and I tried using the  tf.AggregationMethod but that didn't work out.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 270}]
[{"items": [{"tags": ["tensorflow", "keras", "neural-network", "tensorflow2.0", "gradient-descent"], "owner": {"account_id": 17587200, "reputation": 79, "user_id": 13299451, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-J9UbS8685QA/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rd0wsiUzFGNU5vryYEyhyT8z2kWlA/photo.jpg?sz=256", "display_name": "Chao", "link": "https://stackoverflow.com/users/13299451/chao"}, "is_answered": false, "view_count": 835, "answer_count": 2, "score": 2, "last_activity_date": 1606896828, "creation_date": 1602057452, "question_id": 64239696, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64239696/understanding-gradient-tape-with-mini-batches", "title": "Understanding Gradient Tape with mini batches", "body": "<p>In the below example taken from <a href=\"https://keras.io/guides/writing_a_training_loop_from_scratch/\" rel=\"nofollow noreferrer\">Keras documentation</a>,  I want to understand how <code>grads</code> is computed. Does the gradient <code>grads</code> corresponds to the average gradient computed using the batch <code>(x_batch_train, y_batch_train)</code>? In other words, does the algorithm computes the gradient, with respect to each variable, using every sample in the mini batch and then average them to get <code>grads</code>?</p>\n<pre><code>for epoch in range(epochs):\n    print(&quot;\\nStart of epoch %d&quot; % (epoch,))\n\n    # Iterate over the batches of the dataset.\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n\n        # Open a GradientTape to record the operations run\n        # during the forward pass, which enables auto-differentiation.\n        with tf.GradientTape() as tape:\n\n            # Run the forward pass of the layer.\n            # The operations that the layer applies\n            # to its inputs are going to be recorded\n            # on the GradientTape.\n            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n\n            # Compute the loss value for this minibatch.\n            loss_value = loss_fn(y_batch_train, logits)\n\n        # Use the gradient tape to automatically retrieve\n        # the gradients of the trainable variables with respect to the loss.\n        grads = tape.gradient(loss_value, model.trainable_weights)\n\n        # Run one step of gradient descent by updating\n        # the value of the variables to minimize the loss.\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 172}]
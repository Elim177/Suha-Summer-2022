[{"items": [{"tags": ["python-3.x", "tensorflow"], "owner": {"account_id": 5270055, "reputation": 91, "user_id": 4934261, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/02e149a9f3a9e7ee0132f01296c0d976?s=256&d=identicon&r=PG&f=1", "display_name": "Emilio Botero", "link": "https://stackoverflow.com/users/4934261/emilio-botero"}, "is_answered": true, "view_count": 534, "accepted_answer_id": 60290045, "answer_count": 1, "score": 2, "last_activity_date": 1582063197, "creation_date": 1582043030, "last_edit_date": 1582053544, "question_id": 60285334, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60285334/how-to-freeze-a-layers-weights-for-some-number-of-iterations", "title": "How to freeze a layer&#39;s weights for some number of iterations?", "body": "<p>I'm trying to train two MLP's jointly, each one to predict a different real-valued variable. I want to minimize a loss over these two outputs, but I want to fix one of them for some number of \"warm-up\" iterations. </p>\n\n<p>I'm new to tensorflow, but basically I'm looking for the equivalent of something like this in Pytorch:</p>\n\n<pre><code>def loss(self, *args, **kwargs) -&gt; torch.Tensor:\n        # Extract data\n        data, target, probability = args\n\n        # Iterate through each model and sum nll\n        nll = []\n        for index in range(self.num_models):\n            # Extract mean and variance from prediction\n            if self._current_it &lt; self.warm_start_it:\n                predictive_mean = self.mean[index](data)\n                with torch.no_grad():\n                    predictive_variance = softplus(self.variance[index](data))\n            else:\n                with torch.no_grad():\n                    predictive_mean = self.mean[index](data)\n                predictive_variance = softplus(self.variance[index](data))\n\n            # Calculate the loss\n            nll.append(self.calculate_nll(target, predictive_mean, predictive_variance))\n\n        mean_nll = torch.stack(nll).mean()\n\n        # Update current iteration\n        if self.training:\n            self._current_it += 1\n\n        return mean_nll\n</code></pre>\n\n<p>I'm thinking I can do something similar inside my model's <code>call()</code> function, i.e.:</p>\n\n<pre><code>    def call(self, step, inputs, training=None, mask=None):\n\n        if step &lt; self.warmup:\n            with tf.GradientTape() as t:\n                mean_predictions = self.mean(inputs)\n            var_predictions = self.variance(inputs)\n        else:\n            mean_predictions = self.mean(inputs)\n            with tf.GradientTape() as t:\n                var_predictions = self.variance(inputs)\n        return mean_predictions, var_predictions\n</code></pre>\n\n<p>Is this the correct way of getting the above Pytorch equivalent?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 48}]
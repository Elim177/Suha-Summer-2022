[{"items": [{"tags": ["tensorflow", "neural-network"], "owner": {"user_type": "does_not_exist", "display_name": "user9210757"}, "is_answered": true, "view_count": 51, "accepted_answer_id": 51781836, "answer_count": 1, "score": 0, "last_activity_date": 1533888281, "creation_date": 1533847929, "last_edit_date": 1533878274, "question_id": 51775577, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51775577/neural-network-using-tensorflow-has-cost-stuck", "title": "Neural Network Using Tensorflow has Cost Stuck", "body": "<p>I tried to implement a 3 layered neural net using tensorflow but it didn't work.\nCode:-    </p>\n\n<pre><code>import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\nlearning_rate=0.5\nepochs=10\nbatch_size=100\n\nx=tf.placeholder(tf.float32,[None,784])\ny=tf.placeholder(tf.float32,[None,10])\nw1=tf.Variable(tf.random_normal([784,500]))\nb1=tf.Variable(tf.random_normal([500]))\nw2=tf.Variable(tf.random_normal([500,100]))\nb2=tf.Variable(tf.random_normal([100]))\nw3=tf.Variable(tf.random_normal([100,10]))\nb3=tf.Variable(tf.random_normal([10]))\n\nlayer1=tf.add(tf.matmul(x,w1),b1)\nlayer1=tf.nn.relu(layer1)\nlayer2=tf.add(tf.matmul(layer1,w2),b2)\nlayer2=tf.nn.relu(layer2)\noutput_layer=tf.add(tf.matmul(layer2,w3),b3)\noutput_layer=tf.nn.softmax(output_layer)\n\ny_clipped = tf.clip_by_value(output_layer,1e-10,0.9999999)\ncross_entropy = -1*tf.reduce_mean(tf.reduce_sum(y*tf.log(y_clipped)+(1-y)*tf.log(1-y_clipped),axis=1))\noptimiser=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n\ninit_op = tf.global_variables_initializer()\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output_layer,1))\naccuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n    total_batch=int(len(mnist.train.labels)/batch_size)\n    for epoch in range(epochs):\n        avg_cost = 0\n        for i in range(total_batch):\n            batch_x,batch_y = mnist.train.next_batch(batch_size=batch_size)\n            _,c = sess.run([optimiser,cross_entropy],feed_dict={x:batch_x,y:batch_y})\n            avg_cost += c/total_batch\n        print(\"Epoch:\", (epoch+1),\"cost=\",\"{:.3f}\".format(avg_cost))\n    print(sess.run(accuracy,feed_dict={x:mnist.test.images, y:mnist.test.labels}))\n</code></pre>\n\n<p>Output:-</p>\n\n<pre><code>Epoch: 1 cost= 34.984\nEpoch: 2 cost= 34.974\nEpoch: 3 cost= 34.974\nEpoch: 4 cost= 34.974\nEpoch: 5 cost= 34.974\nEpoch: 6 cost= 34.974\nEpoch: 7 cost= 34.974\nEpoch: 8 cost= 34.974\nEpoch: 9 cost= 34.974\nEpoch: 10 cost= 34.974\n0.101\n</code></pre>\n\n<p>The Cost is stuck at 34.974 and I can't find the error.The accuracy is as bad as a guess. I tried reducing the layers to 2 but it still won't run.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 296}]
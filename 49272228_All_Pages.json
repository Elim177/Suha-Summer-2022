[{"items": [{"tags": ["python", "tensorflow", "backpropagation", "tensorflow-gradient"], "owner": {"account_id": 8133134, "reputation": 33, "user_id": 7083967, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0ed32787f41ce5a44d252632da4f3bf6?s=256&d=identicon&r=PG&f=1", "display_name": "rakeshKM", "link": "https://stackoverflow.com/users/7083967/rakeshkm"}, "is_answered": false, "view_count": 518, "answer_count": 1, "score": 0, "last_activity_date": 1521880181, "creation_date": 1521013809, "last_edit_date": 1521880181, "question_id": 49272228, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49272228/tensorflow-typeerror-fetch-argument-none-has-invalid-type-class-nonetype-wh", "title": "Tensorflow TypeError: Fetch argument None has invalid type &lt;class &#39;NoneType&#39;&gt; while finding gradient", "body": "<p>It's a text analysis task with CNN model,I want to visualize which word is triggering most to my particular classification,Here is my code</p>\n\n<pre><code>with tf.Session() as sess:    \n\nsaver = tf.train.import_meta_graph('/home/rakesh/WORK/CNN_Lookout/runs/1519022246/checkpoints/model-200.meta') #load graph\nsaver.restore(sess,tf.train.latest_checkpoint('/home/rakesh/WORK/CNN_Lookout/runs/1519022246/checkpoints/./')) #load weigt\n\ngraph = tf.get_default_graph()\ninput_x = graph.get_tensor_by_name(\"input_x:0\")\ninput_y = graph.get_tensor_by_name(\"input_y:0\")\ndropout_keep_prob=graph.get_tensor_by_name(\"dropout_keep_prob:0\")    \nembedding_W= graph.get_tensor_by_name(\"embedding/W:0\")\n\nembedded_chars = tf.nn.embedding_lookup(embedding_W, input_x)\nembedded_chars_expanded = tf.expand_dims(embedded_chars, -1) \n\nconv_maxpool_5_W= graph.get_tensor_by_name(\"conv-maxpool-5/W:0\")\nconv_maxpool_5_b= graph.get_tensor_by_name(\"conv-maxpool-5/b:0\")\nconv_maxpool_5_conv= graph.get_operation_by_name(\"conv-maxpool-5/conv\")\nconv_maxpool_5_relu= graph.get_tensor_by_name(\"conv-maxpool-5/relu:0\")\nconv_maxpool_5_pool= graph.get_tensor_by_name(\"conv-maxpool-5/pool:0\")\n\nconv_maxpool_7_W= graph.get_tensor_by_name(\"conv-maxpool-7/W:0\")\nconv_maxpool_7_b= graph.get_tensor_by_name(\"conv-maxpool-7/b:0\")\nconv_maxpool_7_conv= graph.get_tensor_by_name(\"conv-maxpool-7/conv:0\")\nconv_maxpool_7_relu= graph.get_tensor_by_name(\"conv-maxpool-7/relu:0\")\nconv_maxpool_7_pool= graph.get_tensor_by_name(\"conv-maxpool-7/pool:0\")\n\nconv_maxpool_9_W= graph.get_tensor_by_name(\"conv-maxpool-9/W:0\")\nconv_maxpool_9_b= graph.get_tensor_by_name(\"conv-maxpool-9/b:0\")\nconv_maxpool_9_conv= graph.get_tensor_by_name(\"conv-maxpool-9/conv:0\")\nconv_maxpool_9_relu= graph.get_tensor_by_name(\"conv-maxpool-9/relu:0\")\nconv_maxpool_9_pool= graph.get_tensor_by_name(\"conv-maxpool-9/pool:0\")\n\n# Combine all the pooled features\nnum_filters_total = 128 * 3\npooled_outputs = [conv_maxpool_5_pool,conv_maxpool_7_pool,conv_maxpool_9_pool]\nprint(conv_maxpool_5_pool.shape)\nh_pool = tf.concat(pooled_outputs,3)\nh_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])   \n\nfinal_w= graph.get_tensor_by_name(\"W:0\")\nfinal_b= graph.get_tensor_by_name(\"output/b:0\")\n\n\n\n# Unpooling       \nPs = (tf.gradients(conv_maxpool_5_pool, conv_maxpool_5_relu))[0]\nunpooled = tf.multiply(Ps, conv_maxpool_5_relu)\nprint(sess.run(unpooled,feed_dict))\nprint(sess.run(unpooled,feed_dict).shape)\n#print(sess.run(conv_maxpool_5_relu,feed_dict))\n\n# Deconv\nbatch_size = tf.shape(input_x)[0]\nds = [batch_size]\nds.append(embedded_chars_expanded.get_shape()[1])\nds.append(embedded_chars_expanded.get_shape()[2])\nds.append(embedded_chars_expanded.get_shape()[3])\ndeconv_shape = tf.stack(ds)\ndeconv = tf.nn.conv2d_transpose(\n    unpooled,\n    conv_maxpool_5_W,\n    deconv_shape,\n    strides=[1, 1, 1, 1],\n    padding='VALID',\n    name=\"Deconv\"\n    )\nprint(sess.run(deconv,feed_dict))\nprint(sess.run(deconv,feed_dict).shape)\nprint(sess.run(embedded_chars_expanded,feed_dict).shape)\n#reshaped to original w2v embeding\nembedded_chars_expanded_shape=sess.run(embedded_chars_expanded,feed_dict).shape\nembedded_chars_back=tf.reshape(deconv,[embedded_chars_expanded_shape[0],embedded_chars_expanded_shape[1],embedded_chars_expanded_shape[2]])\nprint(sess.run(embedded_chars_back,feed_dict))\nprint(sess.run(embedded_chars_back,feed_dict).shape)\nprint(sess.run(embedded_chars,feed_dict))\nprint(sess.run(embedded_chars,feed_dict).shape)\n#original input\nprint(sess.run(input_x, feed_dict))\nprint(sess.run(input_x, feed_dict).shape)\ninput_back = (tf.gradients(embedded_chars_back, input_x))[0]\n#print(sess.run(input_back , feed_dict))\n#input_back = tf.multiply(input_back, input_x)\n#print(sess.run(input_back ,feed_dict).shape)\n</code></pre>\n\n<p>The last part where I want to get the original input by using gradient from embednding layer , the result is becoming None type, I am using Google word2vec model to get embedding in forward pass.</p>\n\n<pre><code>input_back = (tf.gradients(embedded_chars_back, input_x))[0]  #becoming none\n\nprint(sess.run(input_back , feed_dict)) \n</code></pre>\n\n<h3>I---->error of last line</h3>\n\n<pre><code>Traceback (most recent call last):\nFile \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\nFile \"&lt;string&gt;\", line 155, in &lt;module&gt;\nFile \"/home/rakesh/anaconda3/envs/tensorflow/lib/python3.5/site-\npackages/tensorflow/python/client/session.py\", line 889, in run\nrun_metadata_ptr)\nFile \"/home/rakesh/anaconda3/envs/tensorflow/lib/python3.5/site-\npackages/tensorflow/python/client/session.py\", line 1105, in _run\nself._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\nFile \"/home/rakesh/anaconda3/envs/tensorflow/lib/python3.5/site-\npackages/tensorflow/python/client/session.py\", line 414, in __init__\nself._fetch_mapper = _FetchMapper.for_fetch(fetches)\nFile \"/home/rakesh/anaconda3/envs/tensorflow/lib/python3.5/site-\npackages/tensorflow/python/client/session.py\", line 231, in for_fetch\n(fetch, type(fetch)))\nTypeError: Fetch argument None has invalid type &lt;class 'NoneType'&gt;\n</code></pre>\n\n<p>I want to visualize the last cnn layer till the input so I wanted to find out how much is backpropagated till input, which word contribute most, </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 54}]
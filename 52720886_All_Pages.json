[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 3948330, "reputation": 5043, "user_id": 3259896, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/641c30a7b383022f22b53c8cedb04e3f?s=256&d=identicon&r=PG&f=1", "display_name": "SantoshGupta7", "link": "https://stackoverflow.com/users/3259896/santoshgupta7"}, "is_answered": false, "view_count": 44, "answer_count": 0, "score": 1, "last_activity_date": 1539087451, "creation_date": 1539087451, "question_id": 52720886, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/52720886/does-tf-contrib-nn-sampled-sparse-softmax-loss-allow-for-float16-training", "title": "Does tf.contrib.nn.sampled_sparse_softmax_loss allow for float16 training?", "body": "<p>Currently, tf.nn.sampled_softmax_loss does not allow for using float16. </p>\n\n<p><a href=\"https://stackoverflow.com/questions/52711895/how-to-run-define-tensorflow-graph-were-all-variables-are-in-float16-instead-ins\">How to run define Tensorflow graph were all variables are in float16 instead instead of float32</a></p>\n\n<p>I'm looking at </p>\n\n<p>tf.contrib.nn.sampled_sparse_softmax_loss </p>\n\n<p>from </p>\n\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/nn/sampled_sparse_softmax_loss\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/contrib/nn/sampled_sparse_softmax_loss</a></p>\n\n<p>And it seems that this may allow float16 values. This code for this function is here</p>\n\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/nn/python/ops/sampling_ops.py\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/contrib/nn/python/ops/sampling_ops.py</a></p>\n\n<p>Which seems to use tf.nn.sparse_softmax_cross_entropy_with_logits</p>\n\n<p>Which seems to have support for float16</p>\n\n<blockquote>\n  <p>logits: Unscaled log probabilities of shape [d_0, d_1, ..., d_{r-1}, num_classes] and dtype float16, float32, or float64.</p>\n</blockquote>\n\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits</a></p>\n\n<p>However, when I try to use it, I get an error</p>\n\n<p>My code</p>\n\n<pre><code>import math\nimport numpy as np\nimport tensorflow as tf\n\nvocabulary_size = 10\nbatch_size = 64 \nembedding_size = 100 \nnum_inputs =4\nnum_sampled = 128 \n\ngraph = tf.Graph()\n\nwith graph.as_default(): #took out \" , tf.device('/cpu:0')\"\n\n\n    train_dataset = tf.placeholder(tf.int32, shape=[batch_size, num_inputs ])\n    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n\n    embeddings = tf.get_variable( 'embeddings', dtype=tf.float16,\n        initializer= tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0, dtype=tf.float16) )\n\n    softmax_weights = tf.get_variable( 'softmax_weights', dtype=tf.float16,\n        initializer= tf.truncated_normal([vocabulary_size, embedding_size],\n                             stddev=1.0 / math.sqrt(embedding_size), dtype=tf.float16 ) )\n\n    softmax_biases = tf.get_variable('softmax_biases', dtype=tf.float16,\n        initializer= tf.zeros([vocabulary_size], dtype=tf.float16),  trainable=False )\n\n    embed = tf.nn.embedding_lookup(embeddings, train_dataset) #train data set is\n\n    embed_reshaped = tf.reshape( embed, [batch_size*num_inputs, embedding_size] )\n\n    segments= np.arange(batch_size).repeat(num_inputs)\n\n    averaged_embeds = tf.segment_mean(embed_reshaped, segments, name=None)\n\n    sam_sof_los = tf.contrib.nn.sampled_sparse_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=averaged_embeds,\n                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)\n\n    loss = tf.reduce_mean( sam_sof_los )\n\n    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss) \n\n    saver = tf.train.Saver()\n</code></pre>\n\n<p>My error message</p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\n    509                 as_ref=input_arg.is_ref,\n--&gt; 510                 preferred_dtype=default_dtype)\n    511           except TypeError as err:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\n   1143     if ret is None:\n-&gt; 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   1145 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _TensorTensorConversionFunction(t, dtype, name, as_ref)\n    980         \"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\" %\n--&gt; 981         (dtype.name, t.dtype.name, str(t)))\n    982   return t\n\nValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: 'Tensor(\"sampled_sparse_softmax_loss/Log:0\", shape=(64, 1), dtype=float32)'\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-11-b68afd94e9bd&gt; in &lt;module&gt;()\n     41 \n     42     sam_sof_los = tf.contrib.nn.sampled_sparse_softmax_loss(weights=softmax_weights , biases=softmax_biases , inputs=averaged_embeds,\n---&gt; 43                                    labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size)\n     44 \n     45 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/nn/python/ops/sampling_ops.py in sampled_sparse_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, sampled_values, remove_accidental_hits, partition_strategy, name)\n    331       remove_accidental_hits=remove_accidental_hits,\n    332       partition_strategy=partition_strategy,\n--&gt; 333       name=name)\n    334 \n    335   # There is only one true label. _compute_sampled_logits puts the true logit\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py in _compute_sampled_logits(weights, biases, labels, inputs, num_sampled, num_classes, num_true, sampled_values, subtract_log_q, remove_accidental_hits, partition_strategy, name, seed)\n   1126     if subtract_log_q:\n   1127       # Subtract log of Q(l), prior probability that l appears in sampled.\n-&gt; 1128       true_logits -= math_ops.log(true_expected_count)\n   1129       sampled_logits -= math_ops.log(sampled_expected_count)\n   1130 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)\n    860     with ops.name_scope(None, op_name, [x, y]) as name:\n    861       if isinstance(x, ops.Tensor) and isinstance(y, ops.Tensor):\n--&gt; 862         return func(x, y, name=name)\n    863       elif not isinstance(y, sparse_tensor.SparseTensor):\n    864         try:\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in sub(x, y, name)\n   8316   if _ctx is None or not _ctx._eager_context.is_eager:\n   8317     _, _, _op = _op_def_lib._apply_op_helper(\n-&gt; 8318         \"Sub\", x=x, y=y, name=name)\n   8319     _result = _op.outputs[:]\n   8320     _inputs_flat = _op.inputs\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\n    544                   \"%s type %s of argument '%s'.\" %\n    545                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n--&gt; 546                    inferred_from[input_arg.type_attr]))\n    547 \n    548           types = [values.dtype]\n\nTypeError: Input 'y' of 'Sub' Op has type float32 that does not match type float16 of argument 'x'.\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 70}]
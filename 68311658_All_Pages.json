[{"items": [{"tags": ["function", "tensorflow", "tensorflow2.0", "gradient-descent", "gradienttape"], "owner": {"account_id": 9796953, "reputation": 175, "user_id": 7839195, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/YlSRp.png?s=256&g=1", "display_name": "partyphysics", "link": "https://stackoverflow.com/users/7839195/partyphysics"}, "is_answered": true, "view_count": 190, "answer_count": 1, "score": 0, "last_activity_date": 1626726632, "creation_date": 1625808135, "last_edit_date": 1626726632, "question_id": 68311658, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68311658/why-cant-i-perform-gradients-on-a-variable-passed-as-an-argument-to-a-tf-functi", "title": "Why can&#39;t I perform gradients on a variable passed as an argument to a tf.function?", "body": "<p>My training loop was giving me the following warning:</p>\n<blockquote>\n<p>WARNING:tensorflow:Gradients do not exist for variables ['noise:0'] when minimizing the loss.</p>\n</blockquote>\n<p>After some tinkering I determined this only happened when the noise variable was being passed as an argument to my loss function which is a tf.function. The code below shows that there is no problem when the loss function is not a tf.function or when the global noise variable is referenced in the function. It also shows that an error results from trying to perform a gradient on the noise variable when it is used as argument in a tf.function:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom  tensorflow_probability import distributions as tfd \nfrom tensorflow_probability import bijectors as tfb\n\nconstrain_positive = tfb.Shift(np.finfo(np.float64).tiny)(tfb.Exp())\nnoise = tfp.util.TransformedVariable(initial_value=.1, bijector=constrain_positive, dtype=np.float64, name=&quot;noise&quot;)\ntrainable_variables = [noise.variables[0]]\nkernel = tfp.math.psd_kernels.ExponentiatedQuadratic()\noptimizer = tf.keras.optimizers.Adam()\nindex_points = tf.constant([[0]], dtype=np.float64)\nobservations = tf.constant([0], dtype=np.float64)\n\n# I can train noise when it is passed as an argument to a python function \ndef loss_function_1(index_points, observations, kernel, observation_noise_variance):\n    gp = tfd.GaussianProcess(kernel, index_points, observation_noise_variance=observation_noise_variance)\n    return -gp.log_prob(observations)\n\nwith tf.GradientTape() as tape:\n    nll_1 = loss_function_1(index_points, observations, kernel, noise)\ngrad_1 = tape.gradient(nll_1, trainable_variables)\nprint(grad_1)\noptimizer.apply_gradients(zip(grad_1, trainable_variables))\n\n# I can train noise if it is used in a tf.function and not passed as an argument\n@tf.function(autograph=False, experimental_compile=False)\ndef loss_function_2(index_points, observations, kernel):\n    gp = tfd.GaussianProcess(kernel, index_points, observation_noise_variance=noise)\n    return -gp.log_prob(observations)\n\nwith tf.GradientTape() as tape:\n    nll_2 = loss_function_2(index_points, observations, kernel)\ngrad_2 = tape.gradient(nll_2, trainable_variables)\nprint(grad_2)\noptimizer.apply_gradients(zip(grad_2, trainable_variables))\n\n# I can train noise if it is passed as an argument to a tf.function if the tf.function\n# uses the global variable\n@tf.function(autograph=False, experimental_compile=False)\ndef loss_function_3(index_points, observations, kernel, observation_noise_variance):\n    gp = tfd.GaussianProcess(kernel, index_points, observation_noise_variance=noise)\n    return -gp.log_prob(observations)\n\nwith tf.GradientTape() as tape:\n    nll_3 = loss_function_3(index_points, observations, kernel, noise)\ngrad_3 = tape.gradient(nll_3, trainable_variables)\nprint(grad_3)\noptimizer.apply_gradients(zip(grad_3, trainable_variables))\n\n# I cannot train noise if it is passed as an argument to a tf.function if the tf.function\n# the local variable\n@tf.function(autograph=False, experimental_compile=False)\ndef loss_function_4(index_points, observations, kernel, observation_noise_variance):\n    gp = tfd.GaussianProcess(kernel, index_points, observation_noise_variance=observation_noise_variance)\n    return -gp.log_prob(observations)\n\nwith tf.GradientTape() as tape:\n    nll_4 = loss_function_4(index_points, observations, kernel, noise)\ngrad_4 = tape.gradient(nll_4, trainable_variables)\nprint(grad_4)\noptimizer.apply_gradients(zip(grad_4, trainable_variables))\n</code></pre>\n<p>This code prints:</p>\n<blockquote>\n<p>[&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.045454545454545456&gt;]<br />\n[&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.045413242911911206&gt;]<br />\n[&lt;tf.Tensor: shape=(), dtype=float64, numpy=0.04537197429557289&gt;]<br />\n[None]</p>\n</blockquote>\n<p>And then it returns the error message:</p>\n<blockquote>\n<p>ValueError: No gradients provided for any variable: ['noise:0'].</p>\n</blockquote>\n<p>Ideally I would get the performance boost of a tf.function so I don't want to use loss_function_1. Also, I would like to be able to pass different noise variables to my loss function so I do not want to use the global variable like I do in loss_function_2 or loss_function_3.</p>\n<p>Why do I get None when I try to perform a gradient on a variable passed as an argument to a tf.function? How can I get around this?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 107}]
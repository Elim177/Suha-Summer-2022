[{"items": [{"tags": ["python", "python-3.x", "tensorflow", "bert-language-model"], "owner": {"account_id": 12829196, "reputation": 3199, "user_id": 9280994, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/378f51f8fdf92e078e0fd52507fed62f?s=256&d=identicon&r=PG&f=1", "display_name": "Jonathan R", "link": "https://stackoverflow.com/users/9280994/jonathan-r"}, "is_answered": false, "view_count": 672, "answer_count": 0, "score": 0, "last_activity_date": 1561545728, "creation_date": 1561542814, "last_edit_date": 1561545728, "question_id": 56769943, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/56769943/bert-model-does-not-learn-new-task", "title": "BERT model does not learn new task", "body": "<p>I am trying to fine-tune a pretrained BERT model on amazon-review dataset. For that I extended the <code>run_classifier</code> file by the following processor:</p>\n\n<pre><code>class AmazonProcessor(DataProcessor):\n  \"\"\"Processor for the Amazon data set.\"\"\"\n\n  def get_train_examples(self, data_dir):\n    \"\"\"See base class.\"\"\"\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n\n  def get_dev_examples(self, data_dir):\n    \"\"\"See base class.\"\"\"\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n\n  def get_test_examples(self, data_dir):\n    \"\"\"See base class.\"\"\"\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n\n  def get_labels(self):\n    \"\"\"See base class.\"\"\"\n    return [\"0\", \"1\", \"2\"]\n\n  def _create_examples(self, lines, set_type):\n    \"\"\"Creates examples for the training and dev sets.\"\"\"\n    examples = []\n    for (i, line) in enumerate(lines):\n      # header\n      if i == 0:\n        continue\n      guid = \"%s-%s\" % (set_type, i)\n      text_a = tokenization.convert_to_unicode(line[13])\n      label = tokenization.convert_to_unicode(line[7])\n      # only train on 3 labels instead of 5\n      if int(label) &lt;= 2: label = \"0\"\n      if int(label) == 3: label = \"1\"\n      if int(label) &gt;= 4: label = \"2\"\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n      return examples\n</code></pre>\n\n<p>I am training in a colab notebook on GPU, so therefore I adapted the main method for my need as well:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>processors = {\n  \"cola\": run_classifier.ColaProcessor,\n  \"mnli\": run_classifier.MnliProcessor,\n  \"mrpc\": run_classifier.MrpcProcessor,\n  \"xnli\": run_classifier.XnliProcessor,\n  \"amazon\": run_classifier.AmazonProcessor,\n}\n\nbert_config_file = os.path.join(BERT_FOLDER, \"bert_config.json\")\nmax_seq_length = 128\noutput_dir = \"drive/My Drive/model\"\ntask_name = \"amazon\"\nvocab_file = os.path.join(BERT_FOLDER, \"vocab.txt\")\ndo_lower_case = False\nmaster = None\ntpu_cluster_resolver = None\nsave_checkpoints_steps = 1000\niterations_per_loop = 1000\nuse_tpu = False\ndata_dir  = \"drive/My Drive/csv_dataset\"\nlearning_rate = 5e-5\nwarmup_proportion = 0.1\ntrain_batch_size = 16\neval_batch_size = 1\npredict_batch_size = 1\nnum_train_epochs = 10.0\nnum_train_steps = 10000\nnum_tpu_cores = 8\n#init_checkpoint = os.path.join(BERT_FOLDER, \"bert_model.ckpt\")\ninit_checkpoint = \"drive/My Drive/model2/model.ckpt-41000\"\n\ndo_train = True\ndo_eval = True\n\ntokenization.validate_case_matches_checkpoint(do_lower_case, init_checkpoint)\n\n\nbert_config = modeling.BertConfig.from_json_file(bert_config_file)\nprint(bert_config)\n\ntask_name = task_name.lower()\n\nprocessor = processors[task_name]()\n\nlabel_list = processor.get_labels()\n\ntokenizer = tokenization.FullTokenizer(\n  vocab_file=vocab_file, do_lower_case=do_lower_case)\n\nis_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\nrun_config = tf.contrib.tpu.RunConfig(\n  cluster=tpu_cluster_resolver,\n  master=master,\n  model_dir=output_dir,\n  save_checkpoints_steps=save_checkpoints_steps,\n  tpu_config=tf.contrib.tpu.TPUConfig(\n      iterations_per_loop=iterations_per_loop,\n      num_shards=num_tpu_cores,\n      per_host_input_for_training=is_per_host))\n\ntrain_examples = None\nnum_train_steps = None\nnum_warmup_steps = None\nif do_train:\n  train_examples = processor.get_train_examples(data_dir)\n  num_train_steps = int(\n      len(train_examples) / train_batch_size * num_train_epochs)\n  num_warmup_steps = int(num_train_steps * warmup_proportion)\n\nmodel_fn = run_classifier.model_fn_builder(\n  bert_config=bert_config,\n  num_labels=len(label_list),\n  init_checkpoint=init_checkpoint,\n  learning_rate=learning_rate,\n  num_train_steps=num_train_steps,\n  num_warmup_steps=num_warmup_steps,\n  use_tpu=use_tpu,\n  use_one_hot_embeddings=use_tpu)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n  use_tpu=use_tpu,\n  model_fn=model_fn,\n  config=run_config,\n  train_batch_size=train_batch_size,\n  eval_batch_size=eval_batch_size,\n  predict_batch_size=predict_batch_size)\n\nif do_train:\n  train_file = os.path.join(output_dir, \"train.tf_record\")\n  run_classifier.file_based_convert_examples_to_features(\n      train_examples, label_list, max_seq_length, tokenizer, train_file)\n  tf.logging.info(\"***** Running training *****\")\n  tf.logging.info(\"  Num examples = %d\", len(train_examples))\n  tf.logging.info(\"  Batch size = %d\", train_batch_size)\n  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n  train_input_fn = run_classifier.file_based_input_fn_builder(\n      input_file=train_file,\n      seq_length=max_seq_length,\n      is_training=True,\n      drop_remainder=True)\n  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n\nif do_eval:\n  eval_examples = processor.get_test_examples(data_dir)\n  num_actual_eval_examples = len(eval_examples)\n  if use_tpu:\n    # TPU requires a fixed batch size for all batches, therefore the number\n    # of examples must be a multiple of the batch size, or else examples\n    # will get dropped. So we pad with fake examples which are ignored\n    # later on. These do NOT count towards the metric (all tf.metrics\n    # support a per-instance weight, and these get a weight of 0.0).\n    while len(eval_examples) % eval_batch_size != 0:\n      eval_examples.append(PaddingInputExample())\n\n  eval_file = os.path.join(output_dir, \"eval.tf_record\")\n  run_classifier.file_based_convert_examples_to_features(\n      eval_examples, label_list, max_seq_length, tokenizer, eval_file)\n\n  tf.logging.info(\"***** Running evaluation *****\")\n  tf.logging.info(\"  Num examples = %d (%d actual, %d padding)\",\n                  len(eval_examples), num_actual_eval_examples,\n                  len(eval_examples) - num_actual_eval_examples)\n  tf.logging.info(\"  Batch size = %d\", eval_batch_size)\n\n  # This tells the estimator to run through the entire set.\n  eval_steps = None\n  # However, if running eval on the TPU, you will need to specify the\n  # number of steps.\n  if use_tpu:\n    assert len(eval_examples) % eval_batch_size == 0\n    eval_steps = int(len(eval_examples) // eval_batch_size)\n\n  eval_drop_remainder = True if use_tpu else False\n  eval_input_fn = run_classifier.file_based_input_fn_builder(\n      input_file=eval_file,\n      seq_length=max_seq_length,\n      is_training=False,\n      drop_remainder=eval_drop_remainder)\n\n  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n\n  output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n    tf.logging.info(\"***** Eval results *****\")\n    for key in sorted(result.keys()):\n      tf.logging.info(\"  %s = %s\", key, str(result[key]))\n      writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n</code></pre>\n\n<p>I know this is a lot of code but because I cannot pin point the error I want to present all of it.</p>\n\n<p>Note that most of the logging output seems perfectly reasonable:</p>\n\n<p>For example a converted example:</p>\n\n<pre><code>INFO:tensorflow:tokens: [CLS] Ich habe schon viele Klavier ##kon ##zer ##te geh\u00f6rt , aber was Frau Martha Ar ##geri ##ch hier spielt l\u00e4sst einem ge ##wis ##ser ##ma ##\u00dfen den At ##em stock ##en . So geni ##al habe ich diese 2 Klavier ##kon ##zer ##te von Ra ##ch ##mani ##no ##ff und T ##sch ##aik ##ov ##sky noch nie geh\u00f6rt . Sie ent ##fes ##selt einen regel ##rechte ##n Feuer ##stu ##rm an Vir ##tu ##osi ##t\u00e4t . [SEP]\nINFO:tensorflow:input_ids: 101 21023 21404 16363 18602 48021 17423 14210 10216 16706 117 11566 10134 16783 26904 18484 68462 10269 13329 28508 25758 10745 46503 83648 12754 10369 20284 10140 11699 10451 20511 10136 119 12882 107282 10415 21404 12979 12750 123 48021 17423 14210 10216 10166 38571 10269 31124 10343 13820 10130 157 12044 106333 11024 16116 11230 11058 16706 119 11583 61047 58058 26063 10897 46578 55663 10115 68686 19987 19341 10151 106433 10991 20316 24308 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\nINFO:tensorflow:label: 2 (id = 2)\n</code></pre>\n\n<p>Or the model loading from a checkpoint-file:</p>\n\n<pre><code>INFO:tensorflow:  name = output_weights:0, shape = (3, 768), *INIT_FROM_CKPT*\nINFO:tensorflow:  name = output_bias:0, shape = (3,), *INIT_FROM_CKPT*\n</code></pre>\n\n<p>But in the end the eval_accuracy always stays the same:</p>\n\n<pre><code>I0625 15:46:41.328946   eval_accuracy = 0.3338616\n</code></pre>\n\n<p>The full repository can be found here: <a href=\"https://github.com/joroGER/bert/\" rel=\"nofollow noreferrer\">https://github.com/joroGER/bert/</a></p>\n\n<p>And a gist to the notebook here: <a href=\"https://colab.research.google.com/gist/joroGER/75c1c9c6383f0199bb54ce7b63d412d0/untitled4.ipynb\" rel=\"nofollow noreferrer\">https://colab.research.google.com/gist/joroGER/75c1c9c6383f0199bb54ce7b63d412d0/untitled4.ipynb</a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 297}]
[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 19317050, "reputation": 1, "user_id": 14121593, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-PW1ZCrtomD8/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucn5wDCJwYRxO0yCDkBP6xCl9lenPw/photo.jpg?sz=256", "display_name": "W Lin", "link": "https://stackoverflow.com/users/14121593/w-lin"}, "is_answered": false, "view_count": 925, "answer_count": 1, "score": 0, "last_activity_date": 1597706937, "creation_date": 1597694883, "question_id": 63457928, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63457928/tensorflow-load-model-and-load-weights-give-different-behavior", "title": "Tensorflow load model and load_weights give different behavior", "body": "<p>I have one working tensorflow RNN model (called <code>my_model</code>) and I save it using <code>my_model.save(&quot;&lt;dir_to_my_model&gt;&quot;)</code>.</p>\n<p>However, when I reload this model with</p>\n<p><code>model = tf.keras.models.load_model(&quot;&lt;dir_to_my_model&gt;&quot;)</code></p>\n<p>and run a few incremental learning epochs with</p>\n<p><code>with tf.GradientTape() as tape:</code></p>\n<p><code>    logits = model(x_test_batch, training=True)</code></p>\n<p>I got an error (very long, please see at the bottom).</p>\n<p>But, if I save the weights of this model to a checkpoint using <code>model.save_weights()</code>, create a new model and load the weights using <code>model.load_weights</code> from that checkpoint. The code above could run successfully.</p>\n<p>My question is why these 2 methods work differently and what should I do to make method 1 work?</p>\n<hr />\n<pre><code>InvalidArgumentError                      Traceback (most recent call last)\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\n   2325       with c_api_util.tf_buffer() as buf:\n-&gt; 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\n   2327         data = c_api.TF_GetBuffer(buf)\n\nInvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\n    330     try:\n--&gt; 331       xla_compile = op.get_attr(&quot;_XlaCompile&quot;)\n    332       xla_separate_compiled_gradients = op.get_attr(\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\n   2329       # Convert to ValueError for backwards compatibility.\n-&gt; 2330       raise ValueError(str(e))\n   2331     x = attr_value_pb2.AttrValue()\n\nValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\n\nDuring handling of the above exception, another exception occurred:\n\nInvalidArgumentError                      Traceback (most recent call last)\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\n   2325       with c_api_util.tf_buffer() as buf:\n-&gt; 2326         c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf)\n   2327         data = c_api.TF_GetBuffer(buf)\n\nInvalidArgumentError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\n    330     try:\n--&gt; 331       xla_compile = op.get_attr(&quot;_XlaCompile&quot;)\n    332       xla_separate_compiled_gradients = op.get_attr(\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_attr(self, name)\n   2329       # Convert to ValueError for backwards compatibility.\n-&gt; 2330       raise ValueError(str(e))\n   2331     x = attr_value_pb2.AttrValue()\n\nValueError: Operation 'StatefulPartitionedCall' has no attr named '_XlaCompile'.\n\nDuring handling of the above exception, another exception occurred:\n\nLookupError                               Traceback (most recent call last)\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\n    606           try:\n--&gt; 607             grad_fn = ops.get_gradient_function(op)\n    608           except LookupError:\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in get_gradient_function(op)\n   2494     op_type = op.type\n-&gt; 2495   return _gradient_registry.lookup(op_type)\n   2496 \n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/registry.py in lookup(self, name)\n     96       raise LookupError(\n---&gt; 97           &quot;%s registry has no entry for: %s&quot; % (self._name, name))\n\nLookupError: gradient registry has no entry for: While\n\nDuring handling of the above exception, another exception occurred:\n\nLookupError                               Traceback (most recent call last)\n&lt;ipython-input-7-55225939f96a&gt; in &lt;module&gt;\n     10             mode = 'DNN' if 'ANN' in m else 'RNN'\n     11             c_datasets = continual_dataset(datasets[i], mode=mode)\n---&gt; 12             res = learning_rate_test(continual_fine_tune, mname, c_datasets, lr_list=lrs)\n     13             dfs.append(res)\n\n&lt;ipython-input-5-f4d4cc64dc17&gt; in learning_rate_test(continual_func, model_fname, c_datasets, epochs, lr_list, loss)\n     14                 model.save_weights('Model/tmp')\n     15                 model.load_weights('Model/tmp')\n---&gt; 16             test_predictions = continual_func(c_datasets, model, loss_fn, lr, epochs=e, save_model='')\n     17             test_predictions = (np.array(test_predictions) &gt; 0.5).astype(int)\n     18             epoch_test_dict[e].append(np.sum(np.equal(y_test_list, test_predictions)) / len(y_test_list) * 100)\n\n&lt;ipython-input-4-9ae956b3e918&gt; in continual_fine_tune(c_datasets, model, loss_fn, lr, epochs, save_model)\n     12         for e in range(epochs):\n     13             with tf.GradientTape() as tape:\n---&gt; 14                 logits = model(x_test_batch, training=True)\n     15                 loss_value = loss_fn(y_test_batch, logits)\n     16             grads = tape.gradient(loss_value, model.trainable_weights)\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    820           with base_layer_utils.autocast_context_manager(\n    821               self._compute_dtype):\n--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)\n    823           self._handle_activity_regularization(inputs, outputs)\n    824           self._set_mask_metadata(inputs, outputs, input_masks)\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/sequential.py in call(self, inputs, training, mask)\n    265       if not self.built:\n    266         self._init_graph_network(self.inputs, self.outputs, name=self.name)\n--&gt; 267       return super(Sequential, self).call(inputs, training=training, mask=mask)\n    268 \n    269     outputs = inputs  # handle the corner case where self.layers is empty\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in call(self, inputs, training, mask)\n    715     return self._run_internal_graph(\n    716         inputs, training=training, mask=mask,\n--&gt; 717         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n    718 \n    719   def compute_output_shape(self, input_shape):\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in _run_internal_graph(self, inputs, training, mask, convert_kwargs_to_constants)\n    889 \n    890           # Compute outputs.\n--&gt; 891           output_tensors = layer(computed_tensors, **kwargs)\n    892 \n    893           # Update tensor_dict.\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)\n    820           with base_layer_utils.autocast_context_manager(\n    821               self._compute_dtype):\n--&gt; 822             outputs = self.call(cast_inputs, *args, **kwargs)\n    823           self._handle_activity_regularization(inputs, outputs)\n    824           self._set_mask_metadata(inputs, outputs, input_masks)\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in return_outputs_and_add_losses(*args, **kwargs)\n     57     inputs = args[inputs_arg_index]\n     58     args = args[inputs_arg_index + 1:]\n---&gt; 59     outputs, losses = fn(inputs, *args, **kwargs)\n     60     layer.add_loss(losses, inputs)\n     61     return outputs\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in wrap_with_training_arg(*args, **kwargs)\n    111         training,\n    112         lambda: replace_training_and_call(True),\n--&gt; 113         lambda: replace_training_and_call(False))\n    114 \n    115   # Create arg spec for decorated function. If 'training' is not defined in the\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)\n     57         pred, true_fn=true_fn, false_fn=false_fn, name=name)\n     58   return smart_module.smart_cond(\n---&gt; 59       pred, true_fn=true_fn, false_fn=false_fn, name=name)\n     60 \n     61 \n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)\n     52   if pred_value is not None:\n     53     if pred_value:\n---&gt; 54       return true_fn()\n     55     else:\n     56       return false_fn()\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in &lt;lambda&gt;()\n    110     return tf_utils.smart_cond(\n    111         training,\n--&gt; 112         lambda: replace_training_and_call(True),\n    113         lambda: replace_training_and_call(False))\n    114 \n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py in replace_training_and_call(training)\n    106     def replace_training_and_call(training):\n    107       set_training_arg(training, training_arg_index, args, kwargs)\n--&gt; 108       return wrapped_call(*args, **kwargs)\n    109 \n    110     return tf_utils.smart_cond(\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)\n    566         xla_context.Exit()\n    567     else:\n--&gt; 568       result = self._call(*args, **kwds)\n    569 \n    570     if tracing_count == self._get_tracing_count():\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)\n    604       # In this case we have not created variables on the first call. So we can\n    605       # run the first trace but we should fail if variables are created.\n--&gt; 606       results = self._stateful_fn(*args, **kwds)\n    607       if self._created_variables:\n    608         raise ValueError(&quot;Creating variables on a non-first call to a function&quot;\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in __call__(self, *args, **kwargs)\n   2361     with self._lock:\n   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n-&gt; 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\n   2364 \n   2365   @property\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _filtered_call(self, args, kwargs)\n   1609          if isinstance(t, (ops.Tensor,\n   1610                            resource_variable_ops.BaseResourceVariable))),\n-&gt; 1611         self.captured_inputs)\n   1612 \n   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)\n   1695         possible_gradient_type,\n   1696         executing_eagerly)\n-&gt; 1697     forward_function, args_with_tangents = forward_backward.forward()\n   1698     if executing_eagerly:\n   1699       flat_outputs = forward_function.call(\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self)\n   1421     &quot;&quot;&quot;Builds or retrieves a forward function for this call.&quot;&quot;&quot;\n   1422     forward_function = self._functions.forward(\n-&gt; 1423         self._inference_args, self._input_tangents)\n   1424     return forward_function, self._inference_args + self._input_tangents\n   1425 \n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward(self, inference_args, input_tangents)\n   1183       (self._forward, self._forward_graph, self._backward,\n   1184        self._forwardprop_output_indices, self._num_forwardprop_outputs) = (\n-&gt; 1185            self._forward_and_backward_functions(inference_args, input_tangents))\n   1186     return self._forward\n   1187 \n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _forward_and_backward_functions(self, inference_args, input_tangents)\n   1329     outputs = self._func_graph.outputs[:self._num_inference_outputs]\n   1330     return self._build_functions_for_outputs(\n-&gt; 1331         outputs, inference_args, input_tangents)\n   1332 \n   1333 \n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _build_functions_for_outputs(self, outputs, inference_args, input_tangents)\n    888             self._func_graph.inputs,\n    889             grad_ys=gradients_wrt_outputs,\n--&gt; 890             src_graph=self._func_graph)\n    891 \n    892       captures_from_forward = [\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\n    667                 # functions.\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n--&gt; 669                                          lambda: grad_fn(op, *out_grads))\n    670               else:\n    671                 # For function call ops, we add a 'SymbolicGradient'\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\n    334       xla_scope = op.get_attr(&quot;_XlaScope&quot;).decode()\n    335     except ValueError:\n--&gt; 336       return grad_fn()  # Exit early\n    337 \n    338   if not xla_compile:\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in &lt;lambda&gt;()\n    667                 # functions.\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n--&gt; 669                                          lambda: grad_fn(op, *out_grads))\n    670               else:\n    671                 # For function call ops, we add a 'SymbolicGradient'\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)\n    705   def _rewrite_forward_and_call_backward(self, op, *doutputs):\n    706     &quot;&quot;&quot;Add outputs to the forward call and feed them to the grad function.&quot;&quot;&quot;\n--&gt; 707     forward_function, backwards_function = self.forward_backward(len(doutputs))\n    708     if not backwards_function.outputs:\n    709       return backwards_function.structured_outputs\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward_backward(self, num_doutputs)\n    614     if forward_backward is not None:\n    615       return forward_backward\n--&gt; 616     forward, backward = self._construct_forward_backward(num_doutputs)\n    617     self._cached_function_pairs[num_doutputs] = (forward, backward)\n    618     return forward, backward\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _construct_forward_backward(self, num_doutputs)\n    662           args=[], kwargs={},\n    663           signature=signature,\n--&gt; 664           func_graph=backwards_graph)\n    665       backwards_graph_captures = backwards_graph.external_captures\n    666       captures_from_forward = [\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n    976                                           converted_func)\n    977 \n--&gt; 978       func_outputs = python_func(*func_args, **func_kwargs)\n    979 \n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backprop_function(*grad_ys)\n    652             self._func_graph.inputs,\n    653             grad_ys=grad_ys,\n--&gt; 654             src_graph=self._func_graph)\n    655 \n    656     with self._func_graph.as_default():\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\n    667                 # functions.\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n--&gt; 669                                          lambda: grad_fn(op, *out_grads))\n    670               else:\n    671                 # For function call ops, we add a 'SymbolicGradient'\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _MaybeCompile(scope, op, func, grad_fn)\n    334       xla_scope = op.get_attr(&quot;_XlaScope&quot;).decode()\n    335     except ValueError:\n--&gt; 336       return grad_fn()  # Exit early\n    337 \n    338   if not xla_compile:\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in &lt;lambda&gt;()\n    667                 # functions.\n    668                 in_grads = _MaybeCompile(grad_scope, op, func_call,\n--&gt; 669                                          lambda: grad_fn(op, *out_grads))\n    670               else:\n    671                 # For function call ops, we add a 'SymbolicGradient'\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _rewrite_forward_and_call_backward(self, op, *doutputs)\n    705   def _rewrite_forward_and_call_backward(self, op, *doutputs):\n    706     &quot;&quot;&quot;Add outputs to the forward call and feed them to the grad function.&quot;&quot;&quot;\n--&gt; 707     forward_function, backwards_function = self.forward_backward(len(doutputs))\n    708     if not backwards_function.outputs:\n    709       return backwards_function.structured_outputs\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in forward_backward(self, num_doutputs)\n    614     if forward_backward is not None:\n    615       return forward_backward\n--&gt; 616     forward, backward = self._construct_forward_backward(num_doutputs)\n    617     self._cached_function_pairs[num_doutputs] = (forward, backward)\n    618     return forward, backward\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _construct_forward_backward(self, num_doutputs)\n    662           args=[], kwargs={},\n    663           signature=signature,\n--&gt; 664           func_graph=backwards_graph)\n    665       backwards_graph_captures = backwards_graph.external_captures\n    666       captures_from_forward = [\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n    976                                           converted_func)\n    977 \n--&gt; 978       func_outputs = python_func(*func_args, **func_kwargs)\n    979 \n    980       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py in _backprop_function(*grad_ys)\n    652             self._func_graph.inputs,\n    653             grad_ys=grad_ys,\n--&gt; 654             src_graph=self._func_graph)\n    655 \n    656     with self._func_graph.as_default():\n\n~/work/tf/lib/python3.7/site-packages/tensorflow_core/python/ops/gradients_util.py in _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, unconnected_gradients, src_graph)\n    621               raise LookupError(\n    622                   &quot;No gradient defined for operation '%s' (op type: %s)&quot; %\n--&gt; 623                   (op.name, op.type))\n    624         if loop_state:\n    625           loop_state.EnterGradWhileContext(op, before=False)\n\nLookupError: No gradient defined for operation 'while' (op type: While) \n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 286}]
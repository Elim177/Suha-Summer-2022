[{"items": [{"tags": ["python-3.x", "tensorflow2.0"], "owner": {"account_id": 4441934, "reputation": 1952, "user_id": 3616293, "user_type": "registered", "accept_rate": 35, "profile_image": "https://www.gravatar.com/avatar/cf7556b4227065cec9496375d64fea3d?s=256&d=identicon&r=PG&f=1", "display_name": "Arun", "link": "https://stackoverflow.com/users/3616293/arun"}, "is_answered": true, "view_count": 4470, "answer_count": 2, "score": 3, "last_activity_date": 1646484664, "creation_date": 1584893699, "question_id": 60801746, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60801746/tensorflow-2-0-learning-rate-scheduler-with-tf-gradienttape", "title": "TensorFlow 2.0 learning rate scheduler with tf.GradientTape", "body": "<p>I am using TensorFlow 2.0 and Python 3.8 and I want to use a learning rate scheduler for which I have a function. I have to train a neural network for 160 epochs with the following where the learning rate is to be decreased by a factor of 10 at 80 and 120 epochs, where the initial learning rate = 0.01.</p>\n\n<pre><code>def scheduler(epoch, current_learning_rate): \n        if epoch == 79 or epoch == 119: \n            return current_learning_rate / 10 \n        else: \n            return min(current_learning_rate, 0.001) \n</code></pre>\n\n<p>How can I use this learning rate scheduler function with 'tf.GradientTape()'? I know how to use this using \"model.fit()\" as a callback:</p>\n\n<pre><code>callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n</code></pre>\n\n<p>How do I use this while using custom training loops with \"tf.GradientTape()\"?</p>\n\n<p>Thanks!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 0}]
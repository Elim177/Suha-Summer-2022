[{"items": [{"tags": ["python", "tensorflow", "keras", "neural-network", "gradient"], "owner": {"account_id": 19920279, "reputation": 17, "user_id": 14596305, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GjkOU6km-ge8YdIq0_tFr9WSlSmzpJsYZdWl8Tf=k-s256", "display_name": "fardis nadimi", "link": "https://stackoverflow.com/users/14596305/fardis-nadimi"}, "is_answered": true, "view_count": 813, "accepted_answer_id": 66901791, "answer_count": 1, "score": 1, "last_activity_date": 1617271143, "creation_date": 1617268555, "question_id": 66901737, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66901737/how-to-use-multiple-gradients-with-tensorflow-gradienttape", "title": "how to use multiple gradients with TensorFlow GradientTape?", "body": "<p>Having 3 neural networks connected like in the below code, how can we take two gradients from the initial network?? first gradient work but the second one returning the <code>None</code> tensor. seems like they are not related two each other to get the gradient. what is the problem here??</p>\n<pre><code>with tf.GradientTape() as tape1:\n    with tf.GradientTape() as tape2:\n        output1 = NN_model1(input1, training=True)\n        output2 = NN_model2(output1, training=True)\n        output3 = NN_model3([input1, output1, output2], training=True)\n        loss1 = -tf.math.reduce_mean(output3)\n        loss2 = -tf.math.reduce_mean(output2)\n    grad1 = tape2.gradient(loss1, NN_model1.trainable_variables)\ngrad2 = tape1.gradient(loss2, grad1)\noptimizer.apply_gradients(zip(grad2, NN_model1.trainable_variables))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 173}]
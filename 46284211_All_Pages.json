[{"items": [{"tags": ["python-2.7", "tensorflow", "neural-network"], "owner": {"account_id": 2358376, "reputation": 3047, "user_id": 2065691, "user_type": "registered", "accept_rate": 85, "profile_image": "https://i.stack.imgur.com/z304s.png?s=256&g=1", "display_name": "DanielTheRocketMan", "link": "https://stackoverflow.com/users/2065691/danieltherocketman"}, "is_answered": false, "view_count": 1364, "answer_count": 0, "score": 1, "last_activity_date": 1505753716, "creation_date": 1505752402, "last_edit_date": 1505753716, "question_id": 46284211, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/46284211/restoring-a-model-with-tensor-flow-nonetype-object-is-not-iterable", "title": "Restoring a model with tensor flow: &#39;NoneType&#39; object is not iterable", "body": "<p>I am restoring an object with tensor flow. However, I am getting this error</p>\n\n<pre><code>    return [dim.value for dim in self._dims]\nTypeError: 'NoneType' object is not iterable\n</code></pre>\n\n<p>when I define the optimzer:</p>\n\n<pre><code>train = optimizer.minimize(lossBatch)\n</code></pre>\n\n<p>I tested the random generation of weights and it worked well. </p>\n\n<pre><code>def init_weights(shape):\n    return tf.Variable(tf.random_uniform(shape, -0.01, 0.01, seed=0))\n</code></pre>\n\n<p>So I am concluding that the problem is related to the restoration of weights.</p>\n\n<p>To restore the weights I am doing this:</p>\n\n<pre><code>with tf.Session() as sess:\n\n\n\n\n        new_saver = tf.train.import_meta_graph('my-model-88500.meta')\n        new_saver.restore(sess, 'my-model-88500')\n        w_h1=  tf.get_default_graph().get_tensor_by_name(\"w_h1:0\")\n        b_h1 = tf.get_default_graph().get_tensor_by_name(\"b_h1:0\")\n        w_h2 = tf.get_default_graph().get_tensor_by_name(\"w_h2:0\")    \n        b_h2 = tf.get_default_graph().get_tensor_by_name(\"b_h2:0\")    \n        w_h3 = tf.get_default_graph().get_tensor_by_name(\"w_h3:0\")    \n        b_h3 = tf.get_default_graph().get_tensor_by_name(\"b_h3:0\")    \n        w_o =  tf.get_default_graph().get_tensor_by_name(\"w_o:0\")    \n        b_o =  tf.get_default_graph().get_tensor_by_name(\"b_o:0\")\n\n        w_h1=tf.reshape(w_h1,[numberInputs,numberHiddenUnits1],'w_h1')\n        b_h1=tf.reshape(b_h1,[numberHiddenUnits1],'b_h1')\n        w_h2=tf.reshape(w_h2,[numberHiddenUnits1,numberHiddenUnits2],'w_h2')        \n        b_h2=tf.reshape(b_h2,[numberHiddenUnits2],'b_h2')\n        w_h3=tf.reshape(w_h3,[numberHiddenUnits2,numberHiddenUnits3],'w_h3')        \n        b_h3=tf.reshape(b_h3,[numberHiddenUnits3],'b_h3')\n        w_o=tf.reshape(w_o,[numberHiddenUnits3,numberOutputs],'w_o')        \n        b_o=tf.reshape(b_o,[numberOutputs],'b_o')        \n\n        init = tf.initialize_all_variables()\n        sess.run(init)        \n</code></pre>\n\n<p>Then I redefine the network:</p>\n\n<pre><code>        numberEpochs=1500000\n        batchSize=25000    \n        learningRate=0.000001     \n\n        numberOutputs=np.shape(theTrainOutput)[1]    \n        numberTrainSamples=np.shape(theTrainInput)[0]\n        numberInputs=np.shape(theTrainInput)[1]\n\n        xTrain=tf.placeholder(\"float\",[numberTrainSamples,numberInputs])\n        yTrain=tf.placeholder(\"float\",[numberTrainSamples,numberOutputs])  \n        yTrainModel=model(xTrain,w_h1,b_h1,w_h2,b_h2,w_h3,b_h3,w_o,b_o)   \n\n\n        xBatch=tf.placeholder(\"float\",[batchSize,numberInputs])\n        yBatch=tf.placeholder(\"float\",[batchSize,numberOutputs])  \n        yBatchModel=model(xBatch,w_h1,b_h1,w_h2,b_h2,w_h3,b_h3,w_o,b_o)   \n\n\n        lossBatch = tf.reduce_mean(tf.abs(yBatch-yBatchModel)) \n        optimizer = tf.train.AdamOptimizer(learningRate)\n        train = optimizer.minimize(lossBatch)\n</code></pre>\n\n<p>I get an error in this last line above! Note that before I redefined the entire network to reuse the weights.</p>\n\n<p>It is worth mentioning that I am able to get the shape of a weight, namely</p>\n\n<pre><code>w_h1.get_shape()\nTensorShape([Dimension(13), Dimension(50)])\n</code></pre>\n\n<p>On the other hand,</p>\n\n<pre><code> w_h1.dtype\ntf.float32\n</code></pre>\n\n<p>furthermore, I am also able to print the weights:</p>\n\n<pre><code>print sess.run(w_h1)  \n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 27}]
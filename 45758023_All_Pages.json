[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "tensorboard"], "owner": {"account_id": 7434948, "reputation": 2260, "user_id": 5654715, "user_type": "registered", "accept_rate": 83, "profile_image": "https://graph.facebook.com/10153295750512607/picture?type=large", "display_name": "Baptiste Arnaud", "link": "https://stackoverflow.com/users/5654715/baptiste-arnaud"}, "is_answered": true, "view_count": 749, "answer_count": 2, "score": 1, "last_activity_date": 1503085611, "creation_date": 1503063351, "question_id": 45758023, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/45758023/loss-is-equal-to-0-from-the-beginning", "title": "Loss is equal to 0 from the beginning", "body": "<p>I'm trying to attempt the Titanic Kaggle competition using Tensorflow.</p>\n\n<p>My pre processed train data looks like this:</p>\n\n<pre><code>data_x:\n\nPassengerId  Pclass  Sex   Age  SibSp  Parch  Ticket Fare  Cabin  \\ Embarked\n1              2       1    1  38.0      1      0     500   71.2833    104\n2              3       3    1  26.0      0      0     334    7.9250      0\n3              4       1    1  35.0      1      0     650   53.1000    130\n4              5       3    0  35.0      0      0     638    8.0500      0\n\ndata_y:\n\nSurvived\n0\n1\n1\n1\n0\n</code></pre>\n\n<p>A softmax function should do the work to predict if a passenger survived or not since it's binary, right?</p>\n\n<p>So here is how I build my model:</p>\n\n<pre><code>X = tf.placeholder(tf.float32, [None, data_x.shape[1]])\nY_ = tf.placeholder(tf.float32, [None, 1])\n\nW = tf.Variable(tf.truncated_normal([10, 1]))\nb = tf.Variable(tf.zeros([1]))\n\n# Parameters\nlearning_rate = 0.001\n\n#The model\nY = tf.matmul(X,W) + b\n\n# Loss function\nentropy = tf.nn.softmax_cross_entropy_with_logits(labels=Y_, logits=Y)\nloss = tf.reduce_mean(entropy) # computes the mean over examples in the batch\n\noptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n\nacc = tf.equal(tf.argmax(Y_, 1), tf.argmax(Y, 1))\nacc = tf.reduce_mean(tf.cast(acc, tf.float32))\n\ntf.summary.scalar('loss', loss)\ntf.summary.scalar('accuracy', acc)\nmerged_summary = tf.summary.merge_all()\n\ninit = tf.global_variables_initializer()\n</code></pre>\n\n<p>And finallyn, the training part:</p>\n\n<pre><code>with tf.Session() as sess:\n    sess.run(init)\n    writer = tf.summary.FileWriter(\"./graphs\", sess.graph)\n    for i in range(1000):\n        _, l, summary = sess.run([optimizer, loss, merged_summary], feed_dict={X: data_x, Y_: data_y})\n        writer.add_summary(summary, i)\n        if i%100 == 0:\n            print (i)\n            print (\"loss = \", l)\n</code></pre>\n\n<p>But loss is equals to 0 since the first step...</p>\n\n<p>Here is Tensorboard visualization:</p>\n\n<p><a href=\"https://i.stack.imgur.com/rBbof.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/rBbof.png\" alt=\"enter image description here\"></a></p>\n\n<p>Any idea what's going on here?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 89}]
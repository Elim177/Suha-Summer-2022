[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning"], "owner": {"account_id": 15587361, "reputation": 651, "user_id": 11245475, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/77dcf0ea160a4e6b27eed736d2b8df83?s=256&d=identicon&r=PG&f=1", "display_name": "krenerd", "link": "https://stackoverflow.com/users/11245475/krenerd"}, "is_answered": false, "view_count": 140, "answer_count": 0, "score": 1, "last_activity_date": 1614042546, "creation_date": 1613971443, "last_edit_date": 1614042546, "question_id": 66310417, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66310417/training-tensorflow-model-with-multiple-common-output", "title": "Training tensorflow model with multiple common output", "body": "<p>I'm trying to train a Tensorflow model with multiple same outputs. A recurrent neural network predicts labels for an image for t iterative timesteps. Each timestep is supposed to predict the image class, and deeper timesteps are expected to improve the performance. You could simply imagine a deep model with intermediate outputs that are expected to predict one common task. All the output is compared with the same label.</p>\n<p>The problem is that there are too much outputs. All the outputs are successfully compiled, but I don't think my approach of feeding a tiled data to the model isn't the optimal solution for my problem. How could I improve this in terms of memory, clean code?</p>\n<pre><code>CHO_RNN=CustomLayers.RNN_Decoder(units,...)\nprediction_list=[]\n\nfor x in range(settings['refinement_t']):\n  pred, hidden = CHO_RNN(features, hidden)\n  prediction_list.append(pred)\n\nmodel=tf.keras.models.Model(input_image, prediction_list)\n...\n\ndata={'output1':labels,'output2':labels,'output3':labels,...'output15':labels}\nmodel.fit(x=images,y=data,...)\n</code></pre>\n<p>Edit:\nOne more reason I want to achieve this is because at inference, the model output is very complex, and functions for evaluation ex) model.evaluate doesn't work well. Also, the verbose print and tensorboard plot is extremely long.</p>\n<p>Edit2: RNN decoder code</p>\n<pre><code>class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 36, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # attention_hidden_layer shape == (batch_size, 36, units)\n    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n                                         self.W2(hidden_with_time_axis)))\n\n    # score shape == (batch_size, 36, 1)\n    # This gives you an unnormalized score for each image feature.\n    score = self.V(attention_hidden_layer)\n\n    # attention_weights shape == (batch_size, 36, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n\nclass RNN_Decoder(tf.keras.Model):\n    def __init__(self, units, class_types):\n        #units: # classes\n        super(RNN_Decoder, self).__init__()\n        self.units = units\n        self.gru = tf.keras.layers.GRU(self.units,\n                                        return_sequences=True,\n                                        return_state=True,\n                                        recurrent_initializer='glorot_uniform')\n        self.fc1 = tf.keras.layers.Dense(self.units)\n        self.fc2 = tf.keras.layers.Dense(class_types)\n\n        self.attention = BahdanauAttention(self.units)\n\n    def call(self, features, hidden):\n        # hidden: previous states   features: feature map(conv output)\n        # defining attention as a separate model\n\n        context_vector, attention_weights = self.attention(features, hidden)\n\n            # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.expand_dims(context_vector, 1)\n\n        # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n\n        # shape == (batch_size, max_length, hidden_size)\n        x = self.fc1(output)\n\n        # x shape == (batch_size * max_length, hidden_size)\n        x = tf.reshape(x, (-1, x.shape[2]))\n\n        # output shape == (batch_size * max_length, class_types)\n        x = self.fc2(x)\n\n        return x, state, attention_weights\n\n</code></pre>\n<p>Edit 3:\nI managed to solve this partly by defining a custom loss function, therefore the model still outputs very many vectors, but it can be trained without tiling the data.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 202}]
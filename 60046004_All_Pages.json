[{"items": [{"tags": ["tensorflow", "keras", "tensorflow2.0"], "owner": {"account_id": 963556, "reputation": 1375, "user_id": 987397, "user_type": "registered", "accept_rate": 32, "profile_image": "https://www.gravatar.com/avatar/fa7ae7d9bd13c2d04335c3209865c262?s=256&d=identicon&r=PG", "display_name": "Derk", "link": "https://stackoverflow.com/users/987397/derk"}, "is_answered": false, "view_count": 138, "answer_count": 2, "score": 0, "last_activity_date": 1592431639, "creation_date": 1580758499, "last_edit_date": 1580823572, "question_id": 60046004, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60046004/tf-keras-model-fit-enormous-difference-between-train-loss-and-val-loss-on-the", "title": "tf.keras model.fit(): enormous difference between train loss and val loss on the same data", "body": "<p>Tensorflow version 2.1</p>\n\n<p>See the colab notebook to reproduce the issue: <a href=\"https://drive.google.com/file/d/1Fvc6G_9v5mek015cai7qYT6HoY-fLkzk/view?usp=sharing\" rel=\"nofollow noreferrer\">https://drive.google.com/file/d/1Fvc6G_9v5mek015cai7qYT6HoY-fLkzk/view?usp=sharing</a></p>\n\n<p>When the training loss goes down the val_loss does not change, although this is exactly the same data.</p>\n\n<blockquote>\n  <p>Train on 2 samples, validate on 2 samples<br>\n  Epoch 1/30 2/2 [==============================] - 3s - 2s/sample - loss: 0.4630 - val_loss: 302.4763<br>\n  Epoch 2/30 2/2 [==============================] -1s - 457ms/sample - loss: 0.8565 - val_loss: 496.9578<br>\n  Epoch 3/30 2/2 [==============================] - 1s - 457ms/sample - loss: 0.7886 - val_loss: 1050.9148<br>\n  Epoch 4/30 2/2 [==============================] - 1s - 450ms/sample - loss: 0.1080 - val_loss: 744.4895<br>\n  Epoch 5/30 2/2 [==============================] - 1s - 474ms/sample - loss: 0.1144 - val_loss: 1353.2678<br>\n  Epoch 6/30 2/2 [==============================] - 1s - 465ms/sample - loss: 0.0402 - val_loss: 3237.9683<br>\n  Epoch 7/30 2/2 [==============================] - 1s - 465ms/sample - loss: 0.0635 - val_loss: 3946.7822<br>\n  Epoch 8/30 2/2 [==============================] - 1s - 470ms/sample - loss: 0.0355 - val_loss: 4054.5461<br>\n  Epoch 9/30 2/2 [==============================] - 1s - 462ms/sample - loss: 0.0345 - val_loss: 4991.5400</p>\n</blockquote>\n\n<p>How is this possible?\nThe code is pretty straightforward:</p>\n\n<pre><code>ResNet18, preprocess_input = Classifiers.get('resnet18')\nbase_model = ResNet18(input_shape=(180, 320, 3), weights=None, include_top=False)\nx = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput = tf.keras.layers.Dense(8)(x)\nmodel = tf.keras.models.Model(inputs=base_model.input, outputs=output)\nmodel.compile(optimizer='adam', loss='mse')\ndata = np.random.rand(2, 180, 320, 3)\nlabels = np.random.rand(2, 8)\n\nmodel.fit(data, labels, validation_data=(data,labels), batch_size=2, epochs=30)\n</code></pre>\n\n<p>There are known issues with keras and batch normalization (see for example keras-team/keras#6977). This is probably related, but I don't see directly how. What do I have to change to make this working as expected? Is this something in the included package <a href=\"https://github.com/qubvel/classification_models\" rel=\"nofollow noreferrer\">https://github.com/qubvel/classification_models</a> or where to solve it?</p>\n\n<p>Edit: the behaviour of batch normalization is changed as of TF 2.0, so the other issues might not be related, see <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization</a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 288}]
[{"items": [{"tags": ["python", "machine-learning", "keras", "tensorflow2.0", "tf.keras"], "owner": {"account_id": 438612, "reputation": 22264, "user_id": 826983, "user_type": "registered", "accept_rate": 69, "profile_image": "https://i.stack.imgur.com/B9PSD.jpg?s=256&g=1", "display_name": "Stefan Falk", "link": "https://stackoverflow.com/users/826983/stefan-falk"}, "is_answered": true, "view_count": 15872, "accepted_answer_id": 61348633, "answer_count": 1, "score": 7, "last_activity_date": 1598361461, "creation_date": 1587038439, "last_edit_date": 1587487976, "question_id": 61249708, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61249708/valueerror-no-gradients-provided-for-any-variable-tensorflow-2-0-keras", "title": "ValueError: No gradients provided for any variable - Tensorflow 2.0/Keras", "body": "<p>I am trying to implement a simple sequence-to-sequence model using Keras. However, I keep seeing the following <code>ValueError</code>:</p>\n\n<pre><code>ValueError: No gradients provided for any variable: ['simple_model/time_distributed/kernel:0', 'simple_model/time_distributed/bias:0', 'simple_model/embedding/embeddings:0', 'simple_model/conv2d/kernel:0', 'simple_model/conv2d/bias:0', 'simple_model/dense_1/kernel:0', 'simple_model/dense_1/bias:0'].\n</code></pre>\n\n<p>Other questions like <a href=\"https://stackoverflow.com/questions/41689451/valueerror-no-gradients-provided-for-any-variable\">this</a> or looking at <a href=\"https://github.com/tensorflow/tensorflow/issues/1511\" rel=\"noreferrer\">this issue</a> on Github suggests that this might have something to do with the cross-entropy loss function; but I fail to see what I am doing wrong here.</p>\n\n<p>I do not think that this is the problem, but I want to mention that I am on a nightly build of TensorFlow, <code>tf-nightly==2.2.0.dev20200410</code> to be precise.</p>\n\n<p>This following code is a standalone example and should reproduce the exception from above:</p>\n\n<pre><code>import random\nfrom functools import partial\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow_datasets.core.features.text import SubwordTextEncoder\n\nEOS = '&lt;eos&gt;'\nPAD = '&lt;pad&gt;'\n\nRESERVED_TOKENS = [EOS, PAD]\nEOS_ID = RESERVED_TOKENS.index(EOS)\nPAD_ID = RESERVED_TOKENS.index(PAD)\n\ndictionary = [\n    'verstehen',\n    'verstanden',\n    'vergessen',\n    'verlegen',\n    'verlernen',\n    'vertun',\n    'vertan',\n    'verloren',\n    'verlieren',\n    'verlassen',\n    'verhandeln',\n]\n\ndictionary = [word.lower() for word in dictionary]\n\n\nclass SimpleModel(keras.models.Model):\n\n    def __init__(self, params, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self.params = params\n        self.out_layer = keras.layers.Dense(1, activation='softmax')\n\n        self.model_layers = [\n            keras.layers.Embedding(params['vocab_size'], params['vocab_size']),\n            keras.layers.Lambda(lambda l: tf.expand_dims(l, -1)),\n            keras.layers.Conv2D(1, 4),\n            keras.layers.MaxPooling2D(1),\n            keras.layers.Dense(1, activation='relu'),\n            keras.layers.TimeDistributed(self.out_layer)\n        ]\n\n    def call(self, example, training=None, mask=None):\n        x = example['inputs']\n        for layer in self.model_layers:\n            x = layer(x)\n        return x\n\n\ndef sample_generator(text_encoder: SubwordTextEncoder, max_sample: int = None):\n    count = 0\n\n    while True:\n        random.shuffle(dictionary)\n\n        for word in dictionary:\n\n            for i in range(1, len(word)):\n\n                inputs = word[:i]\n                targets = word\n\n                example = dict(\n                    inputs=text_encoder.encode(inputs) + [EOS_ID],\n                    targets=text_encoder.encode(targets) + [EOS_ID],\n                )\n                count += 1\n\n                yield example\n\n                if max_sample is not None and count &gt;= max_sample:\n                    print('Reached max_samples (%d)' % max_sample)\n                    return\n\n\ndef make_dataset(generator_fn, params, training):\n\n    dataset = tf.data.Dataset.from_generator(\n        generator_fn,\n        output_types={\n            'inputs': tf.int64,\n            'targets': tf.int64,\n        }\n    ).padded_batch(\n        params['batch_size'],\n        padded_shapes={\n            'inputs': (None,),\n            'targets': (None,)\n        },\n    )\n\n    if training:\n        dataset = dataset.map(partial(prepare_example, params=params)).repeat()\n\n    return dataset\n\n\ndef prepare_example(example: dict, params: dict):\n    # Make sure targets are one-hot encoded\n    example['targets'] = tf.one_hot(example['targets'], depth=params['vocab_size'])\n    return example\n\n\ndef main():\n\n    text_encoder = SubwordTextEncoder.build_from_corpus(\n        iter(dictionary),\n        target_vocab_size=1000,\n        max_subword_length=6,\n        reserved_tokens=RESERVED_TOKENS\n    )\n\n    generator_fn = partial(sample_generator, text_encoder=text_encoder, max_sample=10)\n\n    params = dict(\n        batch_size=20,\n        vocab_size=text_encoder.vocab_size,\n        hidden_size=32,\n        max_input_length=30,\n        max_target_length=30\n    )\n\n    model = SimpleModel(params)\n\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n    )\n\n    train_dataset = make_dataset(generator_fn, params, training=True)\n    dev_dataset = make_dataset(generator_fn, params, training=False)\n\n    # Peek data\n    for train_batch, dev_batch in zip(train_dataset, dev_dataset):\n        print(train_batch)\n        print(dev_batch)\n        break\n\n    model.fit(\n        train_dataset,\n        epochs=1000,\n        steps_per_epoch=100,\n        validation_data=dev_dataset,\n        validation_steps=100,\n    )\n\n\nif __name__ == '__main__':\n    main()\n\n</code></pre>\n\n<h3>Update</h3>\n\n<ul>\n<li>Gist <a href=\"https://gist.github.com/stefan-falk/42ef89c6636fd9f91fc471584659512f\" rel=\"noreferrer\">link</a></li>\n<li>Github issue <a href=\"https://github.com/tensorflow/tensorflow/issues/38631\" rel=\"noreferrer\">link</a></li>\n</ul>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 86}]
[{"items": [{"tags": ["python", "keras", "deep-learning"], "owner": {"account_id": 22652516, "reputation": 1, "user_id": 16826919, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gjg-LGjY1CRXfrP6NZPB7z9-jZlISQK5t0mXz76=k-s256", "display_name": "Retro Li", "link": "https://stackoverflow.com/users/16826919/retro-li"}, "is_answered": false, "view_count": 32, "answer_count": 0, "score": 0, "last_activity_date": 1630807146, "creation_date": 1630807146, "question_id": 69060096, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/69060096/shufflenet-v2-suddenly-classified-the-vast-majority-of-samples-into-the-same-cat", "title": "ShuffleNet v2 suddenly classified the vast majority of samples into the same category during training", "body": "<p>The problem was discovered by checking the confusion matrix. Before looking at the confusion matrix, the problem I observed was a sharp drop in accuracy. I have tried the following solutions, but this kind of problem has not been fundamentally solved.</p>\n<ol>\n<li>Adjust the learning rate, from a fixed learning rate to adjusting with the progress of training and changes in accuracy.</li>\n<li>Change the activation function and change ReLU to LeakyReLU.</li>\n<li>Add the clipnorm parameter in the optimizer.</li>\n<li>Add the clip_by_value() function to the output of the model to limit the minimum value and avoid the result of all 0s.\nAfter making the above changes, the problem of gradient explosion has been solved, but the situation described in the question has not been avoided.The code and confusion matrix picture are attached below.</li>\n</ol>\n<pre><code>    newModel = ShuffleNetV2(input_shape=(224, 224, 1), scale_factor=1, classes=3)\n    if os.listdir('/Users/returnyg/PycharmProjects/MicroExpressionRecognition/models/save_weight').count('max_acc_' + datasetName + '.h5') &gt; 0:\n        print(&quot;\u52a0\u8f7d\u4e0a\u6b21\u51c6\u786e\u7387\u6700\u9ad8\u7684\u6743\u91cd&quot;)\n        newModel.load_weights(\n            '/Users/returnyg/PycharmProjects/MicroExpressionRecognition/models/save_weight/max_acc_' + datasetName + '.h5')\n        print(&quot;\u6743\u91cd\u52a0\u8f7d\u6210\u529f&quot;)\n    datasetName = datasetName\n    startTime = time.time()\n    print(&quot;\u5f53\u524d\u5728{}\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u3002&quot;.format(datasetName))\n    loss_fuc = tf.keras.losses.CategoricalCrossentropy()\n    val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n    # optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True, name='SGD')\n    # optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, clipnorm=1., amsgrad=False, name='Adam')\n    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, rho=0.9, momentum=0.0, epsilon=1e-7, clipnorm=1., centered=True, name=&quot;RMSprop&quot;)\n\n    current_time = datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;)\n    train_log_dir = '/Users/returnyg/PycharmProjects/MicroExpressionRecognition/models/log/train' + current_time\n    test_log_dir = '/Users/returnyg/PycharmProjects/MicroExpressionRecognition/models/log/test' + current_time\n    cm_train_log_dir = '/Users/returnyg/PycharmProjects/MicroExpressionRecognition/models/log/cm/train' + current_time\n    cm_val_log_dir = '/Users/returnyg/PycharmProjects/MicroExpressionRecognition/models/log/cm/val' + current_time\n    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n    test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n    cm_train_summary_writer = tf.summary.create_file_writer(cm_train_log_dir)\n    cm_val_summary_writer = tf.summary.create_file_writer(cm_val_log_dir)\n\n    def loss(model, x, y):\n        y_ = model(x)\n        y_pred = tf.clip_by_value(y_, 10e-8, 1.-10e-8)\n        return loss_fuc(y_true=y, y_pred=y_pred)\n\n    def grad(model, inputs, targets):\n        with tf.GradientTape() as tape:\n            loss_value = loss(model, inputs, targets)\n        return loss_value, tape.gradient(loss_value, model.trainable_variables)\n\n    train_loss_result = []\n    train_accuracy_result = []\n    max_acc = 0\n    for epoch in range(0, len(dataset)):\n        print(&quot;\u73b0\u5728\u662f\u7b2c{}\u4e2aepoch\uff0c\u5171\u6709{}\u4e2aepoch\u3002&quot;.format(epoch + 1, len(dataset)))\n        testset = []\n        epoch_loss_avg = tf.keras.metrics.Mean()\n        epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n            for x, y in zip(trainfaceslist, trainemotionslist):\n            train_y_true.append(np.argmax(y, axis=1).tolist())\n            train_y_pred.append(np.argmax(newModel(x), axis=1).tolist())\n            loss_value, grads = grad(newModel, x, y)\n            optimizer.apply_gradients(zip(grads, newModel.trainable_variables))\n\n            epoch_loss_avg(loss_value)\n            epoch_accuracy(y, newModel(x))\n\n            if step % 100 == 0:\n                print(&quot;\u5f53\u524d\u662f\u7b2c{}\u4e2abatch\uff0c\u8fd8\u6709{}\u4e2abatch\u5f85\u505a&quot;.format(step, len(trainemotionslist_withBatch) - step))\n\n            with train_summary_writer.as_default():\n                tf.summary.scalar('loss', epoch_loss_avg.result(), step=epoch)\n                tf.summary.scalar('accuracy', epoch_accuracy.result(), step=epoch)\n            step = step + 1\n\n        train_loss_result.append(epoch_loss_avg.result())\n        train_accuracy_result.append(epoch_accuracy.result())\n        print(&quot;Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}&quot;.format(epoch + 1, epoch_loss_avg.result(), epoch_accuracy.result()))\n\n        train_y_true = flatten(train_y_true)\n        train_y_pred = flatten(train_y_pred)\n        cm = confusion_matrix(y_true=train_y_true, y_pred=train_y_pred, labels=class_name)\n        plt.figure()\n        now_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n        train_cm_image = plot_confusion_matrix(cm, now_time=now_time, epoch=epoch+1, status=&quot;Train&quot;, normalize=False, target_names=class_name, title=&quot;Confusion Matrix&quot;)\n        with cm_train_summary_writer.as_default():\n            tf.summary.image(&quot;Train Confusion Matrix&quot;, train_cm_image, step=epoch + 1)\n\n        if epoch_accuracy.result() &gt; max_acc:\n            print(f&quot;\u7b2c{epoch + 1}\u4e2aepoch\u7684\u8bad\u7ec3\u51c6\u786e\u7387\u4e3a\uff1a{epoch_accuracy.result():.3%}\uff0c\u6bd4\u672c\u6b21\u8bad\u7ec3\u5386\u53f2\u6700\u9ad8\u51c6\u786e\u7387{max_acc:.3%}\u9ad8\uff0c\u4fdd\u5b58\u6743\u91cd\u3002&quot;)\n            max_acc = epoch_accuracy.result()\n            newModel.save_weights('/Users/returnyg/PycharmProjects/MicroExpressionRecognition/models/save_weight/max_acc_' + datasetName + '.h5', overwrite=True, save_format='h5')\n\n        val_y_true = []\n        val_y_pred = []\n        for x_val, y_val in zip(testfaceslist, testemotionslist):\n            val_logits = newModel(x_val, training=False)\n            val_acc_metric.update_state(y_val, val_logits)\n            val_y_true.append(np.argmax(y_val, axis=1).tolist())\n            val_y_pred.append(np.argmax(val_logits, axis=1).tolist())\n\n        val_acc = val_acc_metric.result()\n        val_acc_metric.reset_states()\n\n        val_y_true = flatten(val_y_true)\n        val_y_pred = flatten(val_y_pred)\n        cm = confusion_matrix(y_true=val_y_true, y_pred=val_y_pred, labels=class_name)\n        now_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n        plt.figure()\n        val_cm_image = plot_confusion_matrix(cm, now_time=now_time, epoch=epoch+1, status=&quot;Test&quot;, normalize=False, target_names=class_name, title=&quot;Confusion Matrix&quot;)\n        with cm_val_summary_writer.as_default():\n            tf.summary.image(&quot;Val Confusion Matrix&quot;, val_cm_image, step=epoch + 1)\n\n        with test_summary_writer.as_default():\n            tf.summary.scalar('accuracy', val_acc, step=epoch + 1)\n\n        print(&quot;Validation acc: %.4f&quot; % (float(val_acc),))\n        learning_rate = dynamic_learning_rate(epoch, learning_rate, training_accuracy=float(epoch_accuracy.result()))\n        if float(epoch_accuracy.result()) &lt; 0.45:\n            newModel.save(&quot;/Users/returnyg/PycharmProjects/MicroExpressionRecognition/models/error_model&quot;)\n            sys.exit(&quot;Bad Model! Please Check Weight and Loss!&quot;)\n        print(f&quot;\u8fd0\u884c\u5b8c\u7b2c{epoch+1}\u4e2aepoch\u540e\uff0c\u5b66\u4e60\u7387\u66f4\u65b0\u4e3a{learning_rate:.4f}&quot;)\n\n    endTime = time.time()\n    print(&quot;\u8fd0\u884c\u603b\u65f6\u957f\u4e3a{}&quot;.format(endTime - startTime))\n    return train_accuracy_result, train_loss_result\n</code></pre>\n<p>The code omits the data processing part.\n<a href=\"https://i.stack.imgur.com/H2M6J.png\" rel=\"nofollow noreferrer\">Normal confusion matrix</a>\n<a href=\"https://i.stack.imgur.com/FL9kx.png\" rel=\"nofollow noreferrer\">Problematic confusion matrix</a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 20}]
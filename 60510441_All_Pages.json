[{"items": [{"tags": ["python", "tensorflow", "reinforcement-learning"], "owner": {"account_id": 17857149, "reputation": 21, "user_id": 12971481, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bd6f25cafc77f872a58f4fab91177055?s=256&d=identicon&r=PG&f=1", "display_name": "jh1783", "link": "https://stackoverflow.com/users/12971481/jh1783"}, "is_answered": false, "view_count": 1140, "answer_count": 2, "score": 0, "last_activity_date": 1612142472, "creation_date": 1583248990, "last_edit_date": 1596590572, "question_id": 60510441, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60510441/implementing-a3c-on-tensorflow-2", "title": "Implementing A3C on TensorFlow 2", "body": "<p>After finishing Coursera's Practical RL course on A3C, I'm trying to implement my own A3C agent using tensorflow 2. To start, I'm training it on the Cartpole environment but I can't get good results. For now, I've already launched several training with the following code, changing the entropy coefficient to see its impact (the results are shown below). Does it come from my implementation, or is it more a fine-tuning issue ?</p>\n<pre><code>class A3C:\n  def __init__(self, state_dim, n_actions, optimizer=tf.keras.optimizers.Adam(1e-3)):\n    self.state_input = Input(shape=state_dim)\n    self.x = Dense(256, activation='relu')(self.state_input)\n\n    self.head_v = Dense(1, activation='linear')(self.x)\n    self.head_p = Dense(n_actions, activation='linear')(self.x)\n\n    self.network = tf.keras.Model(inputs=[self.state_input], outputs=[self.head_v, self.head_p])\n    self.optimizer = optimizer\n\n  def forward(self, state):\n    return self.network(state)\n\n  def sample(self, logits):\n    policy = np.exp(logits.numpy()) / np.sum(np.exp(logits.numpy()), axis=-1, keepdims=True)\n    return np.array([np.random.choice(len(p), p=p) for p in policy])\n\ndef evaluate(agent, env, n_games=1): &quot;&quot;&quot;Plays an a game from start till done, returns per-game rewards &quot;&quot;&quot;\n  game_rewards = []\n  for _ in range(n_games):\n    state = env.reset()\n\n    total_reward = 0\n    while True:\n      action = agent.sample(agent.forward(np.array([state]))[1])[0]\n      state, reward, done, info = env.step(action)\n      total_reward += reward\n      if done: break\n\n    game_rewards.append(total_reward)\n  return game_rewards\n\nclass EnvBatch:\n  def __init__(self, n_envs = 10):\n    self.envs = [gym.make(env_id) for _ in range(n_envs)]\n    \n  def reset(self):\n    return np.array([env.reset() for env in self.envs])\n\n  def step(self, actions):\n    results = [env.step(a) for env, a in zip(self.envs, actions)]\n    new_obs, rewards, done, infos = map(np.array, zip(*results))\n    \n    for i in range(len(self.envs)):\n      if done[i]:\n        new_obs[i] = self.envs[i].reset()\n    \n    return new_obs, rewards, done, infos\n\n \n\nenv_id = &quot;CartPole-v0&quot;\nenv = gym.make(env_id)\nstate_dim = env.observation_space.shape\nn_actions = env.action_space.n\nagent = A3C(state_dim, n_actions)    \nenv_batch = EnvBatch(10)\nbatch_states = env_batch.reset()\ngamma=0.99\n\nrewards_history = []\nentropy_history = []\n\nfor i in trange(200000):\n  with tf.GradientTape() as t:\n    batch_values, batch_logits = agent.forward(batch_states)\n    batch_actions = agent.sample(batch_logits)\n    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n    batch_next_values, btach_next_logits = agent.forward(batch_next_states)\n    batch_next_values *= (1 - batch_dones)\n\n    probs = tf.nn.softmax(batch_logits)\n    logprobs = tf.nn.log_softmax(batch_logits)\n\n    logp_actions = tf.reduce_sum(logprobs * tf.one_hot(batch_actions, n_actions), axis=-1)\n\n    advantage = batch_rewards + gamma*batch_next_values - batch_values\n    entropy = -tf.reduce_sum(probs * logprobs, 1, name=&quot;entropy&quot;)\n\n    actor_loss =  - tf.reduce_mean(logp_actions * tf.stop_gradient(advantage)) - 0.005 * tf.reduce_mean(entropy)\n\n    target_state_values = batch_rewards + gamma*batch_next_values\n    critic_loss = tf.reduce_mean((batch_values - tf.stop_gradient(target_state_values))**2 )\n\n    loss = actor_loss + critic_loss\n\n  var_list = agent.network.trainable_variables\n  grads = t.gradient(loss,var_list)\n  agent.optimizer.apply_gradients(zip(grads, var_list))\n  batch_states = batch_next_states\n\n  entropy_history.append(np.mean(entropy))\n\n  if i % 500 == 0:\n    if i % 2500 == 0:\n      rewards_history.append(np.mean(evaluate(agent, env, n_games=3)))\n\n    clear_output(True)\n    plt.figure(figsize=[8, 4])\n    plt.subplot(1, 2, 1)\n    plt.plot(rewards_history, label='rewards')\n    plt.title(&quot;Session rewards&quot;)\n    plt.grid()\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(entropy_history, label='entropy')\n    plt.title(&quot;Policy entropy&quot;)\n    plt.grid()\n    plt.legend()\n    plt.show()\n</code></pre>\n<ol>\n<li><a href=\"https://i.stack.imgur.com/mIxQO.png\" rel=\"nofollow noreferrer\">Beta = 0.005 - Training 1</a></li>\n<li><a href=\"https://i.stack.imgur.com/yBxJ4.png\" rel=\"nofollow noreferrer\">Beta = 0.005 - Training 2</a></li>\n<li><a href=\"https://i.stack.imgur.com/aW657.png\" rel=\"nofollow noreferrer\">Beta = 0.005 - Training 3</a></li>\n<li><a href=\"https://i.stack.imgur.com/HZN8i.png\" rel=\"nofollow noreferrer\">Beta = 0.05 - Training 1</a></li>\n<li><a href=\"https://i.stack.imgur.com/GQ0p2.png\" rel=\"nofollow noreferrer\">Beta = 0.05 - Training 2</a></li>\n<li><a href=\"https://i.stack.imgur.com/vrd8U.png\" rel=\"nofollow noreferrer\">Beta = 0.05 - Training 3</a></li>\n</ol>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 245}]
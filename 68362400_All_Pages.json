[{"items": [{"tags": ["tensorflow", "loss"], "owner": {"account_id": 22197155, "reputation": 11, "user_id": 16438408, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/0ff4ec9c7eb6089ee83378c9f2e2f382?s=256&d=identicon&r=PG&f=1", "display_name": "biaocu", "link": "https://stackoverflow.com/users/16438408/biaocu"}, "is_answered": false, "view_count": 53, "answer_count": 1, "score": 1, "last_activity_date": 1627629874, "creation_date": 1626179200, "question_id": 68362400, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68362400/train-loss-not-decreasingconvert-pytorch-code-to-tensorflow", "title": "train loss not decreasing(convert pytorch code to tensorflow)", "body": "<p>I am training a model with roberta using transformers, and the train loss did not decrease after a few steps, I can not find out the reason, any suggestions will be thankful.</p>\n<p>here is the model:</p>\n<pre><code>class TriggerExtractor(keras.Model):\n    def __init__(self,\n                 bert_dir,\n                 dropout_prob=0.1,\n                 use_distant_trigger=True,\n                 **kwargs):\n        super(TriggerExtractor, self).__init__()\n\n        config_path = os.path.join(bert_dir, 'config.json')\n        assert os.path.exists(bert_dir) and os.path.exists(config_path), 'pretrained bert file does not exist'\n        self.bert_module = TFBertModel.from_pretrained(bert_dir)\n        self.bert_config = self.bert_module.config\n\n        self.use_distant_trigger = use_distant_trigger\n        out_dims = self.bert_config.hidden_size\n\n        if use_distant_trigger:\n            embedding_dim = kwargs.pop('embedding_dims', 256)\n            self.distant_trigger_embedding = tf.keras.layers.Embedding(input_dim=3, output_dim=embedding_dim, embeddings_initializer=keras.initializers.HeNormal)\n            out_dims += embedding_dim\n\n        mid_linear_dims = kwargs.pop('mid_linear_dims', 128)\n\n        self.mid_linear = keras.Sequential([\n            keras.layers.Dense(mid_linear_dims, input_shape=(out_dims,), activation=None),\n            keras.layers.ReLU(),\n            keras.layers.Dropout(dropout_prob)\n        ])\n\n        self.classifier = keras.layers.Dense(2, input_shape=(mid_linear_dims, ), activation=None)\n        self.criterion = keras.losses.BinaryCrossentropy()\n\n    def call(self, inputs):\n        # print('inputs:', inputs)\n        token_ids = inputs['token_ids']\n        attention_masks = inputs['attention_masks']\n        token_type_ids = inputs['token_type_ids']\n        distant_trigger = inputs['distant_trigger']\n        labels = inputs['labels']\n\n        bert_outputs = self.bert_module(\n            input_ids=token_ids,\n            attention_mask=attention_masks,\n            token_type_ids=token_type_ids\n        )\n\n        seq_out = bert_outputs[0]\n\n        if self.use_distant_trigger:\n            assert distant_trigger is not None, \\\n                'When using distant trigger features, distant trigger should be implemented'\n\n            distant_trigger_feature = self.distant_trigger_embedding(distant_trigger)\n            seq_out = keras.layers.concatenate([seq_out, distant_trigger_feature], axis=-1)\n\n        seq_out = self.mid_linear(seq_out)\n        logits = keras.activations.sigmoid(self.classifier(seq_out))\n\n        out = (logits,)\n        if labels is not None:\n            labels = tf.cast(labels, dtype=tf.float32)\n            loss = self.criterion(logits, labels)\n            out = (loss,) + out\n        return out\n</code></pre>\n<p>here is the train code:</p>\n<pre><code>train_loader = tf.data.Dataset.from_tensor_slices(train_dataset.__dict__).shuffle(10000).batch(opt.train_batch_size)\n\n    for epoch in range(opt.train_epochs):\n        for step, batch_data in enumerate(train_loader):\n\n            with tf.GradientTape() as tape:\n                loss = model(batch_data)\n\n            grads = tape.gradient(loss, model.variables)\n\n            # for (grad, var) in zip(grads, model.variables):\n            #     if grad is not None:\n            #         name = var.name\n            #         space = name.split('/')\n            #         if space[0] == 'tf_bert_model':\n            #             optimizer_bert.apply_gradients([(tf.clip_by_norm(grad, opt.max_grad_norm), var)])\n            #         else:\n            #             optimizer_other.apply_gradients([(tf.clip_by_norm(grad, opt.max_grad_norm), var)])\n\n\n            optimizer.apply_gradients([\n                (tf.clip_by_norm(grad, opt.max_grad_norm), var)\n                for (grad, var) in zip(grads, model.variables)\n                if grad is not None\n            ])\n\n            global_step += 1\n            if global_step % log_loss_steps == 0:\n                avg_loss /= log_loss_steps\n                logger.info('epoch:%d Step: %d / %d ----&gt; total loss: %.5f' % (epoch, global_step, t_total, avg_loss))\n                avg_loss = 0.\n            else:\n                avg_loss += loss[0]\n</code></pre>\n<p>here is the result:</p>\n<pre><code>07/13/2021 20:09:22 - INFO - src_final.utils.trainer -   ***** Running training *****\n07/13/2021 20:09:22 - INFO - src_final.utils.trainer -     Num Epochs = 10\n07/13/2021 20:09:22 - INFO - src_final.utils.trainer -     Total training batch size = 8\n07/13/2021 20:09:22 - INFO - src_final.utils.trainer -     Total optimization steps = 3070\n07/13/2021 20:09:22 - INFO - src_final.utils.trainer -   Save model in 307 steps; Eval model in 307 steps\n07/13/2021 20:09:36 - INFO - src_final.utils.trainer -   epoch:0 Step: 20 / 3070 ----&gt; total loss: 1.73774\n07/13/2021 20:09:50 - INFO - src_final.utils.trainer -   epoch:0 Step: 40 / 3070 ----&gt; total loss: 0.04631\n07/13/2021 20:10:03 - INFO - src_final.utils.trainer -   epoch:0 Step: 60 / 3070 ----&gt; total loss: 0.04586\n07/13/2021 20:10:17 - INFO - src_final.utils.trainer -   epoch:0 Step: 80 / 3070 ----&gt; total loss: 0.04734\n07/13/2021 20:10:31 - INFO - src_final.utils.trainer -   epoch:0 Step: 100 / 3070 ----&gt; total loss: 0.04554\n07/13/2021 20:10:44 - INFO - src_final.utils.trainer -   epoch:0 Step: 120 / 3070 ----&gt; total loss: 0.04733\n07/13/2021 20:10:58 - INFO - src_final.utils.trainer -   epoch:0 Step: 140 / 3070 ----&gt; total loss: 0.04613\n07/13/2021 20:11:12 - INFO - src_final.utils.trainer -   epoch:0 Step: 160 / 3070 ----&gt; total loss: 0.04643\n07/13/2021 20:11:26 - INFO - src_final.utils.trainer -   epoch:0 Step: 180 / 3070 ----&gt; total loss: 0.04613\n07/13/2021 20:11:39 - INFO - src_final.utils.trainer -   epoch:0 Step: 200 / 3070 ----&gt; total loss: 0.04643\n07/13/2021 20:11:53 - INFO - src_final.utils.trainer -   epoch:0 Step: 220 / 3070 ----&gt; total loss: 0.04553\n07/13/2021 20:12:07 - INFO - src_final.utils.trainer -   epoch:0 Step: 240 / 3070 ----&gt; total loss: 0.04582\n07/13/2021 20:12:21 - INFO - src_final.utils.trainer -   epoch:0 Step: 260 / 3070 ----&gt; total loss: 0.04642\n07/13/2021 20:12:35 - INFO - src_final.utils.trainer -   epoch:0 Step: 280 / 3070 ----&gt; total loss: 0.04582\n07/13/2021 20:12:48 - INFO - src_final.utils.trainer -   epoch:0 Step: 300 / 3070 ----&gt; total loss: 0.04672\n07/13/2021 20:12:53 - INFO - src_final.utils.trainer -   Saving model &amp; optimizer &amp; scheduler checkpoint to ./out/final/trigger/roberta_wwm_distant_trigger_pgd/checkpoint-307\n07/13/2021 20:13:05 - INFO - src_final.utils.trainer -   epoch:1 Step: 320 / 3070 ----&gt; total loss: 0.04582\n07/13/2021 20:13:18 - INFO - src_final.utils.trainer -   epoch:1 Step: 340 / 3070 ----&gt; total loss: 0.04552\n07/13/2021 20:13:32 - INFO - src_final.utils.trainer -   epoch:1 Step: 360 / 3070 ----&gt; total loss: 0.04672\n07/13/2021 20:13:46 - INFO - src_final.utils.trainer -   epoch:1 Step: 380 / 3070 ----&gt; total loss: 0.04762\n07/13/2021 20:13:59 - INFO - src_final.utils.trainer -   epoch:1 Step: 400 / 3070 ----&gt; total loss: 0.04642\n07/13/2021 20:14:13 - INFO - src_final.utils.trainer -   epoch:1 Step: 420 / 3070 ----&gt; total loss: 0.04612\n07/13/2021 20:14:27 - INFO - src_final.utils.trainer -   epoch:1 Step: 440 / 3070 ----&gt; total loss: 0.04582\n07/13/2021 20:14:41 - INFO - src_final.utils.trainer -   epoch:1 Step: 460 / 3070 ----&gt; total loss: 0.04702\n07/13/2021 20:14:54 - INFO - src_final.utils.trainer -   epoch:1 Step: 480 / 3070 ----&gt; total loss: 0.04672\n07/13/2021 20:15:08 - INFO - src_final.utils.trainer -   epoch:1 Step: 500 / 3070 ----&gt; total loss: 0.04672\n07/13/2021 20:15:22 - INFO - src_final.utils.trainer -   epoch:1 Step: 520 / 3070 ----&gt; total loss: 0.04552\n07/13/2021 20:15:36 - INFO - src_final.utils.trainer -   epoch:1 Step: 540 / 3070 ----&gt; total loss: 0.04552\n07/13/2021 20:15:49 - INFO - src_final.utils.trainer -   epoch:1 Step: 560 / 3070 ----&gt; total loss: 0.04672\n07/13/2021 20:16:03 - INFO - src_final.utils.trainer -   epoch:1 Step: 580 / 3070 ----&gt; total loss: 0.04552\n</code></pre>\n<p>PS: I am converting the pytorch code to tensorflow for some reason, the pytorch version is all right(<a href=\"https://github.com/WuHuRestaurant/xf_event_extraction2020Top1\" rel=\"nofollow noreferrer\">https://github.com/WuHuRestaurant/xf_event_extraction2020Top1</a>). thanks again</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 105}]
[{"items": [{"tags": ["tensorflow", "nlp", "bert-language-model", "tensorflow-hub", "gradienttape"], "owner": {"account_id": 9968983, "reputation": 2859, "user_id": 7375754, "user_type": "registered", "accept_rate": 77, "profile_image": "https://www.gravatar.com/avatar/830f6d236ab0502a68fd9b9daf7f729f?s=256&d=identicon&r=PG&f=1", "display_name": "Jane Sully", "link": "https://stackoverflow.com/users/7375754/jane-sully"}, "is_answered": true, "view_count": 104, "answer_count": 1, "score": 0, "last_activity_date": 1629269971, "creation_date": 1629227847, "last_edit_date": 1629245229, "question_id": 68822906, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68822906/internal-error-tried-to-take-gradients-or-similar-of-a-variable-without-handl", "title": "Internal error: Tried to take gradients (or similar) of a variable without handle data in Tensorflow", "body": "<p>I am finetuning BERT for a binary sentiment analysis class using Tensorflow. I want to use a custom training loop/loss function. However, when I train the model I get the following error: <code>ValueError: Internal error: Tried to take gradients (or similar) of a variable without handle data: Tensor(&quot;transformer_encoder/StatefulPartitionedCall:1019&quot;, shape=(), dtype=resource)</code>.</p>\n<p>To debug, I tried simplifying my training loop to just compute standard binary cross entropy, which should be equivalent to if I called model.fit() with binary cross entropy as the loss function (which works completely fine). However, I get the same error as above when running this simplified training loop and I am not sure what's causing it. Note: I am using tensorflow 2.3.0.</p>\n<p>Here is the model:</p>\n<pre><code>def create_model():\n  max_seq_length = 512\n  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                        name=&quot;input_word_ids&quot;)\n  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                     name=&quot;input_mask&quot;)\n  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                      name=&quot;input_type_ids&quot;)\n  \n  bert_layer = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2&quot;, trainable=True)\n  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])\n  drop = tf.keras.layers.Dropout(0.3)(pooled_output)\n  output = tf.keras.layers.Dense(1, activation='sigmoid', name=&quot;output&quot;)(drop)\n\n  model = tf.keras.Model(\n      inputs={\n          'input_word_ids': input_word_ids,\n          'input_mask': input_mask,\n          'input_type_ids': input_type_ids\n      },\n      outputs= output \n  )\n\n  return model\n</code></pre>\n<p>Here is the training loop function. The issue seems to come up when running <code>ypred = model(train_x)</code> inside tf.GradientTape():</p>\n<pre><code>def train_step(train_batch):\n  train_x, train_y = train_batch\n  with tf.GradientTape() as tape:\n    ypred = model(train_x)\n    loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(train_y, ypred))\n  grads = tape.gradient(loss, model.trainable_weights)\n  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n  return loss\n</code></pre>\n<p>Again, this seems to only happen with tf.GradientTape(), since model.fit() does not result in any issues.</p>\n<pre><code>model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n          loss=tf.keras.losses.BinaryCrossentropy(),\n          metrics=[tf.keras.metrics.BinaryAccuracy()])\n\nmodel.fit(train_data,\n          validation_data=valid_data,\n          epochs=epochs,\n          verbose=1)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 4}]
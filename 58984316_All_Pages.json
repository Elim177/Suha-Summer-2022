[{"items": [{"tags": ["tensorflow", "conv-neural-network", "gradient"], "owner": {"account_id": 16973519, "reputation": 11, "user_id": 12397233, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-nF9i2otbNIw/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rcx_sGkNIE_FdLSW_ziXjHGAl_2Sw/photo.jpg?sz=256", "display_name": "Asif Khan", "link": "https://stackoverflow.com/users/12397233/asif-khan"}, "is_answered": false, "view_count": 731, "answer_count": 1, "score": 1, "last_activity_date": 1579196384, "creation_date": 1574373167, "last_edit_date": 1579196384, "question_id": 58984316, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58984316/gradients-are-none-for-custom-convolution-layer", "title": "Gradients are None for Custom Convolution Layer", "body": "<p>I have implemented the Basic MNIST model with Custom convolution layer as shown below. The problem is that the <strong>Gradients are always 'None' for the Custom Layer</strong> and so the learning does not happens during back propagation,  as the Grad has None values.\nI have debugged the outputs of the layers during forward pass and they are OK. \nHere is the sample code, for simplicity I have passed image of 'Ones' and have just returned the matrix from the custom layer.\nI have tried my best but could make it work any help is very much appreciated in advance\nfollowing code is executable and raises the</p>\n\n<p><strong>warning</strong>\n:tensorflow:Gradients do not exist for variables ['cnn/custom_conv2d/kernel:0', 'cnn/custom_conv2d/bias:0', 'cnn/custom_conv2d_1/kernel:0', 'cnn/custom_conv2d_1/bias:0', 'cnn/custom_conv2d_2/kernel:0', 'cnn/custom_conv2d_2/bias:0'] when minimizing the loss.</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\nfrom grpc.beta import interfaces\nclass CustomConv2D(tf.keras.layers.Conv2D):\n    def __init__(self, filters,\n                 kernel_size,\n                 strides=(1, 1),\n                 padding='valid',\n                 data_format=None,\n                 dilation_rate=(1, 1),\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='glorot_uniform',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 __name__ = 'CustomConv2D',\n                 **kwargs\n                 ):\n        super(CustomConv2D, self).__init__(\n            filters=filters,\n            kernel_size=kernel_size,\n            strides=strides,\n            padding=padding,\n            data_format=data_format,\n            dilation_rate=dilation_rate,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs )\n\n     def call(self, input):\n\n        (unrolled_mat, filters, shape) = self.prepare(input)\n#unrolled_mat=unrolled inputs\n#filters=unrolled kernels of the lAYER\n#convolution through unrolling\n        conv_result = tf.tensordot(unrolled_mat, filters, axes=1)\n        result=tf.convert_to_tensor(tf.reshape(conv_result, shape))\n        return result\n\n\n    def prepare(self, matrix):\n        batches,rows,cols,channels=matrix.shape\n        kernel_size = self.kernel_size[0]\n        unrolled_matrices=None\n        # start = timer()\n        for batch in range(batches):\n            unrolled_maps=None\n            for chanel in range(channels):\n                unrolled_map = self.unroll(batch, cols, kernel_size, matrix, rows,chanel)\n                if unrolled_maps is None:\n                    unrolled_maps = unrolled_map\n                else:\n                    unrolled_maps=np.append(unrolled_maps,unrolled_map,axis=1)\n            unrolled_maps = np.reshape(unrolled_maps,(-1,unrolled_maps.shape[0],unrolled_maps.shape[1]))\n            if unrolled_matrices is None:\n                unrolled_matrices = unrolled_maps\n            else:\n                unrolled_matrices = np.concatenate((unrolled_matrices, unrolled_maps))\n        kernels=self.get_weights()\n        kernels=np.reshape(kernels[0],(unrolled_matrices[0].shape[1],-1))\n        shp=(batches,rows-(kernel_size-1),cols-(kernel_size-1),self.filters)\n        matrix=unrolled_matrices\n        return (matrix, kernels, shp)\n\n    def unroll(self, batch, cols, kernel_size, matrix, rows, chanel):\n        # a=np.zeros((shape))\n        unrolled_feature_map = None\n        for x in range(0, rows - (kernel_size - 1)):\n            for y in range(0, (cols - (kernel_size - 1))):\n                temp_row = None  # flattened kernal at single position\n                for k in range(kernel_size):\n                    for l in range(kernel_size):\n                        if temp_row is None:\n                            temp_row = matrix[batch, x + k, y + l, chanel]\n                            # print(matrix[batch, x + k, y + l])\n                        else:\n                            temp_row = np.append(temp_row, matrix[batch, x + k, y + l, chanel])\n                            # print(matrix[batch, x + k, y + l])\n                if unrolled_feature_map is None:\n                    unrolled_feature_map = np.reshape(temp_row,\n                        (-1, kernel_size * kernel_size))  # first row of unrolled matrix added\n                else:\n                    unrolled_feature_map = np.concatenate((unrolled_feature_map, np.reshape(temp_row,\n                        (-1, kernel_size * kernel_size))))  # concatinate subsequent row to un_mat\n        unrolled_feature_map = np.reshape(unrolled_feature_map,( unrolled_feature_map.shape[0], unrolled_feature_map.shape[1]))\n        # print(unrolled_feature_map.shape)\n        matrix=unrolled_feature_map\n        return matrix\n\nclass CNN(tf.keras.Model):\n  def __init__(self):\n    super(CNN, self).__init__()\n    self.learning_rate = 0.001\n    self.momentum = 0.9\n    self.optimizer = tf.keras.optimizers.Adam(self.learning_rate, self.momentum)\n    self.conv1 = CustomConv2D(filters = 6, kernel_size= 3, activation = 'relu')  ## valid means no padding\n    self.pool1 = tf.keras.layers.MaxPool2D(pool_size=2) # default stride??-\n    self.conv2 = CustomConv2D(filters = 16, kernel_size = 3,  activation = 'relu')\n    self.pool2 = tf.keras.layers.MaxPool2D(pool_size = 2)\n    self.conv3 = CustomConv2D(filters=120, kernel_size=3,  activation='relu')\n    self.flatten = tf.keras.layers.Flatten()\n    self.fc1 = tf.keras.layers.Dense(units=82,kernel_initializer='glorot_uniform')\n    self.fc2 = tf.keras.layers.Dense(units=10, activation = 'softmax',kernel_initializer='glorot_uniform')\n  def call(self, x):\n      x = self.conv1(x)  # shap(32,26,26,6) all (6s 3s 6s 3s)\n      x = self.pool1(x)  # shap(32,13,13,6) all (6s)\n      x = self.conv2(x)  # shap(32,11,11,16) all(324s)\n      x = self.pool2(x)  # shap(32,5,5,16)\n      x = self.conv3(x)  # shap(32,3,3,120)all(46656)\n      x = self.flatten(x)  # shap(32,1080)\n      x = self.fc1(x)  # shap(32,82)\n      x = self.fc2(x)  # shap(32,10)\n      return x\n  def feedForward(self, image, label):\n            accuracy_object = tf.metrics.Accuracy()\n            loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n            with tf.GradientTape() as tape:\n                feedForwardCompuation = self(image, training=True)\n                self.loss_value = loss_object(label, feedForwardCompuation)\n            grads = tape.gradient(self.loss_value, self.variables)\n            self.optimizer.apply_gradients(zip(grads, self.variables))\n            accuracy = accuracy_object(tf.argmax(feedForwardCompuation, axis=1, output_type=tf.int32), label)\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\nx_train=x_train.astype('float32')\ny_train = y_train.astype('float32')\nimage=x_train[0].reshape((1,28,28,1)) \nlabel=y_train[0]\ncnn=CNN()\ncnn.feedForward(image,label)\n</code></pre>\n\n<p><strong>UPDATE:</strong> I am not using the builtin TF conv fucntion rather I am implementing my own custom convolution operation via Matrix unrolling method(unrolled map*unrolled filters). But the Tap.gradient returns \"None\" for the custom layers however when I use the builtin conv2d function of TF then it works fine!\nI have <strong>Added</strong> the actual code of the operation<br>\n<a href=\"https://i.stack.imgur.com/vZxMh.png\" rel=\"nofollow noreferrer\">Snapshot of grads while debugging </a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 34}]
[{"items": [{"tags": ["tensorflow", "keras", "deep-learning", "training-data", "model-fitting"], "owner": {"account_id": 17866167, "reputation": 11, "user_id": 12978479, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/bd16c5dc694d212349027f33f7d30923?s=256&d=identicon&r=PG&f=1", "display_name": "AndAgio", "link": "https://stackoverflow.com/users/12978479/andagio"}, "is_answered": false, "view_count": 733, "answer_count": 2, "score": 1, "last_activity_date": 1621593988, "creation_date": 1605890665, "question_id": 64933424, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64933424/error-using-keras-imagedatagenerator-with-custom-train-step-in-subclassed-model", "title": "Error using Keras ImageDataGenerator with custom train_step in subclassed model", "body": "<p>I am trying to code a knowledge distillation model using keras. I started from keras example <a href=\"https://keras.io/examples/vision/knowledge_distillation/\" rel=\"nofollow noreferrer\">here</a>. In order to train the model the train_step and test_step method were overwritten. Unlike keras example, I want to fit the model using an ImageDataGenerator object to preprocess the images in the CIFAR10 dataset. The problem is that whenever I call the model.fit function passing X_train and Y_train the training works fine, if instead I call model.fit passing the ImageDataGenerator.flow(X_train, Y_train, batch_size) the code returns the following error:</p>\n<p><em><strong>NotImplementedError: When subclassing the Model class, you should implement a call method.</strong></em></p>\n<p>I have also tried to modify the way that train_step handles the data input that it receives, but it seems that no approach has worked so far.</p>\n<p>Why is that so? Is there any issue with overwriting the train_step method of the Model class with ImageDataGenereator objects? Should the fit method of the class Model be overwritten too?</p>\n<p>To make things clear and reproducible here is the sample code:</p>\n<pre><code>import time\nimport copy\nimport tensorflow as tf\nimport keras\nfrom keras import regularizers\nfrom keras.engine import Model\nfrom keras.layers import Dropout, Flatten, Dense, Conv2D, MaxPooling2D, Activation, BatchNormalization\nfrom keras.models import Sequential\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom tensorflow.python.keras.engine import data_adapter\n# Imported from files\nimport settings_parser\nfrom utils import progressive_learning_rate\nfrom teacher import Teacher, build_teacher\nfrom student import Student, build_student\n\n\nclass Distiller(tf.keras.Model):\n    def __init__(self, student, teacher):\n        super(Distiller, self).__init__()\n        self.teacher = teacher\n        self.student = student\n\n\n    def compile(self, optimizer, metrics, student_loss_fn, distillation_loss_fn, alpha=0.1, temperature=3):\n        &quot;&quot;&quot; Configure the distiller.\n        Args:\n            optimizer: Keras optimizer for the student weights\n            metrics: Keras metrics for evaluation\n            student_loss_fn: Loss function of difference between student\n                predictions and ground-truth\n            distillation_loss_fn: Loss function of difference between soft\n                student predictions and soft teacher predictions\n            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n            temperature: Temperature for softening probability distributions.\n                Larger temperature gives softer distributions.\n        &quot;&quot;&quot;\n        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n        self.student_loss_fn = student_loss_fn\n        self.distillation_loss_fn = distillation_loss_fn\n        self.alpha = alpha\n        self.temperature = temperature\n\n    # @tf.function\n    def train_step(self, data):\n        # Treat data in different ways if it is a tuple or an iterator\n        x = None\n        y = None\n        if isinstance(data, tuple):\n            x, y = data\n        if isinstance(data, tf.keras.preprocessing.image.NumpyArrayIterator):\n            x, y = data.next()\n        # Forward pass of teacher\n        teacher_predictions = self.teacher(x, training=False)\n\n        with tf.GradientTape() as tape:\n            # Forward pass of student\n            student_predictions = self.student(x, training=True)\n            # Compute losses\n            student_loss = self.student_loss_fn(y, student_predictions)\n            distillation_loss = self.distillation_loss_fn(\n                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n            )\n            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n        # Compute gradients\n        trainable_vars = self.student.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update the metrics configured in `compile()`.\n        self.compiled_metrics.update_state(y, student_predictions)\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update(\n            {&quot;student_loss&quot;: student_loss, &quot;distillation_loss&quot;: distillation_loss}\n        )\n        return results\n\n    # @tf.function\n    def test_step(self, data):\n        # Treat data in different ways if it is a tuple or an iterator\n        x = None\n        y = None\n        if isinstance(data, tuple):\n            x, y = data\n        if isinstance(data, tf.keras.preprocessing.image.NumpyArrayIterator):\n            x, y = data.next()\n\n        # Compute predictions\n        y_prediction = self.student(x, training=False)\n        # Calculate the loss\n        student_loss = self.student_loss_fn(y, y_prediction)\n        # Update the metrics.\n        self.compiled_metrics.update_state(y, y_prediction)\n        # Return a dict of performance\n        results = {m.name: m.result() for m in self.metrics}\n        results.update({&quot;student_loss&quot;: student_loss})\n\n        return results\n\n\n#Define method to build the teacher model (VGG16)\ndef build_teacher():\n    input = keras.Input(shape=(32, 32, 3), name=&quot;img&quot;)\n    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(input)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    # Block 2\n    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Block 3\n    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    # Block 4\n    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Block 5\n    x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    # Block 6\n    x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    # Block 7\n    x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Block 8\n    x = Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    # Block 9\n    x = Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    # Block 10\n    x = Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Block 11\n    x = Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    # Block 12\n    x = Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    # Block 13\n    x = Conv2D(512, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(0.5)(x)\n    # Flatten and classification\n    x = Flatten()(x)\n    x = Dense(512)(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    # Out\n    x = Dense(10)(x)\n    output = Activation('softmax')(x)\n    # Define model from input and output\n    model = keras.Model(input, output, name=&quot;teacher&quot;)\n    print(model.summary())\n\n    return model\n\n\n#Define method to build the teacher model (VGG16)\ndef build_student():\n    input = keras.Input(shape=(32, 32, 3), name=&quot;img&quot;)\n    x = Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(input)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    # Block 2\n    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Block 3\n    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    # Block 4\n    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Block 5\n    x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.4)(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    # Flatten and classification\n    x = Flatten()(x)\n    x = Dense(512)(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    # Out\n    x = Dense(10)(x)\n    output = Activation('softmax')(x)\n    # Define model from input and output\n    model = keras.Model(input, output, name=&quot;student&quot;)\n    print(model.summary())\n\n    return model\n\nif __name__ == '__main__':\n    args = settings_parser.arg_parse()\n    print_during_epochs = True\n    student = build_student()\n    student_clone = build_student()\n    student_clone.set_weights(student.get_weights())\n    teacher = build_teacher()\n\n    (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n    X_train = X_train.astype('float32')\n    X_test = X_test.astype('float32')\n    Y_train = np_utils.to_categorical(y_train, 10)\n    Y_test = np_utils.to_categorical(y_test, 10)\n    train_datagen = ImageDataGenerator(\n        rescale=1. / 255,  # rescale input image\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images)\n\n    train_datagen.fit(X_train)\n    train_generator = train_datagen.flow(X_train, Y_train, batch_size=64)\n\n    test_datagen = ImageDataGenerator(rescale=1. / 255)\n    test_generator = test_datagen.flow(X_test, Y_test, batch_size=64)\n\n    # Train teacher as usual\n    teacher.compile(optimizer=keras.optimizers.SGD(),\n                    loss=keras.losses.categorical_crossentropy,\n                    metrics=['accuracy'])\n\n    # Train and evaluate teacher on data.\n    teacher.fit(train_generator, validation_data=test_generator, epochs=5, verbose=print_during_epochs)\n    loss, acc = teacher.evaluate(test_generator)\n    print(&quot;Teacher model, accuracy: {:5.2f}%&quot;.format(100 * acc))\n\n    # Train student as doen usually\n    student_clone.compile(optimizer=keras.optimizers.SGD(),\n                          loss=keras.losses.categorical_crossentropy,\n                          metrics=['accuracy'])\n    # Train and evaluate student trained from scratch.\n    student_clone.fit(train_generator, validation_data=test_generator, epochs=5, verbose=print_during_epochs)\n    loss, acc = student_clone.evaluate(test_generator)\n    print(&quot;Student scratch model, accuracy: {:5.2f}%&quot;.format(100 * acc))\n\n    #print('{}\\n\\n{}'.format(teacher.summary(), student_clone.summary()))\n\n    # Train student using knowledge distillation\n    distiller = Distiller(student=student, teacher=teacher)\n    distiller.compile(optimizer=keras.optimizers.SGD(),\n                      metrics=['accuracy'],\n                      student_loss_fn=keras.losses.CategoricalCrossentropy(),  # categorical_crossentropy,\n                      distillation_loss_fn=keras.losses.KLDivergence(),\n                      alpha=0.1,\n                      temperature=10)\n    # Distill teacher to student\n\n    distiller.fit(X_train, Y_train, epochs=5) #THIS WORKS FINE\n    distiller.fit(train_generator, validation_data=test_generator, epochs=5,\n                  verbose=print_during_epochs)  # THIS DOESN'T WORK\n\n\n\n    # Evaluate student on test dataset\n    loss, acc = distiller.evaluate(test_generator)\n    print(&quot;Student distilled model, accuracy: {:5.2f}%&quot;.format(100 * acc))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 35}]
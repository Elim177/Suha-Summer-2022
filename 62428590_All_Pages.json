[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras"], "owner": {"account_id": 5615269, "reputation": 391, "user_id": 6057582, "user_type": "registered", "profile_image": "https://graph.facebook.com/848350359/picture?type=large", "display_name": "Borun Chowdhury", "link": "https://stackoverflow.com/users/6057582/borun-chowdhury"}, "is_answered": true, "view_count": 908, "accepted_answer_id": 62691360, "answer_count": 2, "score": 3, "last_activity_date": 1593675681, "creation_date": 1592395419, "last_edit_date": 1592418253, "question_id": 62428590, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62428590/gradienttape-gives-different-gradients-depending-on-loss-function-being-decorate", "title": "GradientTape gives different gradients depending on loss function being decorated by tf.function or not", "body": "<p>I find that the gradients computed depend on the interplay of tf.function decorators in the following way.</p>\n\n<p>First I create some synthetic data for a binary classification </p>\n\n<pre><code>tf.random.set_seed(42)\nnp.random.seed(42)\nx=tf.random.normal((2,1))\ny=tf.constant(np.random.choice([0,1],2))\n</code></pre>\n\n<p>Then I define two loss functions that differ only in the tf.function decorator</p>\n\n<pre><code>weights=tf.constant([1.,.1])[tf.newaxis,...]\n\ndef customloss1(y_true,y_pred,sample_weight=None):\n    y_true_one_hot=tf.one_hot(tf.cast(y_true,tf.uint8),2)\n    y_true_scale=tf.multiply(weights,y_true_one_hot)\n    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true_scale,y_pred))\n\n@tf.function\ndef customloss2(y_true,y_pred,sample_weight=None):\n    y_true_one_hot=tf.one_hot(tf.cast(y_true,tf.uint8),2)\n    y_true_scale=tf.multiply(weights,y_true_one_hot)\n    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true_scale,y_pred))\n</code></pre>\n\n<p>Then I make a very simple logistic regression model with all the bells and whistles removed to keep it simple</p>\n\n<pre><code>tf.random.set_seed(42)\nnp.random.seed(42)\nmodel=tf.keras.Sequential([\n    tf.keras.layers.Dense(2,use_bias=False,activation='softmax',input_shape=[1,])\n])\n</code></pre>\n\n<p>and finally define two functions to calculate the gradients of the aforementioned loss functions with one being decorated by tf.function and the other not being decorated by it</p>\n\n<pre><code>def get_gradients1(x,y):\n    with tf.GradientTape() as tape1:\n        p1=model(x)\n        l1=customloss1(y,p1)\n    with tf.GradientTape() as tape2:\n        p2=model(x)\n        l2=customloss2(y,p2)\n\n    gradients1=tape1.gradient(l1,model.trainable_variables)\n    gradients2=tape2.gradient(l2,model.trainable_variables)\n\n    return gradients1, gradients2\n\n@tf.function\ndef get_gradients2(x,y):\n    with tf.GradientTape() as tape1:\n        p1=model(x)\n        l1=customloss1(y,p1)\n    with tf.GradientTape() as tape2:\n        p2=model(x)\n        l2=customloss2(y,p2)\n\n    gradients1=tape1.gradient(l1,model.trainable_variables)\n    gradients2=tape2.gradient(l2,model.trainable_variables)\n\n    return gradients1, gradients2\n</code></pre>\n\n<p>Now when I run</p>\n\n<pre><code>get_gradients1(x,y)\n</code></pre>\n\n<p>I get </p>\n\n<pre><code>([&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.11473544, -0.11473544]], dtype=float32)&gt;],\n [&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.11473544, -0.11473544]], dtype=float32)&gt;])\n</code></pre>\n\n<p>and the gradients are equal as expected. However when I run</p>\n\n<pre><code>get_gradients2(x,y)\n</code></pre>\n\n<p>I get</p>\n\n<pre><code>([&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.02213785, -0.5065186 ]], dtype=float32)&gt;],\n [&lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[ 0.11473544, -0.11473544]], dtype=float32)&gt;])\n</code></pre>\n\n<p>where only the second answer is correct. Thus, when my outer function is decorated I only get the correct answer from the inner function that is decorated as well. I was under the impression that decorating the outer one (which is the training loop in many applications) is sufficient but here we see its not. I want to understand why and also then how deep does one have to go to decorate the functions being used? </p>\n\n<p><strong>Added some debugging info</strong></p>\n\n<p>I added some debugging info and I show the code only for customloss2 (the other is identical)</p>\n\n<pre><code>@tf.function\ndef customloss2(y_true,y_pred,sample_weight=None):\n    y_true_one_hot=tf.one_hot(tf.cast(y_true,tf.uint8),2)\n    y_true_scale=tf.multiply(weights,y_true_one_hot)\n    tf.print('customloss2',type(y_true_scale),type(y_pred))\n    tf.print('y_true_scale','\\n',y_true_scale)\n    tf.print('y_pred','\\n',y_pred)\n    return tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true_scale,y_pred))\n</code></pre>\n\n<p>and on running get_gradients1 I get</p>\n\n<pre><code>customloss1 &lt;type 'EagerTensor'&gt; &lt;type 'EagerTensor'&gt;\ny_true_scale \n [[1 0]\n [0 0.1]]\ny_pred \n [[0.510775387 0.489224613]\n [0.529191136 0.470808864]]\ncustomloss2 &lt;class 'tensorflow.python.framework.ops.Tensor'&gt; &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;\ny_true_scale \n [[1 0]\n [0 0.1]]\ny_pred \n [[0.510775387 0.489224613]\n [0.529191136 0.470808864]]\n</code></pre>\n\n<p>we see that the tensors for customloss1 are Eager but for customloss2 are Tensor and yet we get same value for gradients. </p>\n\n<p>On the other hand when I run it on get_gradients2</p>\n\n<pre><code>customloss1 &lt;class 'tensorflow.python.framework.ops.Tensor'&gt; &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;\ny_true_scale \n [[1 0]\n [0 0.1]]\ny_pred \n [[0.510775387 0.489224613]\n [0.529191136 0.470808864]]\ncustomloss2 &lt;class 'tensorflow.python.framework.ops.Tensor'&gt; &lt;class 'tensorflow.python.framework.ops.Tensor'&gt;\ny_true_scale \n [[1 0]\n [0 0.1]]\ny_pred \n [[0.510775387 0.489224613]\n [0.529191136 0.470808864]]\n</code></pre>\n\n<p>we see everything is identical with no tensors being Eager and yet I get different gradients!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 44}]
[{"items": [{"tags": ["c++", "microcontroller", "tensorflow-lite"], "owner": {"account_id": 12472041, "reputation": 550, "user_id": 9079812, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4c793a79ef7652f31abd64a5e81cf9cf?s=256&d=identicon&r=PG&f=1", "display_name": "Francesco Cariaggi", "link": "https://stackoverflow.com/users/9079812/francesco-cariaggi"}, "is_answered": true, "view_count": 1193, "accepted_answer_id": 60386862, "answer_count": 1, "score": 0, "last_activity_date": 1582599204, "creation_date": 1582561886, "question_id": 60380190, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60380190/cannot-load-tensorflow-lite-model-on-microcontroller", "title": "Cannot load TensorFlow Lite model on microcontroller", "body": "<p>I'm trying to run a TensorFlow lite model on a microcontroller, namely on a <a href=\"https://www.sparkfun.com/products/15170\" rel=\"nofollow noreferrer\">Sparkfun Edge</a> board, however I'm having some trouble loading the model to the device. Here are the steps I went through:</p>\n\n<ol>\n<li>Trained my own model in TensorFlow 2.1 using the <code>tf.keras</code> API</li>\n<li>Performed full integer quantization of weights and activations using the instructions provided on the <a href=\"https://www.tensorflow.org/lite/performance/post_training_quantization#full_integer_quantization_of_weights_and_activations\" rel=\"nofollow noreferrer\">TensorFlow website</a>. I'm not sure why, but it seems like I wasn't able to get a model with INT8 inputs and outputs even though I followed the instructions provided. In fact, here's what my model looks like after quantization:\n<a href=\"https://i.stack.imgur.com/WfAVX.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/WfAVX.png\" alt=\"input\"></a>\nAs you can see, my model's input type is still <code>float32</code>, which becomes <code>int8</code> after flowing through the <code>Quantize</code> node. Similarly, I have a <code>Dequantize</code> node towards the output of the graph that does the exact opposite, namely it takes <code>int8</code> values and converts them back to <code>float32</code>, as shown below:\n<a href=\"https://i.stack.imgur.com/Vnm2O.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/Vnm2O.png\" alt=\"enter image description here\"></a>\nThough this is not how it was supposed to be (i.e. <code>int8</code> inputs and outputs <em>without</em> <code>Quantize</code> and <code>Dequantize</code> nodes), it's fine as long as I can get it to work</li>\n<li>Edited <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/person_detection/main_functions.cc\" rel=\"nofollow noreferrer\">this file</a> (as well as some other files actually, but this one is the most important one), which is part of an <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection\" rel=\"nofollow noreferrer\">example application</a> of an image classification model running on a Sparkfun Edge board and hosted in TensorFlow's GitHub repository (this application uses TensorFlow Lite's C++ API for use with microcontrollers). More specifically, I replaced the following code:</li>\n</ol>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>static tflite::MicroOpResolver&lt;3&gt; micro_op_resolver;\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_DEPTHWISE_CONV_2D,\n    tflite::ops::micro::Register_DEPTHWISE_CONV_2D());\nmicro_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,\n                             tflite::ops::micro::Register_CONV_2D());\nmicro_op_resolver.AddBuiltin(tflite::BuiltinOperator_AVERAGE_POOL_2D,\n                             tflite::ops::micro::Register_AVERAGE_POOL_2D());\n</code></pre>\n\n<p>with this:</p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>static tflite::MicroOpResolver&lt;10&gt; micro_op_resolver;\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_DEPTHWISE_CONV_2D,\n    tflite::ops::micro::Register_DEPTHWISE_CONV_2D(),\n    1,\n    3\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_CONV_2D, \n    tflite::ops::micro::Register_CONV_2D(),\n    1,\n    3\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_AVERAGE_POOL_2D, \n    tflite::ops::micro::Register_AVERAGE_POOL_2D()\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_ADD, \n    tflite::ops::micro::Register_ADD()\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_SOFTMAX, \n    tflite::ops::micro::Register_SOFTMAX()\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_FULLY_CONNECTED, \n    tflite::ops::micro::Register_FULLY_CONNECTED()\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_QUANTIZE, \n    tflite::ops::micro::Register_QUANTIZE()\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_DEQUANTIZE, \n    tflite::ops::micro::Register_DEQUANTIZE(),\n    1,\n    2\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_RELU, \n    tflite::ops::micro::Register_RELU()\n);\n\nmicro_op_resolver.AddBuiltin(\n    tflite::BuiltinOperator_RELU6, \n    tflite::ops::micro::Register_RELU6()\n);\n</code></pre>\n\n<p>which essentially registers all the different layers that are included in my custom model.\nFollowing the <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection#running-on-sparkfun-edge\" rel=\"nofollow noreferrer\">instructions</a> provided for the Sparkfun Edge board, I managed to flash the application to the board, but when I run it, it outputs the following error:</p>\n\n<pre><code>Didn't find op for builtin opcode 'QUANTIZE' version '1'\n\nFailed to get registration from op code  d\n\nAllocateTensors() failed\n</code></pre>\n\n<p>I don't understand what I'm doing wrong, since the <code>QUANTIZE</code> operation gets registered with <code>micro_op_resolver.AddBuiltin(...)</code> (see the last code snippet)</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 7}]
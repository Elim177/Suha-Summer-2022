[{"items": [{"tags": ["tensorflow", "machine-learning", "neural-network", "svm"], "owner": {"account_id": 14970419, "reputation": 153, "user_id": 10807476, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-5vIKLk0bjKM/AAAAAAAAAAI/AAAAAAAAAFk/Qf795_LVlQE/photo.jpg?sz=256", "display_name": "MangLion", "link": "https://stackoverflow.com/users/10807476/manglion"}, "is_answered": false, "view_count": 579, "answer_count": 1, "score": 0, "last_activity_date": 1597392269, "creation_date": 1586615015, "question_id": 61158278, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61158278/is-it-possible-to-have-a-multiclass-svm-as-the-last-layer-of-a-cnn-using-tensorf", "title": "Is it possible to have a multiclass SVM as the last layer of a CNN using Tensorflow", "body": "<p>I would like to use a multiclass RBF SVM as the last layer of my CNN model built in Tensorflow. </p>\n\n<p>I have currently got the following. But instead of the last layer, is it possible to slip an SVM in?\nWhat are my options. \nI found that Tensorflow have something called Random Fourier Features where I can use kernel methods to mimic an SVM? Is this an option? If so how would I go about implementing it into what I currently have? </p>\n\n<pre><code>net = x_noisy_image\n\n# 1st convolutional layer.\nnet = tf.layers.conv2d(inputs=net, name='layer_conv1', padding='same',\n                       filters=32, kernel_size=3, activation=tf.nn.relu)\n# 2nd convolutional layer.\nnet = tf.layers.conv2d(inputs=net, name='layer_conv2', padding='same',\n                       filters=32, kernel_size=3, activation=tf.nn.relu)\n# Pooling layer\nnet = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n\n# 3rd convolutional layer.\nnet = tf.layers.conv2d(inputs=net, name='layer_conv3', padding='same',\n                       filters=64, kernel_size=3, activation=tf.nn.relu)\n# 4th convolution layer\nnet = tf.layers.conv2d(inputs=net, name='layer_conv4', padding='same',\n                       filters=64, kernel_size=3, activation=tf.nn.relu)\n# Pooling layer\nnet = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)\n\n# Flatten layer.This should eventually be replaced by:\n# net = tf.layers.flatten(net)\nnet = tf.contrib.layers.flatten(net)\n\n# 1st fully-connected / dense layer.\nnet = tf.layers.dense(inputs=net, name='layer_fc1',\n                      units=200, activation=tf.nn.relu)\n# 2nd fully-connected / dense layer.\nnet = tf.layers.dense(inputs=net, name='layer_fc2',\n                      units=200, activation=tf.nn.relu)\n# 3rd fully-connected / dense layer.\nnet = tf.layers.dense(inputs=net, name='layer_fc_out',\n                      units=num_classes, activation=tf.nn.softmax)\n\n\n\n# Unscaled output of the network.\nlogits = net\n# Softmax output of the network.\ny_pred = tf.nn.softmax(logits=logits)\n# Loss measure to be optimized.\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true,\n                                                           logits=logits)\nloss = tf.reduce_mean(cross_entropy)\n\n\"\"\"\nOptimizer for Normal Training\n\"\"\"\n[var.name for var in tf.trainable_variables()]\noptimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 135}]
[{"items": [{"tags": ["tensorflow2.0"], "owner": {"account_id": 108161, "reputation": 5951, "user_id": 287238, "user_type": "registered", "accept_rate": 75, "profile_image": "https://i.stack.imgur.com/oQJH2.jpg?s=256&g=1", "display_name": "mathtick", "link": "https://stackoverflow.com/users/287238/mathtick"}, "is_answered": true, "view_count": 577, "answer_count": 1, "score": 1, "last_activity_date": 1558458155, "creation_date": 1558451887, "question_id": 56241477, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/56241477/how-to-structure-loops-for-gradient-updates-when-data-is-not-changing-in-tensorf", "title": "How to structure loops for gradient updates when data is not changing in tensorflow 2.0?", "body": "<p>The two code snippets show use of the same data to do n updates where one uses a persistent gradient tape and the other just calls it over and over again. The perf difference seems to be about 2x. Is there a better way to structure this going forward? I suppose moving data to device would matter on GPU? </p>\n\n<pre><code>@tf.function\ndef train_n_steps_same_data(tau, y, n=1):\n    \"\"\"\n    In [218]: %timeit r = q.train_n_steps_same_data(q.tau, q.y, n=100)\n    25.3 ms \u00b1 926 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n    \"\"\"\n    with tf.GradientTape(persistent=True) as tape:\n        d = model([tau, y])\n        loss = tf.reduce_mean(d['rho'])\n    for i in range(n):\n        gradients = tape.gradient(loss, model.trainable_variables)\n        l = optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    names = [x.name for x in gradients]\n    g = dict(zip(names, gradients))\n    reduced = dict()\n    reduced['loss'] = loss\n    return reduced, d, g\n\n@tf.function\ndef train_n_steps_same_data2(tau, y, n=1):\n    \"\"\"\n    In [220]: %timeit r = q.train_n_steps_same_data2(q.tau, q.y, n=100)\n    41.6 ms \u00b1 1.47 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n    \"\"\"\n    for i in range(n):\n        with tf.GradientTape() as tape:\n            d = model([tau, y])\n            loss = tf.reduce_mean(d['rho'])\n        gradients = tape.gradient(loss, model.trainable_variables)\n        l = optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    names = [x.name for x in gradients]\n    g = dict(zip(names, gradients))\n    reduced = dict()\n    reduced['loss'] = loss\n    return reduced, d, g\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 83}]
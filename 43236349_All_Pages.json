[{"items": [{"tags": ["machine-learning", "tensorflow", "deep-learning", "multi-gpu"], "owner": {"account_id": 4608321, "reputation": 654, "user_id": 4352606, "user_type": "registered", "accept_rate": 92, "profile_image": "https://graph.facebook.com/100002363644960/picture?type=large", "display_name": "Tai Christian", "link": "https://stackoverflow.com/users/4352606/tai-christian"}, "is_answered": true, "view_count": 477, "accepted_answer_id": 43236645, "answer_count": 1, "score": 0, "last_activity_date": 1491426995, "creation_date": 1491408419, "last_edit_date": 1491426995, "question_id": 43236349, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43236349/how-to-use-multiple-gpus-effectively-when-training-deep-networks", "title": "How to use multiple GPUs effectively when training deep networks?", "body": "<p>I am using a machine which has 2 GPUs Titan Black to train my deep learning model which has 3 layers (3x3, 3x3 and 5x5).</p>\n\n<p>The training runs pretty well but when I watch nvidia-smi (watch every 1 sec), I realized that my program uses only one GPU for computation, the second one always 0% even when the first one reach 100%.</p>\n\n<p>I am trying to use <em>tf.device</em> to assign specific tasks for each of them but then they run one-by-one, not in parallel, and the total time was even increased, not reduced (I guess because 2 GPUs had to exchange values with each other)</p>\n\n<p>Below is my program. It is quite messy, maybe you just need to pay attention at the graph where I use <strong>tf.device</strong> is enough...</p>\n\n<p>Thank you so much!</p>\n\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom six.moves import cPickle as pickle\nimport matplotlib.pyplot as plt\n\nfrom os import listdir, sys\nfrom os.path import isfile, join\n\nfrom time import gmtime, strftime\nimport time\n\ndef validatePath(path):\n    path = path.replace(\"\\\\\",\"/\")\n    if (path[len(path)-1] != \"/\"):\n        path = path + \"/\"\n    return path\n\nhidden_size_default = np.array([16, 32, 64, 32])\ncnn1_default = 3\ncnn2_default = 3\ncnn3_default = 5\n\nSIZE_BATCH_VALID = 200\n\ninput_path = 'ARCHIVES-sub-dataset'\n\noutput_path = 'ARCHIVES-model'\n\nlog_address = \"trainlog.txt\"\n\ntf.app.flags.DEFINE_integer('h0', hidden_size_default[0], 'Size of hidden layer 0th')\ntf.app.flags.DEFINE_integer('h1', hidden_size_default[1], 'Size of hidden layer 1st')\ntf.app.flags.DEFINE_integer('h2', hidden_size_default[2], 'Size of hidden layer 2nd')\ntf.app.flags.DEFINE_integer('h3', hidden_size_default[3], 'Size of hidden layer 3rd')\n\ntf.app.flags.DEFINE_integer('k1', cnn1_default , 'Size of kernel 1st')\ntf.app.flags.DEFINE_integer('k2', cnn2_default , 'Size of kernel 2nd')\ntf.app.flags.DEFINE_integer('k3', cnn3_default , 'Size of kernel 3rd')\n\ntf.app.flags.DEFINE_string('input_path', input_path, 'The parent directory which contains 2 directories: dataset and label')\ntf.app.flags.DEFINE_string('output_path', output_path, 'The directory which will store models (you have to create)')\ntf.app.flags.DEFINE_string('log_address', log_address, 'The file name which will store the log')\n\nFLAGS = tf.app.flags.FLAGS\n\nload_path = FLAGS.input_path\nsave_model_path = FLAGS.output_path\n\nlog_addr = FLAGS.log_address\n\nload_path = validatePath(load_path)\nsave_model_path = validatePath(save_model_path)\n\ncnn1 = FLAGS.k1\ncnn2 = FLAGS.k2\ncnn3 = FLAGS.k3\n\nhidden_size = np.array([FLAGS.h0, FLAGS.h1, FLAGS.h2, FLAGS.h3])\n\n# Shuffle the dataset and its label\ndef randomize(dataset, labels):\n    permutation = np.random.permutation(labels.shape[0])\n    shuffled_dataset = dataset[permutation,:]\n    shuffled_labels = labels[permutation]\n    return shuffled_dataset, shuffled_labels\n\n\ndef writemyfile(mystring):\n    with open(log_addr, \"a\") as myfile:\n        myfile.write(str(mystring + \"\\n\"))\n\nnum_labels = 5\n\ndef accuracy(predictions, labels):\n    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))/ predictions.shape[0])\n\ndef weight_variable(shape):\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\ndef bias_variable(shape):\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\ndef conv2d(x, W):\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool_2x2(x):\n    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\ndef DivideSets(input_set):\n    length_set = input_set.shape[0]\n    index_70 = int(length_set*0.7)\n    index_90 = int(length_set*0.9)\n\n    set_train = input_set[0:index_70]\n    set_valid = input_set[index_70:index_90]\n    set_test = input_set[index_90:length_set]\n    return np.float32(set_train), np.float32(set_valid), np.float32(set_test)\n\n# from 1-value labels to 5 values of (0 and 1)\ndef LabelReconstruct(label_set):\n    label_set = label_set.astype(int)\n    new_label_set = np.zeros(shape=(len(label_set),num_labels))\n    for i in range(len(label_set)):\n        new_label_set[i][label_set[i]] = 1\n    return new_label_set.astype(int)\n\ndef LoadDataSet(load_path):\n    list_data = [f for f in listdir(load_path + \"dataset/\") if isfile(join(load_path + \"dataset/\", f))]\n    list_label = [f for f in listdir(load_path + \"label/\") if isfile(join(load_path + \"dataset/\", f))]\n    if list_data.sort() == list_label.sort():    \n        return list_data\n    else:\n        print(\"data and labels are not suitable\")\n        return 0\n\n# load, randomize, normalize images and reconstruct labels\ndef PrepareData(*arg):\n    filename = arg[0]\n    loaded_dataset = pickle.load( open( load_path + \"dataset/\" + filename, \"rb\" ))\n    loaded_labels = pickle.load( open( load_path + \"label/\" +  filename, \"rb\" ))\n    if len(arg) == 1:\n        datasize = len(loaded_labels)\n    elif len(arg) == 2:\n        datasize = int(arg[1])\n    else:\n        print(\"not more than 2 arguments please!\")\n    dataset_full,labels_full = randomize(loaded_dataset[0:datasize], loaded_labels[0:datasize])\n    return NormalizeData(dataset_full), LabelReconstruct(labels_full)\n\ndef NormalizeData(dataset):\n    dataset = dataset - (dataset.mean())\n    dataset = dataset / (dataset.std())\n    return dataset\n\n### LOAD DATA\nlistfiles = LoadDataSet(load_path)\n\n# divide\nlistfiles_train = listfiles[0:15]\nlistfiles_valid = listfiles[15:25]\nlistfiles_test = listfiles[25:len(listfiles)]\n\n\ngraphCNN = tf.Graph()\n\nwith graphCNN.as_default():\n    with tf.device('/gpu:0'):\n\n        x = tf.placeholder(tf.float32, shape=(None, 224,224,3)) # X\n        y_ = tf.placeholder(tf.float32, shape=(None, num_labels)) # Y_\n\n        dropout = tf.placeholder(tf.float32)\n        if dropout == 1.0:\n            keep_prob = tf.constant([0.2, 0.3, 0.5], dtype=tf.float32)\n        else:\n            keep_prob = tf.constant([1.0, 1.0, 1.0], dtype=tf.float32)\n\n        weights_1 = weight_variable([cnn1,cnn1,3, hidden_size[0]])\n        biases_1 = bias_variable([hidden_size[0]])\n        weights_2 = weight_variable([cnn2,cnn2,hidden_size[0], hidden_size[1]])\n        biases_2 = bias_variable([hidden_size[1]])\n        weights_3 = weight_variable([cnn3,cnn3,hidden_size[1], hidden_size[2]])\n        biases_3 = bias_variable([hidden_size[2]])\n        weights_4 = weight_variable([56 * 56 * hidden_size[2], hidden_size[3]])\n        biases_4 = bias_variable([hidden_size[3]])\n        weights_5 = weight_variable([hidden_size[3], num_labels])\n        biases_5 = bias_variable([num_labels])\n\n        def model(data):\n            with tf.device('/gpu:1'):\n                train_hidden_1 = tf.nn.relu(conv2d(data, weights_1) + biases_1)\n                train_hidden_2 = max_pool_2x2(tf.nn.relu(conv2d(train_hidden_1, weights_2) + biases_2))\n                train_hidden_2_drop = tf.nn.dropout(train_hidden_2, keep_prob[0])\n\n                train_hidden_3 = max_pool_2x2(tf.nn.relu(conv2d(train_hidden_2_drop, weights_3) + biases_3))\n                train_hidden_3_drop = tf.nn.dropout(train_hidden_3, keep_prob[1])\n                train_hidden_3_drop = tf.reshape(train_hidden_3_drop,[-1, 56 * 56 * hidden_size[2]])\n\n                train_hidden_4 = tf.nn.relu(tf.matmul(train_hidden_3_drop, weights_4) + biases_4)\n                train_hidden_4_drop = tf.nn.dropout(train_hidden_4, keep_prob[2])\n\n                logits = tf.matmul(train_hidden_4_drop, weights_5) + biases_5\n            return logits\n\n        t_train_labels = tf.argmax(y_, 1)   # From one-hot (one and zeros) vectors to values    \n        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model(x), labels=t_train_labels))\n\n        optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n\n        y = tf.nn.softmax(model(x))\n\n### RUNNING\n\nprint(\"log address: %s\" % (log_addr))\n\n#num_steps = 10001\ntimes_repeat = 20 # number of epochs\nbatch_size = 100\n\nwith tf.Session(graph=graphCNN,config=tf.ConfigProto(log_device_placement=True)) as session:\n    tf.initialize_all_variables().run()\n    saver = tf.train.Saver(max_to_keep=0)\n\n    writemyfile(\"---ARCHIVES_M1----\")\n    mytime = strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n    writemyfile(str(\"\\nTime: %s \\nLayers: %d,%d,%d \\epochs: %d\"  % (mytime,cnn1,cnn2,cnn3,times_repeat)))\n\n    writemyfile(\"Train files:\" + str(listfiles_train))\n    writemyfile(\"Valid files:\" + str(listfiles_valid))\n    writemyfile(\"Test files:\" + str(listfiles_test))\n\n    print(\"Model will be saved in file: %s\" % save_model_path)\n    writemyfile(str(\"Model will be saved in file: %s\" % save_model_path))\n\n    ### TRAINING &amp; VALIDATION\n    valid_accuracies_epochs = np.array([])\n    for time_repeat in range(times_repeat):\n        print(\"- time_repeat:\",time_repeat)\n        writemyfile(\"- time_repeat:\"+str(time_repeat))\n\n        for file_train in listfiles_train:\n            file_train_id = int(file_train[0:len(file_train)-4])\n\n            time_start_this_file = time.time()\n\n            #LOAD DATA\n            print(\"- - file:\",file_train_id, end=' ')\n            writemyfile(\"- - file:\" + str(file_train_id))\n\n            Data_train, Label_train= PrepareData(file_train)\n\n            for step in range(0,len(Data_train)-batch_size,batch_size):\n                batch_data = Data_train[step:step+batch_size]\n                batch_labels = Label_train[step:step+batch_size]\n                feed_dict = {x : batch_data, y_ : batch_labels, dropout: 1.0}\n                opti, l, predictions = session.run([optimizer, loss, y], feed_dict=feed_dict)\n\n            train_accuracies = np.array([])\n            for index_tr_accu in range(0,len(Data_train)-SIZE_BATCH_VALID,SIZE_BATCH_VALID):\n                current_predictions = y.eval(feed_dict={x: Data_train[index_tr_accu:index_tr_accu+SIZE_BATCH_VALID],dropout: 0.0})\n                current_accuracy = accuracy(current_predictions, Label_train[index_tr_accu:index_tr_accu+SIZE_BATCH_VALID])\n                train_accuracies = np.r_[train_accuracies,current_accuracy]\n\n            train_accuracy = train_accuracies.mean()                    \n            print(\"batch accu: %.2f%%\" %(train_accuracy),end=\" | \")\n            writemyfile(\"batch accu: %.2f%%\" %(train_accuracy))\n\n            time_done_this_file = time.time() - time_start_this_file\n            print(\"time: %.2fs\" % (time_done_this_file))\n            writemyfile(\"time: %.2fs\" % (time_done_this_file))\n\n        # save model\n        model_addr = save_model_path + \"model335\" + \"-epoch-\" + str(time_repeat) + \".ckpt\"\n        save_path = saver.save(session, model_addr,) # max_to_keep default was 5\n\n        mytime = strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n        print(\"epoch finished at %s \\n model address: %s\" % (mytime,model_addr))\n        writemyfile(\"epoch finished at %s \\n model address: %s\" % (mytime,model_addr))\n\n        # validation\n        valid_accuracies = np.array([])\n        for file_valid in listfiles_valid:\n            file_valid_id = int(file_valid[0:len(file_valid)-4])\n            Data_valid, Label_valid = PrepareData(file_valid)\n            for index_vl_accu in range(0,len(Data_valid)-SIZE_BATCH_VALID,SIZE_BATCH_VALID):\n                current_predictions = y.eval(feed_dict={x: Data_valid[index_vl_accu:index_vl_accu+SIZE_BATCH_VALID],dropout: 0.0})\n                current_accuracy = accuracy(current_predictions, Label_valid[index_vl_accu:index_vl_accu+SIZE_BATCH_VALID])\n                valid_accuracies = np.r_[valid_accuracies,current_accuracy]\n\n        valid_accuracy = valid_accuracies.mean()\n        print(\"epoch %d - valid accu: %.2f%%\" %(time_repeat,valid_accuracy))\n        writemyfile(\"epoch %d - valid accu: %.2f%%\" %(time_repeat,valid_accuracy))\n\n        valid_accuracies_epochs = np.hstack([valid_accuracies_epochs,valid_accuracy])\n\nprint('Done!!')\nwritemyfile(str('Done!!'))\nsession.close()\n</code></pre>\n\n<p><strong>Update:</strong> I found <a href=\"https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py\" rel=\"nofollow noreferrer\">cifar10_multi_gpu_train.py</a> seems to be a good example for training with multi GPUs, but honestly I don't know how to apply on my case.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 108}]
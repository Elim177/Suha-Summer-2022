[{"items": [{"tags": ["python", "tensorflow", "distributed-computing", "tensorflow-estimator"], "owner": {"account_id": 406431, "reputation": 555, "user_id": 775938, "user_type": "registered", "accept_rate": 58, "profile_image": "https://www.gravatar.com/avatar/832ef6bda320b48e1feb903f011196ec?s=256&d=identicon&r=PG", "display_name": "Zwackelmann", "link": "https://stackoverflow.com/users/775938/zwackelmann"}, "is_answered": false, "view_count": 1044, "answer_count": 1, "score": 1, "last_activity_date": 1576241128, "creation_date": 1532428412, "last_edit_date": 1532690846, "question_id": 51496412, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51496412/distributed-tensorflow-estimator-execution-does-not-trigger-evaluation-or-export", "title": "Distributed Tensorflow Estimator execution does not trigger evaluation or export", "body": "<p>I am testing distributed training with tensorflow Estimators. In my example I fit a simple sinus function with a custom estimator using <a href=\"https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate\" rel=\"nofollow noreferrer\"><code>tf.estimator.train_and_evaluation</code></a>. After training and evaluation I want to export the model to have it ready for <a href=\"https://www.tensorflow.org/serving/serving_basic\" rel=\"nofollow noreferrer\">tensorflow serving</a>. However the evaluation and export is only triggered when executing the estimator in non-distributed way.</p>\n\n<p>The model and Estimators are defined as follows:</p>\n\n<pre><code>def my_model(features, labels, mode):\n    # define simple dense network\n    net = tf.layers.dense(features['x'], units=8, activation=tf.nn.tanh)\n    net = tf.layers.dense(net, units=8, activation=tf.nn.tanh)\n    net = tf.layers.dense(net, units=8, activation=tf.nn.tanh)\n    net = tf.layers.dense(net, units=8, activation=tf.nn.tanh)\n    net = tf.layers.dense(net, units=8, activation=tf.nn.tanh)\n    net = tf.layers.dense(net, units=8, activation=tf.nn.tanh)\n    net = tf.layers.dense(net, units=8, activation=tf.nn.tanh)\n    net = tf.layers.dense(net, units=8, activation=tf.nn.tanh)\n\n    # output layer\n    predictions = tf.layers.dense(net, units=1, activation=tf.nn.tanh)\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        # define output message for tensorflow serving\n        export_outputs = {'predict_output': tf.estimator.export.PredictOutput({\"predictions\": predictions})}\n\n        return tf.estimator.EstimatorSpec(mode=mode, predictions={'predictions': predictions}, export_outputs=export_outputs)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        # for evaluation simply use mean squared error\n        loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n        metrics = {'mse': tf.metrics.mean_squared_error(labels, predictions)}\n\n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n    elif mode == tf.estimator.ModeKeys.TRAIN:\n        # train on mse with Adagrad optimizer\n        loss = tf.losses.mean_squared_error(labels=labels, predictions=predictions)\n        optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n    else:\n        raise ValueError(\"unhandled mode: %s\" % str(mode))\n\n\ndef main(_):\n    # prepare training data\n    default_batch_size = 50\n    examples = [{'x': x, 'y': math.sin(x)} for x in [random.random()*2*math.pi for _ in range(10000)]]\n\n    estimator = tf.estimator.Estimator(model_fn=my_model,\n                                       config=tf.estimator.RunConfig(model_dir='sin_model',\n                                                                     save_summary_steps=100))\n\n    # function converting examples to dataset\n    def dataset_fn():\n        # returns a dataset serving batched (feature_map, label)-pairs\n        # e.g. ({'x': [1.0, 0.3, 1.1...]}, [0.84, 0.29, 0.89...])\n        return tf.data.Dataset.from_generator(\n            lambda: iter(examples),\n            output_types={\"x\": tf.float32, \"y\": tf.float32},\n            output_shapes={\"x\": [], \"y\": []}) \\\n            .map(lambda x: ({'x': [x['x']]}, [x['y']])) \\\n            .repeat() \\\n            .batch(default_batch_size)\n\n    # function to export model to be used for serving\n    feature_spec = {'x': tf.FixedLenFeature([1], tf.float32)}\n    def serving_input_fn():\n        serialized_tf_example = tf.placeholder(dtype=tf.string, shape=[default_batch_size])\n\n        receiver_tensors = {'examples': serialized_tf_example}\n        features = tf.parse_example(serialized_tf_example, feature_spec)\n        return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)\n\n    # train, evaluate and export\n    train_spec = tf.estimator.TrainSpec(input_fn=dataset_fn, max_steps=1000)\n    eval_spec = tf.estimator.EvalSpec(input_fn=dataset_fn,\n                                      steps=100,\n                                      exporters=[tf.estimator.FinalExporter('sin', serving_input_fn)])\n\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n\nif __name__ == '__main__':\n    tf.app.run(main)\n</code></pre>\n\n<p>When executing this code in one single process, I receive an output folder that contains model checkpoints, evaluation data and the model export</p>\n\n<pre><code>$ ls sin_model/\ncheckpoint                                  model.ckpt-0.index\neval                                        model.ckpt-0.meta\nevents.out.tfevents.1532426226.simon        model.ckpt-1000.data-00000-of-00001\nexport                                      model.ckpt-1000.index\ngraph.pbtxt                                 model.ckpt-1000.meta\nmodel.ckpt-0.data-00000-of-00001\n</code></pre>\n\n<p>However, when distributing the training process (in this test setup only on local machine) the eval and export folders are missing.</p>\n\n<p>I start the individual nodes using the following cluster config:</p>\n\n<pre><code>{\"cluster\": {\n    \"ps\": [\"localhost:2222\"],\n    \"chief\": [\"localhost:2223\"], \n    \"worker\": [\"localhost:2224\"]\n}\n</code></pre>\n\n<p>The starting of ps server looks as follows</p>\n\n<pre><code>$ TF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2223\"], \"worker\": [\"localhost:2224\"], \"ps\": [\"localhost:2222\"]}, \"task\": {\"type\": \"ps\", \"index\": 0}}' CUDA_VISIBLE_DEVICES= python custom_estimator.py\n2018-07-24 12:09:04.913967: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUDA_ERROR_NO_DEVICE\n2018-07-24 12:09:04.914008: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:132] retrieving CUDA diagnostic information for host: simon\n2018-07-24 12:09:04.914013: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:139] hostname: simon\n2018-07-24 12:09:04.914035: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] libcuda reported version is: 384.130.0\n2018-07-24 12:09:04.914059: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:167] kernel reported version is: 384.130.0\n2018-07-24 12:09:04.914079: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:249] kernel version seems to match DSO: 384.130.0\n2018-07-24 12:09:04.914961: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -&gt; {0 -&gt; localhost:2223}\n2018-07-24 12:09:04.914971: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2222}\n2018-07-24 12:09:04.914976: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2224}\n2018-07-24 12:09:04.915658: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Started server with target: grpc://localhost:2222\n</code></pre>\n\n<p>(I appended <code>CUDA_VISIBLE_DEVICES=</code> to the command line to prevent the worker and chief to allocate GPU memory. This causes an <code>failed call to cuInit: CUDA_ERROR_NO_DEVICE</code> Error which is however not critical)</p>\n\n<p>The chief is then started as follows</p>\n\n<pre><code>$ TF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2223\"], \"worker\": [\"localhost:2224\"], \"ps\": [\"localhost:2222\"]}, \"task\": {\"type\": \"chief\", \"index\": 0}}' CUDA_VISIBLE_DEVICES= python custom_estimator.py\n2018-07-24 12:09:10.532171: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUDA_ERROR_NO_DEVICE\n2018-07-24 12:09:10.532234: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:132] retrieving CUDA diagnostic information for host: simon\n2018-07-24 12:09:10.532241: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:139] hostname: simon\n2018-07-24 12:09:10.532298: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] libcuda reported version is: 384.130.0\n2018-07-24 12:09:10.532353: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:167] kernel reported version is: 384.130.0\n2018-07-24 12:09:10.532359: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:249] kernel version seems to match DSO: 384.130.0\n2018-07-24 12:09:10.533195: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -&gt; {0 -&gt; localhost:2223}\n2018-07-24 12:09:10.533207: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2222}\n2018-07-24 12:09:10.533211: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2224}\n2018-07-24 12:09:10.533835: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Started server with target: grpc://localhost:2223\n2018-07-24 12:09:14.038636: I tensorflow/core/distributed_runtime/master_session.cc:1165] Start master session 71a2748ad69725ae with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } }\n</code></pre>\n\n<p>And the worker is then started as follows:</p>\n\n<pre><code>$ TF_CONFIG='{\"cluster\": {\"chief\": [\"localhost:2223\"], \"worker\": [\"localhost:2224\"], \"ps\": [\"localhost:2222\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}' CUDA_VISIBLE_DEVICES= python custom_estimator.py\n2018-07-24 12:09:13.172260: E tensorflow/stream_executor/cuda/cuda_driver.cc:397] failed call to cuInit: CUDA_ERROR_NO_DEVICE\n2018-07-24 12:09:13.172320: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:132] retrieving CUDA diagnostic information for host: simon\n2018-07-24 12:09:13.172327: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:139] hostname: simon\n2018-07-24 12:09:13.172362: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] libcuda reported version is: 384.130.0\n2018-07-24 12:09:13.172399: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:167] kernel reported version is: 384.130.0\n2018-07-24 12:09:13.172405: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:249] kernel version seems to match DSO: 384.130.0\n2018-07-24 12:09:13.173230: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job chief -&gt; {0 -&gt; localhost:2223}\n2018-07-24 12:09:13.173242: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2222}\n2018-07-24 12:09:13.173247: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2224}\n2018-07-24 12:09:13.173783: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:369] Started server with target: grpc://localhost:2224\n2018-07-24 12:09:18.774264: I tensorflow/core/distributed_runtime/master_session.cc:1165] Start master session 1d13ac84816fdc80 with config: allow_soft_placement: true graph_options { rewrite_options { meta_optimizer_iterations: ONE } }\n</code></pre>\n\n<p>After a short time the chief process stops and the <code>sin_model</code> folder exists with model checkpoint but no export or evaluation:</p>\n\n<pre><code>$ ls sin_model/\ncheckpoint                                  model.ckpt-0.meta\nevents.out.tfevents.1532426950.simon        model.ckpt-1001.data-00000-of-00001\ngraph.pbtxt                                 model.ckpt-1001.index\nmodel.ckpt-0.data-00000-of-00001            model.ckpt-1001.meta\nmodel.ckpt-0.index\n</code></pre>\n\n<p>Is any further configuration needed in order to evaluate or export in distributed setting?</p>\n\n<p>I am working with python 3.5 and tensorflow 1.8</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 4}]
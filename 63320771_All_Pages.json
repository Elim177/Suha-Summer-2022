[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "custom-training"], "owner": {"account_id": 18265720, "reputation": 1, "user_id": 13297517, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GhO5rck-EgCtqp4B2M7om-hRjdUr4UeutbEqFDbYA=k-s256", "display_name": "Tessan MOLINIE", "link": "https://stackoverflow.com/users/13297517/tessan-molinie"}, "is_answered": false, "view_count": 1518, "answer_count": 1, "score": 0, "last_activity_date": 1596933746, "creation_date": 1596925476, "last_edit_date": 1596933746, "question_id": 63320771, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63320771/errors-when-training-my-saved-tensorflow-model", "title": "Errors when training my saved tensorflow model", "body": "<p>I'm trying to build a custom keras model whith the subclassing API but I get some errors when I load a previous instance of my model and I try to train it :</p>\n<p>Here is the class of my model, it has 3 inputs and 1 output :</p>\n<pre><code>import tensorflow as tf\n\n\nspec1 = tf.TensorSpec(shape=(1,40,5,1))\nspec2 = tf.TensorSpec(shape=(1,3))\n\nclass Conv_Rnn_model(tf.keras.Model):\n    def __init__(self):\n        # super() permet d'appeler le constructeur de la classe m\u00e8re dans la classe fille\n        # permet \u00e9galemet de r\u00e9soudre le probl\u00e8me d'appel multiple de m\u00e9thode dans les configuration de classe en diamant\n        super().__init__() \n       \n        self.loss_object = tf.keras.losses.MeanSquaredError()\n        self.optimizer   = tf.keras.optimizers.Adam()\n        # Convolutions :\n            # input :  [batch_size, rows, cols, channels]\n            # return : [batch_size, new_rows, new_cols, filters]\n        self.conv1 = tf.keras.layers.Conv2D(filters = 32, kernel_size = (8,2),input_shape=(40,5,1), activation='relu', name =&quot;conv1&quot;)\n        self.conv2 = tf.keras.layers.Conv2D(filters = 64, kernel_size = (6,1),input_shape=(40,5,1), activation='relu', name =&quot;conv2&quot;)\n        self.conv3 = tf.keras.layers.Conv2D(filters =128, kernel_size = (6,1),input_shape=(40,5,1), activation='relu', name =&quot;conv3&quot;)\n       \n        \n        # recurrent cells : \n            #input :  [batch_size, time_steps, features]\n            #return : [batch_size, time_steps, units (if return_sequence=True)]\n        self.lstm1A = tf.keras.layers.LSTM(64, return_sequences=True, name = &quot;lstm1A&quot;)\n        self.lstm1B = tf.keras.layers.LSTM(64, name = &quot;lstm1B&quot;)\n        \n        self.lstm2A = tf.keras.layers.LSTM(64, return_sequences=True, name = &quot;lstm2A&quot;)\n        self.lstm2B = tf.keras.layers.LSTM(64, name = &quot;lstm2B&quot;)\n         \n        self.lstm3A = tf.keras.layers.LSTM(64, return_sequences=True, name = &quot;lstm3A&quot;)\n        self.lstm3B = tf.keras.layers.LSTM(64, name = &quot;lstm3B&quot;)\n        \n        # Concat layer : \n        self.concat = tf.keras.layers.Concatenate(axis=1)\n\n        # fully connected layers :\n            #input  : [batch_size, ... , input_dim]\n            #return : [batch_size, ... , units]\n        self.dense = tf.keras.layers.Dense(32)\n        self.out = tf.keras.layers.Dense(3, activation='softmax')\n        \n    @tf.function(input_signature=[[tf.TensorSpec(shape=(1,40,5,1),name=&quot;M15&quot;),\n                                   tf.TensorSpec(shape=(1,40,5,1),name=&quot;H1&quot;),\n                                   tf.TensorSpec(shape=(1,40,5,1),name=&quot;H4&quot;)]\n                                 ])\n    def call(self, data):\n        &quot;&quot;&quot; \n        TODO: comprendre comment se calculer les outputs des conv2D \n        pour remplacer les conv_res.shape par des constantes\n        &quot;&quot;&quot;\n        \n        #tf.no_gradient(&quot;concatenate&quot;)\n        #tf.no_gradient(&quot;reshape&quot;)\n        \n        conv1_res = self.conv1(data[0])\n        conv2_res = self.conv2(data[1])\n        conv3_res = self.conv3(data[2])\n            \n        shape1 = (1, conv1_res.shape[1],conv1_res.shape[2]*conv1_res.shape[3])\n        shape2 = (1, conv2_res.shape[1],conv2_res.shape[2]*conv2_res.shape[3])\n        shape3 = (1, conv3_res.shape[1],conv3_res.shape[2]*conv3_res.shape[3])\n\n\n        f1 = self.lstm1B(self.lstm1A( tf.reshape(conv1_res, shape1) ))\n        f2 = self.lstm2B(self.lstm2A( tf.reshape(conv2_res, shape2) ))\n        f3 = self.lstm3B(self.lstm3A( tf.reshape(conv3_res, shape3) ))  \n        \n        # returns of fully connected layers\n        pre_output = self.dense(self.concat([f1,f2,f3]))\n        output     = self.out(pre_output)\n        \n        return output\n    \n    @tf.function(input_signature=[[spec1,spec1,spec1], spec2])\n    def train_step(self,X_train, y_train):\n        \n        #X_train, y_train = data\n        with tf.GradientTape() as tape:\n            y_pred = self(X_train)  # Forward pass\n            # Compute the loss value\n            loss = self.loss_object(y_train, y_pred)\n\n        # Compute gradients\n        gradients = tape.gradient(loss, self.trainable_variables)\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n\n</code></pre>\n<p>When I train my model and then I save it all work very well :</p>\n<pre><code>from Conv_Rnn_model import Conv_Rnn_model\nfrom tensorflow.keras.models import load_model\nfrom numpy.random import rand\nfrom functions import getDataset_3period\n\nmodel_1 = Conv_Rnn_model()\nmodel_1.compile(loss='mse', optimizer='adam')\niterations = 5\ndata_environment = getDataset_3period(window_size=40)# get dataframe of EURUSD \n\nfor i in range(iterations):\n    state   = state = data_environment[i] \n    # target  = tf.constant(rand(1,3),dtype=tf.float32)\n    target= rand(1,3)\n    X_train = [state[:1],state[1:2],state[2:3]]\n    # X_train = tf.constant(X_train, dtype=tf.float32)\n    model_1.train_step(X_train, target)\n    print(&quot;epoch&quot;, i)\n\nmodel_1.save(&quot;models/model_test1&quot;)\n</code></pre>\n<p>But when I try to reload my trained model and train it again I get errors :</p>\n<pre><code>model_2 = load_model(&quot;models/model_test1&quot;, compile=False)\n\n2020-08-08 18:17:27.277841: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 20 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:28.048269: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 20 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:28.651946: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 20 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:28.946418: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 20 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:32.857832: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 20 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:32.872207: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'while' has 11 outputs but the _output_shapes attribute specifies shapes for 20 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:32.891483: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'lstm1A/PartitionedCall' has 5 outputs but the _output_shapes attribute specifies shapes for 22 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:32.892203: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'lstm1B/PartitionedCall' has 5 outputs but the _output_shapes attribute specifies shapes for 22 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:32.892926: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'lstm2A/PartitionedCall' has 5 outputs but the _output_shapes attribute specifies shapes for 22 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:32.893593: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'lstm2B/PartitionedCall' has 5 outputs but the _output_shapes attribute specifies shapes for 22 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:32.894289: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'lstm3A/PartitionedCall' has 5 outputs but the _output_shapes attribute specifies shapes for 22 outputs. Output shapes may be inaccurate.\n2020-08-08 18:17:32.894950: W tensorflow/core/common_runtime/graph_constructor.cc:808] Node 'lstm3B/PartitionedCall' has 5 outputs but the _output_shapes attribute specifies shapes for 22 outputs. Output shapes may be inaccurate.\n</code></pre>\n<pre><code>model_2(X_train)\nOut[3]: &lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[0.2654996 , 0.40409103, 0.33040944]], dtype=float32)&gt;\n</code></pre>\n<p>with train_step function :</p>\n<pre><code>model_2.train_step(X_train, target)\nTraceback (most recent call last):\n\n  File &quot;&lt;ipython-input-4-27db33666dda&gt;&quot;, line 1, in &lt;module&gt;\n    model_2.train_step(X_train, target)\n\nTypeError: train_step() takes 2 positional arguments but 3 were given\n</code></pre>\n<p>or with fit function :</p>\n<pre><code> model_2.compile(loss='mse', optimizer='adam')\nmodel_2.fit(X_train, target)\nTraceback (most recent call last):\n\n  File &quot;&lt;ipython-input-6-d013b7a5a810&gt;&quot;, line 2, in &lt;module&gt;\n    model_2.fit(X_train, target)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 103, in _method_wrapper\n    return method(self, *args, **kwargs)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py&quot;, line 1102, in fit\n    tmp_logs = self.train_function(iterator)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py&quot;, line 787, in __call__\n    result = self._call(*args, **kwds)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py&quot;, line 830, in _call\n    self._initialize(args, kwds, add_initializers_to=initializers)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py&quot;, line 702, in _initialize\n    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py&quot;, line 2948, in _get_concrete_function_internal_garbage_collected\n    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py&quot;, line 3319, in _maybe_define_function\n    graph_function = self._create_graph_function(args, kwargs)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py&quot;, line 3171, in _create_graph_function\n    func_graph_module.func_graph_from_py_func(\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 987, in func_graph_from_py_func\n    func_outputs = python_func(*func_args, **func_kwargs)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py&quot;, line 613, in wrapped_fn\n    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n\n  File &quot;/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py&quot;, line 974, in wrapper\n    raise e.ag_error_metadata.to_exception(e)\n\nValueError: in user code:\n\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:809 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:799 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1261 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2794 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3217 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:792 run_step  **\n        outputs = model.train_step(data)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:750 train_step\n        y_pred = self(x, training=True)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:990 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:71 return_outputs_and_add_losses\n        outputs, losses = fn(inputs, *args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:167 wrap_with_training_arg\n        return control_flow_util.smart_cond(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/utils/control_flow_util.py:112 smart_cond\n        return smart_module.smart_cond(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/smart_cond.py:54 smart_cond\n        return true_fn()\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:168 &lt;lambda&gt;\n        training, lambda: replace_training_and_call(True),\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/saved_model/utils.py:165 replace_training_and_call\n        return wrapped_call(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:787 __call__\n        result = self._call(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:821 _call\n        results = self._stateful_fn(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2921 __call__\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3319 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3171 _create_graph_function\n        func_graph_module.func_graph_from_py_func(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:987 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:613 wrapped_fn\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py:251 restored_function_body\n        raise ValueError(\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (2 total):\n        * (&lt;tf.Tensor 'data:0' shape=(None, 40, 5, 1) dtype=float32&gt;, &lt;tf.Tensor 'data_1:0' shape=(None, 40, 5, 1) dtype=float32&gt;, &lt;tf.Tensor 'data_2:0' shape=(None, 40, 5, 1) dtype=float32&gt;)\n        * True\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (2 total):\n        * [TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='data/0'), TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='data/1'), TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='data/2')]\n        * False\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (2 total):\n        * [TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='M15'), TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='H1'), TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='H4')]\n        * False\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (2 total):\n        * [TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='M15'), TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='H1'), TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='H4')]\n        * True\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (2 total):\n        * [TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='data/0'), TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='data/1'), TensorSpec(shape=(1, 40, 5, 1), dtype=tf.float32, name='data/2')]\n        * True\n      Keyword arguments: {}\n\n</code></pre>\n<p>I'm trying to resolve this bug since 1 week and I've already read many times the tensorflow guide.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 199}]
[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0"], "owner": {"account_id": 18372511, "reputation": 3, "user_id": 13382324, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-zA35Cfx5XYI/AAAAAAAAAAI/AAAAAAAAAAA/AAKWJJMdgTWdSrGK0bqMBRxA2Wn_Ew2okA/photo.jpg?sz=256", "display_name": "Tobias Johnathan", "link": "https://stackoverflow.com/users/13382324/tobias-johnathan"}, "is_answered": true, "view_count": 1854, "accepted_answer_id": 61368768, "answer_count": 2, "score": 0, "last_activity_date": 1617444366, "creation_date": 1587567844, "last_edit_date": 1587789356, "question_id": 61368378, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61368378/how-to-efficiently-load-large-training-data-too-big-for-ram-for-training-in-tens", "title": "How to efficiently load large training data too big for RAM for training in tensorflow?", "body": "<p>I've got around 10 GB of training data in numpy array format. However, my RAM is not big enough to load the data and the tensorflow 2.0 model at the same time. I've done plenty of research into tf.data, tf.TFRecords and generators, but I'm stuck in the actual implementation of the training loop. I currently have the following standard training loop, when using a loadable subset of the data to test whether everything worked. </p>\n\n<p>The options I considered are:</p>\n\n<p>1) Splitting the 10GB of numpy files into a large number of tf.TFRecords files (shuffled) with each being the size of the BATCH_SIZE and then loading them in every train_step and epoch</p>\n\n<p>2) Splitting the 10GB of numpy files into smaller files of 1-2 GB each (shuffled) and then sequentially training the model by loading the smaller files every epoch sequentially</p>\n\n<p>Is there a best practice for this, or a better option I am not considering? I imagine this should be a trivial problem, but I can't find any good solutions online.</p>\n\n<pre><code>if __name__ == '__main__':\n    # get the original_dataset\n    #train_dataset, valid_dataset\n    train_dataset = tf.data.Dataset.from_tensor_slices((mtr, labels))\n    train_dataset = train_dataset.shuffle(buffer_size=mtr.shape[0]).batch(BATCH_SIZE)\n\n    valid_dataset = tf.data.Dataset.from_tensor_slices((val_mtr, val_labels))\n    valid_dataset = valid_dataset.batch(BATCH_SIZE)\n\n    # create model\n    model = get_model()\n\n    # define loss and optimizer \n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-04, beta_1=0.9, beta_2=0.999, epsilon=1e-08,)\n\n    train_loss = tf.keras.metrics.Mean(name='train_loss')\n    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\n    valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n    valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n\n    @tf.function\n    def train_step(images, labels):\n        with tf.GradientTape() as tape:\n            predictions = model(images, training=True)\n            loss = loss_object(y_true=labels, y_pred=predictions)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))\n        train_loss(loss)\n        train_accuracy(labels, predictions)\n\n    @tf.function\n    def valid_step(images, labels):\n        predictions = model(images, training=False)\n        v_loss = loss_object(labels, predictions)\n        valid_loss(v_loss)\n        valid_accuracy(labels, predictions)\n\n    # start training\n    for epoch in range(EPOCHS):\n        train_loss.reset_states()\n        train_accuracy.reset_states()\n        valid_loss.reset_states()\n        valid_accuracy.reset_states()\n        step = 0\n        for imgs, lbls in train_dataset:\n            step += 1\n            train_step(imgs, lbls)\n            print(\"Epoch: {}/{}, step: {}/{}, loss: {:.5f}, accuracy: {:.5f}\".format(epoch + 1, EPOCHS, step, math.ceil(mtr.shape[0] / BATCH_SIZE),\n                                                                                     train_loss.result(), train_accuracy.result()))\n        for valid_images, valid_labels in valid_dataset:\n            valid_step(valid_images, valid_labels)\n\n        print(\"Epoch: {}/{}, train loss: {:.5f}, train accuracy: {:.5f}, \"\n              \"valid loss: {:.5f}, valid accuracy: {:.5f}\".format(epoch + 1, EPOCHS, train_loss.result(),\n                                                                  train_accuracy.result(), valid_loss.result(), valid_accuracy.result()))\n\n    model.save_weights(filepath=save_model_dir, save_format='tf')\n\n</code></pre>\n\n<p><strong>Based on the answer below this seemed to work:</strong></p>\n\n<pre><code>def load_data(filename: tf.Tensor):\n     coll = pickle.load(open(\"./TrainData2016/%s\" %(str(filename.numpy().decode('utf-8'))), \"rb\")) #coll[0] = data, coll[1] = label\n     return tf.convert_to_tensor(coll[0],dtype=tf.float32), tf.convert_to_tensor(coll[1],dtype=tf.int32)\n\nfilenames = os.listdir(\"./TrainData2016\") # You can just shuffle this array and not use tf.data.Dataset.shuffle()\ntrain_dataset = tf.data.Dataset.from_tensor_slices(filenames)\ntrain_dataset = train_dataset.map(lambda x: tf.py_function(func=load_data, inp=[x], Tout=(tf.float32, tf.int32)))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 236}]
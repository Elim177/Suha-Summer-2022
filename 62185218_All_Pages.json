[{"items": [{"tags": ["python", "python-3.x", "tensorflow2.0", "tensorflow2.x"], "owner": {"account_id": 474979, "reputation": 2169, "user_id": 1447953, "user_type": "registered", "accept_rate": 58, "profile_image": "https://www.gravatar.com/avatar/7950906f179a27d0b6f71d84841374cb?s=256&d=identicon&r=PG", "display_name": "Ben Farmer", "link": "https://stackoverflow.com/users/1447953/ben-farmer"}, "is_answered": false, "view_count": 143, "answer_count": 2, "score": 0, "last_activity_date": 1591268084, "creation_date": 1591233015, "last_edit_date": 1591233317, "question_id": 62185218, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62185218/unique-identifiers-for-tensorflow-2-tensors", "title": "Unique identifiers for TensorFlow 2 tensors?", "body": "<p>I have a need to be able to tell tensors clearly apart from each other in my debugging output. Let me illustrate this with an example problem:</p>\n\n<pre><code>import tensorflow as tf\n\ndef loss(x):\n    return x**2\n\nx = tf.Variable(5,dtype=float)\ny = tf.Variable(x,dtype=float)\n\nprint(\"x:\", x)\nprint(\"y:\", y)\n\nwith tf.GradientTape() as tape1:\n    z1 = loss(x)\ngrad_z1 = tape1.gradient(z1, [x])\n\nwith tf.GradientTape() as tape2:\n    z2 = loss(y)\ngrad_z2 = tape2.gradient(z2, [x]) # Variable should be y here!\n\nprint(\"grad_z1:\", grad_z1)\nprint(\"grad_z2:\", grad_z2)\n</code></pre>\n\n<p>where the output is:</p>\n\n<pre><code>x: &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0&gt;\ny: &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0&gt;\ngrad_z1: [&lt;tf.Tensor: id=25, shape=(), dtype=float32, numpy=10.0&gt;]\ngrad_z2: [None]\n</code></pre>\n\n<p>Here, I am trying to get the gradient of a simple loss function with respect to some input variable. In the \"z1\" case, the example works fine because there is a graph connection from <code>x</code> to <code>z1</code>. However it breaks in the z2 case, because there is no graph connection from <code>x</code> to <code>z2</code>. This connection was \"accidentally\" broken by initialising a new variable, <code>y</code>, from the value of <code>x</code>. The problem is obvious in this example, but in my much more complicated real code it is a lot easier to accidentally replace variables like this, destroying the calculation. I then have to dig around trying to figure out where I made some mistake like this.</p>\n\n<p>This process would be a lot easier if I could inspect the tensors and find out where they become different objects. For example, is there some sort of unique ID property or something that I can inspect? In the example above, I cannot tell that <code>x</code> and <code>y</code> are in fact completely different variables from the printed output. They look identical, but of course are not. </p>\n\n<p>So I need something else that I can print to help track down when <code>x</code> accidentally was swapped for <code>y</code>. Is there any such property? Surely there must be, but I cannot find one. Maybe I could print the memory address of the objects or something instead, but I'm not sure how to do that in Python either.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 179}]
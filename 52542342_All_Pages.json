[{"items": [{"tags": ["python", "tensorflow", "keras", "lstm"], "owner": {"account_id": 12080232, "reputation": 73, "user_id": 10425539, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/7f21aa16917e6c126d32b09706dcfe42?s=256&d=identicon&r=PG&f=1", "display_name": "danajer", "link": "https://stackoverflow.com/users/10425539/danajer"}, "is_answered": false, "view_count": 304, "answer_count": 0, "score": 2, "last_activity_date": 1538069684, "creation_date": 1538069684, "question_id": 52542342, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/52542342/keras-derivatives-of-output-with-respect-to-time-with-lstm", "title": "Keras: Derivatives of output with respect to time with LSTM", "body": "<p>I have been trying to model nonlinear dynamic systems with LSTM networks using Keras. I have had success by simply using the Keras LSTM networks, where I define my input/output relationship something like the following pseudo-code:</p>\n\n<p><code>x[t] = NN(y[t-200:t],x[t-200-1:t-1])</code></p>\n\n<p>Where <strong>y</strong> would be my forcing function and <strong>x</strong> is the variable I'm after. So I use the past 200 points to estimate the next point. I do this recursively by adding the newly predicted point to my \"past outputs\" vector. </p>\n\n<p>Now I would like to add some information about the PDE that I'm solving to the loss function, so I need to compute derivatives with respect to time. I have read <a href=\"https://stackoverflow.com/questions/50040056/derivatives-of-n-dimensional-function-in-keras?noredirect=1&amp;lq=1\">this answer</a> and the related answers to get started but I can't seem to get that workflow to work with LSTMs. First of all, <strong>time</strong> is not an explicit variable in my workflow, so I would need to add it as an input to accommodate the workflow in that answer. </p>\n\n<p>So I could add the <strong>time</strong> vector to the list of inputs, and then try to compute derivatives of the output with respect to the input:</p>\n\n<pre><code>_df1 = grad(model.output,model.input)\ndf1 = tf.Print( _df1, [ _df1 ], message = \"df1\" )\n</code></pre>\n\n<p>For reference, my input dimension is (?,200,3) and my output dimension is simply (?,1). The code above works and I get a (?,200,3) tensor. But when I try to compute the second derivative like so:</p>\n\n<pre><code>_df2 = grad(df1,model.input)\ndf2 = tf.Print( _df2, [ _df2 ], message = \"df2\" )\n</code></pre>\n\n<p>Then I get the error:\n<strong>TypeError: Second-order gradient for while loops not supported.</strong></p>\n\n<p>Since I only need the derivatives at the current timestep (t), I have tried slicing the tensor, but that doesn't work either. </p>\n\n<pre><code>_df2 = grad(df1[:,-1,-1],model.input)\ndf2 = tf.Print( _df2, [ _df2 ], message = \"df2\" )\n</code></pre>\n\n<p>Even if I could do something like that, I am not too comfortable adding the <strong>time</strong> vector as an explicit input. I have considered computing the derivative numerically with <em>diff()</em> (given a constant dt), but I am not sure how to do that here when dealing with tensors. </p>\n\n<p>So I'd appreciate any suggestions or ideas to help me solve this problem. Ultimately, I'd like to add the homogeneous portion of the PDE to the loss function. At this point, my equation only has derivatives with respect to time. </p>\n\n<p>Thanks.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 71}]
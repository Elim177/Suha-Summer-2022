[{"items": [{"tags": ["python-3.x", "tensorflow", "machine-learning", "keras", "tensorflow-datasets"], "owner": {"account_id": 2900726, "reputation": 1231, "user_id": 11918892, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/CiMYh.jpg?s=256&g=1", "display_name": "maurera", "link": "https://stackoverflow.com/users/11918892/maurera"}, "is_answered": true, "view_count": 140, "accepted_answer_id": 60714146, "answer_count": 2, "score": 1, "last_activity_date": 1584398796, "creation_date": 1584379980, "question_id": 60710598, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60710598/how-to-use-tensorflow-2-dataset-api-with-keras", "title": "How to use Tensorflow 2 Dataset API with Keras?", "body": "<p>This question has been answered for Tensorflow 1, eg: <a href=\"https://stackoverflow.com/questions/46135499/how-to-properly-combine-tensorflows-dataset-api-and-keras\">How to Properly Combine TensorFlow's Dataset API and Keras?</a>, but this answer hasn't helped for my use case.</p>\n\n<p>Below is an example of a model with three float32 inputs and one float32 output. I have a large amount of data that doesn't all fit into memory at once, so it's split into separate files. I'm trying to use the <a href=\"https://www.tensorflow.org/api_docs/python/tf/data/Dataset\" rel=\"nofollow noreferrer\">Dataset API</a> to train a model by bringing in a portion of the training data at once.</p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow.keras.layers as layers\nimport numpy as np\n\n# Create TF model of a given architecture (number of hidden layers, layersize, #outputs, activation function)\ndef create_model(h=2, l=64, activation='relu'):\n    model = tf.keras.Sequential([\n        layers.Dense(l, activation=activation, input_shape=(3,), name='input_layer'),\n        *[layers.Dense(l, activation=activation) for _ in range(h)],\n        layers.Dense(1, activation='linear', name='output_layer')])\n    return model\n\n# Load data (3 X variables, 1 Y variable) split into 5 files\n# (for this example, just create a list 5 numpy arrays)\nlist_of_training_datasets = [np.random.rand(10,4).astype(np.float32) for _ in range(5)]\nvalidation_dataset = np.random.rand(30,4).astype(np.float32)\n\ndef data_generator():\n    for data in list_of_training_datasets:\n        x_data = data[:, 0:3]\n        y_data = data[:, 3:4]\n        yield((x_data,y_data))\n\n# prepare model\nmodel = create_model(h=2,l=64,activation='relu')\nmodel.compile(loss='mse', optimizer=tf.keras.optimizers.Adam())\n\n# load dataset\ndataset = tf.data.Dataset.from_generator(data_generator,(np.float32,np.float32))\n\n# fit model\nmodel.fit(dataset, epochs=100, validation_data=(validation_dataset[:,0:3],validation_dataset[:,3:4]))\n</code></pre>\n\n<p>Running this, I get the error:</p>\n\n<blockquote>\n  <p><strong>ValueError: Cannot take the length of shape with unknown rank.</strong></p>\n</blockquote>\n\n<p>Does anyone know how to get this working? I would also like to be able to use the batch dimension, to load two data files at a time, for example.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 2}]
[{"items": [{"tags": ["python", "deep-learning", "tensorflow2.0", "style-transfer"], "owner": {"account_id": 97477, "reputation": 4181, "user_id": 264410, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/98a0fcc9a9390a0863a901945c45212c?s=256&d=identicon&r=PG", "display_name": "michael", "link": "https://stackoverflow.com/users/264410/michael"}, "is_answered": false, "view_count": 562, "answer_count": 1, "score": 1, "last_activity_date": 1625064046, "creation_date": 1582182214, "question_id": 60314478, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60314478/how-to-write-a-keras-custom-loss-function-when-you-need-the-input-value-to-calcu", "title": "how to write a keras custom loss function when you need the input value to calculate loss?", "body": "<p><img src=\"https://miro.medium.com/max/855/1*TdkNFoecrvBZZbLOHGse0Q.png\" alt=\"perceptual losses\"></p>\n\n<p>I'm trying to duplicate a fast style transfer paper (see diagram above) using the method described in <a href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate#part_i_using_built-in_training_evaluation_loops\" rel=\"nofollow noreferrer\">keras built-in training and evaluation loops</a></p>\n\n<p>I'm having problems understanding how to do this with a custom loss class (see below). </p>\n\n<p>In order to calculate the loss components, I need the following:</p>\n\n<ul>\n<li><code>y_hat</code>, the generated image to get </li>\n</ul>\n\n<pre><code>(generated_content_features, generated_style_features) = VGG(y_hat)\ngenerated_style_gram = [ utils.gram(value) for value in generated_style_features ]\n</code></pre>\n\n<ul>\n<li><code>target_style_gram</code> which is static so I can derive once from <code>target_style_features</code> and cache, <code>(_,target_style_features) = VGG(y_s)</code></li>\n<li><code>x</code>, the InputImage (same as <code>y_c</code> ContentTarget) to get <code>(target_content_features, _) = VGG(x)</code></li>\n</ul>\n\n<p>I find that I'm monkey-patching a whole lot of stuff in the loss class, <code>tf.keras.losses.Loss</code>, in order to derive these values and ultimately perform a loss calculation. This is particularly true of the <code>target_content_features</code> which requires the <strong>input image</strong>, something that I passed in through <code>y_true</code>, but that is obviously a hack</p>\n\n<pre><code>y_pred = generated_image # y_hat from diagram, shape=(b,256,256,3)\ny_true = x # hack: access the input image here\n\nlossFn = PerceptualLosses_Loss(VGG, target_style_gram)\nloss = lossFn(y_true, y_pred)\n\n</code></pre>\n\n<pre><code>\nclass PerceptualLosses_Loss(tf.losses.Loss):\n  name=\"PerceptualLosses_Loss\"\n  reduction=tf.keras.losses.Reduction.AUTO\n  RGB_MEAN_NORMAL_VGG = tf.constant( [0.48501961, 0.45795686, 0.40760392], dtype=tf.float32)\n\n  def __init__(self, loss_network, target_style_gram, loss_weights=None):\n    super(PerceptualLosses_Loss, self).__init__( name=self.name, reduction=self.reduction )\n    self.target_style_gram = target_style_gram # repeated in y_true\n    print(\"PerceptualLosses_Loss init()\", type(target_style_gram), type(self.target_style_gram))\n    self.VGG = loss_network\n\n  def call(self, y_true, y_pred):\n\n    b,h,w,c = y_pred.shape\n    #???: y_pred.shape=(None, 256,256,3), need batch dim for utils.gram(value)\n    generated_batch = tf.reshape(y_pred, (BATCH_SIZE,h,w,c) )\n\n    # generated_batch: expecting domain=(+-int), mean centered\n    generated_batch = tf.nn.tanh(generated_batch) # domain=(-1.,1.), mean centered\n\n    # reverse VGG mean_center\n    generated_batch = tf.add( generated_batch, self.RGB_MEAN_NORMAL_VGG) # domain=(0.,1.)\n    generated_batch_BGR_centered = tf.keras.applications.vgg19.preprocess_input(generated_batch*255.)/255.\n    generated_content_features, generated_style_features = self.VGG( generated_batch_BGR_centered, preprocess=False )\n    generated_style_gram = [ utils.gram(value)  for value in generated_style_features ]  # list\n\n    y_pred = generated_content_features + generated_style_gram\n    # print(\"PerceptualLosses_Loss: y_pred, output_shapes=\", type(y_pred), [v.shape for v in y_pred])\n    # PerceptualLosses_Loss: y_pred, output_shapes= [\n    #   TensorShape([4, 16, 16, 512]), \n    #   TensorShape([4, 64, 64]), \n    #   TensorShape([4, 128, 128]), \n    #   TensorShape([4, 256, 256]), \n    #   TensorShape([4, 512, 512]), \n    #   TensorShape([4, 512, 512])\n    # ]\n\n    if tf.is_tensor(y_true):\n      # print(\"detect y_true is image\", type(y_true), y_true.shape)\n      x_train = y_true\n      x_train_BGR_centered = tf.keras.applications.vgg19.preprocess_input(x_train*255.)/255.\n      target_content_features, _ = self.VGG(x_train_BGR_centered, preprocess=False )\n      # ???: target_content_features[0].shape=(None, None, None, 512), should be shape=(4, 16, 16, 512)\n      target_content_features = [tf.reshape(v, generated_content_features[i].shape) for i,v in enumerate(target_content_features)]\n    elif isinstance(y_true, tuple):\n      print(\"detect y_true is tuple(target_content_features + self.target_style_gram)\", y_true[0].shape)\n      target_content_features = y_true[:len(generated_content_features)]\n      if self.target_style_gram is None:\n        self.target_style_gram = y_true[len(generated_content_features):]\n    else:\n      assert False, \"unexpected result for y_true\"\n\n    # losses = tf.keras.losses.MSE(y_true, y_pred)\n    def batch_reduce_sum(y_true, y_pred, weight, name):\n      losses = tf.zeros(BATCH_SIZE)\n      for a,b in zip(y_true, y_pred):\n        # batch_reduce_sum()\n        loss = tf.keras.losses.MSE(a,b)\n        loss = tf.reduce_sum(loss, axis=[i for i in range(1,len(loss.shape))] )\n        losses = tf.add(losses, loss)\n      return tf.multiply(losses, weight, name=\"{}_loss\".format(name)) # shape=(BATCH_SIZE,)\n\n    c_loss = batch_reduce_sum(target_content_features, generated_content_features, CONTENT_WEIGHT, 'content_loss')\n    s_loss = batch_reduce_sum(self.target_style_gram, generated_style_gram, STYLE_WEIGHT, 'style_loss')\n    return (c_loss, s_loss)\n</code></pre>\n\n<p>I also tried to pre-calculate <code>y_true</code> in the <code>tf.data.Dataset</code>, but while it worked fine under <code>eager execution</code>, it caused an error during <code>model.fit()</code></p>\n\n<pre><code>xy_true_Dataset = tf.data.Dataset.from_generator(\n    xyGenerator_y_true(image_ds, VGG, target_style_gram),\n    output_types=(tf.float32, (tf.float32,  tf.float32,tf.float32,tf.float32,tf.float32,tf.float32) ),\n    output_shapes=(\n      (256,256,3),\n      ( (16, 16, 512), (64, 64), (128, 128), (256, 256), (512, 512), (512, 512)) \n    ),\n  )\n\n# eager execution, y_true: &lt;class 'tuple'&gt; [TensorShape([4, 16, 16, 512]), TensorShape([4, 64, 64]), TensorShape([4, 128, 128]), TensorShape([4, 256, 256]), TensorShape([4, 512, 512]), TensorShape([4, 512, 512])]\n# model.fit(), y_true: &lt;class 'tensorflow.python.framework.ops.Tensor'&gt; (None, None, None, None)\n\nValueError: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), for inputs ['output_1'] but instead got the following list of 6 arrays: [&lt;tf.Tensor 'args_1:0' shape=(None, 16, 16, 512) dtype=float32&gt;, &lt;tf.Tensor 'args_2:0' shape=(None, 64, 64) dtype=float32&gt;, &lt;tf.Tensor 'args_3:0' shape=(None, 128, 128) dtype=float32&gt;, &lt;tf.Tensor 'arg...\n</code></pre>\n\n<p>Do I have the complete wrong approach to this problem?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 62}]
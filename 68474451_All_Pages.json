[{"items": [{"tags": ["tensorflow", "machine-learning", "autoencoder", "gradienttape", "custom-training"], "owner": {"account_id": 20445571, "reputation": 117, "user_id": 15001463, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/QwZ6d.jpg?s=256&g=1", "display_name": "Jared Frazier", "link": "https://stackoverflow.com/users/15001463/jared-frazier"}, "is_answered": false, "view_count": 299, "answer_count": 1, "score": 0, "last_activity_date": 1627017664, "creation_date": 1626890580, "last_edit_date": 1626966974, "question_id": 68474451, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68474451/custom-training-loop-for-tensorflow-variational-autoencoder-tape-gradientloss", "title": "Custom Training Loop for Tensorflow Variational Autoencoder: `tape.gradient(loss, decoder_model.trainable_weights)` Always Returns List Full of None&#39;s", "body": "<p>I am trying to write a custom training loop for a variational autoencoder (VAE) that consists of two separate <code>tf.keras.Model</code> objects. The objective of this VAE is multi-class classification. As usual, the outputs of the encoder model are fed as inputs to the decoder model. The decoder is a recurrent decoder. Also as usual, two loss functions are involved in the VAE: reconstruction loss (categorical cross entropy) and latent loss. The inspiration for my current architecture is based on a pytorch implementation at this <a href=\"https://github.com/aspuru-guzik-group/selfies/blob/master/examples/vae_example/chemistry_vae.py\" rel=\"nofollow noreferrer\">github</a>.</p>\n<p><strong>Problem</strong>: Whenever I calculate the gradients using <code>tape.gradient(loss, decoder.trainable_weights)</code> for the decoder model, the returned list has only NoneType objects for each element. I assume I am making some mistake with the use of the <code>reconstruction_tensor</code>, which is near the bottom of the code I have written below. Since I need to have the iterative decoding process, how can I use something like the <code>reconstruction_tensor</code> without returning a list of NoneType elements for gradients? You may run the code using this <a href=\"https://colab.research.google.com/drive/1APBNIGz0O5-376Iaf4oGe5zMswWaTUgS?usp=sharing\" rel=\"nofollow noreferrer\">colab notebook</a> if you wish.</p>\n<p>To further clarify what the tensors in this problem look like, I shall illustrate the original input, the zeros tensor to which predicted 'tokens' will be assigned, and a single update of the zeroes tensor based on the predicted 'tokens' from the decoder:</p>\n<pre><code>Example original input tensor of shape (batch_size, max_seq_length, num_classes):\n _    _         _     _         _     _         _    _\n|    |  1 0 0 0  |   |  0 1 0 0  |   |  0 0 0 1  |    |\n|    |  0 1 0 0  |   |  1 0 0 0  |   |  1 0 0 0  |    |\n|_   |_ 0 0 1 0 _| , |_ 0 0 0 1 _|,  |_ 0 1 0 0 _|   _|\n\nInitial zeros tensor:\n _    _         _     _         _     _         _    _\n|    |  0 0 0 0  |   |  0 0 0 0  |   |  0 0 0 0  |    |\n|    |  0 0 0 0  |   |  0 0 0 0  |   |  0 0 0 0  |    |\n|_   |_ 0 0 0 0 _| , |_ 0 0 0 0 _|,  |_ 0 0 0 0 _|   _|\n\nExample zeros tensor after a single iteration of the decoding loop:\n _    _                 _     _                 _     _                   _    _\n|    |  0.2 0.4 0.1 0.3  |   |  0.1 0.2 0.6 0.1  |   |  0.7 0.05 0.05 0.2  |    |\n|    |  0   0   0   0    |   |  0   0   0   0    |   |  0   0    0    0    |    |\n|_   |_ 0   0   0   0   _| , |_ 0   0   0   0   _|,  |_ 0   0    0    0   _|   _|\n</code></pre>\n<p>Here is the code to reproduce the problem:</p>\n<pre><code># Arbitrary data\nbatch_size = 3  \nmax_seq_length = 3\nnum_classes = 4\noriginal_inputs = tf.one_hot(tf.argmax((np.random.randn(batch_size, max_seq_length, num_classes)), axis=2), depth=num_classes)\nlatent_dims = 5  # Must be less than (max_seq_length * num_classes)\n\ndef sampling(inputs):\n    &quot;&quot;&quot;Reparametrization function. Used for Lambda layer&quot;&quot;&quot;\n\n    mus, log_vars = inputs\n    epsilon = tf.keras.backend.random_normal(shape=tf.keras.backend.shape(mus))\n    z = mus + tf.keras.backend.exp(log_vars/2) * epsilon\n\n    return z\n\ndef latent_loss_fxn(mus, log_vars):\n    &quot;&quot;&quot;Return latent loss for means and log variance.&quot;&quot;&quot;\n\n    return -0.5 * tf.keras.backend.mean(1. + log_vars - tf.keras.backend.exp(log_vars) - tf.keras.backend.pow(mus, 2))\n\nclass DummyEncoder(tf.keras.Model):\n    def __init__(self, latent_dimension):\n        &quot;&quot;&quot;Define the hidden layer (bottleneck) and sampling layers&quot;&quot;&quot;\n\n        super().__init__()\n        self.hidden = tf.keras.layers.Dense(units=32)\n        self.dense_mus = tf.keras.layers.Dense(units=latent_dimension)\n        self.dense_log_vars = tf.keras.layers.Dense(units=latent_dimension)\n        self.sampling = tf.keras.layers.Lambda(function=sampling)\n\n    def call(self, inputs):\n        &quot;&quot;&quot;Define forward computation that outputs z, mu, log_var of input.&quot;&quot;&quot;\n\n        dense_projection = self.hidden(inputs)\n\n        mus = self.dense_mus(dense_projection)\n        log_vars = self.dense_log_vars(dense_projection)\n        z = self.sampling([mus, log_vars])\n\n        return z, mus, log_vars\n        \n\nclass DummyDecoder(tf.keras.Model):\n    def __init__(self, num_classes):\n        &quot;&quot;&quot;Define GRU layer and the Dense output layer&quot;&quot;&quot;\n\n        super().__init__()\n        self.gru = tf.keras.layers.GRU(units=1, return_sequences=True, return_state=True)\n        self.dense = tf.keras.layers.Dense(units=num_classes, activation='softmax')\n\n    def call(self, x, hidden_states=None):\n        &quot;&quot;&quot;Define forward computation&quot;&quot;&quot;\n\n        outputs, h_t = self.gru(x, hidden_states)\n\n        # The purpose of this computation is to use the unnormalized log\n        # probabilities from the GRU to produce normalized probabilities via\n        # the softmax activation function in the Dense layer\n        reconstructions = self.dense(outputs)\n\n        return reconstructions, h_t\n\n# Instantiate the models\nencoder_model = DummyEncoder(latent_dimension=5)\ndecoder_model = DummyDecoder(num_classes=num_classes)\n\n# Instantiate reconstruction loss function\ncce_loss_fxn = tf.keras.losses.CategoricalCrossentropy()\n\n# Begin tape\nwith tf.GradientTape(persistent=True) as tape:\n    # Flatten the inputs for the encoder\n    reshaped_inputs = tf.reshape(original_inputs, shape=(tf.shape(original_inputs)[0], -1))\n\n    # Encode the input\n    z, mus, log_vars = encoder_model(reshaped_inputs, training=True)\n\n    # Expand dimensions of z so it meets recurrent decoder requirements of\n    # (batch, timesteps, features)\n    z = tf.expand_dims(z, axis=1)\n\n    ################################\n    # SUSPECTED CAUSE OF PROBLEM\n    ################################\n\n    # A tensor that will be modified based on model outputs\n    reconstruction_tensor = tf.Variable(tf.zeros_like(original_inputs))\n\n    ################################\n    # END SUSPECTED CAUSE OF PROBLEM\n    ################################\n\n    # A decoding loop to iteratively generate the next token (i.e., outputs)... \n    # in the sequence\n    hidden_states = None\n    for ith_token in range(max_seq_length):\n\n        # Reconstruct the ith_token for a given sample in the batch\n        reconstructions, hidden_states = decoder_model(z, hidden_states, training=True)\n\n        # Reshape the reconstructions to allow assigning to reconstruction_tensor\n        reconstructions = tf.squeeze(reconstructions)\n\n        # After the loop is done iterating, this tensor is the model's prediction of the \n        # original inputs. Therefore, after a single iteration of the loop, \n        # a single token prediction for each sample in the batch is assigned to\n        # this tensor.\n        reconstruction_tensor = reconstruction_tensor[:, ith_token,:].assign(reconstructions)\n\n    # Calculates losses\n    recon_loss = cce_loss_fxn(original_inputs, reconstruction_tensor)\n    latent_loss = latent_loss_fxn(mus, log_vars)\n    loss = recon_loss + latent_loss\n\n# Calculate gradients\nencoder_gradients = tape.gradient(loss, encoder_model.trainable_weights)\ndecoder_gradients = tape.gradient(loss, decoder_model.trainable_weights)\n\n# Release tape\ndel tape\n\n# Inspect gradients\nprint('Valid Encoder Gradients:', not(None in encoder_gradients))\nprint('Valid Decoder Gradients:', not(None in decoder_gradients), ' -- ', decoder_gradients)\n\n&gt;&gt;&gt; Valid Encoder Gradients: True\n&gt;&gt;&gt; Valid Decoder Gradients: False -- [None, None, None, None, None]\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 100}]
[{"items": [{"tags": ["tensorflow", "deep-learning", "pytorch"], "owner": {"account_id": 12711637, "reputation": 493, "user_id": 9235106, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/23bca8cc0465ec0ee449ce605c48b702?s=256&d=identicon&r=PG&f=1", "display_name": "Jiawei Lu", "link": "https://stackoverflow.com/users/9235106/jiawei-lu"}, "is_answered": false, "view_count": 147, "answer_count": 0, "score": 1, "last_activity_date": 1616095979, "creation_date": 1616095979, "question_id": 66697923, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66697923/is-tensorflow-always-faster-than-pytorch-when-running-in-graph-mode", "title": "Is Tensorflow always faster than Pytorch when running in graph mode", "body": "<p>I have a simple model implemented using Tensorflow and Pytorch. I noticed that the tensorflow implementation runs over 3x faster than the Pytorch implementation when I use graph mode in tf (as I know Pytorch only supports eager mode). Does that mean Tensorflow is faster than Pytorch when running in graph mode?</p>\n<p>I run both implementations on CPU.\nTensorflow version: 2.4.1,\nPytorch version: 1.7.1,\nOS: windows 7</p>\n<p>Tensorflow implementation</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport time\n\nnumber_of_neural_units = 10\nnumber_of_points = 101\nnumber_of_epochs = 50000\nloss_eva_interval = 1000\n\n\nx_train = np.linspace(0, 2, number_of_points, endpoint=True).reshape((number_of_points, 1))\ny_actual = np.exp(-0.5 * x_train**2) / (1 + x_train + x_train**3) + x_train**2\nx_train_ts = tf.constant(x_train, dtype=tf.float32)\n\nloss_array = np.zeros([int(number_of_epochs / loss_eva_interval) + 1])\n\n\nclass Model:\n    def __init__(self):    \n        self.W = tf.Variable(tf.random.normal([1, number_of_neural_units]), name='W')\n        self.b = tf.Variable(tf.random.normal([number_of_neural_units]), name='b')\n        self.W1 = tf.Variable(tf.random.normal([number_of_neural_units, 1]), name='W1')\n        self.b1 = tf.Variable(tf.random.normal([1]), name='b1')\n\n        self.y1 = None\n        self.y = None\n\n        self.trainable_weights = [self.W, self.b, self.W1, self.b1]\n\n\n    def __call__(self, x):\n        self.y1 = tf.nn.sigmoid(tf.add(tf.matmul(x, self.W), self.b))\n        self.y = tf.add(tf.matmul(self.y1, self.W1), self.b1)\n        return self.y\n\n\ndef total_loss():\n    model(x_train_ts)\n    lq = (1 + 3 * (x_train_ts ** 2)) / (1 + x_train_ts + x_train_ts ** 3)\n\n    dif = tf.matmul(tf.multiply(model.y1 * (1 - model.y1), model.W), model.W1)\n\n    t_loss = (dif + (x_train_ts + lq) * model.y - x_train_ts**3 - 2 * x_train_ts - lq * x_train_ts**2)**2\n    loss = tf.reduce_mean(t_loss) + (model.y[0] - 1)**2\n    return loss\n\n\n@tf.function\ndef updateWeights():\n    with tf.GradientTape() as tape:\n        loss = total_loss()\n    grads = tape.gradient(loss, model.trainable_weights)\n    opt.apply_gradients(zip(grads, model.trainable_weights))\n\n\ndef trainModel():\n    t_start = time.time()\n\n    current_lost = total_loss().numpy()\n    loss_array[0] = current_lost\n    print('epoch {}: loss: {}'.format(0, current_lost))\n\n    for epoch in range(1, number_of_epochs + 1):\n\n        updateWeights()\n\n        if epoch % loss_eva_interval == 0:\n            current_lost = total_loss().numpy()\n            loss_array[int(epoch / loss_eva_interval)] = current_lost\n            print('epoch {}: loss: {}'.format(epoch, current_lost))\n\n    t_end = time.time()\n    print('total time: {}'.format(t_end - t_start))\n\n\n\nif __name__ == '__main__':\n    model = Model()\n    opt = tf.optimizers.Adam(learning_rate=0.01)\n    trainModel()\n</code></pre>\n<p>Pytorch implementation:</p>\n<pre><code>import torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport time\n\n\nnumber_of_neural_units = 10\nnumber_of_points = 101\nnumber_of_epochs = 50000\nloss_eva_interval = 1000\n\nx_train = np.linspace(0, 2, number_of_points, endpoint=True, dtype=np.float32).reshape((number_of_points, 1))\ny_actual = np.exp(-0.5 * x_train ** 2) / (1 + x_train + x_train ** 3) + x_train ** 2\nx_train_ts = torch.from_numpy(x_train)\n\nloss_array = np.zeros([int(number_of_epochs / loss_eva_interval) + 1])\n\n\nclass Model:\n    def __init__(self):\n        self.W = Variable(torch.randn(1, number_of_neural_units), requires_grad=True)\n        self.b = Variable(torch.randn(number_of_neural_units), requires_grad=True)\n        self.W1 = Variable(torch.randn(number_of_neural_units, 1), requires_grad=True)\n        self.b1 = Variable(torch.randn(1), requires_grad=True)\n\n        self.y1 = None\n        self.y = None\n\n        self.trainable_weights = [self.W, self.b, self.W1, self.b1]\n\n    def __call__(self, x):\n        self.y1 = torch.sigmoid(torch.add(torch.matmul(x, self.W), self.b))\n        self.y = torch.add(torch.matmul(self.y1, self.W1), self.b1)\n        return self.y\n\n\ndef total_loss():\n    model(x_train_ts)\n    lq = (1 + 3 * (x_train_ts ** 2)) / (1 + x_train_ts + x_train_ts ** 3)\n\n    dif = torch.matmul(torch.multiply(model.y1 * (1 - model.y1), model.W), model.W1)  # dy/dx [100,1]\n\n    t_loss = (dif + (x_train_ts + lq) * model.y - x_train_ts ** 3 - 2 * x_train_ts - lq * x_train_ts ** 2) ** 2\n    loss = t_loss.mean() + (model.y[0] - 1) ** 2\n    return loss\n\n\ndef updateWeights():\n    opt.zero_grad()\n    loss = total_loss()\n    loss.backward()\n    opt.step()\n\n\ndef trainModel():\n    t_start = time.time()\n\n    current_lost = total_loss().detach().numpy()\n    loss_array[0] = current_lost\n    print('epoch {}: loss: {}'.format(0, current_lost))\n\n    for epoch in range(1, number_of_epochs + 1):\n\n        updateWeights()\n\n        if epoch % loss_eva_interval == 0:\n            current_lost = total_loss().detach().numpy()\n            loss_array[int(epoch / loss_eva_interval)] = current_lost\n            print('epoch {}: loss: {}'.format(epoch, current_lost))\n\n    t_end = time.time()\n    print('total time: {}'.format(t_end - t_start))\n\n\nif __name__ == '__main__':\n    model = Model()\n\n    opt = torch.optim.Adam(model.trainable_weights, lr=0.01)\n    trainModel()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 240}]
[{"items": [{"tags": ["tensorflow", "neural-network", "deep-learning"], "owner": {"account_id": 1132467, "reputation": 3715, "user_id": 1118236, "user_type": "registered", "accept_rate": 41, "profile_image": "https://i.stack.imgur.com/Bsd9i.jpg?s=256&g=1", "display_name": "Munichong", "link": "https://stackoverflow.com/users/1118236/munichong"}, "is_answered": false, "view_count": 299, "answer_count": 0, "score": 2, "last_activity_date": 1511370824, "creation_date": 1511370824, "question_id": 47440435, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/47440435/tensor-flow-cnn-accuracy-are-always-zero", "title": "Tensor flow CNN accuracy are always zero", "body": "<p>I am using CNN to do short text classification. I know that if overfitting may lead all neurons become zero. But it is wired that, when overfitting, all training batch accuracies are 1, which does not make sense because not all true categories are 0. I think the accuracy should be low, but not 1.</p>\n\n<p>Below is the part of the code that I use:</p>\n\n<pre><code>... Define some input placeholders here ...\n\npooled_outputs = []\nfor filter_size in filter_sizes:\n    filter_shape = [filter_size, embed_dimen, 1, num_filters]\n    W_filter = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1))\n    b_filter = tf.Variable(tf.constant(0.1, shape=[num_filters]))\n    x_embed_expanded = tf.expand_dims(x_embed, -1)\n    conv = tf.nn.conv2d(x_embed_expanded, W_filter, strides=[1, 1, 1, 1], padding=\"VALID\")\n    h = tf.nn.relu(tf.nn.bias_add(conv, b_filter), name=\"relu\")\n    pooled = tf.nn.max_pool(h, ksize=[1, self.params['max_domain_segments_len'] - filter_size + 1, 1, 1],\n                                        strides=[1, 1, 1, 1], padding='VALID')\n    pooled_outputs.append(pooled)\nh_pool = tf.concat(pooled_outputs, axis=3)\nnum_filters_total = num_filters * len(filter_sizes)\noutput_vec = tf.reshape(h_pool, [-1, num_filters_total])\n\nlogits = tf.contrib.layers.fully_connected(output_vec, num_outputs=n_rnn_neurons, activation_fn=tf.nn.relu)\n\nlogits = tf.contrib.layers.fully_connected(logits, self.params['num_targets'], activation_fn=tf.nn.relu)\n\ncrossentropy = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n\nloss_mean = tf.reduce_mean(crossentropy)\noptimizer = tf.train.AdamOptimizer(learning_rate=lr_rate)\ntraining_op = optimizer.minimize(loss_mean)\n\nprediction = tf.argmax(logits, axis=-1)\nis_correct = tf.nn.in_top_k(logits, y, 1) # logits are unscaled, but here we only care the argmax\nn_correct = tf.reduce_sum(tf.cast(is_correct, tf.float32))\naccuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n\ninit = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    init.run()\n    ......\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 83}]
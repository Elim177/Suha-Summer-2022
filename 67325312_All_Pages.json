[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.x"], "owner": {"account_id": 21380008, "reputation": 49, "user_id": 15746088, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b9cbfe75e9c02e2a82afba4aa9a26c6f?s=256&d=identicon&r=PG&f=1", "display_name": "leodreieck", "link": "https://stackoverflow.com/users/15746088/leodreieck"}, "is_answered": false, "view_count": 372, "answer_count": 0, "score": 1, "last_activity_date": 1623085884, "creation_date": 1619731862, "last_edit_date": 1623085884, "question_id": 67325312, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67325312/simultaneously-training-multiple-tensorflow-models", "title": "Simultaneously training multiple tensorflow models", "body": "<p>For a multi-agent RL problem, I am training multiple models simultaneously. Currently my structure looks as follows:</p>\n<pre class=\"lang-py prettyprint-override\"><code>class testmodel(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.l1 = tf.keras.layers.Dense(20)\n        self.l2 = tf.keras.layers.Dense(20)\n        self.l3 = tf.keras.layers.Dense(2, activation = &quot;softmax&quot;)\n\n    def call(self, x):\n        y = self.l1(x)\n        y = self.l2(y)\n        y = self.l3(y)\n        return y\n\nclass MARL():\n    def __init__(self, nAgents, input_shape):\n        self.nAgents = nAgents\n        self.list_of_actors = list()\n        self.list_of_optimizers = list()\n        for agent in range(nAgents):\n            self.list_of_actors.append(testmodel())\n            self.list_of_optimizers.append(tf.keras.optimizers.Adam(learning_rate = 0.001))\n            self.list_of_actors[agent].build(input_shape = input_shape)\n\n    @tf.function\n    def learn_for_loop(self):\n        x = np.random.random_sample((20,)).tolist()\n        x = tf.expand_dims(x, 0)\n\n        for agent in range(self.nAgents):\n            with tf.GradientTape() as g:\n\n                y_hat = self.list_of_actors[agent](x)\n                loss = y_hat - tf.constant([0.,0])\n\n            grads = g.gradient(loss, self.list_of_actors[agent].trainable_variables)\n            self.list_of_optimizers[agent].apply_gradients(zip(grads, self.list_of_actors[agent].trainable_variables))\n\n    @tf.function\n    def learn_tf_loop(self):\n\n        def body(i,x):\n            with tf.GradientTape() as g:\n\n                y_hat = self.list_of_actors[i](x) ### throws error a)\n                loss = y_hat - tf.constant([0.,0])\n\n            grads = g.gradient(loss, self.list_of_actors[i].trainable_variables)\n            self.list_of_optimizers[agent].apply_gradients(zip(grads, self.list_of_actors[agent].trainable_variables)) ### throws error b)\n\n            return (tf.add(i,1),x)\n\n        def condition(i,x):\n            return tf.less(i,self.nAgents)\n\n        i = tf.constant(0)\n        x = np.random.random_sample((20,)).tolist()\n        x = tf.expand_dims(x, 0)\n\n        r = tf.while_loop(condition, body, (i,x))\n\n</code></pre>\n<p>If I now compare the runtimes on my CPU, I get the following results:</p>\n<pre class=\"lang-py prettyprint-override\"><code>test_instance = MARL(10, (1,20))\n\ntic = time.time()   \nfor _ in range(100):\n    test_instance.learn_for_loop()\nprint(time.time() - tic)\n# without @tf.function: ~ 7s\n# with @tf.function: ~ 3.5s # cut runtime by half, GREAT\n\ntic = time.time()   \nfor _ in range(100):\n    test_instance.learn_tf_loop()\nprint(time.time() - tic)\n# without @tf.function: ~ 7s\n# with @tf.function: super problematic\n</code></pre>\n<p>In my understanding, the more tensorflowy &quot;learn_tf_loop&quot; should be faster than &quot;learn_for_loop&quot;, especially for larger models and when using a GPU. I expect these effects to become apparent especially with the tf.function decorator. Unfortunately, this leads to errors like a) &quot;TypeError: list indices must be integers or slices, not Tensor&quot; and b) &quot;ValueError: tf.function-decorated function tried to create variables on non-first call.&quot; These errors have been dealt with at a) <a href=\"https://stackoverflow.com/questions/63838440/select-an-item-from-a-list-of-object-of-any-type-when-using-tensorflow-2-x\">Select an item from a list of object of any type when using tensorflow 2.x</a> and b) <a href=\"https://github.com/tensorflow/tensorflow/issues/27120\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/tensorflow/issues/27120</a> respectively, but unfortunately, I can't make these solutions work as I a) need to give input to my models when calling them and b) do not want to create n number of seperate functions for my n Agents. How can I make &quot;learn_tf_loop&quot; work WITH the tf.function decorator? I think it boils down to the following two questions:</p>\n<h2>tl;dr:</h2>\n<ul>\n<li>How can I store the models in a way that they are still accessible in graph mode with a tensor?</li>\n<li>How can I train multiple models simultaneously in graph mode?</li>\n</ul>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 154}]
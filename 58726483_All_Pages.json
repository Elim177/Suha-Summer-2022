[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "generative-adversarial-network"], "owner": {"account_id": 7521612, "reputation": 1, "user_id": 11432795, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-hf0VJ0SEjbY/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rfgIRgHM0p2TcA8B8J-DNcI2G40WQ/mo/photo.jpg?sz=256", "display_name": "Atul Krishna Singh", "link": "https://stackoverflow.com/users/11432795/atul-krishna-singh"}, "is_answered": false, "view_count": 443, "answer_count": 2, "score": 0, "last_activity_date": 1610021690, "creation_date": 1573031181, "question_id": 58726483, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58726483/should-we-stop-training-discriminator-while-training-generator-in-cyclegan-tutor", "title": "Should we stop training discriminator while training generator in CycleGAN tutorial?", "body": "<p>In the code provided by tensorlfow tutorial for CycleGAN, they have trained discriminator and generator simultaneously.</p>\n\n<pre>\n\n    def train_step(real_x, real_y):\n      # persistent is set to True because the tape is used more than\n      # once to calculate the gradients.\n      with tf.GradientTape(persistent=True) as tape:\n        # Generator G translates X -> Y\n        # Generator F translates Y -> X.\n\n        fake_y = generator_g(real_x, training=True)\n        cycled_x = generator_f(fake_y, training=True)\n\n        fake_x = generator_f(real_y, training=True)\n        cycled_y = generator_g(fake_x, training=True)\n\n        # same_x and same_y are used for identity loss.\n        same_x = generator_f(real_x, training=True)\n        same_y = generator_g(real_y, training=True)\n\n        disc_real_x = discriminator_x(real_x, training=True)\n        disc_real_y = discriminator_y(real_y, training=True)\n\n        disc_fake_x = discriminator_x(fake_x, training=True)\n        disc_fake_y = discriminator_y(fake_y, training=True)\n\n        # calculate the loss\n        gen_g_loss = generator_loss(disc_fake_y)\n        gen_f_loss = generator_loss(disc_fake_x)\n\n        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n\n        # Total generator loss = adversarial loss + cycle loss\n        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n      # Calculate the gradients for generator and discriminator\n      generator_g_gradients = tape.gradient(total_gen_g_loss, \n                                            generator_g.trainable_variables)\n      generator_f_gradients = tape.gradient(total_gen_f_loss, \n                                            generator_f.trainable_variables)\n\n      discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                                discriminator_x.trainable_variables)\n      discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                                discriminator_y.trainable_variables)\n\n      # Apply the gradients to the optimizer\n      generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n                                                generator_g.trainable_variables))\n\n      generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n                                                generator_f.trainable_variables))\n\n      discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                    discriminator_x.trainable_variables))\n\n      discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                    discriminator_y.trainable_variables))\n\n</pre>\n\n<p>But while training a GAN network we need to stop training discriminator when we are training generator network.\nWhat's the benefit of using it?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 97}]
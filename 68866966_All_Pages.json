[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 19785975, "reputation": 17, "user_id": 14489928, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/4d5b12d7544e1f4240beee0390345a93?s=256&d=identicon&r=PG&f=1", "display_name": "Chandra", "link": "https://stackoverflow.com/users/14489928/chandra"}, "is_answered": false, "view_count": 402, "answer_count": 0, "score": 0, "last_activity_date": 1633338901, "creation_date": 1629485947, "last_edit_date": 1633338901, "question_id": 68866966, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68866966/typeerror-the-argument-cell-keras-layers-recurrent-rnn-is-not-an-rnncel", "title": "TypeError: The argument &#39;cell&#39; (&lt;keras.layers.recurrent.RNN ..) is not an RNNCell: &#39;output_size&#39; property is missing, &#39;state_size&#39; property is missing", "body": "<p>I am using TensorFlow.Keras and writing a sequence to sequence model for the time series data classification task. In this model, I am using multilayer BiLSTM in the encoder part and multilayer stacked LSTM cells in the decoder part. I have written the following lines of code:</p>\n<pre><code>class Encoder(tf.keras.Model):\n    def __init__(self, layers, feature_extract, bidirectional=False):\n    super(Encoder, self).__init__()\n\n    self.n_layers=len(layers)\n    self.lstm_cells = [LSTMCell(hidden_dim) for hidden_dim in layers]\n    self.feature_extract=feature_extract\n    self.bidirectional=bidirectional\n\n    def call(self, x):\n       # dimension of x: input shape=[batch_size, time_step, embedding_dim]\n       x=tf.reshape(x, [-1, 3000,1])\n       network=self.feature_extract(x)\n       print(&quot;after feature extraction&quot;)\n       x= tf.reshape(network, (-1, max_time_step, network.shape[-1]))\n       print(&quot;after feature reshape&quot;, x.shape)\n\n       if self.bidirectional:\n           encoder = Bidirectional(RNN(self.lstm_cells, return_state=True))\n           encoder_outputs_and_states = encoder(x)\n           bi_encoder_states = encoder_outputs_and_states[1:]\n           print(len(bi_encoder_states))\n           encoder_states = []\n           for i in range(int(len(bi_encoder_states)/2)):\n               temp1 = concatenate([bi_encoder_states[i][0],bi_encoder_states[self.n_layers + i][0]], axis=-1)\n               temp2 = concatenate([bi_encoder_states[i][1],bi_encoder_states[self.n_layers + i][1]], axis=-1)\n               temp=[temp1, temp2]\n               encoder_states.append(temp)\n       else:  \n               encoder = RNN(self.lstm_cells, return_state=True)\n               encoder_outputs_and_states = encoder(x)\n               encoder_states = encoder_outputs_and_states[1:]\n       return encoder_outputs_and_states, encoder_states\n\nclass Decoder(tf.keras.Model):\n      def __init__(self, char2numY, dec_units, batch_sz, attention_type='luong',  bidirectional=False):\n      super(Decoder, self).__init__()\n      self.batch_sz = batch_sz\n      self.dec_units = dec_units #must be two times of enc_units\n      self.attention_type = attention_type\n\n      # Embedding Layer\n      #self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n\n      #Final Dense layer on which softmax will be applied\n      self.fc = Dense(len(char2numY), use_bias=False)\n\n      # Define the fundamental cell for decoder recurrent structure\n      if bidirectional:\n         decoder_cells = [LSTMCell(hidden_dim*2) for hidden_dim in layers]\n      else:\n         decoder_cells = [LSTMCell(hidden_dim) for hidden_dim in layers]\n \n      self.decoder_lstm = RNN(decoder_cells, return_sequences=True, return_state=True)\n\n      #self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n\n      # Sampler\n      self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n\n      # Create attention mechanism with memory = None\n      self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n                                                          None, None, self.attention_type)\n\n      # Wrap attention mechanism with the fundamental rnn cell of decoder\n      self.rnn_cell = self.build_rnn_cell(batch_sz)\n\n      # Define the decoder with respect to fundamental rnn cell\n      self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, \n                                                                         output_layer=self.fc)\n\n\n      def build_rnn_cell(self, batch_sz):\n          rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_lstm, \n                              self.attention_mechanism, attention_layer_size=self.dec_units)\n          return rnn_cell\n\n      def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, \n                                                                      attention_type='luong'):\n          return tfa.seq2seq.LuongAttention(units=self.dec_units, memory=memory, \n                                                                  memory_sequence_length=None)\n\n     def build_initial_state(self, batch_sz, encoder_state, Dtype):\n         decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, \n                                                                                  dtype=Dtype)\n         decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n         return decoder_initial_state\n\n    def call(self, inputs, initial_state):\n        x = inputs\n        outputs, final_states, final_sequence_lengths = self.decoder(x, \n                                            initial_state=initial_state, sequence_length=None)\n        #dec_outputs, _final_state, _final_sequence_lengths = \n                                      tfa.seq2seq.dynamic_decode(decoder,impute_finished=True)\n        logits = outputs.rnn_output\n        return outputs, final_states\n\nlayers=[128, 128]\nbatch_sz=32\nmax_time_step=10  # 5 3 second best 10# 40 # 100\noutput_max_length=10 + 2  # max_time_step +1\nsample_x = tf.random.uniform((batch_sz, max_time_step, 3000))\nencoder = Encoder(layers, feature_extract, bidirectional=True)\n# sample input\n#sample_hidden = encoder.initialize_hidden_state()\nencoder_outputs_and_states, encoder_states= encoder(sample_x)\nprint ('Encoder output shape: (batch size, sequence length, units) \n{}'.format(len(encoder_outputs_and_states)))\nprint ('Encoder h vecotr shape: (batch size, units) \n{}'.format(len(encoder_states)))\n\n\n#output\nafter feature extraction\nafter feature reshape (32, 10, 2560)\n4\nEncoder output shape: (batch size, sequence length, units) 5\nEncoder h vecotr shape: (batch size, units) 2\n\n# testing decoder\n\ndec_units=256\nembedding_dim=256\noutput_max_length=max_time_step +2\nvocab_size=7 # no. of class + &lt;SOD&gt; + &lt;EOD&gt;= 5+1+1=7\ndecoder = Decoder(vocab_size, embedding_dim, dec_units, layers, batch_sz, 'luong', bidirectional=True)\nsample_x = tf.random.uniform((batch_sz, output_max_length))\ndecoder.attention_mechanism.setup_memory(sample_output)\ninitial_state = decoder.build_initial_state(batch_sz, encoder_states, tf.float32) \nprint(sample_x.shape)\nsample_decoder_outputs, _ = decoder(sample_x, output_max_length, initial_state)  \n\nprint(&quot;Decoder Outputs Shape: &quot;,sample_decoder_outputs.rnn_output.shape)\n</code></pre>\n<p>I am getting the following error</p>\n<pre><code>  TypeError: The argument 'cell' (&lt;keras.layers.recurrent.RNN object at \n  0x7f66b6c5c850&gt;) is not an RNNCell: 'output_size' property is missing, \n  'state_size' property is missing.\n</code></pre>\n<p>I am not able to understand the reason behind this error.\nPlease help me in fixing this issue.\nThank you in advance.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 85}]
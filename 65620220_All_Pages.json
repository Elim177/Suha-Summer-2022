[{"items": [{"tags": ["python", "tensorflow", "keras", "transfer-learning", "tfrecord"], "owner": {"account_id": 8122961, "reputation": 1012, "user_id": 6118987, "user_type": "registered", "accept_rate": 91, "profile_image": "https://i.stack.imgur.com/MZvTG.jpg?s=256&g=1", "display_name": "Masoud Masoumi Moghadam", "link": "https://stackoverflow.com/users/6118987/masoud-masoumi-moghadam"}, "is_answered": false, "view_count": 707, "answer_count": 3, "score": 2, "last_activity_date": 1615316165, "creation_date": 1610055105, "last_edit_date": 1610175391, "question_id": 65620220, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65620220/invalidargumenterror-key-label-cant-parse-serialized-example-how-can-i-find", "title": "InvalidArgumentError: Key: label. Can&#39;t parse serialized Example: How can I find a way to parse the one-hot encoded labels from TFRecords?", "body": "<p>I got 12 folders (which are categories of my data) containing images. This code converts images and their corresponding labels into tfrecord data with compressing it efficiently:</p>\n<pre><code>import tensorflow as tf\nfrom pathlib import Path\nfrom tensorflow.keras.utils import to_categorical\nimport cv2\nfrom tqdm import tqdm\nfrom os import listdir\nimport numpy as np\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm\n\nlabels = {v:k for k, v in enumerate(listdir('train/'))}\nlabels\n\nclass GenerateTFRecord:\n    def __init__(self, path):\n        self.path = Path(path)\n        self.labels = {v:k for k, v in enumerate(listdir(path))}\n\n    def convert_image_folder(self, tfrecord_file_name):\n        # Get all file names of images present in folder\n        img_paths = list(self.path.rglob('*.jpg'))\n\n        with tf.io.TFRecordWriter(tfrecord_file_name) as writer:\n            for img_path in tqdm(img_paths, desc='images converted'):\n                example = self._convert_image(img_path)\n                writer.write(example.SerializeToString())\n\n    def _convert_image(self, img_path):\n        label = self.labels[img_path.parent.stem]\n        img_shape = mpimg.imread(img_path).shape\n\n        # Read image data in terms of bytes\n        with tf.io.gfile.GFile(img_path, 'rb') as fid:\n            image_data = fid.read()\n\n        example = tf.train.Example(features = tf.train.Features(feature = {\n            'rows': tf.train.Feature(int64_list = tf.train.Int64List(value = [img_shape[0]])),\n            'cols': tf.train.Feature(int64_list = tf.train.Int64List(value = [img_shape[1]])),\n            'channels': tf.train.Feature(int64_list = tf.train.Int64List(value = [3])),\n            'image': tf.train.Feature(bytes_list = tf.train.BytesList(value = [image_data])),\n            'label': tf.train.Feature(int64_list = tf.train.Int64List(value = tf.one_hot(label, depth=len(labels), on_value=1, off_value=0))),\n        }))\n        return example\n\nt = GenerateTFRecord(path='train/')\nt.convert_image_folder('data.tfrecord')\n</code></pre>\n<p>Then I use this code here to read the tfrecord data and create my <code>tf.data.Dataset</code>:</p>\n<pre><code>def _parse_function(tfrecord):\n    # Extract features using the keys set during creation\n    features = {\n        'rows': tf.io.FixedLenFeature([], tf.int64),\n        'cols': tf.io.FixedLenFeature([], tf.int64),\n        'channels': tf.io.FixedLenFeature([], tf.int64),\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64)\n    }\n\n    # Extract the data record\n    sample = tf.io.parse_single_example(tfrecord, features)\n\n    image = tf.image.decode_image(sample['image'])\n    label = sample['label']\n    # label = tf.one_hot(label, depth=len(labels), on_value=1, off_value=0)\n    return image, label\n\ndef configure_for_performance(ds, buffer_size, batch_size):\n    ds = ds.cache()\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=buffer_size)\n    return ds\n\n\ndef generator(tfrecord_file, batch_size, n_data, validation_ratio, reshuffle_each_iteration=False):\n    reader = tf.data.TFRecordDataset(filenames=[tfrecord_file])\n    reader.shuffle(n_data, reshuffle_each_iteration=reshuffle_each_iteration)\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n\n    val_size = int(n_data * validation_ratio)\n    train_ds = reader.skip(val_size)\n    val_ds = reader.take(val_size)\n\n    train_ds = train_ds.map(_parse_function, num_parallel_calls=AUTOTUNE)\n    train_ds = configure_for_performance(train_ds, AUTOTUNE, batch_size)\n\n    val_ds = val_ds.map(_parse_function, num_parallel_calls=AUTOTUNE)\n    val_ds = configure_for_performance(val_ds, AUTOTUNE, batch_size)\n    return train_ds, val_ds\n</code></pre>\n<p>And in here I create my model:</p>\n<pre><code>from os.path import isdir, dirname, abspath, join\nfrom os import makedirs\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import SGD, Adam\n\n\ndef create_model(optimizer, freeze_layer=False):\n  densenet = DenseNet121(weights='imagenet', \n                        include_top=False)\n\n  if freeze_layer:\n    for layer in densenet_model.layers:\n      if 'conv5' in layer.name:\n        layer.trainable = True\n      else:\n        layer.trainable = False\n\n  model = Sequential()\n  model.add(densenet)\n  model.add(GlobalAveragePooling2D())\n  model.add(Dense(12, activation='softmax'))\n\n  model.compile(loss='categorical_crossentropy',\n                optimizer=optimizer,\n                metrics=['accuracy'])\n\n  return model\n\nif __name__ == '__main__':\n    optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99, epsilon=1e-6)\n    densenet_model = create_model(optimizer)\n\n    tfrecord_file = 'data.tfrecord'\n    n_data = len(list(Path('train').rglob('*.jpg')))\n    train, val = generator(tfrecord_file, 2, n_data, validation_ratio, True)\n\n    validation_ratio = 0.2\n    val_size = int(n_data * validation_ratio)\n    train_size = n_data - val_size\n    batch_size = 32\n    n_epochs = 300\n    n_workers = 5\n\n    filename = '/content/drive/MyDrive/data.tfrecord'\n\n\n    train_ds, val_ds = generator(filename,\n                            batch_size=batch_size,\n                            n_data=n_data,\n                            validation_ratio=validation_ratio,\n                            reshuffle_each_iteration=True)\n\n\n   hist = densenet_model.fit(train_ds,\n                      validation_data=val_ds,\n                      epochs=n_epochs,\n                      workers=n_workers,\n                      steps_per_epoch=train_size//batch_size,\n                      validation_steps=val_size)\n</code></pre>\n<p>This is the error I get each time:</p>\n<p><code>InvalidArgumentError:  Key: label.  Can't parse serialized Example. [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [[IteratorGetNext]] [Op:__inference_train_function_343514]</code></p>\n<p>Obviously there is something wrong with the <code>label</code> in my tfrecord data.</p>\n<p>I really need to know that based on my model output shape (12,) how can I safely have one hot encoded label store in my tfrecord and parse in <code>tf.data.Dataset</code>?</p>\n<p>Thank you everybody.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 235}]
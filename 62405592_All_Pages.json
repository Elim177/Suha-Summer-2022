[{"items": [{"tags": ["tensorflow", "metrics", "auc", "multi-gpu"], "owner": {"account_id": 14798558, "reputation": 144, "user_id": 10687511, "user_type": "registered", "profile_image": "https://graph.facebook.com/2270851286516090/picture?type=large", "display_name": "\u0396\u03b9 \u0392\u03ac\u03b3\u03b3\u03bf", "link": "https://stackoverflow.com/users/10687511/%ce%96%ce%b9-%ce%92%ce%ac%ce%b3%ce%b3%ce%bf"}, "is_answered": false, "view_count": 414, "answer_count": 1, "score": 4, "last_activity_date": 1646094531, "creation_date": 1592300673, "question_id": 62405592, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62405592/tensorflow-2-metrics-produce-wrong-results-with-2-gpus", "title": "Tensorflow 2 Metrics produce wrong results with 2 GPUs", "body": "<p>I took this piece of code from tensorflow documentation about distributed training with custom loop <a href=\"https://www.tensorflow.org/tutorials/distribute/custom_training\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/tutorials/distribute/custom_training</a> and I just fixed it to work with the tf.keras.metrics.AUC and run it with 2 GPUS (2 Nvidia V100 from a DGX machine). </p>\n\n<pre><code># Import TensorFlow\nimport tensorflow as tf\n\n# Helper libraries\nimport numpy as np\n\n\nprint(tf.__version__)\n\n\nfashion_mnist = tf.keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\n# Adding a dimension to the array -&gt; new shape == (28, 28, 1)\n# We are doing this because the first layer in our model is a convolutional\n# layer and it requires a 4D input (batch_size, height, width, channels).\n# batch_size dimension will be added later on.\ntrain_images = train_images[..., None]\ntest_images = test_images[..., None]\n\n# One hot\ntrain_labels = tf.keras.utils.to_categorical(train_labels, 10)\ntest_labels = tf.keras.utils.to_categorical(test_labels, 10)\n\n# Getting the images in [0, 1] range.\ntrain_images = train_images / np.float32(255)\ntest_images = test_images / np.float32(255)\n\n# If the list of devices is not specified in the\n# `tf.distribute.MirroredStrategy` constructor, it will be auto-detected.\nGPUS = [0, 1]\ndevices = [\"/gpu:\" + str(gpu_id) for gpu_id in GPUS]\nstrategy = tf.distribute.MirroredStrategy(devices=devices)\n\nprint ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n\n\nBUFFER_SIZE = len(train_images)\n\nBATCH_SIZE_PER_REPLICA = 64\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n\nEPOCHS = 10\n\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\ntest_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)\n\ntrain_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\ntest_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)\n\n\ndef create_model():\n  model = tf.keras.Sequential([\n      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Conv2D(64, 3, activation='relu'),\n      tf.keras.layers.MaxPooling2D(),\n      tf.keras.layers.Flatten(),\n      tf.keras.layers.Dense(64, activation='relu'),\n      tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n  return model\n\n\nwith strategy.scope():\n  # Set reduction to `none` so we can do the reduction afterwards and divide by\n  # global batch size.\n  loss_object = tf.keras.losses.CategoricalCrossentropy(\n      from_logits=True,\n      reduction=tf.keras.losses.Reduction.NONE)\n  def compute_loss(labels, predictions):\n    per_example_loss = loss_object(labels, predictions)\n    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n\n\nwith strategy.scope():\n  test_loss = tf.keras.metrics.Mean(name='test_loss')\n\n  train_accuracy = tf.keras.metrics.CategoricalAccuracy(\n      name='train_accuracy')\n  test_accuracy = tf.keras.metrics.CategoricalAccuracy(\n      name='test_accuracy')\n  train_auc = tf.keras.metrics.AUC(name='train_auc')\n  test_auc = tf.keras.metrics.AUC(name='test_auc')\n\n\n# model, optimizer, and checkpoint must be created under `strategy.scope`.\nwith strategy.scope():\n  model = create_model()\n\n  optimizer = tf.keras.optimizers.Adam()\n\n\ndef train_step(inputs):\n  images, labels = inputs\n\n  with tf.GradientTape() as tape:\n    predictions = model(images, training=True)\n    loss = compute_loss(labels, predictions)\n\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n  train_accuracy(labels, predictions)\n  train_auc(labels, predictions)\n  return loss\n\ndef test_step(inputs):\n  images, labels = inputs\n\n  predictions = model(images, training=False)\n  t_loss = loss_object(labels, predictions)\n\n  test_loss.update_state(t_loss)\n  test_accuracy(labels, predictions)\n  test_auc(labels, predictions)\n\n\n# `run` replicates the provided computation and runs it\n# with the distributed input.\n@tf.function\ndef distributed_train_step(dataset_inputs):\n  per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))\n  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n                         axis=None)\n\n@tf.function\ndef distributed_test_step(dataset_inputs):\n  return strategy.run(test_step, args=(dataset_inputs,))\n\n\nfor epoch in range(EPOCHS):\n  # TRAIN LOOP\n  total_loss = 0.0\n  num_batches = 0\n  for x in train_dist_dataset:\n    total_loss += distributed_train_step(x)\n    num_batches += 1\n  train_loss = total_loss / num_batches\n\n  # TEST LOOP\n  for x in test_dist_dataset:\n    distributed_test_step(x)\n\n  template = (\"Epoch {}, Loss: {}, Accuracy: {}, AUC: {},\"\n              \"Test Loss: {}, Test Accuracy: {}, Test AUC: {}\")\n  print (template.format(epoch+1,\n                         train_loss, train_accuracy.result()*100, train_auc.result()*100,\n                         test_loss.result(), test_accuracy.result()*100, test_auc.result()*100))\n\n  test_loss.reset_states()\n  train_accuracy.reset_states()\n  test_accuracy.reset_states()\n  train_auc.reset_states()\n  test_auc.reset_states()\n</code></pre>\n\n<p>The problem is that AUC's evaluation is definitely wrong cause it exceeds its range (should be from 0-100) and i get theese results by running the above code for one time:</p>\n\n<pre><code>Epoch 1, Loss: 1.8061423301696777, Accuracy: 66.00833892822266, AUC: 321.8688659667969,Test Loss: 1.742477536201477, Test Accuracy: 72.0999984741211, Test AUC: 331.33709716796875\nEpoch 2, Loss: 1.7129968404769897, Accuracy: 74.9816665649414, AUC: 337.37017822265625,Test Loss: 1.7084736824035645, Test Accuracy: 75.52999877929688, Test AUC: 337.1878967285156\nEpoch 3, Loss: 1.643971562385559, Accuracy: 81.83333587646484, AUC: 355.96209716796875,Test Loss: 1.6072628498077393, Test Accuracy: 85.3499984741211, Test AUC: 370.603759765625\nEpoch 4, Loss: 1.5887378454208374, Accuracy: 87.27833557128906, AUC: 373.6204528808594,Test Loss: 1.5906082391738892, Test Accuracy: 87.13999938964844, Test AUC: 371.9998474121094\nEpoch 5, Loss: 1.581775426864624, Accuracy: 88.0, AUC: 373.9468994140625,Test Loss: 1.5964380502700806, Test Accuracy: 86.68000030517578, Test AUC: 371.0227355957031\nEpoch 6, Loss: 1.5764907598495483, Accuracy: 88.49166870117188, AUC: 375.2404479980469,Test Loss: 1.5832056999206543, Test Accuracy: 87.94000244140625, Test AUC: 373.41998291015625\nEpoch 7, Loss: 1.5698528289794922, Accuracy: 89.19166564941406, AUC: 376.473876953125,Test Loss: 1.5770654678344727, Test Accuracy: 88.58000183105469, Test AUC: 375.5516662597656\nEpoch 8, Loss: 1.564456820487976, Accuracy: 89.71833801269531, AUC: 377.8564758300781,Test Loss: 1.5792100429534912, Test Accuracy: 88.27000427246094, Test AUC: 373.1791687011719\nEpoch 9, Loss: 1.5612279176712036, Accuracy: 90.02000427246094, AUC: 377.9949645996094,Test Loss: 1.5729509592056274, Test Accuracy: 88.9800033569336, Test AUC: 375.5257263183594\nEpoch 10, Loss: 1.5562015771865845, Accuracy: 90.54000091552734, AUC: 378.9789123535156,Test Loss: 1.56815767288208, Test Accuracy: 89.3499984741211, Test AUC: 375.8636474609375\n</code></pre>\n\n<p>Accuracy is ok but it seems that it's the only one metric that behaves nice. I tried other metrics too but they are not evaluated correctly. It seems that the problems come when using more than one GPU, cause when I run this code with one GPU it produce the right results. </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 222}]
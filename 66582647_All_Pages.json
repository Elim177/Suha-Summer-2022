[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning", "segmentation-fault"], "owner": {"account_id": 12587117, "reputation": 131, "user_id": 9598527, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/eb56a0482eadd7dbdca7b311b155753f?s=256&d=identicon&r=PG&f=1", "display_name": "Ajinkya Ambatwar", "link": "https://stackoverflow.com/users/9598527/ajinkya-ambatwar"}, "is_answered": false, "view_count": 267, "answer_count": 1, "score": 0, "last_activity_date": 1616509241, "creation_date": 1615465907, "last_edit_date": 1615465996, "question_id": 66582647, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66582647/tensorflow-random-segmentation-faults", "title": "Tensorflow Random segmentation faults", "body": "<p>I am trying to run the demo code from official tensorflow <a href=\"https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\" rel=\"nofollow noreferrer\">website</a>\nI am attaching the full code (copied and arranged) here for ease</p>\n<pre><code>import tensorflow as tf\n\n# print(&quot;1&quot;)\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport time\nimport os\n\n# print(&quot;2&quot;)\nos.environ[&quot;TF_CPP_MIN_LOG_LEVEL&quot;] = &quot;3&quot;\n\n\n# @tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        logits = model(x, training=True)\n        loss_value = loss_fn(y, logits)\n    grads = tape.gradient(loss_value, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    train_acc_metric.update_state(y, logits)\n    return loss_value\n\n\n# @tf.function\ndef test_step(x, y):\n    val_logits = model(x, training=False)\n    val_acc_metric.update_state(y, val_logits)\n\n\ninputs = keras.Input(shape=(784,), name=&quot;digits&quot;)\nx1 = layers.Dense(64, activation=&quot;relu&quot;)(inputs)\nx2 = layers.Dense(64, activation=&quot;relu&quot;)(x1)\noutputs = layers.Dense(10, name=&quot;predictions&quot;)(x2)\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n\n# Instantiate an optimizer.\noptimizer = keras.optimizers.SGD(learning_rate=1e-3)\n# Instantiate a loss function.\nloss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\ntrain_acc_metric = keras.metrics.SparseCategoricalAccuracy()\nval_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n# Prepare the training dataset.\nbatch_size = 64\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train = np.reshape(x_train, (-1, 784))\nx_test = np.reshape(x_test, (-1, 784))\n\n# Reserve 10,000 samples for validation.\nx_val = x_train[-10000:]\ny_val = y_train[-10000:]\nx_train = x_train[:-10000]\ny_train = y_train[:-10000]\n\n# Prepare the training dataset.\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n\n# Prepare the validation dataset.\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\nval_dataset = val_dataset.batch(batch_size)\n\nepochs = 2\nfor epoch in range(epochs):\n    print(&quot;\\nStart of epoch %d&quot; % (epoch,))\n    start_time = time.time()\n\n    # Iterate over the batches of the dataset.\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        loss_value = train_step(x_batch_train, y_batch_train)\n\n        # Log every 200 batches.\n        if step % 200 == 0:\n            print(\n                &quot;Training loss (for one batch) at step %d: %.4f&quot;\n                % (step, float(loss_value))\n            )\n            print(&quot;Seen so far: %d samples&quot; % ((step + 1) * 64))\n\n    # Display metrics at the end of each epoch.\n    train_acc = train_acc_metric.result()\n    print(&quot;Training acc over epoch: %.4f&quot; % (float(train_acc),))\n\n    # Reset training metrics at the end of each epoch\n    train_acc_metric.reset_states()\n\n    # Run a validation loop at the end of each epoch.\n    for x_batch_val, y_batch_val in val_dataset:\n        test_step(x_batch_val, y_batch_val)\n\n    val_acc = val_acc_metric.result()\n    val_acc_metric.reset_states()\n    print(&quot;Validation acc: %.4f&quot; % (float(val_acc),))\n    print(&quot;Time taken: %.2fs&quot; % (time.time() - start_time))\n    print(&quot;end&quot;)\n</code></pre>\n<p>Without any reason, this code enters Segmentation Fault in Tensorflow 2.3.1 right at the beginning</p>\n<pre><code>&gt;python dummy.py \n2021-03-11 17:45:52.231509: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\nSegmentation fault (core dumped)\n</code></pre>\n<p>Interestingly if I put some random print statements at the very start(those <code>print(&quot;1&quot;)</code> etc statements, the code will execute till the end and suffer segmentation fault at the end(redundant output not shown)</p>\n<pre><code>Start of epoch 1\nTraining loss (for one batch) at step 0: 1.0215\nSeen so far: 64 samples\nTraining loss (for one batch) at step 200: 0.9116\nSeen so far: 12864 samples\nTraining loss (for one batch) at step 400: 0.4894\nSeen so far: 25664 samples\nTraining loss (for one batch) at step 600: 0.5636\nSeen so far: 38464 samples\nTraining acc over epoch: 0.8416\nValidation acc: 0.8296\nTime taken: 3.16s\nend\nSegmentation fault (core dumped)\n</code></pre>\n<p>Another observation is, if I uncomment the <code>@tf.function</code> on top of my <code>trainStep</code> and <code>testStep</code> functions, the code enters into segfault again but after it prints\n<code>Start of epoch 0</code></p>\n<p>Can someone explain what is going wrong with my Tensorflow package?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 245}]
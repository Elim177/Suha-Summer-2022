[{"items": [{"tags": ["python", "tensorflow", "lstm", "gradient-descent"], "owner": {"account_id": 10716056, "reputation": 2138, "user_id": 7886651, "user_type": "registered", "accept_rate": 76, "profile_image": "https://i.stack.imgur.com/zfb59.jpg?s=256&g=1", "display_name": "I. A", "link": "https://stackoverflow.com/users/7886651/i-a"}, "is_answered": true, "view_count": 386, "accepted_answer_id": 62326282, "answer_count": 2, "score": 1, "last_activity_date": 1591891255, "creation_date": 1591882785, "question_id": 62325764, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62325764/calculating-the-derivates-of-the-output-with-respect-to-input-for-a-give-time-st", "title": "Calculating the derivates of the output with respect to input for a give time step in LSTM tensorflow2.0", "body": "<p>I wrote a sample code to generate the real problem I am facing in my project. I am using an LSTM in tensorflow to model some time series data. Input dimensions are <code>(10, 100, 1)</code>, that is, 10 instances, 100 time steps, and number of features is 1. The output is of the same shape.</p>\n\n<p>What I want to achieve after training the model is to study the influence of each of the inputs to each output at each particular time step. In other words, I would like to see which input variables affect my output the most (or which input has the most influence on the output/maybe large gradient) at each time step. Here is the code for this problem:</p>\n\n<pre><code>tf.keras.backend.clear_session()\ntf.random.set_seed(42)\n\nmodel_input = tf.data.Dataset.from_tensor_slices(np.random.normal(size=(10, 100, 1)))\nmodel_input = model_input.batch(10)\nmodel_output = tf.data.Dataset.from_tensor_slices(np.random.normal(size=(10, 100, 1)))\nmodel_output = model_output.batch(10)\n\nmy_dataset = tf.data.Dataset.zip((model_input, model_output))\n\nm_inputs = tf.keras.Input(shape=(None, 1))\n\nlstm_outputs = tf.keras.layers.LSTM(32, return_sequences=True)(m_inputs)\nm_outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))(lstm_outputs)\n\nmy_model = tf.keras.Model(m_inputs, m_outputs, name=\"my_model\")\n\nmy_optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\nmy_loss_fn = tf.keras.losses.MeanSquaredError()\n\nmy_epochs = 3\n\nfor epoch in range(my_epochs):\n\n    for step, (x_batch_tr, y_batch_tr) in enumerate(my_dataset):\n        x += 1\n        # open a gradient tape to record the operations run during the forward pass, which enables autodifferentiation\n        with tf.GradientTape() as tape:\n\n            # Run the forward pass of the layer\n            logits = my_model(x_batch_tr, training=True)\n\n            # compute the loss value for this mismatch\n            loss_value = my_loss_fn(y_batch_tr, logits)\n\n        # use the gradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss.\n        grads = tape.gradient(loss_value, my_model.trainable_weights)\n\n        # Run one step of gradient descent by updating the value of the variables to minimize the loss.\n        my_optimizer.apply_gradients(zip(grads, my_model.trainable_weights))\n\n        print(f\"Step {step}, loss: {loss_value}\")\n\n\nprint(\"\\n\\nCalculate gradient of ouptuts w.r.t inputs\\n\\n\")\n\nfor step, (x_batch_tr, y_batch_tr) in enumerate(my_dataset):\n    # open a gradient tape to record the operations run during the forward pass, which enables autodifferentiation\n    with tf.GradientTape() as tape:\n\n        tape.watch(x_batch_tr)\n\n        # Run the forward pass of the layer\n        logits = my_model(x_batch_tr, training=True)\n        #tape.watch(logits[:, 10, :])   # this didn't help\n        # compute the loss value for this mismatch\n        loss_value = my_loss_fn(y_batch_tr, logits)\n\n    # use the gradient tape to automatically retrieve the gradients of the trainable variables with respect to the loss.\n#     grads = tape.gradient(logits, x_batch_tr)   # This works\n#     print(grads.numpy().shape)                  # This works\n    grads = tape.gradient(logits[:, 10, :], x_batch_tr)\n    print(grads)\n</code></pre>\n\n<p>In other words, I would like to pay attention to the inputs that affect my output the most (at each particular time step).</p>\n\n<p>To me <code>grads = tape.gradient(logits, x_batch_tr)</code> won't do the job cuz this will add the gradients from all outputs w.r.t each inputs.</p>\n\n<p>However, the gradients are always None.</p>\n\n<p>Any help is much appreciated!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 77}]
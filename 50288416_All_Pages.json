[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "image-segmentation", "tfrecord"], "owner": {"user_type": "does_not_exist", "display_name": "user6038900"}, "is_answered": true, "view_count": 1755, "answer_count": 1, "score": 4, "last_activity_date": 1539617495, "creation_date": 1526028167, "question_id": 50288416, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/50288416/unable-to-retrain-the-instance-segmentation-model", "title": "Unable to retrain the instance segmentation model", "body": "<p>Im trying to train the instance segmentation model. Im using the following code to generate the tfrecord. </p>\n\n<pre><code>flags = tf.app.flags\nflags.DEFINE_string('data_dir', '', 'Root directory to raw pet dataset.')\nflags.DEFINE_string('output_dir', '', 'Path to directory to output TFRecords.')\nflags.DEFINE_string('label_map_path', 'data/pet_label_map.pbtxt',\n                    'Path to label map proto')\nflags.DEFINE_boolean('faces_only', True, 'If True, generates bounding boxes '\n                     'for pet faces.  Otherwise generates bounding boxes (as '\n                     'well as segmentations for full pet bodies).  Note that '\n                     'in the latter case, the resulting files are much larger.')\nflags.DEFINE_string('mask_type', 'png', 'How to represent instance '\n                    'segmentation masks. Options are \"png\" or \"numerical\".')\nFLAGS = flags.FLAGS\n\n\ndef get_class_name_from_filename(file_name):\n\n  match = re.match(r'([A-Za-z_]+)(_[0-9]+\\.jpg)', file_name, re.I)\n  return match.groups()[0]\n\n\ndef dict_to_tf_example(data,\n                       mask_path,\n                       label_map_dict,\n                       image_subdirectory,\n                       ignore_difficult_instances=False,\n                       faces_only=True,\n                       mask_type='png'):\n\n  img_path = os.path.join(image_subdirectory, data['filename'])\n  with tf.gfile.GFile(img_path, 'rb') as fid:\n    encoded_jpg = fid.read()\n  encoded_jpg_io = io.BytesIO(encoded_jpg)\n  image = PIL.Image.open(encoded_jpg_io)\n  if image.format != 'JPEG':\n    raise ValueError('Image format not JPEG')\n  key = hashlib.sha256(encoded_jpg).hexdigest()\n\n  with tf.gfile.GFile(mask_path, 'rb') as fid:\n    encoded_mask_png = fid.read()\n  encoded_png_io = io.BytesIO(encoded_mask_png)\n  mask = PIL.Image.open(encoded_png_io)\n  if mask.format != 'PNG':\n    raise ValueError('Mask format not PNG')\n\n  mask_np = np.asarray(mask)\n  nonbackground_indices_x = np.any(mask_np != 2, axis=0)\n  nonbackground_indices_y = np.any(mask_np != 2, axis=1)\n  nonzero_x_indices = np.where(nonbackground_indices_x)\n  nonzero_y_indices = np.where(nonbackground_indices_y)\n\n  width = int(data['size']['width'])\n  height = int(data['size']['height'])\n\n  xmins = []\n  ymins = []\n  xmaxs = []\n  ymaxs = []\n  classes = []\n  classes_text = []\n  truncated = []\n  poses = []\n  difficult_obj = []\n  masks = []\n  if 'object' in data:\n    for obj in data['object']:\n      difficult = bool(int(obj['difficult']))\n      if ignore_difficult_instances and difficult:\n        continue\n      difficult_obj.append(int(difficult))\n\n      if faces_only:\n        xmin = float(obj['bndbox']['xmin'])\n        xmax = float(obj['bndbox']['xmax'])\n        ymin = float(obj['bndbox']['ymin'])\n        ymax = float(obj['bndbox']['ymax'])\n      else:\n        xmin = float(np.min(nonzero_x_indices))\n        xmax = float(np.max(nonzero_x_indices))\n        ymin = float(np.min(nonzero_y_indices))\n        ymax = float(np.max(nonzero_y_indices))\n\n      xmins.append(xmin / width)\n      ymins.append(ymin / height)\n      xmaxs.append(xmax / width)\n      ymaxs.append(ymax / height)\n      class_name = get_class_name_from_filename(data['filename'])\n      classes_text.append(class_name.encode('utf8'))\n      classes.append(label_map_dict[class_name])\n      truncated.append(int(obj['truncated']))\n      poses.append(obj['pose'].encode('utf8'))\n      if not faces_only:\n        mask_remapped = (mask_np != 2).astype(np.uint8)\n        masks.append(mask_remapped)\n\n  feature_dict = {\n      'image/height': dataset_util.int64_feature(height),\n      'image/width': dataset_util.int64_feature(width),\n      'image/filename': dataset_util.bytes_feature(\n          data['filename'].encode('utf8')),\n      'image/source_id': dataset_util.bytes_feature(\n          data['filename'].encode('utf8')),\n      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n      'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),\n      'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n      'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n      'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n      'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n      'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n      'image/object/class/label': dataset_util.int64_list_feature(classes),\n      'image/object/difficult': dataset_util.int64_list_feature(difficult_obj),\n      'image/object/truncated': dataset_util.int64_list_feature(truncated),\n      'image/object/view': dataset_util.bytes_list_feature(poses),\n  }\n  if not faces_only:\n    if mask_type == 'numerical':\n      mask_stack = np.stack(masks).astype(np.float32)\n      masks_flattened = np.reshape(mask_stack, [-1])\n      feature_dict['image/object/mask'] = (\n          dataset_util.float_list_feature(masks_flattened.tolist()))\n    elif mask_type == 'png':\n      encoded_mask_png_list = []\n      for mask in masks:\n        img = PIL.Image.fromarray(mask)\n        output = io.BytesIO()\n        img.save(output, format='PNG')\n        encoded_mask_png_list.append(output.getvalue())\n      feature_dict['image/object/mask'] = (\n          dataset_util.bytes_list_feature(encoded_mask_png_list))\n\n  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n  return example\n\n\ndef create_tf_record(output_filename,\n                     label_map_dict,\n                     annotations_dir,\n                     image_dir,\n                     examples,\n                     faces_only=True,\n                     mask_type='png'):\n\n  writer = tf.python_io.TFRecordWriter(output_filename)\n  for idx, example in enumerate(examples):\n    if idx % 100 == 0:\n      logging.info('On image %d of %d', idx, len(examples))\n    xml_path = os.path.join(annotations_dir, 'xmls', example + '.xml')\n    mask_path = os.path.join(annotations_dir, 'trimaps', example + '.png')\n\n    if not os.path.exists(xml_path):\n      logging.warning('Could not find %s, ignoring example.', xml_path)\n      continue\n    with tf.gfile.GFile(xml_path, 'r') as fid:\n      xml_str = fid.read()\n    xml = etree.fromstring(xml_str)\n    data = dataset_util.recursive_parse_xml_to_dict(xml)['annotation']\n\n    try:\n      tf_example = dict_to_tf_example(\n          data,\n          mask_path,\n          label_map_dict,\n          image_dir,\n          faces_only=faces_only,\n          mask_type=mask_type)\n      writer.write(tf_example.SerializeToString())\n    except ValueError:\n      logging.warning('Invalid example: %s, ignoring.', xml_path)\n\n  writer.close()\n\n\n# TODO(derekjchow): Add test for pet/PASCAL main files.\ndef main(_):\n  data_dir = FLAGS.data_dir\n  label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)\n\n  logging.info('Reading from Pet dataset.')\n  image_dir = os.path.join(data_dir, 'images')\n  annotations_dir = os.path.join(data_dir, 'annotations')\n  examples_path = os.path.join(annotations_dir, 'trainval.txt')\n  examples_list = dataset_util.read_examples_list(examples_path)\n\n  # Test images are not included in the downloaded data set, so we shall perform\n  # our own split.\n  random.seed(42)\n  random.shuffle(examples_list)\n  num_examples = len(examples_list)\n  num_train = int(0.7 * num_examples)\n  train_examples = examples_list[:num_train]\n  val_examples = examples_list[num_train:]\n  logging.info('%d training and %d validation examples.',\n               len(train_examples), len(val_examples))\n\n  train_output_path = os.path.join(FLAGS.output_dir, 'pet_train.record')\n  val_output_path = os.path.join(FLAGS.output_dir, 'pet_val.record')\n  if FLAGS.faces_only:\n    train_output_path = os.path.join(FLAGS.output_dir,\n                                     'pet_train_with_masks.record')\n    val_output_path = os.path.join(FLAGS.output_dir,\n                                   'pet_val_with_masks.record')\n  create_tf_record(\n      train_output_path,\n      label_map_dict,\n      annotations_dir,\n      image_dir,\n      train_examples,\n      faces_only=FLAGS.faces_only,\n      mask_type=FLAGS.mask_type)\n  create_tf_record(\n      val_output_path,\n      label_map_dict,\n      annotations_dir,\n      image_dir,\n      val_examples,\n      faces_only=FLAGS.faces_only,\n      mask_type=FLAGS.mask_type)\n\n\nif __name__ == '__main__':\n  tf.app.run()\n</code></pre>\n\n<p>The dataset that Im using to train has 37 classes with images and masks. The dataset is in <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_pets.md\" rel=\"noreferrer\">here</a></p>\n\n<p>However when i try to train, Im getting the following error.</p>\n\n<blockquote>\n  <p>Traceback (most recent call last):   File \"train.py\", line 167, in\n  \n      tf.app.run()   File \"/anaconda3/envs/conda/lib/python3.6/site-packages/tensorflow/python/platform/app.py\",\n  line 126, in run\n      _sys.exit(main(argv))   File \"train.py\", line 163, in main\n      worker_job_name, is_chief, FLAGS.train_dir)   File \"/Users/Documents/research/models/research/object_detection/trainer.py\",\n  line 275, in train\n      clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])   File\n  \"/Users/Documents/research/models/research/slim/deployment/model_deploy.py\",\n  line 193, in create_clones\n      outputs = model_fn(*args, **kwargs)   File \"/Users/Documents/research/models/research/object_detection/trainer.py\",\n  line 200, in _create_losses\n      losses_dict = detection_model.loss(prediction_dict, true_image_shapes)   File\n  \"/Users/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py\",\n  line 1608, in loss\n      groundtruth_masks_list,   File \"/Users/Documents/research/models/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py\",\n  line 1837, in _loss_box_classifier\n      raise ValueError('Groundtruth instance masks not provided. ' ValueError: Groundtruth instance masks not provided. Please configure</p>\n</blockquote>\n\n<p>How can I be able to sort it out? </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 297}]
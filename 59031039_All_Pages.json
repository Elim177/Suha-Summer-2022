[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "neural-network", "conv-neural-network"], "owner": {"account_id": 14824066, "reputation": 139, "user_id": 12027232, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/52a2b89c6bdf40545326c02329d52488?s=256&d=identicon&r=PG&f=1", "display_name": "JOKKINATOR", "link": "https://stackoverflow.com/users/12027232/jokkinator"}, "is_answered": true, "view_count": 95, "accepted_answer_id": 59031213, "answer_count": 1, "score": 2, "last_activity_date": 1574682687, "creation_date": 1574682169, "question_id": 59031039, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59031039/understanding-cnn-hyperparameters", "title": "Understanding CNN hyperparameters", "body": "<p>I have some problems regarding Convolutional Neural Networks. My code is by no means clean so I apologize on forehand.</p>\n\n<p>First off, I have a dataset consisting of 10.000 images with dimensions (28,28,1). My wish is to build a convolutional neural network to classify these images into 5 different classes (it's half of the well-known Zalando dataset). </p>\n\n<p>This is my code</p>\n\n<pre><code>class layers(ABC): \n    def __init__(self, filter_size, number_of_neurons, fully_conn_neurons):\n        self.filter_size = filter_size #placeholder for filter \n        self.number_of_neurons = number_of_neurons #The number of neurons\n        self.fully_conn_neurons = fully_conn_neurons #Amount of neurons in the last layer\n\n        return\n\nclass new_conv_layer(ABC):\n    def __init__(self, filters, number_of_filters, initial_input, namew, nameb, defrel):\n        self.filters = filters\n        self.number_of_filters = number_of_filters #16 is amount of filters\n        self.color_chan = 1\n        self.shape = [filters, filters, self.color_chan, number_of_filters]\n        self.defrel = False\n\n        self.weight = tf.get_variable(name=namew, shape =self.shape, initializer = tf.initializers.glorot_normal)\n        self.bias = tf.Variable(tf.constant(0.05, shape = [number_of_filters], name=nameb))\n\n        self.layer = tf.nn.conv2d(input = initial_input, filter = self.weight, strides=[1,2,2,1], padding=\"SAME\")\n        self.layer += self.bias \n\n        self.layer = tf.nn.max_pool(value=self.layer, ksize = [1,2,2,1], strides = [1,2,2,1], padding=\"SAME\")\n\n        if defrel == True:\n            self.layer = tf.nn.relu(self.layer)\n\n\n    def flatten(self):\n        flat_shape = self.layer.shape\n        self.features = flat_shape[1:].num_elements()\n        self.layer = tf.reshape(self.layer, [-1, self.features])\n\n        return self.layer, self.features\n\nx = tf.placeholder(tf.float32, shape=[None, 784], name='x')\nx_image = tf.reshape(x, [-1, 28, 28, 1])\ny = tf.placeholder(tf.float32, [None, 5])\n\n\n\nlayer1 = new_conv_layer(filters=4,number_of_filters=16, initial_input= x_image, namew =\"w\", nameb=\"b\", defrel=True)\nlayer2 = new_conv_layer(filters=4,number_of_filters=32, initial_input=layer1.layer, namew=\"fuckoff\", nameb=\"fuck\", defrel=False)\n\n\nlayer_flat, num_features = layer2.flatten()\n\n\nclass fully_connected(ABC):\n    def __init__(self, previous_layer, inp, outp, namea, nameb):\n\n        self.previous_layer = previous_layer\n        self.weights = tf.get_variable(shape =[inp, outp], initializer = tf.initializers.glorot_normal, name=namea)\n        self.biases = tf.Variable(tf.constant(0.05, shape = [outp], name = nameb))\n        self.temp_layer = tf.matmul(self.previous_layer, self.weights) + self.biases\n        self.new_layer = tf.nn.relu(self.temp_layer)\n\n\n\n\n\n\nlayer_fc1 = fully_connected(layer_flat, inp=num_features, outp=128, namea = \"t\", nameb= \"u\")\nlayer_fc2 = fully_connected(layer_fc1.new_layer, inp=128, outp=5, nameb=\"h\", namea=\"z\")\n\n\n\nepochs = 300\nlearning_rate = 0.05\nbatch_size = 128\n\n\npred = tf.nn.softmax(layer_fc2.new_layer)\nprint(pred.shape)\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = pred, labels = y))\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\ncorrect_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n\n# drop out, regularization \n# call back \n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init) \n    train_loss = []\n    test_loss = []\n    train_accuracy = []\n    test_accuracy = []\n    summary_writer = tf.summary.FileWriter('./Output', sess.graph)\n    for i in range(epochs):\n        for batch in range(len(train_X)//batch_size):\n            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]\n            batch_y = train_y[batch*batch_size:min((batch+1)*batch_size,len(train_y))]    \n            opt = sess.run(optimizer, feed_dict={x: batch_x,\n                                                              y: batch_y})\n            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n                                                              y: batch_y})\n        print(\"Iter \" + str(i) + \", Loss= \" + \\\n                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n                      \"{:.5f}\".format(acc))\n        print(\"Optimization Finished!\")\n\n\n        test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})\n        train_loss.append(loss)\n        test_loss.append(valid_loss)\n        train_accuracy.append(acc)\n        test_accuracy.append(test_acc)\n        print(\"Testing Accuracy:\",\"{:.5f}\".format(test_acc))\n    summary_writer.close()\n\n</code></pre>\n\n<p>And I get two different problems: I can not change the filters, as it will give me the error: InvalidArgumentError: input and filter must have the same depth: 16 vs 1. Secondly, I only get a testing accuracy of 50% which is by no means good..</p>\n\n<p>I know this is super broad, but is there something I am severely missing?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 97}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "generative-adversarial-network"], "owner": {"account_id": 17805186, "reputation": 1, "user_id": 12931039, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/2ee2b6abf95a6c68f56f8127acb06afd?s=256&d=identicon&r=PG&f=1", "display_name": "tzc", "link": "https://stackoverflow.com/users/12931039/tzc"}, "is_answered": false, "view_count": 35, "answer_count": 1, "score": 0, "last_activity_date": 1589198752, "creation_date": 1589173247, "last_edit_date": 1589198752, "question_id": 61722583, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61722583/loss-function-unchange-in-gan-model-training", "title": "Loss function unchange in GAN model training", "body": "<pre><code>from keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nimport numpy as n\n\n# gene model define\nclass gene(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5],\n                                           input_shape=(150,150,1))\n        self.leak1 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.conv2 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5])\n        self.leak2 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.conv3 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[4,4])\n        self.leak3 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.conv4 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[4,4])\n        self.leak4 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.conv5 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[4,4])\n        self.leak5 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.conv6 = tf.keras.layers.Conv2D(filters=64,\n                                           kernel_size=[3,3])\n        self.leak6 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.conv7 = tf.keras.layers.Conv2D(filters=64,\n                                           kernel_size=[3,3])\n        self.leak7 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.conv8 = tf.keras.layers.Conv2D(filters=64,\n                                           kernel_size=[3,3])\n        self.leak8 = tf.keras.layers.LeakyReLU(alpha =0.4)\n\n        self.deconv1 = tf.keras.layers.Conv2DTranspose(64,\n                                        kernel_size=[3,3])\n        self.deleak1 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.add1 = tf.keras.layers.Add()\n        self.deconv2 = tf.keras.layers.Conv2DTranspose(64,\n                                        kernel_size=[3,3])\n        self.deleak2 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.add2 = tf.keras.layers.Add()\n        self.deconv3 = tf.keras.layers.Conv2DTranspose(128,\n                                        kernel_size=[3,3])\n        self.deleak3 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.add3 = tf.keras.layers.Add()\n        self.deconv4 = tf.keras.layers.Conv2DTranspose(128,\n                                        kernel_size=[4,4])\n        self.deleak4 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.add4 = tf.keras.layers.Add()\n        self.deconv5 = tf.keras.layers.Conv2DTranspose(128,\n                                        kernel_size=[4,4])\n        self.deleak5 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.add5 = tf.keras.layers.Add()\n        self.deconv6 = tf.keras.layers.Conv2DTranspose(128,\n                                        kernel_size=[4,4])\n        self.deleak6 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.add6 = tf.keras.layers.Add()\n        self.deconv7 = tf.keras.layers.Conv2DTranspose(128,\n                                        kernel_size=[5,5])\n        self.deleak7 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.add7= tf.keras.layers.Add()\n        self.deconv8 = tf.keras.layers.Conv2DTranspose(128,\n                                        kernel_size=[5,5])\n        self.deleak8 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.dense = tf.keras.layers.Dense(1,\n                                           activation=tf.nn.relu)\n        self.add = tf.keras.layers.Add()\n\n\n    def call(self,inputs):\n        x1 = self.conv1(inputs)\n        x1 = self.leak1(x1)\n        x2 = self.conv2(x1)\n        x2 = self.leak2(x2)\n        x3 = self.conv3(x2)\n        x3 = self.leak3(x3)\n        x4 = self.conv4(x3)\n        x4 = self.leak4(x4)\n        x5 = self.conv5(x4)\n        x5 = self.leak5(x5)\n        x6 = self.conv6(x5)\n        x6 = self.leak6(x6)\n        x7 = self.conv7(x6)\n        x7 = self.leak7(x7)\n        x8 = self.conv8(x7)\n        x8 = self.leak8(x8)\n        y1 = self.deconv1(x8)\n        y1 = self.deleak1(y1)\n        y1 = self.add1([x7,y1])\n        y2 = self.deconv2(y1)\n        y2 = self.deleak2(y2)\n        y2 = self.add2([x6,y2])\n        y3 = self.deconv3(y2)\n        y3 = self.deleak3(y3)\n        y3 = self.add3([x5,y3])\n        y4 = self.deconv4(y3)\n        y4 = self.deleak4(y4)\n        y4 = self.add4([x4,y4])\n        y5 = self.deconv5(y4)\n        y5 = self.deleak5(y5)\n        y5 = self.add5([x3,y5])\n        y6 = self.deconv6(y5)\n        y6 = self.deleak6(y6)\n        y6 = self.add6([x2,y6])\n        y7 = self.deconv7(y6)\n        y7 = self.deleak7(y7)\n        y7 = self.add7([x1,y7])\n        y8 = self.deconv8(y7)\n        y8 = self.deleak8(y8)\n        x = self.dense(y8)\n        output = self.add([x,inputs])\n        return output\n\nclass dis(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.con1 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5],\n                                           input_shape=(150,150,1))\n        self.leak1 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.con2 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5])\n        self.leak2 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.con3 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5])\n        self.leak3 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.con4 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5])\n        self.leak4 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.con5 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5])\n        self.leak5 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.con6 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5])\n        self.leak6 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.con7 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5])\n        self.leak7 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.con8 = tf.keras.layers.Conv2D(filters=128,\n                                           kernel_size=[5,5])\n        self.leak8 = tf.keras.layers.LeakyReLU(alpha =0.4)\n        self.flatten = tf.keras.layers.Flatten()\n\n        self.dense1 = tf.keras.layers.Dense(1,\n                                         activation=tf.nn.softmax)\n    def call(self,inputs):\n        x = self.con1(inputs)\n        x = self.leak1(x)\n        x = self.con2(x)\n        x = self.leak2(x)\n        x = self.con3(x)\n        x = self.leak3(x)\n        x = self.con4(x)\n        x = self.leak4(x)\n        x = self.con5(x)\n        x = self.leak5(x)\n        x = self.con6(x)\n        x = self.leak6(x)\n        x = self.con7(x)\n        x = self.leak7(x)\n        x = self.con8(x)\n        x = self.leak8(x)\n        x = self.flatten(x)\n        x = tf.nn.dropout(x, 0.5)\n        output = self.dense1(x)\n        return output\n\n\n\n\n\ngene1 = gene()\ndis1 = dis()\n</code></pre>\n\n<p>Here is model define</p>\n\n<pre><code>#imagegenerator \nnum=0\nlearning_rate = 0.001\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n\ntrain_datagen = ImageDataGenerator(rescale=1./255)\ntrain_gene = train_datagen.flow_from_directory(\n    'data/',\n    color_mode='grayscale',\n    target_size=(150, 150),\n    batch_size=10,\n    class_mode=None)\n</code></pre>\n\n<p>image generator</p>\n\n<pre><code>from undersample import undersample\nfor num in range(1000):\n    for x in train_gene:\n        x_u = []\n        for i in range(x.shape[0]):\n            x_u.append(undersample(x[i,:,:,:],0.7))\n        x_u = n.array(x_u)\n        x_r = gene1(x_u)\n        D_train = n.vstack((x,x_r)) \n        D_label = n.vstack((n.zeros((x.shape[0],1)),n.ones((x.shape[0],1))))\n        with tf.GradientTape() as tape:\n            D = dis1(D_train)\n\n            loss = tf.keras.losses.binary_crossentropy(y_true=D_label,y_pred=D)\n            loss = tf.reduce_mean(loss)\n            print(\"batch %d: D_loss %f\" % (num, loss.numpy()))\n        grads = tape.gradient(loss , dis1.variables)\n        optimizer.apply_gradients(grads_and_vars=zip(grads , dis1.variables))\n        break\n    for x in train_gene:\n        x_u = []\n        for i in range(x.shape[0]):\n            x_u.append(undersample(x[i,:,:,:],0.7))\n        x_u = n.array(x_u)\n        with tf.GradientTape(watch_accessed_variables=False) as tape:\n            tape.watch(gene1.variables)\n            D = dis1(gene1(x_u))\n            loss = tf.keras.losses.binary_crossentropy(y_true=n.zeros((x.shape[0],1)),y_pred=D)\n            loss = tf.reduce_mean(loss)\n            print('batch %d: G_loss %f' % (num, loss.numpy()))\n        grads = tape.gradient(loss , gene1.variables)\n        optimizer.apply_gradients(grads_and_vars=zip(grads , gene1.variables))\n        break    \n</code></pre>\n\n<p>train loss</p>\n\n<p>I want to train a gan model that reconstruct undersample MRI image to a normal MRI image. But when I train the model with code above, I got D_loss/G_loss which be 7.666619/15.333239  and never change. Is this a code problem organ model train don't work like this?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 140}]
[{"items": [{"tags": ["python", "tensorflow", "deep-learning", "eager-execution", "sonnet"], "owner": {"account_id": 7254915, "reputation": 3462, "user_id": 5533928, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/517d5a0e08d76cd446ac7e6b156093a4?s=256&d=identicon&r=PG&f=1", "display_name": "Susmit Agrawal", "link": "https://stackoverflow.com/users/5533928/susmit-agrawal"}, "is_answered": true, "view_count": 422, "answer_count": 1, "score": 1, "last_activity_date": 1567083763, "creation_date": 1559755708, "last_edit_date": 1567083763, "question_id": 56465346, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/56465346/lstm-timesteps-in-sonnet", "title": "LSTM timesteps in Sonnet", "body": "<p>I'm currently trying to learn <code>Sonnet</code>.</p>\n\n<p>My network (incomplete, the question is based on this):</p>\n\n<pre><code>class Model(snt.AbstractModule):\n\n    def __init__(self, name=\"LSTMNetwork\"):\n        super(Model, self).__init__(name=name)\n        with self._enter_variable_scope():\n            self.l1 = snt.LSTM(100)\n            self.l2 = snt.LSTM(100)\n            self.out = snt.LSTM(10)\n\n    def _build(self, inputs):\n\n        # 'inputs' is of shape (batch_size, input_length)\n        # I need it to be of shape (batch_size, sequence_length, input_length)\n\n        l1_state = self.l1.initialize_state(np.shape(inputs)[0]) # init with batch_size\n        l2_state = self.l2.initialize_state(np.shape(inputs)[0]) # init with batch_size\n        out_state = self.out.initialize_state(np.shape(inputs)[0])\n\n        l1_out, l1_state = self.l1(inputs, l1_state)\n        l1_out = tf.tanh(l1_out)\n        l2_out, l2_state = self.l2(l1_out, l2_state)\n        l2_out = tf.tanh(l2_out)\n        output, out_state = self.out(l2_out, out_state)\n        output = tf.sigmoid(output)\n\n        return output, out_state\n</code></pre>\n\n<p>In other frameworks (eg. Keras), LSTM inputs are of the form <code>(batch_size, sequence_length, input_length)</code>.</p>\n\n<p>However, the Sonnet documentation states that the input to Sonnet's LSTM is of the form <code>(batch_size, input_length)</code>.</p>\n\n<p>How do I use them for sequential input?</p>\n\n<p>So far, I've tried using a for loop inside <code>_build</code>, iterating over each timestep, but that gives seemingly random outputs.</p>\n\n<p>I've tried the same architecture in Keras, which runs without any issues.</p>\n\n<p>I'm executing in eager mode, using <code>GradientTape</code> for training.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 134}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "keras-layer"], "owner": {"account_id": 15116032, "reputation": 30088, "user_id": 10908375, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/L7f8w.jpg?s=256&g=1", "display_name": "Nicolas Gervais", "link": "https://stackoverflow.com/users/10908375/nicolas-gervais"}, "is_answered": true, "view_count": 698, "answer_count": 3, "score": 1, "last_activity_date": 1621316650, "creation_date": 1621019487, "last_edit_date": 1621316650, "question_id": 67539788, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67539788/replacing-the-input-layer-of-a-pre-trained-model-with-different-channels", "title": "Replacing the input layer of a pre-trained model with different channels?", "body": "<p>I want to re-use the pre-trained weights of <code>MobiletNetv2</code>, but with images with <strong>12</strong> channels. I know this needs to create more weights, but that's okay because I want to re-train anyway. I can't find a way to make it work.</p>\n<pre><code>import tensorflow as tf\n\nclass CNN(tf.keras.Model):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.input_layer = tf.keras.layers.InputLayer(input_shape=(None, 224, 224, 12))\n        self.base = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3),\n                                                      include_top=False,\n                                                      weights='imagenet')\n        _ = self.base._layers.pop(0)\n        self.flat1 = tf.keras.layers.Flatten()\n        self.dens3 = tf.keras.layers.Dense(10)\n\n    def call(self, x, **kwargs):\n        x = self.input_layer(x)\n        x = self.base(x)\n        x = self.flat1(x)\n        x = self.dens3(x)\n        return x\n\nmodel = CNN()\nmodel.build(input_shape=(None, 224, 224, 12))\n</code></pre>\n<blockquote>\n<p>ValueError: Input 0 is incompatible with layer mobilenetv2_1.00_224: expected shape=(None, 224, 224, 3), found shape=(None, 224, 224, 12)</p>\n</blockquote>\n<p>I tried popping the first layer like in other answers.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 221}]
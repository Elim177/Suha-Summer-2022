[{"items": [{"tags": ["python", "pytorch", "tensorflow2.0", "softmax"], "owner": {"account_id": 7748979, "reputation": 1722, "user_id": 5865579, "user_type": "registered", "accept_rate": 100, "profile_image": "https://www.gravatar.com/avatar/185a0b751c11ba968e2cf51ec30c37d0?s=256&d=identicon&r=PG&f=1", "display_name": "jason", "link": "https://stackoverflow.com/users/5865579/jason"}, "is_answered": true, "view_count": 1999, "accepted_answer_id": 65458870, "answer_count": 1, "score": 1, "last_activity_date": 1640888987, "creation_date": 1609002372, "last_edit_date": 1609010936, "question_id": 65458736, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65458736/pytorch-equivalent-to-tf-nn-softmax-cross-entropy-with-logits-and-tf-nn-sigmoid", "title": "PyTorch equivalent to tf.nn.softmax_cross_entropy_with_logits and tf.nn.sigmoid_cross_entropy_with_logits", "body": "<p>I found the post <a href=\"https://stackoverflow.com/questions/46218566/pytorch-equivalence-for-softmax-cross-entropy-with-logits\">here</a>. Here, we try to find an equivalence of <code>tf.nn.softmax_cross_entropy_with_logits</code> in PyTorch. The answer is still confusing to me.</p>\n<p>Here is the <code>Tensorflow 2</code> code</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n# here we assume 2 batch size with 5 classes\n\npreds = np.array([[.4, 0, 0, 0.6, 0], [.8, 0, 0, 0.2, 0]])\nlabels = np.array([[0, 0, 0, 1.0, 0], [1.0, 0, 0, 0, 0]])\n\n\ntf_preds = tf.convert_to_tensor(preds, dtype=tf.float32)\ntf_labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n\nloss = tf.nn.softmax_cross_entropy_with_logits(logits=tf_preds, labels=tf_labels)\n</code></pre>\n<p>It give me the <code>loss</code> as</p>\n<pre><code>&lt;tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.2427604, 1.0636061], dtype=float32)&gt;\n</code></pre>\n<p>Here is the <code>PyTorch</code> code</p>\n<pre><code>import torch\nimport numpy as np\n\npreds = np.array([[.4, 0, 0, 0.6, 0], [.8, 0, 0, 0.2, 0]])\nlabels = np.array([[0, 0, 0, 1.0, 0], [1.0, 0, 0, 0, 0]])\n\n\ntorch_preds = torch.tensor(preds).float()\ntorch_labels = torch.tensor(labels).float()\n\nloss = torch.nn.functional.cross_entropy(torch_preds, torch_labels)\n</code></pre>\n<p>However, it raises:</p>\n<blockquote>\n<p>RuntimeError: 1D target tensor expected, multi-target not supported</p>\n</blockquote>\n<p>It seems that the problem is still unsolved. How to implement <code>tf.nn.softmax_cross_entropy_with_logits</code> in PyTorch?</p>\n<p>What about <code>tf.nn.sigmoid_cross_entropy_with_logits</code>?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 242}]
[{"items": [{"tags": ["python", "tensorflow", "fft", "automatic-differentiation"], "owner": {"account_id": 10450698, "reputation": 93, "user_id": 7704419, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/b0bb1d437fd8ff280e7e28922c9db25d?s=256&d=identicon&r=PG&f=1", "display_name": "valade aur&#233;lien", "link": "https://stackoverflow.com/users/7704419/valade-aur%c3%a9lien"}, "is_answered": false, "view_count": 270, "answer_count": 0, "score": 2, "last_activity_date": 1619516887, "creation_date": 1619516887, "question_id": 67280717, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67280717/is-there-a-way-to-define-the-gradient-of-a-3d-fft-using-tensorflows-custom-grad", "title": "Is there a way to define the gradient of a 3D FFT using tensorflow&#39;s custom_gradient decorator", "body": "<p><strong>Context &amp; problem</strong></p>\n<p>I am using the Hamiltonian Monte Carlo (HMC) method of the <code>tensorflow-probability</code> module to explore the most probable states of a self-written probability function. Amongst the parameters I am trying to fit are Fourier modes of a real three dimensional field.</p>\n<p>For the HMC to run, each computing block needs to have its gradient implemented. However, the implementation of the inverse real FFT <code>tf.signal.irfft3d</code> does not have a gradient method associated by default.</p>\n<p><strong>Question</strong></p>\n<p>Is there a way to implement the gradient of the function <code>irfft3d</code>? I already have a running, self-implemented <code>irfft3d</code> with more basic blocks of <code>tensorflow</code> on which automatic differentiation seems to work, but I would like to wrap the actual optimized and stable implementation of <code>tf.signal.irfft3d</code> using the decorator <code>@tf.custom_gradient</code> to make the automatic differentiation work.</p>\n<p><strong>My guess</strong></p>\n<p>The Fourier transform is linear, this problem is then theoretically trivial. However, writing the Jacobian of the Fourier transform on a grid is numerically unfeasible (as its dimensions would be huge). Luckily, <code>tensorflow</code> demands only for a functional that evaluates the Jacobian on a input vector. I believe this can be efficiently done thanks to the FFT algorithm. Unfortunately, it seems to me that <code>tensorflow</code> demands a functional that computes the <em>invert</em> of the Jacobian applied to the &quot;upstream gradient&quot;, which I do not understand:</p>\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/custom_gradient?version=nightly\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/api_docs/python/tf/custom_gradient?version=nightly</a></p>\n<blockquote>\n<p>function f(*x) that returns a tuple (y, grad_fn) where:</p>\n<ul>\n<li>x is a sequence of (nested structures of) Tensor inputs to the function.</li>\n<li>y is a (nested structure of) Tensor outputs of applying TensorFlow operations in f to x.</li>\n<li>grad_fn is a function with the signature g(*grad_ys) which returns a list of Tensors the same size as (flattened) x - the derivatives of Tensors in y with respect to the Tensors in x. grad_ys is a sequence of Tensors the same size as (flattened) y holding the initial value gradients for each Tensor in y.</li>\n</ul>\n<p>In a pure mathematical sense, a vector-argument vector-valued function f's derivatives should be its Jacobian matrix J. Here we are expressing the Jacobian J as a function grad_fn which defines how J will transform a vector grad_ys when left-multiplied with it (grad_ys * J, the vector-Jacobian product, or VJP). This functional representation of a matrix is convenient to use for chain-rule calculation (in e.g. the back-propagation algorithm).</p>\n</blockquote>\n<p>Complying with the dimensions and the formats given in the doc, I cannot imagine any other solution that:</p>\n<pre><code>#!/usr/bin/env python3\n\n# set up\nimport tensorflow as tf\n\nn = 64\n\nnoise = tf.random.normal((n, n, n))\nmodes = tf.signal.rfft3d(noise)\n\n# the function \n@tf.custom_gradient\ndef irfft3d(x):\n    def grad_fn(dy):\n        return (tf.signal.rfft3d(dy))\n\n    return (tf.signal.irfft3d(x), grad_fn)\n\n# test \nwith tf.GradientTape() as gt:\n    gt.watch(modes)\n    rec_noise = irfft3d(modes)\n\ndn_dm = gt.gradient(rec_noise, modes)\n\nprint(dn_dm)\n</code></pre>\n<p>Which does run and return:</p>\n<pre><code>tf.Tensor(\n[[[262144.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j\n        0.+0.j]\n  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j\n        0.+0.j]\n  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j\n        0.+0.j]\n  ...\n  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j\n        0.+0.j]\n  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j\n        0.+0.j]\n  [     0.+0.j      0.+0.j      0.+0.j ...      0.+0.j      0.+0.j\n        0.+0.j]]], shape=(64, 64, 33), dtype=complex64)\n</code></pre>\n<p>I cannot really wrap up my mind around it. First, this would be such a simple solution that I would not understand why it has not been natively  implemented. But more importantly, I am simply lost in what <code>tensorflow</code> expects from this self-written gradient function and I am not able to express its result in a mathematical way that makes sense to me.</p>\n<p>Is there anybody out there that understands the way <code>tensorflow</code> handles differentiation and could help or correct me?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 155}]
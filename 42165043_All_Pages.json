[{"items": [{"tags": ["python", "tensorflow", "deep-learning"], "owner": {"account_id": 963556, "reputation": 1375, "user_id": 987397, "user_type": "registered", "accept_rate": 32, "profile_image": "https://www.gravatar.com/avatar/fa7ae7d9bd13c2d04335c3209865c262?s=256&d=identicon&r=PG", "display_name": "Derk", "link": "https://stackoverflow.com/users/987397/derk"}, "is_answered": false, "view_count": 2261, "answer_count": 0, "score": 5, "last_activity_date": 1486746848, "creation_date": 1486746848, "question_id": 42165043, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/42165043/tensorflow-is-this-normal-behaviour-of-batch-normalization", "title": "Tensorflow: is this normal behaviour of Batch normalization?", "body": "<p>See the screenshot. Without batch normalization is the blue line. Accuracy in the upper plot, loss in the lower one. So without BN the loss decreases slowly and the accuracy increases slowly, expected behaviour.</p>\n\n<p>But then I try batch normalization. Training loss converges quite fast to values near zero. So much better than without, but testing accuracy on both train and test sets give worse results.\n<a href=\"https://i.stack.imgur.com/9ZaS8.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/9ZaS8.png\" alt=\"enter image description here\"></a></p>\n\n<p>I based my implementation on this: <a href=\"http://ruishu.io/2016/12/27/batchnorm/\" rel=\"noreferrer\">http://ruishu.io/2016/12/27/batchnorm/</a>\nSo adding batch norm to a layer like this:</p>\n\n<pre><code>h1_p_bn = tf.contrib.layers.batch_norm(h1_p, center=True, scale=True, is_training=self.is_training,scope='bn1')\n</code></pre>\n\n<p>with model.is_training a placeholder that is set to zero when testing accuracy (upper plot). </p>\n\n<p>I do this as well:</p>\n\n<pre><code># Requirement from tf.contrib.layers.batch_norm\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    # Ensures that we execute the update_ops before performing the train_step\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n</code></pre>\n\n<p>Any ideas or suggestions?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 109}]
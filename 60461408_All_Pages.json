[{"items": [{"tags": ["python", "python-3.x", "tensorflow", "keras"], "owner": {"account_id": 5423068, "reputation": 23, "user_id": 4316180, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/cd828f8f12bc6f15655247e975ee573e?s=256&d=identicon&r=PG&f=1", "display_name": "Sc.Shenoy", "link": "https://stackoverflow.com/users/4316180/sc-shenoy"}, "is_answered": true, "view_count": 1335, "accepted_answer_id": 60491573, "answer_count": 1, "score": 0, "last_activity_date": 1583161464, "creation_date": 1582942569, "last_edit_date": 1583085463, "question_id": 60461408, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60461408/tensorflow-2-0-sparsecategoricalcrossentropy-valueerror-shape-mismatch-the-sha", "title": "TensorFlow 2.0 SparseCategoricalCrossentropy valueError: Shape mismatch: The shape of labels should equal the shape of logits except for the last", "body": "<p>New to ML and TensorFlow in general.  Im getting this issue when I try to run this line (t_loss = loss_object(labels, predictions)) in the train_step function.</p>\n\n<p>I feel i'm missing something super small and stupid!  Checked other solutions and from what i can gather its for older versions of TF or syntax and structure is different.  Below snippet is executable. Just feel like i dont understand enough after googling.  Any help is appreciated.</p>\n\n<p><strong>Error Received</strong></p>\n\n<p><code>ValueError: Shape mismatch: The shape of labels (received (30,)) should equal the shape of logits except for the last dimension (received (2, 10)).</code></p>\n\n<p>I am following this write up and adding my own spin if possible.\n<a href=\"https://cloud.google.com/blog/products/ai-machine-learning/how-to-serve-deep-learning-models-using-tensorflow-2-0-with-cloud-functions\" rel=\"nofollow noreferrer\">GCP TF sample writeup</a>\n'</p>\n\n<pre><code>import tensorflow as tf\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import backend as K;\n\nimport nltk\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\n#try sklearn\nfrom sklearn.model_selection import train_test_split\n\nEPOCHS = 10\n\n# staging and vars\ndata=pd.read_csv('../rando.csv')\ndata = data[pd.notnull(data['utext'])]\ndata=data[data.type != 'None']\n\n# encode unique values of the types\nle = LabelEncoder()\ndata['type'] = le.fit_transform(data['type'])\n\ntraining_data = [] \ntestTrain_data = []\n# create a dictionary of data based on type\nfor index,row in data.iterrows():\n    training_data.append({\"class\":row[\"type\"], \"sentence\":row[\"fulltext\"]})\n\nwords = []\nclasses = []\ndocuments= []\n\nnot_required= ['?']\n# create our training data\ntraining = []\noutput = []\n\nlanStemmer = LancasterStemmer()\n\ndef stemDocWord(words=words, classes=classes):\n    # loop through each sentence in our training data\n    for pattern in training_data:\n        # tokenize each word in the sentence\n        w = nltk.word_tokenize(pattern['sentence'])\n        # add to our words list\n        words.extend(w)\n\n        documents.append((w, pattern['class']))\n        # add to our classes list\n        if pattern['class'] not in classes:\n            classes.append(pattern['class'])\n\n    # stem and lower each word and remove duplicates\n    stemmer = PorterStemmer()\n    words = [stemmer.stem(w.lower()) for w in words if w not in not_required]\n    words = list(set(words))\n\n    # remove duplicates\n    classes = list(set(classes))\n\n    print(len(documents), \"documents\")\n\n\ndef listWordTokensForPattern():\n    # create an empty array for our output\n    output_empty = [0] * len(classes)\n\n    # training set, bag of words for each sentence\n    for doc in documents:\n        # initialize our bag of words\n        bag = []\n        # list of tokenized words for the pattern\n        pattern_words = doc[0]\n        # stem each word\n        pattern_words = [lanStemmer.stem(word.lower()) for word in pattern_words]\n        # create our bag of words array\n        for w in words:\n            bag.append(1) if w in pattern_words else bag.append(0)\n\n        training.append(bag)\n        # output is a '0' for each tag and '1' for current tag\n        output_row = list(output_empty)\n        output_row[classes.index(doc[1])] = 1\n        output.append(output_row)\n\n    print(\"# output\", len(output))\n    print(\"# training\", len(training))\n\n# og training function\nstemDocWord()\nlistWordTokensForPattern()\nX = np.array(training)\ny = np.array(output)\n\nprint(X.shape)\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)\nx_train, x_test = x_train / 255.0, x_test / 255.0 \n\n# Add a channels dimension e.g. (60000, 28, 28) =&gt; (60000, 28, 28, 1)\nx_train = x_train[..., tf.newaxis, tf.newaxis]\nx_test = x_test[..., tf.newaxis, tf.newaxis]\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(100).batch(2)\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(2)\n\nprint(x_train)\nprint(K.image_data_format())\n\n# inputs_ = tf.compat.v1.placeholder(tf.float32, [None, 32, 32, 3])\n# inputs_ = tf.Variable(tf.ones(shape=(0 ,32, 32, 3)), name=\"inputs_\")\nclass CustomModel(Model):\n  def __init__(self):\n    super(CustomModel, self).__init__()\n    self.conv1 = Conv2D(2, 1,activation='relu')#, input_shape=x_train.shape)#x_train.shape())\n    self.flatten = Flatten()\n    self.d1 = Dense(128, activation='relu')\n    self.d2 = Dense(10, activation='softmax')\n\n  def call(self, x):\n    x = self.conv1(x)\n    x = self.flatten(x)\n    x = self.d1(x)\n    return self.d2(x)\n\nmodel = CustomModel()\n\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction='none')\noptimizer = tf.keras.optimizers.Adam()\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n\n@tf.function\ndef train_step(images, labels):\n  with tf.GradientTape() as tape:\n    predictions = model(images)\n    loss = loss_object(labels, predictions)\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n  train_loss(loss)\n  train_accuracy(labels, predictions)\n\n@tf.function\ndef test_step(images, labels):\n  predictions = model(images)\n  t_loss = loss_object(labels, predictions)\n\n  test_loss(t_loss)\n  test_accuracy(labels, predictions)\n\nfor epoch in range(EPOCHS):\n  for images, labels in train_ds:\n    train_step(images, labels)\n\n  for test_images, test_labels in test_ds:\n    test_step(test_images, test_labels)\n\n  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n  print (template.format(epoch+1,\n                         train_loss.result(),\n                         train_accuracy.result()*100,\n                         test_loss.result(),\n                         test_accuracy.result()*100))\n\n# Save the weights\nmodel.save_weights('fashion_mnist_weights')\n</code></pre>\n\n<p>the CSV File looks similar to this</p>\n\n<p>utext,fulltext,type</p>\n\n<p>t1,\"some random sentence\",type1</p>\n\n<p>t2,\"some other random sentence\",type2</p>\n\n<p>t3,\"some more random text\",type3</p>\n\n<hr>\n\n<p>Below is what i get regarding my first comment:</p>\n\n<p><code>WARNING:tensorflow:Entity &lt;function train_step at 0x000001E9480B11E0&gt; could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, 'export AUTOGRAPH_VERBOSITY=10') and attach the full output. Cause: converting &lt;function train_step at 0x000001E9480B11E0&gt;: AttributeError: module 'gast' has no attribute 'Str'\nWARNING:tensorflow:Entity &lt;bound method CustomModel.call of &lt;__main__.CustomModel object at 0x000001E947C9D358&gt;&gt; could not be transformed and will be executed as-is. Please report this to \nthe AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, 'export AUTOGRAPH_VERBOSITY=10') and attach the full output. Cause: converting &lt;bound method CustomModel.call of &lt;__main__.CustomModel object at 0x000001E947C9D358&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4\n2020-02-29 12:14:46.018316: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2683371520 exceeds 10% of system memory.\n2020-02-29 12:14:47.459793: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2683371520 exceeds 10% of system memory.\n2020-02-29 12:14:47.869789: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 2683371520 exceeds 10% of system memory.</code></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 53}]
[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "deep-learning"], "owner": {"account_id": 5556872, "reputation": 952, "user_id": 4557607, "user_type": "registered", "accept_rate": 65, "profile_image": "https://graph.facebook.com/679173583/picture?type=large", "display_name": "Edv Beq", "link": "https://stackoverflow.com/users/4557607/edv-beq"}, "is_answered": true, "view_count": 1587, "accepted_answer_id": 67748059, "answer_count": 1, "score": 1, "last_activity_date": 1622320598, "creation_date": 1622257844, "last_edit_date": 1622320598, "question_id": 67747389, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67747389/typeerror-cannot-convert-a-symbolic-keras-input-output-to-numpy-array", "title": "TypeError: Cannot convert a symbolic Keras input/output to numpy array", "body": "<p>Trying to upgrade this awesome implementation of <strong>gumble-softmax-vae</strong> found <a href=\"https://github.com/EderSantana/gumbel/blob/master/GumbelVAE.ipynb\" rel=\"nofollow noreferrer\">here</a>. However, I keep getting</p>\n<pre><code>TypeError: Cannot convert a symbolic Keras input/output to a numpy array. \n</code></pre>\n<p>I am stumped - tried many many things. Interestingly some searches return with other implementation of VAEs. I believe the error is somewhere in the &quot;KL&quot; term calculation of the loss.</p>\n<p>Here is the almost working code:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nbatch_size = 10\ndata_dim = 784\n\nM = 10  # classes\nN = 30  # how many distributions\n\nnb_epoch = 100\nepsilon_std = 0.01\nanneal_rate = 0.0003\nmin_temperature = 0.5\n\ntau = tf.Variable(5.0, dtype=tf.float32)\n\n\nclass Sampling(keras.layers.Layer):\n    def call(self, logits_y):\n        u = tf.random.uniform(tf.shape(logits_y), 0, 1)\n        y = logits_y - tf.math.log(\n            -tf.math.log(u + 1e-20) + 1e-20\n        )  # logits + gumbel noise\n        y = tf.nn.softmax(tf.reshape(y, (-1, N, M)) / tau)\n        y = tf.reshape(y, (-1, N * M))\n        return y\n\n\nencoder_inputs = keras.Input(shape=(data_dim))\nx = keras.layers.Dense(512, activation=&quot;relu&quot;)(encoder_inputs)\nx = keras.layers.Dense(256, activation=&quot;relu&quot;)(x)\nlogits_y = keras.layers.Dense(M * N, name=&quot;logits_y&quot;)(x)\nz = Sampling()(logits_y)\nencoder = keras.Model(encoder_inputs, z, name=&quot;encoder&quot;)\nencoder.build(encoder_inputs)\n\nprint(encoder.summary())\n\ndecoder_inputs = keras.Input(shape=(N * M))\nx = keras.layers.Dense(256, activation=&quot;relu&quot;)(decoder_inputs)\nx = keras.layers.Dense(512, activation=&quot;relu&quot;)(x)\ndecoder_outputs = keras.layers.Dense(data_dim, activation=&quot;sigmoid&quot;)(x)\ndecoder = keras.Model(decoder_inputs, decoder_outputs, name=&quot;decoder&quot;)\ndecoder.build(decoder_inputs)\n\nprint(decoder.summary())\n\n\nclass VAE(keras.Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        super(VAE, self).__init__(**kwargs)\n        self.encoder = encoder\n        self.decoder = decoder\n        self.bce = tf.keras.losses.BinaryCrossentropy()\n        self.loss_tracker = keras.metrics.Mean(name=&quot;loss&quot;)\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker]\n\n    def call(self, x):\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        return x_hat\n\n    @tf.function\n    def gumbel_loss(self, y_true, y_pred, logits_y):\n        q_y = tf.reshape(logits_y, (-1, N, M))\n        q_y = tf.nn.softmax(q_y)\n        log_q_y = tf.math.log(q_y + 1e-20)\n        kl_tmp = q_y * (log_q_y - tf.math.log(1.0 / M))\n        kl = tf.math.reduce_sum(kl_tmp, axis=(1, 2))\n        kl = tf.squeeze(kl, axis=0)\n        elbo = data_dim * self.bce(y_true, y_pred) - kl\n        return elbo\n\n    def train_step(self, data):\n        x = data\n\n        with tf.GradientTape(persistent=True) as tape:\n            z = self.encoder(x, training=True)\n            x_hat = self.decoder(z, training=True)\n\n            x = tf.cast(x, dtype=tf.float32)\n            x_hat = tf.cast(x_hat, dtype=tf.float32)\n            logits_y = self.encoder.get_layer('logits_y').output\n\n            loss = self.gumbel_loss(x, x_hat, logits_y)\n\n        grads = tape.gradient(loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n        self.loss_tracker.update_state(loss)\n        return {&quot;loss&quot;: self.loss_tracker.result()}\n\n\ndef main():\n\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(\n        path=&quot;mnist.npz&quot;\n    )\n\n    x_train = x_train.astype(&quot;float32&quot;) / 255.0\n    x_test = x_test.astype(&quot;float32&quot;) / 255.0\n    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\n    vae = VAE(encoder, decoder, name=&quot;vae-model&quot;)\n    vae_inputs = (None, data_dim)\n    vae.build(vae_inputs)\n    vae.compile(optimizer=&quot;adam&quot;, loss=None)\n    vae.fit(\n        x_train,\n        shuffle=True,\n        epochs=1,\n        batch_size=batch_size\n    )\n\nif __name__ == &quot;__main__&quot;:\n    main()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 216}]
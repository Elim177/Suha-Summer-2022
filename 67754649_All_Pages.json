[{"items": [{"tags": ["tensorflow", "keras", "tf.keras"], "owner": {"account_id": 13874560, "reputation": 38, "user_id": 10016590, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-Egjn-sxD9SI/AAAAAAAAAAI/AAAAAAAAABk/Z-Ltv_YFKuM/photo.jpg?sz=256", "display_name": "Padma_Rakesh_Sagar", "link": "https://stackoverflow.com/users/10016590/padma-rakesh-sagar"}, "is_answered": true, "view_count": 484, "accepted_answer_id": 67754918, "answer_count": 1, "score": 0, "last_activity_date": 1622315242, "creation_date": 1622313105, "question_id": 67754649, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67754649/mean-of-tensorflow-kerass-glorot-normal-initializer-is-not-zero", "title": "Mean of Tensorflow Keras&#39;s Glorot Normal Initializer is not zero", "body": "<p>As per the documentation of <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal\" rel=\"nofollow noreferrer\">Glorot Normal</a>, <strong><code>mean</code></strong> of the <strong><code>Normal Distribution</code></strong> of the <strong><code>Initial Weights</code></strong> should be <strong><code>zero</code></strong>.</p>\n<blockquote>\n<p>Draws samples from a truncated normal distribution centered on 0</p>\n</blockquote>\n<p>But it doesn't seem to be <strong><code>zero</code></strong>, am I missing something?</p>\n<p>Please find the code below:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nimport numpy as np\n\nprint(tf.__version__)\n\ninitializer = tf.keras.initializers.GlorotNormal(seed = 1234)\n\n\nmodel = Sequential([Dense(units = 3, input_shape = [1], kernel_initializer = initializer,\n                         bias_initializer = initializer),\n                   Dense(units = 1, kernel_initializer = initializer,\n                         bias_initializer = initializer)])\n\nbatch_size = 1\n\nx = np.array([-1.0, 0, 1, 2, 3, 4.0], dtype = 'float32')\ny = np.array([-3, -1.0, 1, 3.0, 5.0, 7.0], dtype = 'float32')\n\nx = np.reshape(x, (-1, 1))\n\n# Prepare the training dataset.\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x, y))\ntrain_dataset = train_dataset.shuffle(buffer_size=64).batch(batch_size)\n\nepochs = 1\nlearning_rate=1e-3\n\n# Instantiate an optimizer.\noptimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n\nfor epoch in range(epochs):\n\n    # Iterate over the batches of the dataset.\n    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n        \n        with tf.GradientTape() as tape:\n            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n\n            # Compute the loss value for this minibatch.\n            loss_value = tf.keras.losses.MSE(y_batch_train, logits)               \n        \n       \n        Initial_Weights_1st_Hidden_Layer = model.trainable_weights[0]\n       \n        Mean_Weights_Hidden_Layer = tf.reduce_mean(Initial_Weights_1st_Hidden_Layer)\n                          \n        Initial_Weights_Output_Layer = model.trainable_weights[2]\n        \n        Mean_Weights_Output_Layer = tf.reduce_mean(Initial_Weights_Output_Layer)  \n               \n        Initial_Bias_1st_Hidden_Layer = model.trainable_weights[1]\n        \n        Mean_Bias_Hidden_Layer = tf.reduce_mean(Initial_Bias_1st_Hidden_Layer)      \n        \n        Initial_Bias_Output_Layer = model.trainable_weights[3]\n        \n        Mean_Bias_Output_Layer = tf.reduce_mean(Initial_Bias_Output_Layer)\n        \n        if epoch ==0 and step==0:\n            \n            print('\\n Initial Weights of First-Hidden Layer = ', Initial_Weights_1st_Hidden_Layer)\n            print('\\n Mean of Weights of Hidden Layer = %s' %Mean_Weights_Hidden_Layer.numpy())\n            \n            print('\\n Initial Weights of Second-Hidden/Output Layer = ', Initial_Weights_Output_Layer)\n            print('\\n Mean of Weights of Output Layer = %s' %Mean_Weights_Output_Layer.numpy())\n                \n            print('\\n Initial Bias of First-Hidden Layer = ', Initial_Bias_1st_Hidden_Layer)\n            print('\\n Mean of Bias of Hidden Layer = %s' %Mean_Bias_Hidden_Layer.numpy())\n\n            print('\\n Initial Bias of Second-Hidden/Output Layer = ', Initial_Bias_Output_Layer)\n            print('\\n Mean of Bias of Output Layer = %s' %Mean_Bias_Output_Layer.numpy())\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 134}]
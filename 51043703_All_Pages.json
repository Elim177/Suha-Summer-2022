[{"items": [{"tags": ["python", "tensorflow", "iterator", "dataset"], "owner": {"account_id": 13457117, "reputation": 671, "user_id": 9710391, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=256", "display_name": "gab", "link": "https://stackoverflow.com/users/9710391/gab"}, "is_answered": true, "view_count": 1292, "accepted_answer_id": 58475682, "answer_count": 2, "score": 1, "last_activity_date": 1571594102, "creation_date": 1530018454, "last_edit_date": 1571594102, "question_id": 51043703, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/51043703/how-to-correctly-map-a-python-function-and-then-batch-the-dataset-in-tensorflow", "title": "How to correctly map a python function and then batch the Dataset in Tensorflow", "body": "<p>I wish to create a pipeline to provide non-standard files to the neural network (for example with extension *.xxx).\nCurrently I have structured my code as follows:</p>\n\n<p>\u00a0 1) I define a list of paths where to find training files</p>\n\n<p>\u00a0 2) I define an instance of the tf.data.Dataset object containing these paths</p>\n\n<p>\u00a0 3) I map to the Dataset a python function that takes each path and returns the associated numpy array (loaded from the folder on the pc); this array is a matrix with dimensions [256, 256, 192].</p>\n\n<p>\u00a0 4) I define an initializable iterator and then use it during network training.</p>\n\n<p>My doubt lies in the size of the batch I provide to the network. I would like to have batches of size 64 supplied to the network. How could I do?\nFor example, if I use the function train_data.batch(b_size) with b_size = 1 the result is that when iterated, the iterator gives one element of shape [256, 256, 192]; what if I wanted to feed the neural net with just 64 slices of this array?</p>\n\n<p>This is an extract of my code:</p>\n\n<pre><code>    with tf.name_scope('data'):\n        train_filenames = tf.constant(list_of_files_train)\n\n        train_data = tf.data.Dataset.from_tensor_slices(train_filenames)\n        train_data = train_data.map(lambda filename: tf.py_func(\n            self._parse_xxx_data, [filename], [tf.float32]))\n\n        train_data.shuffle(buffer_size=len(list_of_files_train))\n        train_data.batch(b_size)\n\n        iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n\n        input_data = iterator.get_next()\n        train_init = iterator.make_initializer(train_data)\n\n  [...]\n\n  with tf.Session() as sess:\n      sess.run(train_init)\n      _ = sess.run([self.train_op])\n</code></pre>\n\n<p>Thanks in advance</p>\n\n<h2>----------</h2>\n\n<p><strong>I posted a solution to my problem in the comments below. I would still be happy to receive any comment or suggestion on possible improvements. Thank you ;)</strong></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 76}]
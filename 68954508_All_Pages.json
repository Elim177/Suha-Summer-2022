[{"items": [{"tags": ["deep-learning", "tensorflow2.0", "tf.keras"], "owner": {"account_id": 285031, "reputation": 5218, "user_id": 583464, "user_type": "registered", "accept_rate": 87, "profile_image": "https://www.gravatar.com/avatar/931448dc20c10c1b799bb9dba38b5e58?s=256&d=identicon&r=PG", "display_name": "George", "link": "https://stackoverflow.com/users/583464/george"}, "is_answered": false, "view_count": 191, "answer_count": 1, "score": 3, "last_activity_date": 1652344603, "creation_date": 1630072011, "last_edit_date": 1630487930, "question_id": 68954508, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68954508/validation-and-train-metrics-very-low-values-images-and-masks-generator", "title": "validation and train metrics very low values (images and masks generator)", "body": "<p>I have images(X_train) and masks data (y_train).</p>\n<p>I want to train a unet network. I am currently using iou metric and the validation iou is very low and constant!</p>\n<p>I am not sure if I can handle right the scaling preprocessing of images and masks.</p>\n<p>I have tried either to use only <code>rescale=1.0/255</code> in the generator, either to scale only <code>X_train</code> and <code>X_val</code> hence <code>(images)</code> values and not <code>masks</code> values, either scale in the unet model <code>(s = Lambda(lambda x: x / 255.0) (inputs))</code> . I am not sure if that is the problem, just wondering.</p>\n<p><a href=\"https://easyupload.io/m/u7t5yl\" rel=\"nofollow noreferrer\">here</a> you can download <code>X_train</code> and <code>y_train</code> data</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D,  MaxPooling2D, Conv2DTranspose, \\\n    Dropout, Input, Concatenate, Lambda\nfrom imgaug import augmenters as iaa\nfrom tensorflow.keras import backend as K\n\n\n# gpu setup\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\n    \n\nX_train = np.load('./X_train.npy')\ny_train = np.load('/y_train.npy')\n\n\nX_train = X_train.astype('uint8')\ny_train = y_train.astype('uint8')\n\n\nBATCH_SIZE=8\nSEED=123\nVAL_SPLIT = 0.2\nIMG_HEIGHT = 256\nIMG_WIDTH = 256\n\ndef augment(images):\n\n    seq =  iaa.Sequential([\n        \n        iaa.Fliplr(0.5), # horizontal flips\n        iaa.Flipud(0.5), # vertical flips\n       \n        iaa.Sometimes(\n             0.1,\n             iaa.GaussianBlur(sigma=(0, 0.5))\n        ),\n        iaa.LinearContrast((0.75, 1.5)),\n       \n        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)),\n              \n        iaa.BlendAlphaSimplexNoise(\n            iaa.EdgeDetect(0.3),\n            upscale_method=&quot;linear&quot;),\n        \n       \n        \n    ], random_order=True) \n    \n    return seq.augment_image(images)\n\n\ndef create_gen(X,\n               y,\n               batch_size=BATCH_SIZE,\n               seed=SEED):\n    \n    X_train, X_val, y_train, y_val = \\\n        train_test_split(X,\n                         y,\n                         test_size=VAL_SPLIT)\n        \n    \n    # Image data generator\n    data_gen_args = dict(rescale = 1.0/255,\n                         preprocessing_function=augment)\n\n    data_gen_args_masks = dict(                      preprocessing_function=augment)\n                             \n    X_datagen = ImageDataGenerator(**data_gen_args)\n    y_datagen = ImageDataGenerator(**data_gen_args_masks)\n    \n    X_datagen.fit(X_train, augment=True, seed=seed)\n    y_datagen.fit(y_train, augment=True, seed=seed)\n    \n    X_train_augmented = X_datagen.flow(X_train,\n                                       batch_size=batch_size,\n                                       shuffle=True,\n                                       seed=seed)\n    y_train_augmented = y_datagen.flow(y_train,\n                                       batch_size=batch_size,\n                                       shuffle=True,\n                                       seed=seed)\n    \n    # Validation data generator     \n    data_gen_args_val = dict(rescale = 1.0/255)\n                                                     \n    X_datagen_val = ImageDataGenerator(**data_gen_args_val)\n    y_datagen_val = ImageDataGenerator()\n    \n    X_datagen_val.fit(X_val, augment=True, seed=seed)\n    y_datagen_val.fit(y_val, augment=True, seed=seed)\n    \n    X_val_after = X_datagen_val.flow(X_val,\n                                     batch_size=batch_size,\n                                     shuffle=False)\n                                    \n    y_val_after = y_datagen_val.flow(y_val,\n                                     batch_size=batch_size,\n                                     shuffle=False)\n                                    \n     \n    train_generator = zip(X_train_augmented, y_train_augmented)\n    val_generator = zip(X_val_after, y_val_after)\n    \n    steps_per_epoch = X_train_augmented.n // X_train_augmented.batch_size\n    validation_steps = X_val_after.n // X_val_after.batch_size\n    return train_generator, val_generator, steps_per_epoch, validation_steps\n\n\ntrain_generator, val_generator, steps_per_epoch, validation_steps =  \\\n    create_gen(X_train,\n               y_train,\n               batch_size=BATCH_SIZE)\n\n\n# Build U-Net model\ninputs = Input((IMG_HEIGHT, IMG_WIDTH, 3))\n#s = Lambda(lambda x: x / 255) (inputs)  # rescale inputs\n\nc1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (inputs)\nc1 = Dropout(0.1) (c1)\nc1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c1)\np1 = MaxPooling2D((2, 2)) (c1)\n\nc2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p1)\nc2 = Dropout(0.1) (c2)\nc2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c2)\np2 = MaxPooling2D((2, 2)) (c2)\n\nc3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p2)\nc3 = Dropout(0.2) (c3)\nc3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c3)\np3 = MaxPooling2D((2, 2)) (c3)\n\nc4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p3)\nc4 = Dropout(0.2) (c4)\nc4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c4)\np4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\nc5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (p4)\nc5 = Dropout(0.3) (c5)\nc5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c5)\n\nu6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\nu6 = Concatenate()([u6, c4])\nc6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u6)\nc6 = Dropout(0.2) (c6)\nc6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c6)\n\nu7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\nu7 = Concatenate()([u7, c3])\nc7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u7)\nc7 = Dropout(0.2) (c7)\nc7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c7)\n\nu8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\nu8 = Concatenate()([u8, c2])\nc8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u8)\nc8 = Dropout(0.1) (c8)\nc8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c8)\n\nu9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\nu9 = Concatenate()([u9, c1])\nc9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (u9)\nc9 = Dropout(0.1) (c9)\nc9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same') (c9)\n\noutputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n\nmodel = Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=[iouMetric])\n\nEPOCHS = 40\n\nmodel.fit( train_generator,\n           validation_data=val_generator,\n           batch_size=BATCH_SIZE,\n           steps_per_epoch= steps_per_epoch,\n           validation_steps=validation_steps,\n epochs=EPOCHS)\n</code></pre>\n<p>code for ioumetric:</p>\n<pre><code>def castF(x):\n    return K.cast(x, K.floatx())\n\ndef castB(x):\n    return K.cast(x, bool)\n\ndef iou_loss_core(true,pred):  #this can be used as a loss if you make it negative\n    intersection = true * pred\n    notTrue = 1 - true\n    union = true + (notTrue * pred)\n\n    return (K.sum(intersection, axis=-1) + K.epsilon()) / (K.sum(union, axis=-1) + K.epsilon())\n\ndef iouMetric(true, pred):\n\n    tresholds = [0.5 + (i * 0.05)  for i in range(5)]\n\n    #flattened images (batch, pixels)\n    true = K.batch_flatten(true)\n    pred = K.batch_flatten(pred)\n    pred = castF(K.greater(pred, 0.5))\n\n    #total white pixels - (batch,)\n    trueSum = K.sum(true, axis=-1)\n    predSum = K.sum(pred, axis=-1)\n\n    #has mask or not per image - (batch,)\n    true1 = castF(K.greater(trueSum, 1))    \n    pred1 = castF(K.greater(predSum, 1))\n\n    #to get images that have mask in both true and pred\n    truePositiveMask = castB(true1 * pred1)\n\n    #separating only the possible true positives to check iou\n    testTrue = tf.boolean_mask(true, truePositiveMask)\n    testPred = tf.boolean_mask(pred, truePositiveMask)\n\n    #getting iou and threshold comparisons\n    iou = iou_loss_core(testTrue,testPred) \n    truePositives = [castF(K.greater(iou, tres)) for tres in tresholds]\n\n    #mean of thressholds for true positives and total sum\n    truePositives = K.mean(K.stack(truePositives, axis=-1), axis=-1)\n    truePositives = K.sum(truePositives)\n\n    #to get images that don't have mask in both true and pred\n    trueNegatives = (1-true1) * (1 - pred1) # = 1 -true1 - pred1 + true1*pred1\n    trueNegatives = K.sum(trueNegatives) \n\n    return (truePositives + trueNegatives) / castF(K.shape(true)[0])\n</code></pre>\n<p>I tried other metrics as well, dice loss is also constant and very low. Accuracy is around 79 and constant.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 80}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "tensorflow-estimator", "tensorflow2.0"], "owner": {"account_id": 4540798, "reputation": 339, "user_id": 3688728, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/0xrzF.png?s=256&g=1", "display_name": "Byest", "link": "https://stackoverflow.com/users/3688728/byest"}, "is_answered": true, "view_count": 4026, "answer_count": 2, "score": 14, "last_activity_date": 1615207242, "creation_date": 1552595705, "last_edit_date": 1553266018, "question_id": 55171526, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/55171526/tensorflow-2-0-keras-is-training-4x-slower-than-2-0-estimator", "title": "Tensorflow 2.0 Keras is training 4x slower than 2.0 Estimator", "body": "<p>We recently switched to Keras for TF 2.0, but when we compared it to the DNNClassifier Estimator on 2.0, we experienced around 4x slower speeds with Keras. But I cannot for the life of me figure out why this is happening. The rest of the code for both are identical, using an input_fn() that returns the same tf.data.Dataset, and using identical feature_columns. Been struggling with this problem for days now. Any help would be greatly greatly appreciated. Thank you</p>\n\n<p>Estimator code:</p>\n\n<pre><code>estimator = tf.estimator.DNNClassifier(\n        feature_columns = feature_columns,\n        hidden_units = [64,64],\n        activation_fn = tf.nn.relu,\n        optimizer = 'Adagrad',\n        dropout = 0.4,\n        n_classes = len(vocab),\n        model_dir = model_dir,\n        batch_norm = false)\n\nestimator.train(input_fn=train_input_fn, steps=400)\n</code></pre>\n\n<p>Keras code:</p>\n\n<pre><code>feature_layer = tf.keras.layers.DenseFeatures(feature_columns);\n\nmodel = tf.keras.Sequential([\n        feature_layer,\n        layers.Dense(64, input_shape = (len(vocab),), activation = tf.nn.relu),\n        layers.Dropout(0.4),\n        layers.Dense(64, activation = tf.nn.relu),\n        layers.Dropout(0.4),\n        layers.Dense(len(vocab), activation = 'softmax')]);\n\nmodel.compile(\n        loss = 'sparse_categorical_crossentropy',\n        optimizer = 'Adagrad'\n        distribute = None)\n\nmodel.fit(x = train_input_fn(),\n          epochs = 1,\n          steps_per_epoch = 400,\n          shuffle = True)\n</code></pre>\n\n<p>UPDATE: To test further, I wrote a custom subclassed Model (See: <a href=\"https://www.tensorflow.org/alpha/tutorials/quickstart/advanced\" rel=\"noreferrer\">Get Started For Experts</a>), which runs faster than Keras but slower than Estimators. If Estimator trains in 100 secs, the custom model takes approx ~180secs, and Keras approx ~350secs. An interesting note is that Estimator runs slower with Adam() than Adagrad() while Keras seems to run faster. With Adam() Keras takes less than twice as long as DNNClassifier. Assuming I didn't mess up the custom code, I'm beginning to think that DNNClassifier just has a lot of backend optimization / efficiencies that make it run faster than Keras.</p>\n\n<p>Custom code: </p>\n\n<pre><code>class MyModel(Model):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.features = layers.DenseFeatures(feature_columns, trainable=False)\n    self.dense = layers.Dense(64, activation = 'relu')\n    self.dropout = layers.Dropout(0.4)\n    self.dense2 = layers.Dense(64, activation = 'relu')\n    self.dropout2 = layers.Dropout(0.4)\n    self.softmax = layers.Dense(len(vocab_of_codes), activation = 'softmax')\n\n  def call(self, x):\n    x = self.features(x)\n    x = self.dense(x)\n    x = self.dropout(x)\n    x = self.dense2(x)\n    x = self.dropout2(x)\n    return self.softmax(x)\n\nmodel = MyModel()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adagrad()\n\n@tf.function\ndef train_step(features, label):\n  with tf.GradientTape() as tape:\n    predictions = model(features)\n    loss = loss_object(label, predictions)\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\nitera = iter(train_input_fn())\nfor i in range(400):\n  features, labels = next(itera)\n  train_step(features, labels)\n</code></pre>\n\n<p>UPDATE: It possibly seems to be the dataset. When I print a row of the dataset within the train_input_fn(), in estimators, it prints out the non-eager Tensor definition. In Keras, it prints out the eager values. Going through the Keras backend code, when it receives a tf.data.dataset as input, it handles it eagerly (and ONLY eagerly), which is why it was crashing whenever I used tf.function on the train_input_fn(). Basically, my guess is DNNClassifier is training faster than Keras because it runs more dataset code in graph mode. Will post any updates/finds.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 56}]
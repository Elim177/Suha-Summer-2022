[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 12451817, "reputation": 147, "user_id": 9066242, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d4fd3bd443ed826b8c8418da4d0377c3?s=256&d=identicon&r=PG&f=1", "display_name": "Rens", "link": "https://stackoverflow.com/users/9066242/rens"}, "is_answered": true, "view_count": 141, "answer_count": 1, "score": 0, "last_activity_date": 1595076827, "creation_date": 1595070187, "last_edit_date": 1595076827, "question_id": 62967769, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62967769/no-gradients-provided-for-any-variable-tf", "title": "No gradients provided for any variable. TF", "body": "<p>I am trying to build an implicit quantile network. I build a custom loss function but do not get it working. I get the error 'no gradients available' but I belief I only use functions that should provide gradients, like tf.tile and stuff. I dont explicityly cast something in my loss_kv_iq() function.</p>\n<p>Below I provide the code for my custom layer ( IQNlayer ) , the network I use (IQN), and my custom loss function. Also a small piece of code in the main that should be able to reproduce the error.</p>\n<p>TF version: 2.1.0</p>\n<pre><code>\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport numpy as np\n  \nclass IQN(keras.Model):\n    def __init__(self, quantile_dims, fc_dims, n_actions, n_quantiles):\n        super(IQN, self).__init__()\n        self.n_quantiles = n_quantiles\n                \n        initializer = keras.initializers.he_uniform()\n    \n        self.iq = IQNlayer(quantile_dims, n_quantiles)\n        self.dense = keras.layers.Dense(fc_dims, activation='relu', kernel_initializer = initializer)\n        self.out = keras.layers.Dense(n_actions, activation = None)\n    \n    def call(self, state, tau):\n        batch_size, state_size = state.shape\n        \n        x = self.iq(state, tau)\n        x = self.dense(x)\n        x = self.out(x)\n        \n        x = tf.transpose(tf.split(x, batch_size, axis=0), perm=[0, 2, 1])\n        return x\n    \n      \nclass IQNlayer(keras.layers.Layer):\n    def __init__(self, quantile_dims, n_quantiles):\n        super(IQNlayer, self).__init__()\n        self.quantile_dims = quantile_dims\n        self.n_quantiles = n_quantiles\n        \n        self.fc1 = keras.layers.Dense(self.quantile_dims, activation = tf.nn.selu)\n        self.fc2 = keras.layers.Dense(self.quantile_dims, activation = tf.nn.relu)\n        \n    def call(self, state, tau):\n        batch_size, state_size = state.shape\n        \n        state_tile = tf.tile(state, [1, self.n_quantiles])\n        state_reshape = tf.reshape(state_tile, [-1, state_size])\n        state_net = self.fc1(state_reshape)\n        \n        tau = tf.reshape(tau, [-1, 1])\n        pi_mtx = tf.constant(np.expand_dims(np.pi * np.arange(0, 64), axis=0), dtype=tf.float32)\n        cos_tau = tf.cos(tf.matmul(tau, pi_mtx))\n        phi = self.fc2(cos_tau)\n        \n        net = tf.multiply(state_net, phi)\n        return net\n    \n\ndef loss_kv_iq(x, tau, action_hot, theta_target):\n    expand_dim_action = tf.expand_dims(action_hot, -1)\n    main_support = tf.reduce_sum(x * expand_dim_action, axis=1)\n\n    theta_loss_tile = tf.tile(tf.expand_dims(main_support, axis=2), [1, 1, N_QUANTILES])\n    logit_valid_tile = tf.tile(tf.expand_dims(theta_target, axis=1), [1, N_QUANTILES, 1])\n    Huber_loss = hloss(logit_valid_tile, theta_loss_tile)\n    \n    inv_tau = 1 - tau\n    tau = tf.tile(tf.expand_dims(tau, axis=1), [1, N_QUANTILES, 1])\n    inv_tau = tf.tile(tf.expand_dims(inv_tau, axis=1), [1, N_QUANTILES, 1])\n    error_loss = logit_valid_tile - theta_loss_tile\n\n    Loss = tf.where(tf.less(error_loss, 0.0), inv_tau * Huber_loss, tau * Huber_loss)\n    loss = tf.reduce_mean(tf.reduce_sum(tf.reduce_mean(Loss, axis=2), axis=1))\n    return loss\n        \nif __name__ == '__main__':\n    hloss = tf.keras.losses.Huber(reduction = tf.keras.losses.Reduction.NONE)\n    \n    N_QUANTILES = 10\n    BATCH_SIZE = 2\n    ACTION_SIZE = 5\n    STATE_SIZE = 16\n    \n    # FOR EXAMPLE: RANDOM BATCH\n    cs = np.random.rand(BATCH_SIZE,STATE_SIZE)\n    a = np.random.randint(0,5,size=(2))\n    r = np.random.randint(0,500,size=(2))\n    ns = np.random.rand(BATCH_SIZE,STATE_SIZE)\n    \n    tau = np.random.uniform(size=(BATCH_SIZE, N_QUANTILES))\n    tau = tau.astype('float32')    \n    iq = IQN(128,128,ACTION_SIZE,N_QUANTILES)\n    \n    action_hot = np.zeros((BATCH_SIZE,ACTION_SIZE), dtype = np.float32)\n    action_hot[np.arange(BATCH_SIZE), a] = 1\n    \n    Q = iq(ns, tau)\n    theta_target = np.random.rand(BATCH_SIZE,N_QUANTILES)\n    theta_target = theta_target.astype('float32')\n    \n    optimizer = tf.keras.optimizers.Adam(lr = 1e-3)\n    \n    with tf.GradientTape() as tape:\n        loss = loss_kv_iq(Q, tau, action_hot, theta_target)\n        grads = tape.gradient(loss, iq.trainable_weights)\n        optimizer.apply_gradients(zip(grads,iq.trainable_weights))\n\n</code></pre>\n<p>Error:</p>\n<pre><code>Traceback (most recent call last):\n\n  File &quot;C:\\Users\\rensj\\.spyder-py3\\Thesis\\test.py&quot;, line 106, in &lt;module&gt;\n    optimizer.apply_gradients(zip(grads,iq.trainable_weights))\n\n  File &quot;C:\\Users\\rensj\\Anaconda3\\envs\\tfnew\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py&quot;, line 426, in apply_gradients\n    grads_and_vars = _filter_grads(grads_and_vars)\n\n  File &quot;C:\\Users\\rensj\\Anaconda3\\envs\\tfnew\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py&quot;, line 1039, in _filter_grads\n    ([v.name for _, v in grads_and_vars],))\n\nValueError: No gradients provided for any variable: ['iqn_4/iq_nlayer_4/dense_16/kernel:0', 'iqn_4/iq_nlayer_4/dense_16/bias:0', 'iqn_4/iq_nlayer_4/dense_17/kernel:0', 'iqn_4/iq_nlayer_4/dense_17/bias:0', 'iqn_4/dense_18/kernel:0', 'iqn_4/dense_18/bias:0', 'iqn_4/dense_19/kernel:0', 'iqn_4/dense_19/bias:0'].\n</code></pre>\n<p>EDIT:\nAs mister Agrawal pointed out, I use numpy operation in pi_mtx. I changed these to their tensorflow counterparts, and together with some other small change to the same line, this becomes:</p>\n<pre><code>pi_mtx = tf.constant(tf.expand_dims(tf.constant(np.pi) * tf.range(0, 64, dtype=tf.float32), axis=0), dtype=tf.float32)\n</code></pre>\n<p>However, I keep having the same ValueError: No gradients provided</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 289}]
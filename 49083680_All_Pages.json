[{"items": [{"tags": ["python", "tensorflow", "tensorboard"], "owner": {"account_id": 27479, "reputation": 83753, "user_id": 72583, "user_type": "registered", "accept_rate": 78, "profile_image": "https://www.gravatar.com/avatar/16a5eec5eca4550033c0e19dba3b32b8?s=256&d=identicon&r=PG", "display_name": "Jakub Arnold", "link": "https://stackoverflow.com/users/72583/jakub-arnold"}, "is_answered": true, "view_count": 4650, "accepted_answer_id": 69079804, "answer_count": 1, "score": 151, "last_activity_date": 1630958763, "creation_date": 1520076843, "last_edit_date": 1630958739, "question_id": 49083680, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/49083680/how-are-the-new-tf-contrib-summary-summaries-in-tensorflow-evaluated", "title": "How are the new tf.contrib.summary summaries in TensorFlow evaluated?", "body": "<p>I'm having a bit of trouble understanding the new <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/summary\" rel=\"nofollow noreferrer\"><code>tf.contrib.summary</code></a> API. In the old one, it seemed that all one was supposed to do was to run <a href=\"https://www.tensorflow.org/api_docs/python/tf/summary/merge_all\" rel=\"nofollow noreferrer\"><code>tf.summary.merge_all()</code></a> and run that as an op.</p>\n<p>But now we have things like <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/summary/record_summaries_every_n_global_steps\" rel=\"nofollow noreferrer\"><code>tf.contrib.summary.record_summaries_every_n_global_steps</code></a>, which can be used like this:</p>\n<pre><code>import tensorflow.contrib.summary as tfsum\n\nsummary_writer = tfsum.create_file_writer(logdir, flush_millis=3000)\nsummaries = []\n\n# First we create one summary which runs every n global steps\nwith summary_writer.as_default(), tfsum.record_summaries_every_n_global_steps(30):\n    summaries.append(tfsum.scalar(&quot;train/loss&quot;, loss))\n\n# And then one that runs every single time?\nwith summary_writer.as_default(), tfsum.always_record_summaries():\n    summaries.append(tfsum.scalar(&quot;train/accuracy&quot;, accuracy))\n\n# Then create an optimizer which uses a global step\nstep = tf.create_global_step()\ntrain = tf.train.AdamOptimizer().minimize(loss, global_step=step)\n</code></pre>\n<p>And now come a few questions:</p>\n<ol>\n<li>If we just run <code>session.run(summaries)</code> in a loop, I assume that the accuracy summary would get written every single time, while the loss one wouldn't, because it only gets written if the global step is divisible by 30?</li>\n<li>Assuming the summaries automatically evaluate their dependencies, I never need to run <code>session.run([accuracy, summaries])</code> but can just run, <code>session.run(summaries)</code> since they have a dependency in the graph, right?</li>\n<li>If 2) is true, can't I just add a control dependency to the training step so that the summaries are written on every train run? Or is this a bad practice?</li>\n<li>Is there any downside to using control dependencies in general for things that are going to be evaluated at the same time anyway?</li>\n<li>Why does <a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/summary/scalar\" rel=\"nofollow noreferrer\"><code>tf.contrib.summary.scalar</code></a> (and others) take in a <code>step</code> parameter?</li>\n</ol>\n<p>By adding a control dependency in 3) I mean doing this:</p>\n<pre><code>tf.control_dependencies(summaries):\n    train = tf.train.AdamOptimizer().minimize(loss, global_step=step)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 104}]
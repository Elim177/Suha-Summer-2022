[{"items": [{"tags": ["python-3.x", "tensorflow", "tensorflow2.0"], "owner": {"account_id": 4441934, "reputation": 1952, "user_id": 3616293, "user_type": "registered", "accept_rate": 35, "profile_image": "https://www.gravatar.com/avatar/cf7556b4227065cec9496375d64fea3d?s=256&d=identicon&r=PG&f=1", "display_name": "Arun", "link": "https://stackoverflow.com/users/3616293/arun"}, "is_answered": false, "view_count": 535, "answer_count": 0, "score": 1, "last_activity_date": 1580725160, "creation_date": 1580725160, "question_id": 60037196, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60037196/tensorflow-2-0-tf-function-annotated-valueerror", "title": "TensorFlow 2.0 &#39;tf.function&#39; annotated - ValueError", "body": "<p>I am using TensorFlow 2.0 and Python 3.7.5 for MNIST dataset using 300-100-10 dense fully connected feed forward neural network for classification using <em>tf.function</em> decorated function using <em>GradientTape</em>. The code I have is as follows:</p>\n\n<pre><code>def nn_model():\n    \"\"\"\n    Function to create LeNet 300-100-10\n    model for MNIST classification\n    \"\"\"\n\n    model = Sequential()\n\n    model.add(l.InputLayer(input_shape=(784, )))\n\n    model.add(Flatten())\n\n    model.add(Dense(units = 300, activation='relu', kernel_initializer=tf.initializers.GlorotUniform()))\n\n    # model.add(l.Dropout(0.2))\n\n    model.add(Dense(units = 100, activation='relu', kernel_initializer=tf.initializers.GlorotUniform()))\n\n    # model.add(l.Dropout(0.1))\n\n    model.add(Dense(units = num_classes, activation='softmax'))\n\n    return model\n\n\n# Create training and testing datasets-\ntrain_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n\n\ntrain_dataset = train_dataset.shuffle(buffer_size = 20000, reshuffle_each_iteration = True).batch(batch_size = batch_size, drop_remainder = False)\n\ntest_dataset = test_dataset.batch(batch_size=batch_size, drop_remainder=False)\n\n# Choose an optimizer and loss function for training-\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam(lr = 0.001)\n\n# Select metrics to measure the error &amp; accuracy of model.\n# These metrics accumulate the values over epochs and then\n# print the overall result-\ntrain_loss = tf.keras.metrics.Mean(name = 'train_loss')\ntrain_accuracy = tf.keras.metrics.BinaryAccuracy(name = 'train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name = 'test_loss')\ntest_accuracy = tf.keras.metrics.BinaryAccuracy(name = 'train_accuracy')\n\n@tf.function\ndef train_one_step(model, optimizer, x, y):\n    '''\n    Function to compute one step of gradient descent optimization\n    '''\n    with tf.GradientTape() as tape:\n        # Make predictions using defined model-\n        y_pred = model(x)\n\n        # Compute loss-\n        loss = loss_fn(y, y_pred)\n\n    # Compute gradients wrt defined loss and weights and biases-\n    grads = tape.gradient(loss, model.trainable_variables)\n\n    # type(grads)\n    # list\n\n    # Apply computed gradients to model's weights and biases-\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\n    # Compute accuracy-\n    train_loss(loss)\n    train_accuracy(y, y_pred)\n\n    return None\n\n\n@tf.function\ndef test_step(model, optimizer, data, labels):\n    \"\"\"\n    Function to test model performance\n    on testing dataset\n    \"\"\"\n\n    predictions = model(data)\n    t_loss = loss_fn(labels, predictions)\n\n    test_loss(t_loss)\n    test_accuracy(labels, predictions)\n\n    return None\n\n\n# Initialize an instance of defined neural network model-\nmodel = nn_model()\n\n# User input for manual implementation of Early Stopping-\nminimum_delta = 0.001\npatience = 3\nbest_val_loss = 1\nloc_patience = 0\n\n\n# Train model ONCE:\nfor epoch in range(num_epochs):\n\n    if loc_patience &gt;= patience:\n        print(\"\\n'EarlyStopping' called!\\n\")\n        break\n\n    # Reset the metrics at the start of the next epoch\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    test_loss.reset_states()\n    test_accuracy.reset_states()\n\n    # Dictionary to hold scalar metrics-\n    history = {}\n\n    history['accuracy'] = np.zeros(num_epochs)\n    history['val_accuracy'] = np.zeros(num_epochs)\n    history['loss'] = np.zeros(num_epochs)\n    history['val_loss'] = np.zeros(num_epochs)\n\n    for x, y in train_dataset:\n        # train_step(x, y)\n        train_one_step(model, optimizer, x, y)\n\n    for x_t, y_t in test_dataset:\n        # test_step(x_t, y_t)\n        test_step(model, optimizer, x_t, y_t)\n\n    template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'\n\n    history['accuracy'][epoch] = train_accuracy.result()\n    history['loss'][epoch] = train_loss.result()\n    history['val_loss'][epoch] = test_loss.result()\n    history['val_accuracy'][epoch] = test_accuracy.result()\n\n    print(template.format(epoch + 1, \n                              train_loss.result(), train_accuracy.result()*100,\n                              test_loss.result(), test_accuracy.result()*100))\n\n\n    # Code for manual Early Stopping:\n    if np.abs(test_loss.result() &lt; best_val_loss) &gt;= minimum_delta:\n        # update 'best_val_loss' variable to lowest loss encountered so far-\n        best_val_loss = test_loss.result()\n\n        # reset 'loc_patience' variable-\n        loc_patience = 0\n\n    else:  # there is no improvement in monitored metric 'val_loss'\n        loc_patience += 1  # number of epochs without any improvement\n\n\n    # Resize numpy arrays according to the epoch when 'EarlyStopping' was called-\n    for metrics in history.keys():\n        history[metrics] = np.resize(history[metrics], new_shape=epoch)\n</code></pre>\n\n<p>Now, if I try to train the model multiple times instead of once, I use the following code:</p>\n\n<pre><code># Instantiate a new model-\nmodel_new = nn_model()\n\n# Choose an optimizer and loss function for training-\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\noptimizer = tf.keras.optimizers.Adam(lr = 0.001)\n\n# Ask user for number of times to train model-\nnum_training_rounds = int(input(\"\\nEnter number of times to train model: \"))\n\n\nfor x in range(num_training_rounds):\n    print(\"\\n\\n\\nTraining round number = {0}\\n\".format(x + 1))\n\n    # Use new values for each iteration-\n    best_val_loss = 1\n    loc_patience = 0\n\n    for epoch in range(num_epochs):\n\n        if loc_patience &gt;= patience:\n            print(\"\\n'EarlyStopping' called!\\n\")\n            break\n\n        # Reset the metrics at the start of the next epoch\n        train_loss.reset_states()\n        train_accuracy.reset_states()\n        test_loss.reset_states()\n        test_accuracy.reset_states()\n\n        # Dictionary to hold scalar metrics-\n        history = {}\n\n        history['accuracy'] = np.zeros(num_epochs)\n        history['val_accuracy'] = np.zeros(num_epochs)\n        history['loss'] = np.zeros(num_epochs)\n        history['val_loss'] = np.zeros(num_epochs)\n\n        for x, y in train_dataset:\n            train_one_step(model_new, optimizer, x, y)\n\n        for x_t, y_t in test_dataset:\n            test_step(model_new, optimizer, x_t, y_t)\n\n        template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f}, Test Accuracy: {4:4f}'\n\n        history['accuracy'][epoch] = train_accuracy.result()\n        history['loss'][epoch] = train_loss.result()\n        history['val_loss'][epoch] = test_loss.result()\n        history['val_accuracy'][epoch] = test_accuracy.result()\n\n        print(template.format(epoch + 1, \n                              train_loss.result(), train_accuracy.result()*100,\n                              test_loss.result(), test_accuracy.result()*100))\n\n\n        # Code for manual Early Stopping:\n        if np.abs(test_loss.result() &lt; best_val_loss) &gt;= minimum_delta:\n            # update 'best_val_loss' variable to lowest loss encountered so far-\n            best_val_loss = test_loss.result()\n\n            # reset 'loc_patience' variable-\n            loc_patience = 0\n\n        else:  # there is no improvement in monitored metric 'val_loss'\n            loc_patience += 1  # number of epochs without any improvement\n\n\n        # Resize numpy arrays according to the epoch when 'EarlyStopping' was called-\n        for metrics in history.keys():\n            history[metrics] = np.resize(history[metrics], new_shape=epoch)\n</code></pre>\n\n<p>But, when I execute this code, I get the following error:</p>\n\n<blockquote>\n  <p>--------------------------------------------------------------------------- ValueError                                Traceback (most recent call\n  last)  in \n       23 \n       24         for x, y in train_dataset:\n  ---> 25             train_one_step(model_new, optimizer, x, y)\n       26 \n       27         for x_t, y_t in test_dataset:</p>\n  \n  <p>~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\n  in <strong>call</strong>(self, *args, **kwds)\n      455 \n      456     tracing_count = self._get_tracing_count()\n  --> 457     result = self._call(*args, **kwds)\n      458     if tracing_count == self._get_tracing_count():\n      459       self._call_counter.called_without_tracing()</p>\n  \n  <p>~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\n  in _call(self, *args, **kwds)\n      485       # In this case we have created variables on the first call, so we run the\n      486       # defunned version which is guaranteed to never create variables.\n  --> 487       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n      488     elif self._stateful_fn is not None:\n      489       # Release the lock early so that multiple threads can perform the call</p>\n  \n  <p>~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\n  in <strong>call</strong>(self, *args, **kwargs)    1820   def <strong>call</strong>(self, *args,\n  **kwargs):    1821     \"\"\"Calls a graph function specialized to the inputs.\"\"\"\n  -> 1822     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)    1823     return\n  graph_function._filtered_call(args, kwargs)  # pylint:\n  disable=protected-access    1824 </p>\n  \n  <p>~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\n  in _maybe_define_function(self, args, kwargs)    2148<br>\n  graph_function = self._function_cache.primary.get(cache_key, None)<br>\n  2149         if graph_function is None:\n  -> 2150           graph_function = self._create_graph_function(args, kwargs)    2151           self._function_cache.primary[cache_key] =\n  graph_function    2152         return graph_function, args, kwargs</p>\n  \n  <p>~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\n  in _create_graph_function(self, args, kwargs,\n  override_flat_arg_shapes)    2039             arg_names=arg_names,<br>\n  2040             override_flat_arg_shapes=override_flat_arg_shapes,\n  -> 2041             capture_by_value=self._capture_by_value),    2042         self._function_attributes,    2043         # Tell the ConcreteFunction\n  to clean up its graph once it goes out of</p>\n  \n  <p>~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\n  in func_graph_from_py_func(name, python_func, args, kwargs, signature,\n  func_graph, autograph, autograph_options, add_control_dependencies,\n  arg_names, op_return_value, collections, capture_by_value,\n  override_flat_arg_shapes)\n      913                                           converted_func)\n      914 \n  --> 915       func_outputs = python_func(*func_args, **func_kwargs)\n      916 \n      917       # invariant: <code>func_outputs</code> contains only Tensors, CompositeTensors,</p>\n  \n  <p>~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\n  in wrapped_fn(*args, **kwds)\n      356         # <strong>wrapped</strong> allows AutoGraph to swap in a converted function. We give\n      357         # the function a weak reference to itself to avoid a reference cycle.\n  --> 358         return weak_wrapped_fn().<strong>wrapped</strong>(*args, **kwds)\n      359     weak_wrapped_fn = weakref.ref(wrapped_fn)\n      360 </p>\n  \n  <p>~/.local/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\n  in wrapper(*args, **kwargs)\n      903           except Exception as e:  # pylint:disable=broad-except\n      904             if hasattr(e, \"ag_error_metadata\"):\n  --> 905               raise e.ag_error_metadata.to_exception(e)\n      906             else:\n      907               raise</p>\n  \n  <p>ValueError: in converted code:</p>\n\n<pre><code>&lt;ipython-input-28-0f4e8ff55201&gt;:21 train_one_step  *\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n/home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:433\n</code></pre>\n  \n  <p>apply_gradients\n          _ = self.iterations\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:541\n  <strong>getattribute</strong>\n          return super(OptimizerV2, self).<strong>getattribute</strong>(name)\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:655\n  iterations\n          aggregation=tf_variables.VariableAggregation.ONLY_FIRST_REPLICA)\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:805\n  add_weight\n          aggregation=aggregation)\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py:744 _add_variable_with_custom_getter\n          **kwargs_for_getter)\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py:139\n  make_variable\n          shape=variable_shape if variable_shape else None)\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:258\n  <strong>call</strong>\n          return cls._variable_v1_call(*args, **kwargs)\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:219\n  _variable_v1_call\n          shape=shape)\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py:65\n  getter\n          return captured_getter(captured_previous, **kwargs)\n      /home/majumdar/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:413\n  invalid_creator_scope\n          \"tf.function-decorated function tried to create \"</p>\n\n<pre><code>ValueError: tf.function-decorated function tried to create variables on non-first call.\n</code></pre>\n</blockquote>\n\n<p>What's going wrong?</p>\n\n<p>Thanks!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 13}]
[{"items": [{"tags": ["python", "python-3.x", "dataframe"], "owner": {"account_id": 6768404, "reputation": 18652, "user_id": 5212614, "user_type": "registered", "accept_rate": 32, "profile_image": "https://i.stack.imgur.com/2qaHz.jpg?s=256&g=1", "display_name": "ASH", "link": "https://stackoverflow.com/users/5212614/ash"}, "is_answered": true, "view_count": 404, "accepted_answer_id": 64038163, "answer_count": 1, "score": 0, "last_activity_date": 1600917708, "creation_date": 1600907987, "last_edit_date": 1600917708, "question_id": 64038031, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64038031/how-can-we-find-the-last-row-in-a-specific-column-in-a-data-frame", "title": "How can we find the last row, in a specific column, in a data frame?", "body": "<p>I have a data frame named 'data[&quot;df&quot;]'.  The data looks like this.</p>\n<p><a href=\"https://i.stack.imgur.com/iXEwh.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/iXEwh.png\" alt=\"enter image description here\" /></a></p>\n<p>How can I get the last row in the column named 'adjclose'?  The number that I want is '28.430000'.</p>\n<p>I am trying to append this to a list.  I tested this piece of code:</p>\n<pre><code>print(str(data.iloc[-1, data.columns.get_loc(&quot;adjclose&quot;)]) = 0))\n</code></pre>\n<p>I get this: <code>AttributeError: 'dict' object has no attribute 'iloc'</code></p>\n<p>What am I doing wrong here?</p>\n<p>Here is my full code.</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom yahoo_fin import stock_info as si\nfrom collections import deque\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport random\n\n\n# set seed, so we can get the same results after rerunning several times\nnp.random.seed(314)\ntf.random.set_seed(314)\nrandom.seed(314)\n\n\n\ndef load_data(ticker, n_steps=50, scale=True, shuffle=True, lookup_step=1, \n                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low']):\n    # see if ticker is already a loaded stock from yahoo finance\n    if isinstance(ticker, str):\n        # load it from yahoo_fin library\n        df = si.get_data(ticker)\n    elif isinstance(ticker, pd.DataFrame):\n        # already loaded, use it directly\n        df = ticker\n    # this will contain all the elements we want to return from this function\n    result = {}\n    # we will also return the original dataframe itself\n    result['df'] = df.copy()\n    # make sure that the passed feature_columns exist in the dataframe\n    for col in feature_columns:\n        assert col in df.columns, f&quot;'{col}' does not exist in the dataframe.&quot;\n    if scale:\n        column_scaler = {}\n        # scale the data (prices) from 0 to 1\n        for column in feature_columns:\n            scaler = preprocessing.MinMaxScaler()\n            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n            column_scaler[column] = scaler\n\n        # add the MinMaxScaler instances to the result returned\n        result[&quot;column_scaler&quot;] = column_scaler\n    # add the target column (label) by shifting by `lookup_step`\n    df['future'] = df['adjclose'].shift(-lookup_step)\n    # last `lookup_step` columns contains NaN in future column\n    # get them before droping NaNs\n    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n    # drop NaNs\n    df.dropna(inplace=True)\n    sequence_data = []\n    sequences = deque(maxlen=n_steps)\n    for entry, target in zip(df[feature_columns].values, df['future'].values):\n        sequences.append(entry)\n        if len(sequences) == n_steps:\n            sequence_data.append([np.array(sequences), target])\n    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n    # this last_sequence will be used to predict in future dates that are not available in the dataset\n    last_sequence = list(sequences) + list(last_sequence)\n    # shift the last sequence by -1\n    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n    # add to result\n    result['last_sequence'] = last_sequence\n    # construct the X's and y's\n    X, y = [], []\n    for seq, target in sequence_data:\n        X.append(seq)\n        y.append(target)\n    # convert to numpy arrays\n    X = np.array(X)\n    y = np.array(y)\n    # reshape X to fit the neural network\n    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n    # split the dataset\n    result[&quot;X_train&quot;], result[&quot;X_test&quot;], result[&quot;y_train&quot;], result[&quot;y_test&quot;] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n    # return the result\n    return result\n\ndef create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n                loss=&quot;mean_absolute_error&quot;, optimizer=&quot;rmsprop&quot;, bidirectional=False):\n    model = Sequential()\n    for i in range(n_layers):\n        if i == 0:\n            # first layer\n            if bidirectional:\n                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n            else:\n                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n        elif i == n_layers - 1:\n            # last layer\n            if bidirectional:\n                model.add(Bidirectional(cell(units, return_sequences=False)))\n            else:\n                model.add(cell(units, return_sequences=False))\n        else:\n            # hidden layers\n            if bidirectional:\n                model.add(Bidirectional(cell(units, return_sequences=True)))\n            else:\n                model.add(cell(units, return_sequences=True))\n        # add dropout after each layer\n        model.add(Dropout(dropout))\n    model.add(Dense(1, activation=&quot;linear&quot;))\n    model.compile(loss=loss, metrics=[&quot;mean_absolute_error&quot;], optimizer=optimizer)\n    return model\n\n\n# Window size or the sequence length\nN_STEPS = 100\n# Lookup step, 1 is the next day\nLOOKUP_STEP = 1\n# test ratio size, 0.2 is 20%\nTEST_SIZE = 0.2\n# features to use\nFEATURE_COLUMNS = [&quot;adjclose&quot;, &quot;volume&quot;, &quot;open&quot;, &quot;high&quot;, &quot;low&quot;]\n# date now\ndate_now = time.strftime(&quot;%Y-%m-%d&quot;)\n### model parameters\nN_LAYERS = 3\n# LSTM cell\nCELL = LSTM\n# 256 LSTM neurons\nUNITS = 256\n# 40% dropout\nDROPOUT = 0.4\n# whether to use bidirectional RNNs\nBIDIRECTIONAL = False\n### training parameters\n# mean absolute error loss\n# LOSS = &quot;mae&quot;\n# huber loss\nLOSS = &quot;huber_loss&quot;\nOPTIMIZER = &quot;adam&quot;\nBATCH_SIZE = 64\nEPOCHS = 2\n\n\n# save the dataframe\nfrom datetime import date\nfrom datetime import datetime\n\nstart_date = date(2020, 3, 1)\nend_date = date(2020, 9, 18)\ndays = np.busday_count(start_date, end_date)\n\n\n\n# Apple stock market\n\ntickers = ['CHIS']\n\nthelen = len(tickers)\n\nz=0\nall_stocks=[]\n\nfor ticker in tickers:\n    ticker_data_filename = os.path.join(&quot;data&quot;, f&quot;{ticker}_{date_now}.csv&quot;)\n    # model name to save, making it as unique as possible based on parameters\n    model_name = f&quot;{date_now}_{ticker}-{LOSS}-{OPTIMIZER}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}&quot;\n    if BIDIRECTIONAL:\n        model_name += &quot;-b&quot;\n    \n    \n    # create these folders if they does not exist\n    if not os.path.isdir(&quot;results&quot;):\n        os.mkdir(&quot;results&quot;)\n    if not os.path.isdir(&quot;logs&quot;):\n        os.mkdir(&quot;logs&quot;)\n    if not os.path.isdir(&quot;data&quot;):\n        os.mkdir(&quot;data&quot;)\n        \n    \n    # load the data\n    data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n    \n    # save the dataframe\n    data[&quot;df&quot;].to_csv(ticker_data_filename)\n    \n    # construct the model\n    model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n                        dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n    \n    # some tensorflow callbacks\n    checkpointer = ModelCheckpoint(os.path.join(&quot;results&quot;, model_name + &quot;.h5&quot;), save_weights_only=True, save_best_only=True, verbose=1)\n    tensorboard = TensorBoard(log_dir=os.path.join(&quot;logs&quot;, model_name))\n    \n    history = model.fit(data[&quot;X_train&quot;], data[&quot;y_train&quot;],\n                        batch_size=BATCH_SIZE,\n                        epochs=EPOCHS,\n                        validation_data=(data[&quot;X_test&quot;], data[&quot;y_test&quot;]),\n                        callbacks=[checkpointer, tensorboard],\n                        verbose=1)\n    \n    model.save(os.path.join(&quot;results&quot;, model_name) + &quot;.h5&quot;)\n    \n    \n    # after the model ends running...or during training, run this\n    # tensorboard --logdir=&quot;logs&quot;\n    # http://localhost:6006/\n    \n    \n    data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n                    feature_columns=FEATURE_COLUMNS, shuffle=False)\n    \n    # construct the model\n    model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n                        dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n    \n    model_path = os.path.join(&quot;results&quot;, model_name) + &quot;.h5&quot;\n    model.load_weights(model_path)\n    \n    \n    # evaluate the model\n    mse, mae = model.evaluate(data[&quot;X_test&quot;], data[&quot;y_test&quot;], verbose=0)\n    # calculate the mean absolute error (inverse scaling)\n    mean_absolute_error = data[&quot;column_scaler&quot;][&quot;adjclose&quot;].inverse_transform([[mae]])[0][0]\n    #print(&quot;Mean Absolute Error:&quot;, mean_absolute_error)\n    \n    \n    def predict(model, data, classification=False):\n        # retrieve the last sequence from data\n        last_sequence = data[&quot;last_sequence&quot;][:N_STEPS]\n        # retrieve the column scalers\n        column_scaler = data[&quot;column_scaler&quot;]\n        # reshape the last sequence\n        last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n        # expand dimension\n        last_sequence = np.expand_dims(last_sequence, axis=0)\n        # get the prediction (scaled from 0 to 1)\n        prediction = model.predict(last_sequence)\n        # get the price (by inverting the scaling)\n        predicted_price = column_scaler[&quot;adjclose&quot;].inverse_transform(prediction)[0][0]\n        return predicted_price\n    \n    \n    # predict the future price\n    future_price = predict(model, data)\n    print(f&quot;Future price after {LOOKUP_STEP} days is ${future_price:.2f}&quot;)\n\n    z=z+1\n    print(str(z) + ' of ' + str(thelen))\n\n    \n    # print(new_seriesdata['Adj Close'].iloc[-1])\n    all_stocks.append(ticker + ' act: ' + str(future_price) + ' prd: ' + str(future_price))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 105}]
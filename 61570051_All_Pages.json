[{"items": [{"tags": ["tensorflow", "keras", "conv-neural-network"], "owner": {"account_id": 18474357, "reputation": 41, "user_id": 13458316, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6c42aa7e1dda5033d2cec57c20dac2af?s=256&d=identicon&r=PG&f=1", "display_name": "penguin911", "link": "https://stackoverflow.com/users/13458316/penguin911"}, "is_answered": true, "view_count": 6205, "answer_count": 1, "score": 4, "last_activity_date": 1590720881, "creation_date": 1588484204, "last_edit_date": 1588484939, "question_id": 61570051, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61570051/valueerror-no-gradients-provided-for-any-variable-conv2d-kernel0-conv2d", "title": "ValueError: No gradients provided for any variable: [&#39;conv2d/kernel:0&#39;, &#39;conv2d/bias:0&#39;, &#39;conv2d_1/kernel:0&#39;, &#39;conv2d_1/bias:0&#39;,", "body": "<p>System information\nColab tensorflow 2.2.0</p>\n\n<p>Describe the current behavior:\nI faced this error when i tried to solve my own data issues, which is multiple label semantic segmentations. </p>\n\n<p>Below is the code </p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow.keras.backend as K\n\nIMG_WIDTH = 512\nIMG_HEIGHT = 512\nIMG_CHANNELS = 3\n\n# batch_shape=(512,512,3)\n# inputs = Input(batch_shape=(4, 512, 512, 3))\n#Build the model\ninputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n#s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n\n#Contraction path\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\nc1 = tf.keras.layers.Dropout(0.1)(c1)\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\np1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\nc2 = tf.keras.layers.Dropout(0.1)(c2)\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\np2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\nc3 = tf.keras.layers.Dropout(0.2)(c3)\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\np3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\nc4 = tf.keras.layers.Dropout(0.2)(c4)\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\np4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\nc5 = tf.keras.layers.Dropout(0.3)(c5)\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n#Expansive path \nu6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\nu6 = tf.keras.layers.concatenate([u6, c4])\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\nc6 = tf.keras.layers.Dropout(0.2)(c6)\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n\nu7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\nu7 = tf.keras.layers.concatenate([u7, c3])\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\nc7 = tf.keras.layers.Dropout(0.2)(c7)\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n\nu8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\nu8 = tf.keras.layers.concatenate([u8, c2])\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\nc8 = tf.keras.layers.Dropout(0.1)(c8)\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n\nu9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\nu9 = tf.keras.layers.concatenate([u9, c1], axis=3)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\nc9 = tf.keras.layers.Dropout(0.1)(c9)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n\noutputs = tf.keras.layers.Conv2D(6, (1, 1), activation='softmax')(c9)\n\nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n\n# define optomizer\noptim = tf.keras.optimizers.Adam()\n\ndef dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f*y_true_f) + K.sum(y_pred_f*y_pred_f) + smooth)\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.-dice_coef(y_true, y_pred)\n\nsmooth = 1.\nloss= tf.keras.losses.CategoricalCrossentropy()\n\nmodel.compile(optim, loss, metrics=[dice_coef,'accuracy'])\n\n#model.compile(optim, metrics, loss)\nmodel.summary()\n\n#SET UP FOR DATA TRAINING\n\nBATCH_SIZE = 4\nCLASSES = ['0', '1','2','3','4','5']\nLR = 0.0001\nEPOCHS = 40\nn_classes = len(CLASSES)\n\n# Dataset for train images\ntrain_dataset = Dataset(\n    x_train_dir, \n    y_train_dir, \n    classes=CLASSES, \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(),\n    with_shape_assert= True,\n)\n\n# Dataset for validation images\nvalid_dataset = Dataset(\n    x_valid_dir, \n    y_valid_dir, \n    classes=CLASSES, \n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(),\n    with_shape_assert= True,\n)\n\ntrain_dataloader = Dataloader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataloader = Dataloader(valid_dataset, batch_size=4, shuffle=False)\n\n# check shapes for errors\nassert train_dataloader[0][0].shape == (BATCH_SIZE, 512, 512, 3)\nassert train_dataloader[0][1].shape == (BATCH_SIZE, 512, 512, n_classes)\n\n# define callbacks for learning rate scheduling and best checkpoints saving\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint('./best_model.h5', save_weights_only=False, save_best_only=True, mode='min'),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n]\nresults_2704 = model.fit(\n    train_dataloader, \n    steps_per_epoch=len(train_dataloader), \n    epochs=EPOCHS, \n    validation_data=valid_dataloader, \n    callbacks=callbacks, \n    validation_steps=len(valid_dataloader),verbose=1\n)\n\n</code></pre>\n\n<p>This will give the error:</p>\n\n<p><code>ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'conv2d_6/kernel:0', 'conv2d_6/bias:0', 'conv2d_7/kernel:0', 'conv2d_7/bias:0', 'conv2d_8/kernel:0', 'conv2d_8/bias:0', 'conv2d_9/kernel:0', 'conv2d_9/bias:0', 'conv2d_transpose/kernel:0', 'conv2d_transpose/bias:0', 'conv2d_10/kernel:0', 'conv2d_10/bias:0', 'conv2d_11/kernel:0', 'conv2d_11/bias:0', 'conv2d_transpose_1/kernel:0', 'conv2d_transpose_1/bias:0', 'conv2d_12/kernel:0', 'conv2d_12/bias:0', 'conv2d_13/kernel:0', 'conv2d_13/bias:0', 'conv2d_transpose_2/kernel:0', 'conv2d_transpose_2/bias:0', 'conv2d_14/kernel:0', 'conv2d_14/bias:0', 'conv2d_15/kernel:0', 'conv2d_15/bias:0', 'conv2d_transpose_3/kernel:0', 'conv2d_transpose_3/bias:0', 'conv2d_16/kernel:0', 'conv2d_16/bias:0', 'conv2d_17/kernel:0', 'conv2d_17/bias:0', 'conv2d_18/kernel:0', 'conv2d_18/bias:0'].</code></p>\n\n<p>I know it is maybe due to the dead gradients and I have been trying to solve this problem while also posted on Tensorflow github for a month but till now i still do not figure out the solution. So I post here to seek for the help of other Tensorflow experts who may give me some hints while waiting for updating from Tensorflow support person. I searched around and I knew that using <code>tf.GradientTape()</code> may help solve the issue but I myself still could not figure out the right way. </p>\n\n<p>Really looking forward to any advise. Thank you very much</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 157}]
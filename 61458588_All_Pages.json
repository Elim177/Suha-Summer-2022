[{"items": [{"tags": ["tensorflow", "machine-learning", "deep-learning", "gradient", "gradienttape"], "owner": {"account_id": 10431265, "reputation": 1, "user_id": 13416549, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6a540f183922af760f2b3e710b487fe9?s=256&d=identicon&r=PG", "display_name": "Tarun Gupta", "link": "https://stackoverflow.com/users/13416549/tarun-gupta"}, "is_answered": false, "view_count": 132, "answer_count": 0, "score": 0, "last_activity_date": 1587989665, "creation_date": 1587989364, "last_edit_date": 1587989665, "question_id": 61458588, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61458588/gradient-calculation-in-tensorflow-using-gradienttape-getting-unexpected-none", "title": "Gradient Calculation in Tensorflow using GradientTape - Getting unexpected None value", "body": "<p>I am having a problem calculating the gradient in TensorFlow 1.15. I think it's something related context manager or keras session, but I am not sure about it. Following is the code I have written:</p>\n\n<pre><code>def create_adversarial_pattern_CW(input_patch, input_label, target_label):\n input_patch_T = tf.cast(input_patch,tf.float32)\n with tf.GradientTape() as tape:\n  tape.watch(input_patch_T)\n  patch_pred = model(input_patch_T)\n  loss_input_label = soft_dice_loss(input_label, patch_pred[0])\n  loss_target_label = soft_dice_loss(target_label, patch_pred[0])\n  f = loss_input_label - loss_target_label\n f_grad = tape.gradient(f, input_patch_T)\n\n #-------------------------#\n print(type(f_grad)) \n #-------------------------#\n\n f_grad_sign = tf.sign(f_grad)\n return f_grad_sign\n\n\ndef DAG():\n sess = K.get_session()\n with sess.as_default() as sess:\n  adv_x_old = tf.cast(X,dtype=tf.float32)\n  for i in range(iters):\n   #-------------------------#\n   #y_pred = model(adv_x_old) -&gt; If I uncomment this line the value of f_grad returned is None, otherwise it works fine, but I need this line\n   #-------------------------#\n   perturbations = create_adversarial_pattern_CW(adv_x_old, y, y_target)\n   adv_x_new = adv_x_old - alpha*perturbations\n   adv_x_old = adv_x_new\n  adv_patch_pred = model(adv_x_old)\n</code></pre>\n\n<p>To fix it, I tried to wrap the commented line as :</p>\n\n<pre><code>with tf.GradientTape() as tape:\n  with tape.stop_recording():\n    y_pred = model(adv_x_old)\n</code></pre>\n\n<p>but I still get the value of f_grad as None.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 136}]
[{"items": [{"tags": ["tensorflow", "lstm", "loss", "overfitting-underfitting"], "owner": {"account_id": 20171991, "reputation": 1, "user_id": 14794604, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-HuNTTD9U4oQ/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucktRwAe1ipm4vdtHI0OR0uVCQbQiQ/s96-c/photo.jpg?sz=256", "display_name": "R2D2", "link": "https://stackoverflow.com/users/14794604/r2d2"}, "is_answered": false, "view_count": 463, "answer_count": 0, "score": 0, "last_activity_date": 1607597492, "creation_date": 1607524044, "last_edit_date": 1607524674, "question_id": 65218721, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65218721/validation-loss-doesnt-decrease-tensorflow", "title": "Validation loss doesn&#39;t decrease (tensorflow)", "body": "<p>first of all, sorry for my english,</p>\n<p>I am facing a problem with my project using tensorflow, i have to code a dictionnary (english--&gt;german), i know it is not easy to figure out where is the problem :/ it is killing me, i am here for any question.\nTHANKS</p>\n<p>The inputs:</p>\n<ol>\n<li>En_input:</li>\n</ol>\n<p>has shape of (16, 14, 128) :</p>\n<p>-batch\n-number of words (1&lt;words&lt;13) and using a function to map over the datasets that pads each English sequence of embeddings with some distinct padding value before the sequence, so that each sequence is length 13</p>\n<pre class=\"lang-py prettyprint-override\"><code>    def pad_eng_embeddings(data):\n        def pad_eng_embeddings_(x,y):  \n            return (tf.pad(x, [tf.math.maximum([13-tf.shape(x)[0] ,0],   tf.constant([0,0])),tf.constant([0,0])], &quot;CONSTANT&quot;, constant_values=0), y)\n        return data.map(pad_eng_embeddings_)\n</code></pre>\n<p>128 after the</p>\n<pre class=\"lang-py prettyprint-override\"><code>embedding_layer = hub.KerasLayer(&quot;https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1&quot;,output_shape=[128], input_shape=[], dtype=tf.string) \n</code></pre>\n<ol start=\"2\">\n<li>Input_german</li>\n</ol>\n<p>shape (16,14)  with &quot;<strong>&lt; start&gt;</strong>&quot; and &quot;<strong>&lt; end&gt;</strong>&quot; token to the beginning and end</p>\n<p>Problem with the Validation loss :</p>\n<pre><code>Epoch : 1 --&gt; Loss train :  1.4463926553726196  --&gt; Loss Validation :  1.1669453382492065\nEpoch : 2 --&gt; Loss train :  0.9627977609634399  --&gt; Loss Validation :  0.9346983432769775\nEpoch : 3 --&gt; Loss train :  0.6962901949882507  --&gt; Loss Validation :  0.8172098994255066\nEpoch : 4 --&gt; Loss train :  0.4979133903980255  --&gt; Loss Validation :  0.7540919184684753\nEpoch : 5 --&gt; Loss train :  0.34379565715789795  --&gt; Loss Validation :  0.7045937776565552\n.\n.\n.\n.\n.\nEpoch : 111 --&gt; Loss train :  0.0012935919221490622  --&gt; Loss Validation :  0.43797847628593445\nEpoch : 112 --&gt; Loss train :  0.0010554787004366517  --&gt; Loss Validation :  0.4402512311935425\nEpoch : 113 --&gt; Loss train :  0.001183984917588532  --&gt; Loss Validation :  0.4351470470428467\nEpoch : 114 --&gt; Loss train :  0.0008711100090295076  --&gt; Loss Validation :  0.43835172057151794\nEpoch : 115 --&gt; Loss train :  0.0008662969921715558  --&gt; Loss Validation :  0.4418365955352783\nEpoch : 116 --&gt; Loss train :  0.0015571219846606255  --&gt; Loss Validation :  0.4526227116584778\nEpoch : 117 --&gt; Loss train :  0.002025176538154483  --&gt; Loss Validation :  0.442545086145401\nEpoch : 118 --&gt; Loss train :  0.0014257029397413135  --&gt; Loss Validation :  0.43709230422973633\nEpoch : 119 --&gt; Loss train :  0.0010628846939653158  --&gt; Loss Validation :  0.43659183382987976\nEpoch : 120 --&gt; Loss train :  0.0008744939113967121  --&gt; Loss Validation :  0.44265955686569214\n</code></pre>\n<p>Here mon code:</p>\n<pre><code>optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')\n@tf.function\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    #print(loss_[0])\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    #print(loss_[0])\n    return tf.reduce_mean(loss_)\n\n@tf.function\ndef Inputs_model(Input_german):\n    return (tf.cast(Input_german[:,0:-1],tf.float32),tf.cast(Input_german[:,1:],tf.float32))\n    \n\n@tf.function\ndef grad(En_input,GR_input,GR_output):\n    with tf.GradientTape() as tape:\n        state_h_En,state_c_En = model(En_input)\n        de_tensor = tf.squeeze(tf.convert_to_tensor(GR_input))\n        lstm_decoder, state_h_decoder, state_c_decoder=decoder(de_tensor,state_h_En, state_c_En)\n        loss_value = loss_function(GR_output, lstm_decoder)\n        variables = model.trainable_variables + decoder.trainable_variables\n        gradients=tape.gradient(loss_value,variables)\n    return loss_value,gradients\n\n\n@tf.function\ndef train_step(En_input, Input_german):\n  GR_input,GR_output=Inputs_model(Input_german)\n\n  loss_value,gradients=grad(En_input,GR_input,GR_output)\n  variables = model.trainable_variables + decoder.trainable_variables\n  optimizer.apply_gradients(zip(gradients,variables))\n  return loss_value\n\n@tf.function\ndef validation_step(En_input, Input_german):\n  GR_input,GR_output=Inputs_model(Input_german)\n  state_h_En,state_c_En = model(En_input)\n  de_tensor = tf.squeeze(tf.convert_to_tensor(GR_input))\n  lstm_decoder, state_h_decoder, state_c_decoder=decoder(de_tensor,state_h_En, state_c_En)\n  loss_value = loss_function(GR_output, lstm_decoder)\n  return loss_value\n  \n def train( num_epochs, dataset_train,dataset_valid):\n    train_loss_results=[]\n    validation_loss_results=[]\n    \n    for epoch in range(num_epochs):\n      mean_loss_t=tf.keras.metrics.Mean()\n      mean_loss_v=tf.keras.metrics.Mean()\n\n      for  En_input, Input_german in dataset_train:\n        loss_value=train_step(En_input, Input_german)\n        mean_loss_t(loss_value)\n      train_loss_results.append(mean_loss_t.result())\n      \n      \n      for  En_input, Input_german in dataset_valid:\n        loss_value=validation_step(En_input, Input_german)\n        mean_loss_v(loss_value)\n      validation_loss_results.append(mean_loss_v.result()) \n\n      dataset_train.shuffle(2)\n      dataset_valid.shuffle(2)\n      #checkpoint.save(file_prefix = checkpoint_prefix)\n      print(&quot;Epoch :&quot;,epoch+1,&quot;--&gt; Loss train : &quot;,float(train_loss_results[epoch]),&quot; --&gt; Loss Validation : &quot;,float(validation_loss_results[epoch]))\n      if float(validation_loss_results[epoch])&lt; 0.02:\n        return train_loss_results, validation_loss_results\n    return train_loss_results, validation_loss_results\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 278}]
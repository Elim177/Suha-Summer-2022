[{"items": [{"tags": ["tensorflow", "gradienttape"], "owner": {"account_id": 5284220, "reputation": 161, "user_id": 4218681, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d1705461ba7447ef6fc917229f839023?s=256&d=identicon&r=PG&f=1", "display_name": "Ulf W&#229;llgren", "link": "https://stackoverflow.com/users/4218681/ulf-w%c3%a5llgren"}, "is_answered": true, "view_count": 819, "answer_count": 1, "score": 1, "last_activity_date": 1580736997, "creation_date": 1580051716, "question_id": 59919691, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59919691/why-is-gradienttape-returning-none-when-i-use-numpy-math", "title": "Why is GradientTape returning None when I use numpy math", "body": "<p>Why is GradientTape returning None when I use numpy math</p>\n\n<p>I am trying to understand tensorflow GradientTape calculation for RL loss function. When I call a function using np.math the GradientTape returns None. If I use tf.math in the function it works fine. I have looked at tf-agents like ppo and sac and they are doing exactly(?) what I am trying to do (I have tried at last 50 other versions). \nWhat's wrong in the code below? What am I missing?</p>\n\n<p>window 10, python 3.6.8,  tensorflow 2.0.0\nref:<a href=\"https://github.com/chagmgang/tf2.0_reinforcement_learning/blob/master/policy/ppo.py\" rel=\"nofollow noreferrer\">https://github.com/chagmgang/tf2.0_reinforcement_learning/blob/master/policy/ppo.py</a></p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\ndef my_loss1(x):\n    y=tf.sin(x)\n    y=tf.abs(y)\n    return y\n\ndef my_loss2(x):\n    y=np.sin(x)\n    y=np.abs(y)\n    return y\n\ndef main(ver):    \n    x = np.linspace(0,10,25)\n    dsin_dx=np.cos(x)    \n    xx = tf.constant(x)\n    with tf.GradientTape() as tape:\n        tape.watch(xx)\n        if ver==0:\n            # my_loss1 with tf math \n            loss1=my_loss1(xx)\n        if ver==1:\n            #my loss with numpy math\n            loss1=my_loss2(np.array(xx))    \n            loss1 = tf.convert_to_tensor(loss1, dtype=tf.float64)\n        print(loss1)\n        loss=tf.reduce_sum(loss1)\n        print('loss=',loss)\n    grads = tape.gradient(loss, xx)\n\n    fig, ax = plt.subplots(2)\n    ax[0].plot(x,loss1,'r')\n    print('grads', grads)\n\n    if not grads is None:\n        ax[1].plot(x, grads)\n        ax[1].plot(x,dsin_dx)\n    plt.show()\n\nif __name__ == '__main__':\n    main(ver=0)  # This works ok\n    main(ver=1)  # This returns grads = None \n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 40}]
[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "neural-network"], "owner": {"account_id": 19060542, "reputation": 1, "user_id": 13916754, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gi9H_QCNeC2YTbYYfhBIl4xPV6L6nICDaMpiecp=k-s256", "display_name": "Nikita Lokhmachev", "link": "https://stackoverflow.com/users/13916754/nikita-lokhmachev"}, "is_answered": false, "view_count": 177, "answer_count": 0, "score": 0, "last_activity_date": 1594919131, "creation_date": 1594562382, "last_edit_date": 1594637114, "question_id": 62861991, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62861991/minimizing-the-maximal-loss-in-a-batch-of-augmented-data", "title": "Minimizing the maximal loss in a batch of augmented data", "body": "<p>I'm trying to implement the MaxUp approach (<a href=\"https://arxiv.org/pdf/2002.09024v1.pdf\" rel=\"nofollow noreferrer\">https://arxiv.org/pdf/2002.09024v1.pdf</a>) in order to improve my image classifier's generalization.\nAs far as I understand, the essence of the method is that for each data point from our dataset we generate a small set of perturbed or augmented data points (in my case, I used augmentation) and get a batch of <em>m</em> augmented images. Then we pass this batch through the neural network and calculate losses for each example in the batch. After that, we optimize the network using only the maximum loss. We basically minimize the maximum loss.</p>\n<p>I'm using Imagenette dataset(<a href=\"https://github.com/fastai/imagenette\" rel=\"nofollow noreferrer\">https://github.com/fastai/imagenette</a>) which I balanced beforehand.</p>\n<p>So, I built a simple convnet consisting of 3 VGG blocks.</p>\n<pre><code>def define_model_VGG(loss):\n    momentum = 0.9\n    #VGG\n    model = Sequential()\n    model.add(Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(MaxPooling2D(2, 2))\n\n    model.add(Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(MaxPooling2D(2, 2))\n\n    model.add(Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(MaxPooling2D(2, 2))\n\n    model.add(Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(MaxPooling2D(2, 2))\n\n    model.add(Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal'))\n    model.add(BatchNormalization(momentum = momentum, center=True, scale=False))\n    model.add(MaxPooling2D(2, 2))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation = 'relu', kernel_initializer = 'he_normal'))\n    model.add(Dense(256, activation = 'relu', kernel_initializer = 'he_normal'))\n    model.add(Dense(10, activation = 'softmax'))\n\n    opt = SGD(lr=0.001, momentum=0.9)\n    model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n    return model\n</code></pre>\n<p>I defined a custom loss function so that it would return max loss instead of average loss.</p>\n<pre><code>def max_loss_minimization(y_true, y_pred):\n    &quot;&quot;&quot;\n    Returns max loss\n    &quot;&quot;&quot;\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n    max_loss = tf.keras.backend.max(loss)\n    return max_loss\n</code></pre>\n<p>I made a generator that returns any number of augmented images based on one image.</p>\n<pre><code>def augmentation_generator(aug_batch_size=AUG_BATCH_SIZE):\n    &quot;&quot;&quot;\n    Returns aug_batch_size augmented images of one data point\n    &quot;&quot;&quot;\n    maxup_datagen = ImageDataGenerator(rotation_range=30,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True)\n\n    train_datagen = ImageDataGenerator(rescale=1./255)\n    maxup_train_generator = train_datagen.flow_from_directory(\n            train_dir,\n            target_size=(IMAGE_SIZE, IMAGE_SIZE),\n            shuffle=True,\n            batch_size=1,\n            class_mode='categorical')\n\n    while True:\n        \n        next_image, next_label = next(maxup_train_generator)\n        i = 0\n        X = np.zeros((aug_batch_size, IMAGE_SIZE, IMAGE_SIZE, 3))\n        y = np.zeros((aug_batch_size, 10))\n\n        for x_batch, y_batch in maxup_datagen.flow(next_image, next_label, batch_size=1):\n            X[i,:,:,:] = x_batch\n            y[i,:] = y_batch\n            i += 1\n            if i == aug_batch_size:\n                break\n\n        yield X, y\n</code></pre>\n<p>During training, I generate a batch of augmented images, do a forward pass, calculate max loss, and update the model by applying gradients.</p>\n<pre><code>test_datagen = ImageDataGenerator(rescale=1./255)\ntest_gen = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=TEST_BATCH_SIZE,\n    shuffle=True,\n    class_mode='categorical'\n)\n\nval_data, val_labels = next(test_gen)\nval_labels = np.array([np.where(a==1)[0][0] for a in val_labels]) # validation data\n\nmodel_maxup = define_model_VGG(loss=max_loss_minimization)\n\nepochs = 20\nfor epoch in range(epochs):\n    print(&quot;\\nStart of epoch %d&quot; % (epoch,))\n    \n    aug_generator = augmentation_generator()\n\n    # Iterate over the batches of the dataset.\n    for step in range(0, IMAGE_NUMBER):\n        \n        x_batch_train, y_batch_train = next(aug_generator)\n        x_batch_train = tf.convert_to_tensor(x_batch_train, dtype='float32')\n        y_batch_train = tf.convert_to_tensor(y_batch_train, dtype='float32')\n        # Open a GradientTape to record the operations run\n        # during the forward pass, which enables autodifferentiation.\n        with tf.GradientTape() as tape:\n\n            # Run the forward pass of the layer.\n            # The operations that the layer applies\n            # to its inputs are going to be recorded\n            # on the GradientTape.\n            logits = model_maxup(x_batch_train)  # Logits for this minibatch\n\n            # Compute the loss value for this minibatch.\n            loss_value = max_loss_minimization(y_batch_train, logits)\n\n        # Use the gradient tape to automatically retrieve\n        # the gradients of the trainable variables with respect to the loss.\n        grads = tape.gradient(loss_value, model_maxup.trainable_weights)\n\n        # Run one step of gradient descent by updating\n        # the value of the variables to minimize the loss.\n        opt.apply_gradients(zip(grads, model_maxup.trainable_weights))\n\n        # Log every 200 batches.\n        if step % 200 == 0:\n            print(\n                &quot;Training loss (for one batch) at step %d: %.4f&quot;\n                % (step, float(loss_value))\n            )\n            print(&quot;Seen so far: %s samples&quot; % ((step + 1)))\n            \n        if step % 1000 == 0:\n            pred_labels = np.argmax(model_maxup.predict(val_data),axis=1)\n            print(&quot;Validation accuracy: {}&quot;.format(sum(val_labels==pred_labels)/TEST_BATCH_SIZE))\n</code></pre>\n<p>However, my neural network doesn't seem to train at all. I get the following output:</p>\n<pre><code>Start of epoch 0\nFound 9300 images belonging to 10 classes.\nTraining loss (for one batch) at step 0: 2.3029\nSeen so far: 1 samples\nValidation accuracy: 0.0975\nTraining loss (for one batch) at step 200: 2.4079\nSeen so far: 201 samples\nTraining loss (for one batch) at step 400: 2.3045\nSeen so far: 401 samples\nTraining loss (for one batch) at step 600: 2.1799\nSeen so far: 601 samples\nTraining loss (for one batch) at step 800: 2.3446\nSeen so far: 801 samples\nTraining loss (for one batch) at step 1000: 2.3806\nSeen so far: 1001 samples\nValidation accuracy: 0.085\nTraining loss (for one batch) at step 1200: 2.2879\nSeen so far: 1201 samples\nTraining loss (for one batch) at step 1400: 2.3160\nSeen so far: 1401 samples\n</code></pre>\n<p>I also tried the high-level model.fit method and got the same results.\nThe problem is most likely in either my custom loss or data generator.\nCan you tell me what I'm doing wrong? I'm pretty sure there is a way to do it easier.</p>\n<p>Thank you in advance.</p>\n<p>UPD:\nI tried random gaussian perturbations instead of augmentation, and it didn't seem to change much...\nAlso, as I continued exploring the problem, I noticed that when I replace the &quot;reduce_max&quot; operation with &quot;reduce_mean&quot;, the training accuracy, as well as validation accuracy, starts increasing. However, the paper states that they somehow minimize the maximal loss.\nIs there some other method of minimizing the maximal loss?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 279}]
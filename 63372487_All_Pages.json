[{"items": [{"tags": ["python", "python-3.x", "tensorflow", "tensorflow2.0", "generative-adversarial-network"], "owner": {"account_id": 18077962, "reputation": 177, "user_id": 13140685, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14GgUaFQERpsW0eQT8nCbB5fixS5ND7mYZYvRcltS=k-s256", "display_name": "jigar", "link": "https://stackoverflow.com/users/13140685/jigar"}, "is_answered": true, "view_count": 6520, "accepted_answer_id": 63373201, "answer_count": 1, "score": 3, "last_activity_date": 1597222806, "creation_date": 1597220047, "question_id": 63372487, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63372487/operatornotallowedingrapherror-using-a-tf-tensor-as-a-python-bool-is-not-al", "title": "OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: (error in tf Cycle GAN)", "body": "<p>I am trying to code along with <a href=\"https://www.tensorflow.org/tutorials/generative/cyclegan\" rel=\"nofollow noreferrer\">Cycle_gan Tensorflow</a></p>\n<p>I received an error saying:</p>\n<pre><code>OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n</code></pre>\n<p>My full error:</p>\n<pre><code>---------------------------------------------------------------------------\nOperatorNotAllowedInGraphError            Traceback (most recent call last)\n&lt;ipython-input-161-bde4fc92c3b9&gt; in &lt;module&gt;\n      4     n = 0\n      5     for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n----&gt; 6         train_step(image_x, image_y)\n      7         if n % 10 == 0:\n      8             print ('.', end='')\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py in __call__(self, *args, **kwds)\n    778       else:\n    779         compiler = &quot;nonXla&quot;\n--&gt; 780         result = self._call(*args, **kwds)\n    781 \n    782       new_tracing_count = self._get_tracing_count()\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py in _call(self, *args, **kwds)\n    821       # This is the first call of __call__, so we have to initialize.\n    822       initializers = []\n--&gt; 823       self._initialize(args, kwds, add_initializers_to=initializers)\n    824     finally:\n    825       # At this point we know that the initialization is complete (or less\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py in _initialize(self, args, kwds, add_initializers_to)\n    694     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)\n    695     self._concrete_stateful_fn = (\n--&gt; 696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n    697             *args, **kwds))\n    698 \n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n   2853       args, kwargs = None, None\n   2854     with self._lock:\n-&gt; 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)\n   2856     return graph_function\n   2857 \n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _maybe_define_function(self, args, kwargs)\n   3211 \n   3212       self._function_cache.missed.add(call_context_key)\n-&gt; 3213       graph_function = self._create_graph_function(args, kwargs)\n   3214       self._function_cache.primary[cache_key] = graph_function\n   3215       return graph_function, args, kwargs\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n   3063     arg_names = base_arg_names + missing_arg_names\n   3064     graph_function = ConcreteFunction(\n-&gt; 3065         func_graph_module.func_graph_from_py_func(\n   3066             self._name,\n   3067             self._python_function,\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n    984         _, original_func = tf_decorator.unwrap(python_func)\n    985 \n--&gt; 986       func_outputs = python_func(*func_args, **func_kwargs)\n    987 \n    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py in wrapped_fn(*args, **kwds)\n    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n    599         # the function a weak reference to itself to avoid a reference cycle.\n--&gt; 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    601     weak_wrapped_fn = weakref.ref(wrapped_fn)\n    602 \n\n~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\func_graph.py in wrapper(*args, **kwargs)\n    971           except Exception as e:  # pylint:disable=broad-except\n    972             if hasattr(e, &quot;ag_error_metadata&quot;):\n--&gt; 973               raise e.ag_error_metadata.to_exception(e)\n    974             else:\n    975               raise\n\nOperatorNotAllowedInGraphError: in user code:\n\n    &lt;ipython-input-160-538af916a6fd&gt;:28 train_step  *\n        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_losss(real_y, cycled_y)\n    &lt;ipython-input-151-74a790ebcddf&gt;:2 calc_cycle_loss  *\n        loss1 = tf.reduce_mean(tf.abs(real_image, cycled_image))\n    C:\\Users\\astro\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201 wrapper  **\n        return target(*args, **kwargs)\n    C:\\Users\\astro\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\math_ops.py:388 abs\n        with ops.name_scope(name, &quot;Abs&quot;, [x]) as name:\n    C:\\Users\\astro\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:6492 __enter__\n        return self._name_scope.__enter__()\n    c:\\users\\astro\\appdata\\local\\programs\\python\\python38\\lib\\contextlib.py:113 __enter__\n        return next(self.gen)\n    C:\\Users\\astro\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:4176 name_scope\n        if name:\n    C:\\Users\\astro\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:877 __bool__\n        self._disallow_bool_casting()\n    C:\\Users\\astro\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:486 _disallow_bool_casting\n        self._disallow_when_autograph_enabled(\n    C:\\Users\\astro\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:472 _disallow_when_autograph_enabled\n        raise errors.OperatorNotAllowedInGraphError(\n\n    OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.\n</code></pre>\n<p>I have modified the code a bit,</p>\n<p>My code:</p>\n<pre><code>import tensorflow as tf\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# In[ ]:\n\n\n# In[118]:\n\n\nprint(tf.__version__)\n\n# In[119]:\n\n\nimport time\nfrom IPython.display import clear_output\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# In[120]:\n\n\ntrain_horses = []\ntrain_horses_path = 'C:/Users/astro/pythonprojects/cycleGANs/horse2zebra/trainA/'\nfor image_name in os.listdir(train_horses_path):\n    img_path = os.path.join(train_horses_path, image_name)\n    img_arr = plt.imread(img_path)\n    train_horses.append(img_arr)\ntrain_horses = np.array(train_horses)\n\n# In[121]:\n\n\nprint(train_horses.shape)\n# (1067, 256, 256, 3)\n\n# In[122]:\n\n\ntrain_zebras_path = 'C:/Users/astro/pythonprojects/cycleGANs/horse2zebra/trainB/'\ntrain_zebras = []\nfor image_name in os.listdir(train_zebras_path):\n    img_path = os.path.join(train_zebras_path, image_name)\n    img_arr = plt.imread(img_path)\n    train_zebras.append(img_arr)\ntrain_zebras = np.array(train_zebras)\n\n# In[123]:\n\n\ntest_horses_path = 'C:/Users/astro/pythonprojects/cycleGANs/horse2zebra/testA/'\ntest_horses = []\nfor image_name in os.listdir(test_horses_path):\n    img_path = os.path.join(test_horses_path, image_name)\n    img_arr = plt.imread(img_path)\n    test_horses.append(img_arr)\ntest_horses = np.array(test_horses)\n\n# In[124]:\n\n\ntest_zebras_path = 'C:/Users/astro/pythonprojects/cycleGANs/horse2zebra/testB/'\ntest_zebras = []\nfor image_name in os.listdir(test_zebras_path):\n    img_path = os.path.join(test_zebras_path, image_name)\n    img_arr = plt.imread(img_path)\n    test_zebras.append(img_arr)\ntest_zebras = np.array(test_zebras)\n\n# In[125]:\n\n\n# In[126]:\n\n\n# Setting image constants\nBUFFER_SIZE = 1000\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\n\n# In[127]:\n\n\n# Inputing pipeline:\ntrain_horses = tf.data.Dataset.from_tensor_slices(train_horses)\ntrain_zebras = tf.data.Dataset.from_tensor_slices(train_zebras)\n\ntest_horses = tf.data.Dataset.from_tensor_slices(test_horses)\ntest_zebras = tf.data.Dataset.from_tensor_slices(test_zebras)\n\n\n# In[128]:\n\n\n# Function: random crops\ndef random_crop(image):\n    cropped_image = tf.image.random_crop(image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n    return cropped_image\n\n\n# In[129]:\n\n\n# Function: normalize (- for normalizing images to [-1, 1])\ndef normalize(image):\n    image = tf.cast(image, tf.float32)\n    # Note: the image pixel values lie from 0 to 255\n    image = (image / 127.5) - 1\n    return image\n\n\n# In[130]:\n\n\n# Function: random jitter\ndef random_jitter(image):\n    # Resizing the image to 286x286x3\n    image = tf.image.resize(image, size=[286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n    # Randomly cropping to 256x256x3\n    image = random_crop(image)\n\n    # Random mirroring\n    image = tf.image.random_flip_left_right(image)\n\n    return image\n\n\n# In[131]:\n\n\ndef preprocess_image_train(image):\n    image = random_jitter(image)\n    image = normalize(image)\n    return image\n\n\n# In[132]:\n\n\ndef preprocess_image_test(image):\n    image = normalize(image)\n    return image\n\n\n# In[133]:\n\n\ntrain_horses = train_horses.map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(buffer_size=BUFFER_SIZE).batch(1)\n\ntrain_zebras = train_zebras.map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(buffer_size=BUFFER_SIZE).batch(1)\n\ntest_horses = test_horses.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(buffer_size=BUFFER_SIZE).batch(1)\n\ntest_zebras = test_zebras.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(buffer_size=BUFFER_SIZE).batch(1)\n\n# In[134]:\n\n\nsample_horse = next(iter(train_horses))\nsample_zebra = next(iter(train_zebras))\n\n# In[135]:\n\n\nplt.subplot(121)\nplt.title('Horse')\nplt.imshow(sample_horse[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Horse with random jitter')\nplt.imshow(random_jitter(sample_horse[0]) * 0.5 + 0.5)\n\n# In[136]:\n\n\npix2pix_model_dir = 'C:/Users/astro/pythonprojects/cycleGANs/venv/Lib/site-packages/tensorflow_examples/models/pix2pix'\n\n# In[137]:\n\n\n# Generator G to transform image from X(horse) to Y(zebra). (G: Horse -&gt; Zebra)\n# Generator F to transform image from Y(zebra) to X(horse). (F: Zebra -&gt; Horse)\n\n\n# In[138]:\n\n\n# Discriminator (Dx) Learns to differentiate between X and F(Y)(generated horse img)\n# Discriminator (Dy) Learns to differentiate between Y and G(X)(generated zebra img)\n\n\n# In[ ]:\n\n\n# In[139]:\n\n\n# Building Generator G\nfrom tensorflow.keras import layers\n\n\ndef Generator():\n    # Creating input layer\n    input_layer = layers.Input(shape=[256, 256, 3])  # (bs, 256, 256, 3)\n\n    # Downsampling\n    conv1 = layers.Conv2D(filters=32, kernel_size=4, strides=2, padding='same', activation='relu')(\n        input_layer)  # (bs, 128, 128, 32)\n    bat_norm = layers.BatchNormalization()(conv1)\n    conv2 = layers.Conv2D(filters=64, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 64, 64, 64)\n    bat_norm = layers.BatchNormalization()(conv2)\n    conv3 = layers.Conv2D(filters=128, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 32, 32, 128)\n    bat_norm = layers.BatchNormalization()(conv3)\n    conv4 = layers.Conv2D(filters=256, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 16, 16, 256)\n    bat_norm = layers.BatchNormalization()(conv4)\n    conv5 = layers.Conv2D(filters=256, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 4, 4, 512)\n    bat_norm = layers.BatchNormalization()(conv5)\n    conv6 = layers.Conv2D(filters=256, kernel_size=4, strides=2, padding='same', activation='relu')(\n        bat_norm)  # (bs, 2, 2, 512)\n    bat_norm = layers.BatchNormalization()(conv6)\n    conv7 = layers.Conv2D(filters=256, kernel_size=4, strides=2, padding='same', activation='relu')(\n        bat_norm)  # (bs, 1, 1, 512)\n    bat_norm = layers.BatchNormalization()(conv7)\n\n    # Upsampling\n    convt1 = layers.Conv2DTranspose(filters=256, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 2, 2, 1024)\n    convt1 = layers.concatenate([convt1, conv6])\n    drop = layers.Dropout(0.1)(convt1)\n    bat_norm = layers.BatchNormalization()(drop)\n    convt2 = layers.Conv2DTranspose(filters=256, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 4, 4, 1024)\n    convt2 = layers.concatenate([convt2, conv5])\n    drop = layers.Dropout(0.1)(convt2)\n    bat_norm = layers.BatchNormalization()(drop)\n    convt3 = layers.Conv2DTranspose(filters=256, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 8, 8, 1024)\n    convt3 = layers.concatenate([convt3, conv4])\n    drop = layers.Dropout(0.1)(convt3)\n    bat_norm = layers.BatchNormalization()(drop)\n    convt4 = layers.Conv2DTranspose(filters=128, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 16, 16, 512)\n    convt4 = layers.concatenate([convt4, conv3])\n    bat_norm = layers.BatchNormalization()(convt4)\n    convt5 = layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 64, 64, 256)\n    convt5 = layers.concatenate([convt5, conv2])\n    bat_norm = layers.BatchNormalization()(convt5)\n    convt6 = layers.Conv2DTranspose(filters=32, kernel_size=4, strides=2, activation='relu', padding='same')(\n        bat_norm)  # (bs, 64, 64, 128)\n    convt6 = layers.concatenate([convt6, conv1])\n    bat_norm = layers.BatchNormalization()(convt6)\n    # convt7 = layers.Conv2DTranspose(filters=64, kernel_size=4, strides=2, activation='relu',padding='same')(bat_norm) #(bs, 128, 128, 128)\n    # bat_norm = layers.BatchNormalization()(convt7)\n\n    convt_op = layers.Conv2DTranspose(filters=3, kernel_size=4, strides=2, activation='tanh', padding='same')(bat_norm)\n\n    return tf.keras.models.Model(inputs=input_layer, outputs=convt_op)\n\n\n# In[140]:\n\n\ngenerator_g = Generator()\n\n# Displaying Generator\ntf.keras.utils.plot_model(generator_g, show_shapes=True, dpi=64)\n\n# In[141]:\n\n\n# Creating generator_f\ngenerator_f = Generator()\n\n\n# In[142]:\n\n\n# Defining discriminator\ndef Discriminator(target=False):\n    inp = layers.Input(shape=[256, 256, 3], name='input_image')\n    tar = layers.Input(shape=[256, 256, 3], name='target_image')\n    x = inp\n    if target:\n        x = layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n\n    # downsampling\n    conv1 = layers.Conv2D(filters=64, kernel_size=4, strides=2, padding='same', activation='relu')(\n        x)  # (bs, 128, 128, 64)\n    leaky1 = layers.LeakyReLU()(conv1)\n\n    conv2 = layers.Conv2D(filters=128, kernel_size=4, strides=2, activation='relu', padding='same')(\n        leaky1)  # (bs, 64, 64, 128)\n    bat_norm = layers.BatchNormalization()(conv2)\n    leaky2 = layers.LeakyReLU()(bat_norm)\n\n    conv3 = layers.Conv2D(filters=256, kernel_size=4, strides=2, activation='relu', padding='same')(\n        leaky2)  # (bs, 32, 32, 256)\n    bat_norm = layers.BatchNormalization()(conv3)\n    leaky3 = layers.LeakyReLU()(bat_norm)\n\n    zero_pad1 = layers.ZeroPadding2D()(leaky3)  # (bs, 34, 34, 256)\n    conv = tf.keras.layers.Conv2D(filters=512, kernel_size=4, strides=1, use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n\n    batch_norm = layers.BatchNormalization()(conv)\n\n    leaky_relu = layers.LeakyReLU()(batch_norm)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n\n    last = tf.keras.layers.Conv2D(1, 4, strides=1)(zero_pad2)  # (bs, 30, 30, 1)\n\n    if target:\n        return tf.keras.Model(inputs=[inp, tar], outputs=last)\n    else:\n        return tf.keras.Model(inputs=inp, outputs=last)\n\n\n# In[143]:\n\n\n# Setting up discriminator x\ndiscriminator_x = Discriminator()\ntf.keras.utils.plot_model(discriminator_x, show_shapes=True, dpi=64)\n\n# In[144]:\n\n\n# Setting up discriminator_y\ndiscriminator_y = Discriminator()\n\n# In[145]:\n\n\nwith tf.device('/cpu:0'):\n    to_zebra = generator_g(sample_horse)\n    to_horse = generator_f(sample_zebra)\nplt.figure(figsize=(8, 8))\ncontrast = 8\n\nimgs = [sample_horse, to_zebra, sample_zebra, to_horse]\ntitle = ['Horse', 'To Zebra', 'Zebra', 'To Horse']\n\nfor i in range(len(imgs)):\n    plt.subplot(2, 2, i + 1)\n    plt.title(title[i])\n    if i % 2 == 0:\n        plt.imshow(imgs[i][0] * 0.5 + 0.5)\n    else:\n        plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\nplt.show()\n\n# In[146]:\n\n\nplt.figure(figsize=(8, 8))\n\nplt.subplot(121)\nplt.title('Is a real zebra?')\nwith tf.device('/cpu:0'):\n    plt.imshow(discriminator_y(sample_zebra)[0, ..., -1], cmap='RdBu_r')\n\nplt.subplot(122)\nplt.title('Is a real horse?')\nwith tf.device('/cpu:0'):\n    plt.imshow(discriminator_x(sample_horse)[0, ..., -1], cmap='RdBu_r')\n\nplt.show()\n\n# In[147]:\n\n\nLAMBDA = 10\n\n# In[148]:\n\n\nloss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True,\n                                              label_smoothing=True)\n\n\n# In[149]:\n\n\ndef discriminator_loss(real, generated):\n    real_loss = loss_obj(tf.ones_like(real), real)\n\n    generated_loss = loss_obj(tf.zeros_like(generated), generated).numpy()\n\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss * 0.5\n\n\n# In[150]:\n\n\ndef generator_loss(generated):\n    return loss_obj(tf.ones_like(generated), generated)\n\n\n# In[ ]:\n\n\n# In[151]:\n\n\ndef calc_cycle_loss(real_image, cycled_image):\n    loss1 = tf.reduce_mean(tf.abs(real_image, cycled_image))\n    return LAMBDA * loss1\n\n\n# In[152]:\n\n\n\n\n\n# In[154]:\n\n\n''' generator_g is responsible for translating image X to image Y. Identity \nloss says that, if you fed image Y to generator G, it should yield the real \nimage Y or something close to image Y.\n    identity_loss = |G(Y)-Y|+|F(X)-X|'''\n\n\ndef identity_loss(real_image, same_image):\n    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n    print(loss)\n    return LAMBDA * 0.5 * loss\n\n\n# In[155]:\n\n\n# Optimizers\ngenerator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n# In[156]:\n\n\ncheckpoint_path = &quot;./checkpoints/train&quot;\n\nckpt = tf.train.Checkpoint(generator_g=generator_g,\n                           generator_f=generator_f,\n                           discriminator_x=discriminator_x,\n                           discriminator_y=discriminator_y,\n                           generator_g_optimizer=generator_g_optimizer,\n                           generator_f_optimizer=generator_f_optimizer,\n                           discriminator_x_optimizer=discriminator_x_optimizer,\n                           discriminator_y_optimizer=discriminator_y_optimizer)\n\n# In[157]:\n\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=15)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print('Latest checkpoint restored!!')\n\n\n# In[158]:\n\n\ndef generate_images(model, test_input):\n    prediction = model(test_input)\n\n    plt.figure(figsize=(12, 12))\n\n    display_list = [test_input[0], prediction[0]]\n    title = ['Input Image', 'Predicted Image']\n\n    for i in range(2):\n        plt.subplot(1, 2, i + 1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()\n\n\n# Even though the training loop looks complicated, it consists of four basic steps:\n# \n# Get the predictions.\n# Calculate the loss.\n# Calculate the gradients using backpropagation.\n# Apply the gradients to the optimizer.\n\n# In[159]:\n\n\nEPOCHS = 40\n\n\n# In[160]:\n\n\n@tf.function\ndef train_step(real_x, real_y):\n    # persistent is set to True because the tape is used more than once to calculate the gradients.\n    with tf.GradientTape(persistent=True) as tape:\n        # Generator G translates X -&gt; Y\n        # Generator F translates Y -&gt; X.\n\n        fake_y = generator_g(real_x, training=True)\n        cycled_x = generator_f(fake_y, training=True)\n\n        fake_x = generator_f(real_y, training=True)\n        cycled_y = generator_g(fake_x, training=True)\n\n        # same_x and same_y are used for identity loss.\n        same_x = generator_f(real_x, training=True)\n        same_y = generator_g(real_y, training=True)\n\n        disc_real_x = discriminator_x(real_x, training=True)\n        disc_real_y = discriminator_y(real_y, training=True)\n\n        disc_fake_x = discriminator_x(fake_x, training=True)\n        disc_fake_y = discriminator_y(fake_y, training=True)\n\n        # calculate the loss\n        gen_g_loss = generator_loss(disc_fake_y)\n        gen_f_loss = generator_loss(disc_fake_x)\n\n        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n\n        # Total generator loss = adversarial loss + cycle loss\n        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n    # Calculate the gradients for generator and discriminator\n    generator_g_gradients = tape.gradient(total_gen_g_loss, generator_g.trainable_variables)\n    generator_f_gradients = tape.gradient(total_gen_f_loss, generator_f.trainable_variables)\n\n    discriminator_x_gradients = tape.gradient(disc_x_loss, discriminator_x.trainable_variables)\n    discriminator_y_gradients = tape.gradient(disc_y_loss, discriminator_y.trainable_variables)\n\n    # Apply the gradients to the optimizer\n    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, generator_g.trainable_variables))\n\n    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, generator_f.trainable_variables))\n\n    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients, discriminator_x.trainable_variables))\n\n    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients, discriminator_y.trainable_variables))\n\n\n# In[161]:\n\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n\n    n = 0\n    for image_x, image_y in tf.data.Dataset.zip((train_horses, train_zebras)):\n        train_step(image_x, image_y)\n        if n % 10 == 0:\n            print('.', end='')\n        n += 1\n\n    clear_output(wait=True)\n    # Using a consistent image (sample_horse) so that the progress of the model\n    # is clearly visible.\n    generate_images(generator_g, sample_horse)\n\n    if (epoch + 1) % 5 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n\n    print(f'Time taken for epoch {epoch + 1} is {time.time() - start}')\n</code></pre>\n<p>The error tries to point me in the direction of <code>calc_cycle_loss</code></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 66}]
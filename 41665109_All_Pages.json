[{"items": [{"tags": ["tensorflow", "nlp", "conv-neural-network"], "owner": {"account_id": 438612, "reputation": 22264, "user_id": 826983, "user_type": "registered", "accept_rate": 69, "profile_image": "https://i.stack.imgur.com/B9PSD.jpg?s=256&g=1", "display_name": "Stefan Falk", "link": "https://stackoverflow.com/users/826983/stefan-falk"}, "is_answered": true, "view_count": 649, "accepted_answer_id": 41987897, "answer_count": 1, "score": 1, "last_activity_date": 1494377836, "creation_date": 1484506957, "last_edit_date": 1495540962, "question_id": 41665109, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/41665109/trying-to-understand-cnns-for-nlp-tutorial-using-tensorflow", "title": "Trying to understand CNNs for NLP tutorial using Tensorflow", "body": "<p>I am following <a href=\"http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\" rel=\"nofollow noreferrer\">this tutorial</a> in order to understand CNNs in NLP. There are a few things which I don't understand despite having the code in front of me. I hope somebody can clear a few things up here.</p>\n\n<hr>\n\n<p>The first rather minor thing is the <code>sequence_length</code>parameter of the <code>TextCNN</code> object. In the example on github this is just <code>56</code> which I think is the max-length of all sentences in the training data. This means that  <code>self.input_x</code> is  a 56-dimensional vector which will contain just the indices from the dictionary of a sentence for each word. </p>\n\n<p>This list goes into <code>tf.nn.embedding_lookup(W, self.intput_x)</code> which will return a matrix consisting of the word embeddings of those words given by <code>self.input_x</code>. According to <a href=\"https://stackoverflow.com/a/34877590/826983\">this answer</a> this operation is similar to using indexing with numpy:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>matrix = np.random.random([1024, 64]) \nids = np.array([0, 5, 17, 33])\nprint matrix[ids]\n</code></pre>\n\n<p>But the problem here is that <code>self.input_x</code> most of the time looks like <code>[1 3 44 25 64 0 0 0 0 0 0 0 .. 0 0]</code>. So am I correct if I assume that <code>tf.nn.embedding_lookup</code> ignores the value 0?</p>\n\n<hr>\n\n<p>Another thing I don't get is how <code>tf.nn.embedding_lookup</code> is working here:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Embedding layer\nwith tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n    W = tf.Variable(\n        tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n            name=\"W\")\n    self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n    self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n</code></pre>\n\n<p>I assume, taht <code>self.embedded_chars</code> is the matrix which is the <em>actual</em> input to the CNN where each row represents the word embedding of <em>one</em> word. But how can <code>tf.nn.embedding_lookup</code> know about those indices given by <code>self.input_x</code>? </p>\n\n<hr>\n\n<p>The last thing which I don't understand here is </p>\n\n<blockquote>\n  <p><code>W</code> is our embedding matrix that we learn during training. We initialize it using a random uniform distribution. <code>tf.nn.embedding_lookup</code> creates the actual embedding operation. The result of the embedding operation is a 3-dimensional tensor of shape <code>[None, sequence_length, embedding_size]</code>.</p>\n</blockquote>\n\n<p>Does this mean that we are <em>actually</em> learning the word embeddings <em>here</em>? The tutorial states at the beginning:</p>\n\n<blockquote>\n  <p>We will not used pre-trained word2vec vectors for our word embeddings. Instead, we learn embeddings from scratch.</p>\n</blockquote>\n\n<p>But I don't see a line of code where this is actually happening. The <a href=\"https://github.com/dennybritz/cnn-text-classification-tf/blob/master/text_cnn.py#L22\" rel=\"nofollow noreferrer\">code of the embedding layer</a> does not look like as if there is anything being trained or learned - so where is it happening?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 298}]
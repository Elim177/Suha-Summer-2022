[{"items": [{"tags": ["tensorflow", "keras", "deep-learning", "tensorflow2.0", "attention-model"], "owner": {"account_id": 16230354, "reputation": 2719, "user_id": 11725056, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/3fb8aa3bd56b90f894e9805de55ff840?s=256&d=identicon&r=PG&f=1", "display_name": "Deshwal", "link": "https://stackoverflow.com/users/11725056/deshwal"}, "is_answered": false, "view_count": 115, "answer_count": 0, "score": 0, "last_activity_date": 1622126200, "creation_date": 1622126200, "question_id": 67724391, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67724391/what-is-the-difference-between-the-two-different-codes-of-bahdanaus-attention-g", "title": "What is the difference between the two different codes of Bahdanau&#39;s Attention given in Official Tensorflow tutorials?", "body": "<p>I was reading and coding for Machine Translation Task and stumped across the two different tutorials.</p>\n<p>One of them is <a href=\"https://www.tensorflow.org/tutorials/text/image_captioning\" rel=\"nofollow noreferrer\">Caption Generation using Visual Attention</a> paper implementation where they have used Image features of <code>[64,2048]</code> in a way such that each image is a sentence of 64 words and each word in the sentence having an embedding of 2048 length. I totally get that implementation and here is the code below for <code>Bahdanau's Additive style Attention</code>:</p>\n<pre><code>class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n    attention_hidden_layer = (tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n  \n    score = self.V(attention_hidden_layer)\n\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n    \n    return context_vector, attention_weights\n</code></pre>\n<p>But when I went to <a href=\"https://www.tensorflow.org/text/tutorials/nmt_with_attention\" rel=\"nofollow noreferrer\">Neural Machine Language Translation Task</a>, I found this complex there which I am not able to comprehend what is happening here:</p>\n<pre><code>class BahdanauAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super().__init__()\n    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n    \n    self.attention = tf.keras.layers.AdditiveAttention()\n\n  def call(self, query, value, mask):\n    w1_query = self.W1(query)\n    w2_key = self.W2(value)\n\n    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n    value_mask = mask\n\n    context_vector, attention_weights = self.attention(inputs = [w1_query, value, w2_key],mask=[query_mask, value_mask],return_attention_scores = True,)\n    return context_vector, attention_weights\n</code></pre>\n<p>I want to ask</p>\n<ol>\n<li><strong>What is the difference between the two?</strong></li>\n<li><strong>Why can't we use the Code for Caption Generation in the Second one or vice versa?</strong></li>\n</ol>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 65}]
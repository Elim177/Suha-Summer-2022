[{"items": [{"tags": ["tensorflow", "deep-learning", "word2vec"], "owner": {"account_id": 17064857, "reputation": 97, "user_id": 12347331, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/-ZRxEhyKGzv4/AAAAAAAAAAI/AAAAAAAAAAA/ACHi3rcFijzzf5SvpruW-cZbDwrfhM7LhA/photo.jpg?sz=256", "display_name": "LiverToll92", "link": "https://stackoverflow.com/users/12347331/livertoll92"}, "is_answered": false, "view_count": 75, "answer_count": 1, "score": 0, "last_activity_date": 1580344679, "creation_date": 1580337153, "question_id": 59976586, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59976586/accuracy-of-the-skip-grammword2vec-model-for-the-words-similarity-using-brown", "title": "Accuracy of the skip-gramm(word2vec) model for the words similarity using brown dataset(NLTK)", "body": "<p>I want to create similarity matrix based on the brown dataset from the NLTK library. The problem is that loss</p>\n\n<pre><code>tf.reduce_mean(tf.nn.sampled_softmax_loss(weights = softmax_weight, biases = softmax_bias, inputs = embed,\n                  labels = y, num_sampled = num_sampled, num_classes = num_words))\n</code></pre>\n\n<p>decreases from 4.2 to 2.0 and then it starts to go up and down.\nThe question is: how can I improve accuracy of my model?</p>\n\n<p>Here is my full code:</p>\n\n<pre><code>import pandas as pd\nimport numpy as np\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding,Layer\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom numpy.random import choice\nimport random\nfrom itertools import repeat\nimport tensorflow as tf\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import brown\nimport string\nnltk.download('brown')\nnltk.download('stopwords')\n\n\n#Dataset loading and preparation:\ndataset = brown.sents()\n\npunct = list(string.punctuation)\npunct.append(\"``\")\npunct.append(\"''\")\npunct.append(\"--\")\nstops = set(stopwords.words(\"english\")) \n\ndataset = [[word.lower() for word in sentence if word not in punct and word.lower() not in stops] for sentence in dataset] \n\n\n#tokenization\ntokenizer = Tokenizer(num_words = 5000)\ntokenizer.fit_on_texts(dataset)\n\nword2index = tokenizer.word_index\nindex_word = tokenizer.index_word\n\ntotal_words = 5000\n\ndata_prep = tokenizer.texts_to_sequences(dataset) \ndata_prep = [sentence for sentence in data_prep if len(sentence) &gt;2] \n\n#word2vec\ndef word2vec_preparation(data,window_size,num_skips):\n    grams = []\n    context = []\n    target = []\n\n    assert window_size &gt;= 1,'windows_size argument is &lt;1!'\n    assert num_skips &gt;= 1,'num_skips argument &lt;1!'\n    for sentence in data:\n        if len(sentence) - window_size &gt; 1:\n            #print(sentence)\n\n            for i in range(len(sentence)):\n                if i - window_size &lt; 0:\n                    gram = sentence[i+1:i+window_size + 1]\n                    check = num_skips - len(set(gram))\n                    #print(gram)\n                    grams.append(gram)\n                    if check &gt; 0:\n                        context.extend(random.sample(set(gram), len(set(gram))))\n                        target.extend(repeat(sentence[i], len(set(gram))))\n                    else:\n                        context.extend(random.sample(set(gram), num_skips))\n                        target.extend(repeat(sentence[i], num_skips))\n\n                elif i + window_size &gt; len(sentence) -1:\n                    gram = sentence[i-window_size:i]\n                    check = num_skips - len(set(gram))\n                    #print(gram)\n                    grams.append(gram)\n                    if check &gt; 0:\n                        context.extend(random.sample(set(gram), len(set(gram))))\n                        target.extend(repeat(sentence[i], len(set(gram))))\n                    else:\n                        context.extend(random.sample(set(gram), num_skips))\n                        target.extend(repeat(sentence[i], num_skips))\n\n                else:\n                    gram = sentence[i-window_size:i] + sentence[i+1:i+window_size + 1]\n                    check = num_skips - len(set(gram))\n                    #print(gram)\n                    grams.append(gram)\n                    if check &gt; 0:\n                        context.extend(random.sample(set(gram), len(set(gram))))\n                        target.extend(repeat(sentence[i], len(set(gram))))\n                    else:\n                        context.extend(random.sample(set(gram), num_skips))\n                        target.extend(repeat(sentence[i], num_skips))\n\n        #print('----------------------')\n\n    return grams, context, target\n\ngrams,context,target = word2vec_preparation(data_prep,window_size = 2,num_skips = 3)\n\ntarget = np.array(target,dtype= np.int64)\ncontext = np.array(context,dtype= np.int64)\n\n\ncontext = context.reshape(len(context),1)\ndataset_train = tf.data.Dataset.from_tensor_slices((target, context))\ndataset_train = dataset_train.shuffle(buffer_size=1024).batch(64)\n\n#Parameters:\nnum_words = 5000\nembed_size = 300\nnum_sampled = 64\ninitializer_softmax = tf.keras.initializers.GlorotUniform()\n#Variables:\nembeddings_weight = tf.Variable(tf.random.uniform([num_words,embed_size],-1.0,1.0))\nsoftmax_weight = tf.Variable(initializer_softmax([num_words,embed_size]))\nsoftmax_bias = tf.Variable(initializer_softmax([num_words]))\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\n@tf.function\ndef training(X,y):\n  with tf.GradientTape() as tape:\n    embed = tf.nn.embedding_lookup(embeddings_weight,X)#embeddings_weight are parameters and X is a collection of indecies for looking up in the embedding table\n    loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights = softmax_weight, biases = softmax_bias, inputs = embed,\n                  labels = y, num_sampled = num_sampled, num_classes = num_words))\n  variables = [embeddings_weight,softmax_weight,softmax_bias]  \n  gradients = tape.gradient(loss,variables)\n  optimizer.apply_gradients(zip(gradients,variables))\n  return loss\n  #tf.print('Loss:',loss)\n\n\n\nEPOCHS = 100\n\nfor epoch in range(EPOCHS):\n  for step, (X,y) in enumerate(dataset_train):\n    loss = training(X,y)\n  tf.print('Epoch:',epoch + 1, 'loss:',loss)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 53}]
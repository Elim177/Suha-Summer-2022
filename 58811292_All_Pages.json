[{"items": [{"tags": ["python", "tensorflow", "keras", "tensorflow2.0"], "owner": {"account_id": 7277007, "reputation": 17062, "user_id": 10133797, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ElNKG.png?s=256&g=1", "display_name": "OverLordGoldDragon", "link": "https://stackoverflow.com/users/10133797/overlordgolddragon"}, "is_answered": true, "view_count": 66, "accepted_answer_id": 58827274, "answer_count": 1, "score": 1, "last_activity_date": 1573595855, "creation_date": 1573526977, "last_edit_date": 1573595855, "question_id": 58811292, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58811292/cant-load-optimizer-weights-after-adding-layer-without-parameters", "title": "Can&#39;t load optimizer weights after adding layer without parameters", "body": "<p><strong>MODEL A</strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ipt = Input(batch_shape=(32, 240, 4))\nx1  = Conv1D(16, 20,  strides=200, padding='same')(ipt)\nx1  = BatchNormalization()(x1)\nx2  = Conv1D(16, 200, strides=120, padding='same')(ipt)\nx2  = BatchNormalization()(x2) # ...\n</code></pre>\n\n<p><strong>MODEL B</strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ipt = Input(batch_shape=(32, 250, 4))\nx1  = Conv1D(16, 20,  strides=200)(ipt)\nx1  = BatchNormalization()(x1)\nx2  = Conv1D(16, 200, strides=120)(ipt)\nx2  = BatchNormalization()(x2) # ...\n</code></pre>\n\n<p><hr>\nThe two have identical weight shapes - however, A's optimizer <code>weights</code> <em>cannot</em> be loaded onto B, as B has a different build order (images &amp; code below). </p>\n\n<p>This is a tiny snippet of a much larger model which needs its <code>timesteps</code> parameter changed every X epochs, and <code>ZeroPadding1D</code> appears to <em>change layer build order</em> whenever it's used; this doesn't affect <em>model</em> weights, as they're mapped via a dictionary - whereas optimizer weights are mapped sequentially, list-to-list.</p>\n\n<p>Reproducible in both TF1 &amp; TF2, and w/ <code>keras</code> &amp; <code>tf.keras</code> imports. What's the problem, and how to fix? <a href=\"https://github.com/tensorflow/tensorflow/issues/34182\" rel=\"nofollow noreferrer\">Relevant Git</a></p>\n\n<hr>\n\n<p><strong>Environment</strong>: Win-10 OS, CUDA 10.0.130, cuDNN 7.6.0, Python 3.7.4, GTX 1070</p>\n\n<p><strong>Observations</strong>:</p>\n\n<ul>\n<li>Swaps any other layer, not just <code>BatchNormalization</code> - and any <em>number of layers</em> before <code>concatenate</code>; optimizer weights end up being simply swapped in <code>.get_weights()</code></li>\n<li>Can change <code>strides</code> instead of <code>batch_shape[1]</code></li>\n<li>Can use <code>MaxPooling1D</code> w/ <code>strides &gt; 1</code></li>\n<li><code>padding='valid'</code> leads to <code>ZeroPadding1D</code>, but it <em>doesn't</em> change build order (don't know why)</li>\n</ul>\n\n<hr>\n\n<p><strong><code>model_A.summary()</code></strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>Layer (type)                    Output Shape         Param #     Connected to     \n==================================================================================\ninput_1 (InputLayer)            [(32, 240, 4)]       0                            \n__________________________________________________________________________________\nconv1d (Conv1D)                 (32, 2, 16)          1296        input_1[0][0]    \n__________________________________________________________________________________\nconv1d_1 (Conv1D)               (32, 2, 16)          12816       input_1[0][0]    \n__________________________________________________________________________________\nbn_1 (BatchNormalization)       (32, 2, 16)          64          conv1d[0][0]     \n__________________________________________________________________________________\nbn_2 (BatchNormalization)       (32, 2, 16)          64          conv1d_1[0][0]   \n__________________________________________________________________________________\nconcatenate (Concatenate)       (32, 2, 32)          0           bn_1[0][0]       \n                                                                 bn_2[0][0]       \n__________________________________________________________________________________\ngap_0 (GlobalAveragePooling1D)  (32, 32)             0           concatenate[0][0]\n__________________________________________________________________________________\ndense (Dense)                   (32, 1)              33          gap_0[0][0]      \n</code></pre>\n\n<p><strong><code>model_B.summary()</code></strong> <em>(note the swapped layers)</em></p>\n\n<pre class=\"lang-py prettyprint-override\"><code>input_2 (InputLayer)            [(32, 250, 4)]       0                               \n_____________________________________________________________________________________\nconv1d_2 (Conv1D)               (32, 2, 16)          1296        input_2[0][0]       \n_____________________________________________________________________________________\nbn_1 (BatchNormalization)       (32, 2, 16)          64          conv1d_2[0][0]      \n_____________________________________________________________________________________\nconv1d_3 (Conv1D)               (32, 3, 16)          12816       input_2[0][0]       \n_____________________________________________________________________________________\nzero_padding1d (ZeroPadding1D)  (32, 3, 16)          0           bn_1[0][0]          \n_____________________________________________________________________________________\nbn_2 (BatchNormalization)       (32, 3, 16)          64          conv1d_3[0][0]      \n_____________________________________________________________________________________\nconcatenate_1 (Concatenate)     (32, 3, 32)          0           zero_padding1d[0][0]\n                                                                 bn_2[0][0]          \n_____________________________________________________________________________________\ngap_0 (GlobalAveragePooling1D)  (32, 32)             0           concatenate_1[0][0] \n_____________________________________________________________________________________\ndense_1 (Dense)                 (32, 1)              33          gap_0[0][0]  \n</code></pre>\n\n<hr>\n\n<p><strong>Minimally reproducible code</strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code># also works with `from keras`\nfrom tensorflow.keras.layers import Input, Conv1D, ZeroPadding1D, concatenate\nfrom tensorflow.keras.layers import BatchNormalization, Dense, GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model\nimport numpy as np\n\ndef make_model(batch_shape):\n    ipt = Input(batch_shape=batch_shape)\n\n    x1  = Conv1D(16, 20,  strides=200, padding='same')(ipt)\n    x1  = BatchNormalization()(x1)\n    x2  = Conv1D(16, 200, strides=120, padding='same')(ipt)\n    x2  = BatchNormalization()(x2)\n\n    x1, x2 = zero_pad(x1, x2)\n    preout = concatenate([x1, x2])\n    preout = GlobalAveragePooling1D()(preout)\n    out    = Dense(1)(preout)\n\n    model  = Model(ipt, out)\n    model.compile('adam', 'mse')\n    return model \n\ndef zero_pad(x1, x2):\n    diff = int(x2.shape[1]) - int(x1.shape[1])\n    if   diff &gt; 0:\n        x1 = ZeroPadding1D((diff, 0))(x1)\n    elif diff &lt; 0:\n        x2 = ZeroPadding1D((abs(diff), 0))(x2)\n    return x1, x2\n\ndef make_data(batch_shape):\n    return (np.random.randn(*batch_shape), \n            np.random.randint(0, 2, (batch_shape[0], 1)))\n\nbatch_shape_A = (32, 240, 4)\nbatch_shape_B = (32, 250, 4)\nbatch_shape_C = (32, 240, 4)\nmodel_A  = make_model(batch_shape_A)\nmodel_B  = make_model(batch_shape_B)\nmodel_C  = make_model(batch_shape_C) # 'control group'\nx_A, y_A = make_data(batch_shape_A)\nx_B, y_B = make_data(batch_shape_B)\nx_C, y_C = make_data(batch_shape_C)\n\nmodel_A.train_on_batch(x_A, y_A)\nmodel_B.train_on_batch(x_B, y_B)\nmodel_C.train_on_batch(x_C, y_C)\n\noptimizer_weights_A = model_A.optimizer.get_weights()\n\nmodel_C.optimizer.set_weights(optimizer_weights_A)\nprint(\"model_C optimizer weights set successfully\")\n\nmodel_B.optimizer.set_weights(optimizer_weights_A)\nprint(\"model_B optimizer weights set successfully\") # will not print\n</code></pre>\n\n<p><strong>Output</strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>model_C optimizer weights set successfully\n\nValueError: Optimizer weight shape (16,) not compatible with provided \nweight shape (200, 4, 16)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 289}]
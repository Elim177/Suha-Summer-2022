[{"items": [{"tags": ["python", "tensorflow", "lstm", "tensorflow-datasets"], "owner": {"account_id": 1720655, "reputation": 820, "user_id": 1757224, "user_type": "registered", "accept_rate": 85, "profile_image": "https://i.stack.imgur.com/yUXOQ.jpg?s=256&g=1", "display_name": "ARAT", "link": "https://stackoverflow.com/users/1757224/arat"}, "is_answered": true, "view_count": 208, "accepted_answer_id": 53452692, "answer_count": 1, "score": 2, "last_activity_date": 1544023617, "creation_date": 1542992247, "last_edit_date": 1544023617, "question_id": 53450495, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/53450495/tf-data-api-cannot-print-all-the-batches", "title": "tf.data API cannot print all the batches", "body": "<p>I am self-teaching myself about <code>tf.data</code> API. I am using <code>MNIST</code> dataset for binary classification. The training x and y data is zipped together in the full train_dataset. Chained along together with this zip method is first the <code>batch()</code> dataset method. the data is batched with a batch size of 30. Since my training set size is 11623, with batch size 128, I will have 91 batches. The size of the last batch will be 103 which is fine since this is LSTM. Additionally, I am using drop-out. When I compute batch accuracy, I am turning off the drop-out.</p>\n\n<p>The full code is given below:</p>\n\n<pre><code>#Ignore the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport tensorflow as tf\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (8,7)\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\")\n\nXtrain = mnist.train.images[mnist.train.labels &lt; 2]\nytrain = mnist.train.labels[mnist.train.labels &lt; 2]\n\nprint(Xtrain.shape)\nprint(ytrain.shape)\n\n#Data parameters\nnum_inputs = 28\nnum_classes = 2\nnum_steps=28\n\n# create the training dataset\nXtrain = tf.data.Dataset.from_tensor_slices(Xtrain).map(lambda x: tf.reshape(x,(num_steps, num_inputs)))\n# apply a one-hot transformation to each label for use in the neural network\nytrain = tf.data.Dataset.from_tensor_slices(ytrain).map(lambda z: tf.one_hot(z, num_classes))\n# zip the x and y training data together and batch and Prefetch data for faster consumption\ntrain_dataset = tf.data.Dataset.zip((Xtrain, ytrain)).batch(128).prefetch(128)\n\niterator = tf.data.Iterator.from_structure(train_dataset.output_types,train_dataset.output_shapes)\nX, y = iterator.get_next()\n\ntraining_init_op = iterator.make_initializer(train_dataset)\n\n\n#### model is here ####\n\n#Network parameters\nnum_epochs = 2\nbatch_size = 128\noutput_keep_var = 0.5\n\nwith tf.Session() as sess:\n    init.run()\n\n    print(\"Initialized\")\n    # Training cycle\n    for epoch in range(0, num_epochs):\n        num_batch = 0\n        print (\"Epoch: \", epoch)\n        avg_cost = 0.\n        avg_accuracy =0\n        total_batch = int(11623 / batch_size + 1)\n        sess.run(training_init_op)\n       while True:\n            try:\n                _, miniBatchCost = sess.run([trainer, loss], feed_dict={output_keep_prob: output_keep_var})\n                miniBatchAccuracy = sess.run(accuracy, feed_dict={output_keep_prob: 1.0})\n               print('Batch %d: loss = %.2f, acc = %.2f' % (num_batch, miniBatchCost, miniBatchAccuracy * 100))\n                num_batch +=1\n            except tf.errors.OutOfRangeError:\n                break\n</code></pre>\n\n<p>When I run this code, it seems it is working and printing:</p>\n\n<pre><code>Batch 0: loss = 0.67276, acc = 0.94531\nBatch 1: loss = 0.65672, acc = 0.92969\nBatch 2: loss = 0.65927, acc = 0.89062\nBatch 3: loss = 0.63996, acc = 0.99219\nBatch 4: loss = 0.63693, acc = 0.99219\nBatch 5: loss = 0.62714, acc = 0.9765\n......\n......\nBatch 39: loss = 0.16812, acc = 0.98438\nBatch 40: loss = 0.10677, acc = 0.96875\nBatch 41: loss = 0.11704, acc = 0.99219\nBatch 42: loss = 0.10592, acc = 0.98438\nBatch 43: loss = 0.09682, acc = 0.97656\nBatch 44: loss = 0.16449, acc = 1.00000\n</code></pre>\n\n<p>However, as one can see easily, there is something wrong. Only 45 batches are printed not 91 and I do not know why this is happening. I tried so many things and I think I am missing something out. </p>\n\n<p>I can use <code>repeat()</code> function but I do not want that because I have redundant observations for last batches and I want LSTM to handle it. </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 120}]
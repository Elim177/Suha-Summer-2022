[{"items": [{"tags": ["tensorflow", "keras", "optimization", "tensorflow2.0", "adam"], "owner": {"account_id": 7606564, "reputation": 1, "user_id": 5769044, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-4CeHGaozEts/AAAAAAAAAAI/AAAAAAAAJGU/X1G7w1p3RtM/photo.jpg?sz=256", "display_name": "fatemeh .b", "link": "https://stackoverflow.com/users/5769044/fatemeh-b"}, "is_answered": false, "view_count": 154, "answer_count": 0, "score": 0, "last_activity_date": 1626942584, "creation_date": 1626942584, "question_id": 68481589, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68481589/custom-optimizer-error-typeerror-expected-tf-group-expected-tensor-argumen", "title": "Custom Optimizer error--&gt; TypeError: Expected tf.group() expected Tensor arguments not &#39;None&#39; with type &#39;&lt;class &#39;NoneType&#39;&gt;&#39;", "body": "<p>I've implemented an <a href=\"https://towardsdatascience.com/gradient-accumulation-overcoming-memory-constraints-in-deep-learning-36d411252d01\" rel=\"nofollow noreferrer\">accumulated gradient optimizer</a> but when I want to train model it gives me this error.So what is the problem?\nThe idea behind gradient accumulation is that it calculates the loss and gradients after each mini-batch, but instead of updating the model parameters, it waits and accumulates the gradients over consecutive batches. And then ultimately updates the parameters based on the cumulative gradient after a specified number of batches. It serves the same purpose as having a mini-batch with higher number of images.</p>\n<pre><code>class AccumAdamOptimizer(keras.optimizers.Optimizer):\n\n    def __init__(self,learning_rate=0.001,steps=1,beta_1=0.9,beta_2=0.999,epsilon=1e- \n        7,amsgrad=False,name='AccumAdamOptimizer',**kwargs):\n        super(AccumAdamOptimizer, self).__init__(name, **kwargs)\n        self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n        self._set_hyper('decay', self._initial_decay)\n        self._set_hyper('beta_1', beta_1)\n        self._set_hyper('beta_2', beta_2)\n        self.epsilon = epsilon\n        self.amsgrad = amsgrad\n        self.iterations = tf.Variable(1, dtype='int64', name='iterations')\n        self.steps = steps\n        self.condition = tf.math.equal(self.iterations % self.steps , 0)\n\n    def _create_slots(self, var_list):\n        for var in var_list:\n          self.add_slot(var, 'm')\n        for var in var_list:\n          self.add_slot(var, 'v')\n        # if self.amsgrad:\n        #   for var in var_list:\n        #     self.add_slot(var, 'vhat')\n        for var in var_list:\n          self.add_slot(var, &quot;ag&quot;) #accumulated gradient        \n\n    def _resource_apply_dense(self, grad, var, apply_state=None):\n        var_device, var_dtype = var.device, var.dtype.base_dtype\n        m = self.get_slot(var, 'm')\n        v = self.get_slot(var, 'v')\n        ag = self.get_slot(var, 'ag')\n        lr=self._get_hyper('learning_rate', var_dtype)\n        beta1= self._get_hyper('beta_1', var_dtype)\n        beta2=self._get_hyper('beta_2', var_dtype)\n        t =  tf.cast(self.iterations, tf.float32)\n        beta1_power=tf.math.pow(beta1, t )\n        beta2_power=tf.math.pow(beta2, t)\n\n        if self.condition:\n          new_m = beta1 * m + (1-beta1) * ag\n          new_v = beta2 * v + (1-beta2) * tf.math.square(ag)\n          m_corrected = new_m/(1-beta1_power)\n          v_corrected = new_v/(1-beta2_power)\n          new_var =  var - lr * m_corrected/(tf.math.sqrt(v_corrected)+self.epsilon))                       \n          var.assign(new_var) # update weights\n          shape_var = tf.shape(var)\n          ag.assign(tf.zeros(shape_var, dtype=var.dtype))\n          m.assign(m_corrected)\n          v.assign(v_corrected)\n          self.iterations.assign_add(1)\n        else:\n          ag.assign_add(grad)\n          self.iterations.assign_add(1)\n\n    def _resource_apply_sparse(self, grad, var):\n        raise NotImplementedError\n\n    def get_config(self):\n        config = super(AccumAdamOptimizer, self).get_config()\n        config.update({\n            'learning_rate': self._serialize_hyperparameter('learning_rate'),\n            'decay': self._initial_decay,\n            'beta_1': self._serialize_hyperparameter('beta_1'),\n            'beta_2': self._serialize_hyperparameter('beta_2'),\n            'epsilon': self.epsilon,\n            'amsgrad': self.amsgrad,\n        })\n        return config\n</code></pre>\n<p>This is my complete error :\nTypeError: in user code:</p>\n<pre><code>/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:830 train_function  *\n    return step_function(self, iterator)\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:813 run_step  *\n    outputs = model.train_step(data)\n/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:774 train_step  *\n    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:530 minimize  **\n    return self.apply_gradients(grads_and_vars, name=name)\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:668 apply_gradients\n    apply_state)\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:732 _distributed_apply\n    with ops.control_dependencies([control_flow_ops.group(update_ops)]):\n/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:2966 group\n    &quot;'%s' with type '%s'&quot; % (inp, type(inp)))\n\nTypeError: Expected tf.group() expected Tensor arguments not 'None' with type '&lt;class 'NoneType'&gt;'\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 39}]
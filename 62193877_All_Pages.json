[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning"], "owner": {"account_id": 15459496, "reputation": 51, "user_id": 11152508, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/8803ae070e2fa517e1abb51072f41d7f?s=256&d=identicon&r=PG&f=1", "display_name": "TheOneTheOnly2", "link": "https://stackoverflow.com/users/11152508/theonetheonly2"}, "is_answered": false, "view_count": 448, "answer_count": 1, "score": 3, "last_activity_date": 1591697023, "creation_date": 1591271319, "question_id": 62193877, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62193877/keras-fit-giving-better-performance-than-manual-tensorflow", "title": "Keras .fit giving better performance than manual Tensorflow", "body": "<p>I'm new to Tensorflow and Keras. To get started, I followed the <a href=\"https://www.tensorflow.org/tutorials/quickstart/advanced\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/tutorials/quickstart/advanced</a> tutorial. I'm now adapting it to train on CIFAR10 instead of MNIST dataset. I recreated this model <a href=\"https://keras.io/examples/cifar10_cnn/\" rel=\"nofollow noreferrer\">https://keras.io/examples/cifar10_cnn/</a> and I'm trying to run it in my own codebase. </p>\n\n<p>Logically, if the model, batch size and optimizer are all the same, then the two should perform identically, but they don't. I thought it might be that I'm making a mistake in preparing the data. So I copied the model.fit function from the keras code into my script, and it still performs better. Using .fit gives me around 75% accuracy in 25 epochs, while with the manual method it takes around 60 epochs. With .fit I also achieve slightly better max accuracy.</p>\n\n<p>What I want to know is: Is .fit doing something behind the scenes that's optimizing training? What do I need to add to my code to get the same performance? Am I doing something obviously wrong? </p>\n\n<p>Thanks for your time.</p>\n\n<p>Main code:</p>\n\n<pre><code>\nimport tensorflow as tf\nfrom tensorflow import keras\nimport msvcrt\nfrom Plotter import Plotter\n\n\n#########################Configuration Settings#############################\n\nBatchSize = 32\nModelName = \"CifarModel\"\n\n############################################################################\n\n\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\nprint(\"x_train\",x_train.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"x_test\",x_test.shape)\nprint(\"y_test\",y_test.shape)\n\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, 10)\ny_test = keras.utils.to_categorical(y_test, 10)\n\n\n\ntrain_ds = tf.data.Dataset.from_tensor_slices(\n    (x_train, y_train)).batch(BatchSize)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BatchSize)\n\n\nloss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001,decay=1e-6)\n\n# Create an instance of the model\nmodel = ModelManager.loadModel(ModelName,10)\n\n\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n\ntest_loss = tf.keras.metrics.Mean(name='test_loss')\ntest_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n\n\n\n########### Using this function I achieve better results ##################\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\nmodel.fit(x_train, y_train,\n              batch_size=BatchSize,\n              epochs=100,\n              validation_data=(x_test, y_test),\n              shuffle=True,\n              verbose=2)\n\n############################################################################\n\n########### Using the below code I achieve worse results ##################\n\n@tf.function\ndef train_step(images, labels):\n  with tf.GradientTape() as tape:\n    predictions = model(images, training=True)\n    loss = loss_object(labels, predictions)\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n\n  train_loss(loss)\n  train_accuracy(labels, predictions)\n\n@tf.function\ndef test_step(images, labels):\n  predictions = model(images, training=False)\n  t_loss = loss_object(labels, predictions)\n\n  test_loss(t_loss)\n  test_accuracy(labels, predictions)\n\nepoch = 0\nInterruptLoop = False\nwhile InterruptLoop == False:\n  #Shuffle training data\n  train_ds.shuffle(1000)\n  epoch = epoch + 1\n  # Reset the metrics at the start of the next epoch\n  train_loss.reset_states()\n  train_accuracy.reset_states()\n  test_loss.reset_states()\n  test_accuracy.reset_states()\n\n  for images, labels in train_ds:\n    train_step(images, labels)\n\n  for test_images, test_labels in test_ds:\n    test_step(test_images, test_labels)\n\n  test_accuracy = test_accuracy.result() * 100\n  train_accuracy = train_accuracy.result() * 100\n\n  #Print update to console\n  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n  print(template.format(epoch,\n                        train_loss.result(),\n                        train_accuracy ,\n                        test_loss.result(),\n                        test_accuracy))\n\n  # Check if keyboard pressed\n  while msvcrt.kbhit():\n    char = str(msvcrt.getch())\n    if char == \"b'q'\":\n      InterruptLoop = True\n      print(\"Stopping loop\")\n\n</code></pre>\n\n<p>The model:</p>\n\n<pre><code>from tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPool2D\nfrom tensorflow.keras import Model\n\nclass ModelData(Model):\n  def __init__(self,NumberOfOutputs):\n    super(ModelData, self).__init__()\n    self.conv1 = Conv2D(32, 3, activation='relu', padding='same', input_shape=(32,32,3))\n    self.conv2 = Conv2D(32, 3, activation='relu')\n    self.maxpooling1 = MaxPool2D(pool_size=(2,2))\n    self.dropout1 = Dropout(0.25)\n    ############################\n    self.conv3 = Conv2D(64,3,activation='relu',padding='same')\n    self.conv4 = Conv2D(64,3,activation='relu')\n    self.maxpooling2 = MaxPool2D(pool_size=(2,2))\n    self.dropout2 = Dropout(0.25)\n    ############################\n    self.flatten = Flatten()\n    self.d1 = Dense(512, activation='relu')\n    self.dropout3 = Dropout(0.5)\n    self.d2 = Dense(NumberOfOutputs,activation='softmax')\n\n  def call(self, x):\n    x = self.conv1(x)\n    x = self.conv2(x)\n    x = self.maxpooling1(x)\n    x = self.dropout1(x)\n    x = self.conv3(x)\n    x = self.conv4(x)\n    x = self.maxpooling2(x)\n    x = self.dropout2(x)\n    x = self.flatten(x)\n    x = self.d1(x)\n    x = self.dropout3(x)\n    x = self.d2(x)\n    return x\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 182}]
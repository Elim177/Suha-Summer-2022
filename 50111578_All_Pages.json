[{"items": [{"tags": ["tensorflow", "google-cloud-ml"], "owner": {"account_id": 528061, "reputation": 110, "user_id": 1864220, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/67071a7cbdeca64a11ce6362e772e75d?s=256&d=identicon&r=PG", "display_name": "Dan", "link": "https://stackoverflow.com/users/1864220/dan"}, "is_answered": true, "view_count": 218, "accepted_answer_id": 50179335, "answer_count": 1, "score": 0, "last_activity_date": 1525450955, "creation_date": 1525151248, "question_id": 50111578, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/50111578/google-cloud-ml-engine-does-not-return-objective-values-when-hyperparameter-tuni", "title": "Google Cloud ML Engine does not return objective values when hyperparameter tuning", "body": "<p>In the training output for a hyperparameter tuning job on Google Cloud ML Engine, I do not see the values of the objective calculated for each trial. The training output is the following:</p>\n\n<pre><code>    {\n  \"completedTrialCount\": \"4\",\n  \"trials\": [\n    {\n      \"trialId\": \"2\",\n      \"hyperparameters\": {\n        \"learning-rate\": \"0.0010000350944297609\"\n      }\n    },\n    {\n      \"trialId\": \"3\",\n      \"hyperparameters\": {\n        \"learning-rate\": \"0.0053937227881987841\"\n      }\n    },\n    {\n      \"trialId\": \"4\",\n      \"hyperparameters\": {\n        \"learning-rate\": \"0.099948384760813816\"\n      }\n    },\n    {\n      \"trialId\": \"1\",\n      \"hyperparameters\": {\n        \"learning-rate\": \"0.02917661111653325\"\n      }\n    }\n  ],\n  \"consumedMLUnits\": 0.38,\n  \"isHyperparameterTuningJob\": true\n}\n</code></pre>\n\n<p>The hyperparameter tuning job appears to run correctly and displays a green check mark next to the job. However, I expected that I would see the value of the objective function for each trial in the training output. Without this, I don't know which trial is best. I have attempted to add the value of the objective into the summary graph as follows:</p>\n\n<pre><code>with tf.Session() as sess:\n    ...\n    final_cost = sess.run(tf.reduce_sum(tf.square(Y-y_model)), feed_dict={X: trX, Y:trY})\n    summary = Summary(value=[Summary.Value(tag='hyperparameterMetricTag', simple_value=final_cost)])\n    summary_writer.add_summary(summary)\n    summary_writer.flush()\n</code></pre>\n\n<p>I believe I have followed all the steps discussed in the documentation to set up a hyperparameter tuning job. What else is required to ensure that I get an output that lets me compare different trials?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 25}]
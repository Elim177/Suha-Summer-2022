[{"items": [{"tags": ["python", "tensorflow", "gradienttape"], "owner": {"account_id": 13654159, "reputation": 140, "user_id": 9851177, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/00b3886544429cb7e5ed25523518e103?s=256&d=identicon&r=PG&f=1", "display_name": "Marie M.", "link": "https://stackoverflow.com/users/9851177/marie-m"}, "is_answered": false, "view_count": 99, "answer_count": 0, "score": 0, "last_activity_date": 1629645374, "creation_date": 1629645374, "question_id": 68882646, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68882646/tensorflow-gradienttape-computes-only-none-gradients-with-custom-loss", "title": "Tensorflow GradientTape computes only &#39;None&#39; gradients with custom loss", "body": "<p>So I have a model (A) that I\u00b4m training with a <em>custom training procedure</em>. It is supported by a second model (B). This means of course I have to use <strong>tf.GradientTape</strong> and compute + apply the gradients myself.</p>\n<p>However, it doesn\u00b4t work as expected and instead as <strong>gradients</strong>, just a list [None, None ...] is returned. Code snippet:</p>\n<pre><code>with tf.GradientTape() as tape:\n  outputs = model(input_batch, training=True)  # output of model A\n  critic_output = critic_model(outputs, training=True)  # output of model B\n  loss = critic_loss(critic_output, 1)  # loss of model B with input generated by A\n  model_grads = tape.gradient(loss, model.trainable_variables)  # returns [None, ...]\n</code></pre>\n<p>The models are correct, I tested every aspect of them. This is not the first time I use Gradient tapes to compute gradients. However, here this time, instead of returning a list of tensors, the gradient call just returns a list of 'None's. I don\u00b4t know what\u00b4s going on.</p>\n<p>I also looked into this <a href=\"https://stackoverflow.com/questions/56916313/tensorflow-2-0-doesnt-compute-the-gradient/56917148\">other post</a> and added every variable to the gradient tape with .watch(), and it didn\u00b4t change anything.\nAll other gradients are computed without problems. I tried pretty much everything and am desperate for help.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 11}]
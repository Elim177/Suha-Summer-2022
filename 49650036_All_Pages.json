[{"items": [{"tags": ["python", "tensorflow", "neural-network", "recurrent-neural-network"], "owner": {"user_type": "does_not_exist", "display_name": "user8587747"}, "is_answered": false, "view_count": 81, "answer_count": 1, "score": 0, "last_activity_date": 1522933565, "creation_date": 1522841932, "question_id": 49650036, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49650036/tensorflow-rnn-not-training-saving", "title": "TensorFlow | RNN not training (saving)", "body": "<p>I am currently working on a RNN which should generate text based on sample text with which it was trained. But for some reason no matter how long I train it it's always spilling out nonsense. You can find the GitHub repo <a href=\"https://github.com/MrGrimod/rnn_text_gen\" rel=\"nofollow noreferrer\">here</a>.</p>\n\n<p><strong>Network definition</strong></p>\n\n<pre><code>## RNN with num_layers LSTM layers and a fully-connected output layer\n## The network allows for a dynamic number of iterations, depending on the         inputs it receives.\n##\n##    out   (fc layer; out_size)\n##     ^\n##    lstm\n##     ^\n##    lstm  (lstm size)\n##     ^\n##     in   (in_size)\nclass ModelNetwork:\n    def __init__(self, in_size, lstm_size, num_layers, out_size, session,     learning_rate=0.003, name=\"rnn\"):\n        self.scope = name\n\n        self.in_size = in_size\n        self.lstm_size = lstm_size\n        self.num_layers = num_layers\n        self.out_size = out_size\n\n        self.session = session\n\n        self.learning_rate = tf.constant( learning_rate )\n\n        # Last state of LSTM, used when running the network in TEST mode\n        self.lstm_last_state = np.zeros((self.num_layers*2*self.lstm_size,))\n\n        with tf.variable_scope(self.scope):\n            ## (batch_size, timesteps, in_size)\n            self.xinput = tf.placeholder(tf.float32, shape=(None, None,     self.in_size), name=\"xinput\")\n            self.lstm_init_value = tf.placeholder(tf.float32, shape=(None, self.num_layers*2*self.lstm_size), name=\"lstm_init_value\")\n\n        # LSTM\n        self.lstm_cells = [ tf.contrib.rnn.BasicLSTMCell(self.lstm_size, forget_bias=1.0, state_is_tuple=False) for i in range(self.num_layers)]\n        self.lstm = tf.contrib.rnn.MultiRNNCell(self.lstm_cells, state_is_tuple=False)\n\n        # Iteratively compute output of recurrent network\n        outputs, self.lstm_new_state = tf.nn.dynamic_rnn(self.lstm, self.xinput, initial_state=self.lstm_init_value, dtype=tf.float32)\n\n        # Linear activation (FC layer on top of the LSTM net)\n        self.rnn_out_W = tf.Variable(tf.random_normal( (self.lstm_size, self.out_size), stddev=0.01 ))\n        self.rnn_out_B = tf.Variable(tf.random_normal( (self.out_size, ), stddev=0.01 ))\n\n        outputs_reshaped = tf.reshape( outputs, [-1, self.lstm_size] )\n        network_output = ( tf.matmul( outputs_reshaped, self.rnn_out_W ) + self.rnn_out_B )\n\n        batch_time_shape = tf.shape(outputs)\n        self.final_outputs = tf.reshape( tf.nn.softmax( network_output), (batch_time_shape[0], batch_time_shape[1], self.out_size) )\n\n\n        ## Training: provide target outputs for supervised training.\n        self.y_batch = tf.placeholder(tf.float32, (None, None, self.out_size))\n        y_batch_long = tf.reshape(self.y_batch, [-1, self.out_size])\n\n        self.cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=network_output, labels=y_batch_long) )\n        self.train_op = tf.train.RMSPropOptimizer(self.learning_rate, 0.9).minimize(self.cost)\n\n\n## Input: X is a single element, not a list!\ndef run_step(self, x, init_zero_state=True):\n    ## Reset the initial state of the network.\n    if init_zero_state:\n        init_value = np.zeros((self.num_layers*2*self.lstm_size,))\n    else:\n        init_value = self.lstm_last_state\n\n    out, next_lstm_state = self.session.run([self.final_outputs, self.lstm_new_state], feed_dict={self.xinput:[x], self.lstm_init_value:[init_value]   } )\n\n    self.lstm_last_state = next_lstm_state[0]\n\n    return out[0][0]\n\n\n## xbatch must be (batch_size, timesteps, input_size)\n## ybatch must be (batch_size, timesteps, output_size)\ndef train_batch(self, xbatch, ybatch):\n    init_value = np.zeros((xbatch.shape[0], self.num_layers*2*self.lstm_size))\n\n    cost, _ = self.session.run([self.cost, self.train_op], feed_dict={self.xinput:xbatch, self.y_batch:ybatch, self.lstm_init_value:init_value   } )\n\n    return cost\n\n\n# Embed string to character-arrays -- it generates an array len(data) x     len(vocab)\n# Vocab is a list of elements\ndef embed_to_vocab(data_, vocab):\n    data = np.zeros((len(data_), len(vocab)))\n\n    cnt=0\n    for s in data_:\n        v = [0.0]*len(vocab)\n        v[vocab.index(s)] = 1.0\n        data[cnt, :] = v\n        cnt += 1\n\n    return data\n\ndef decode_embed(array, vocab):\n    return vocab[ array.index(1) ]\n</code></pre>\n\n<p><strong>Restoring</strong></p>\n\n<pre><code>    if options.mode == 'test':\n    if not options.uid:\n        print(\"Please enter model id, use -u &lt;model id&gt;\")\n        return\n    model_uuid = options.uid\n    with tf.Session() as sess_res:\n        saver_res = tf.train.import_meta_graph(\"model/\"+model_uuid+\"/model.ckpt.meta\")\n        saver_res.restore(sess, \"model/\"+model_uuid+\"/model.ckpt\")\n\n        TEST_PREFIX = TEST_PREFIX.lower()\n        for i in range(len(TEST_PREFIX)):\n            out = net.run_step( embed_to_vocab(TEST_PREFIX[i], vocab) , i==0)\n\n        gen_str = TEST_PREFIX\n        for i in range(LEN_TEST_TEXT):\n            element = np.random.choice( range(len(vocab)), p=out ) # Sample character from the network according to the generated output probabilities\n            gen_str += vocab[element]\n\n            out = net.run_step( embed_to_vocab(vocab[element], vocab) , False )\n        print ('----------------Text----------------')\n        print (gen_str)\n        print ('----------------End----------------')\n        text_file = open(\"data/output.txt\", \"w\")\n        text_file.write(gen_str)\n        text_file.close()\n</code></pre>\n\n<p><strong>Training</strong> </p>\n\n<pre><code>    ## Initialize the network\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\nsess = tf.InteractiveSession(config=config)\n\nnet = ModelNetwork(in_size = in_size,\n                    lstm_size = lstm_size,\n                    num_layers = num_layers,\n                    out_size = out_size,\n                    session = sess,\n                    learning_rate = learning_rate_prop,\n                    name = \"char_rnn_network\")\n\nsess.run(tf.global_variables_initializer())\n\nsaver = tf.train.Saver(tf.global_variables())\n\nlast_time = time.time()\n\nbatch = np.zeros((batch_size, time_steps, in_size))\nbatch_y = np.zeros((batch_size, time_steps, in_size))\n\npossible_batch_ids = range(data.shape[0]-time_steps-1)\n\nif options.mode == 'train':\n    for i in tqdm(range(NUM_TRAIN_BATCHES)):\n        # Sample time_steps consecutive samples from the dataset text file\n        batch_id = random.sample( possible_batch_ids, batch_size )\n        for j in range(time_steps):\n            ind1 = [k+j for k in batch_id]\n            ind2 = [k+j+1 for k in batch_id]\n\n            batch[:, j, :] = data[ind1, :]\n            batch_y[:, j, :] = data[ind2, :]\n\n\n        cst = net.train_batch(batch, batch_y)\n        if (i%100) == 0:\n            new_time = time.time()\n            diff = new_time - last_time\n            last_time = new_time\n\n            print(\"batch: \",i,\"   loss: \",cst,\"   speed: \",(100.0/diff),\" batches / s\")\n    model_uuid = str(uuid.uuid1())\n    saver.save(sess, \"model/\"+model_uuid+\"/model.ckpt\")\n    print(\"Finished training model, model id: \" + model_uuid)\n</code></pre>\n\n<p><strong>Results</strong></p>\n\n<pre><code>   0%|          | 0/500 [00:00&lt;?, ?it/s]batch:  0    loss:  4.19432    speed:        239.7453419095813  batches / s\n   19%|#9        | 96/500 [00:01&lt;00:13, 30.77it/s]batch:  100    loss:        3.9609    speed:  114.48676714604412  batches / s\n   38%|###8      | 192/500 [00:02&lt;00:03, 100.71it/s]batch:  200    loss:  3.08484    speed:  116.24018792187363  batches / s\n   60%|######    | 300/500 [00:03&lt;00:01, 112.94it/s]batch:  300    loss:        2.65907    speed:  112.51337575482982  batches / s\n   79%|#######9  | 396/500 [00:03&lt;00:00, 114.93it/s]batch:  400    loss:        2.29085    speed:  113.07714974572966  batches / s\n  100%|##########| 500/500 [00:04&lt;00:00, 104.44it/s]\n  Finished training model, model id: 335c3de2-37f9-11e8-a493-4ccc6abbb6f6\n</code></pre>\n\n<p><strong>Assumption</strong></p>\n\n<p>I suspect that it trains ok, because the loss after 100k batches is approximately 0.9-1. But all the saved <strong>.cpkt</strong> files have the same size <em>(no matter how long I trained the model)</em>. All that leads me to the presumption that it doesn't save the Model for some reason altough I followed <a href=\"https://www.tensorflow.org/programmers_guide/saved_model\" rel=\"nofollow noreferrer\">this</a> blog post on how to save and restore a model.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 83}]
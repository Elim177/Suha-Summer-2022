[{"items": [{"tags": ["python", "tensorflow", "tensorflow-probability", "bernoulli-probability"], "owner": {"account_id": 2811841, "reputation": 872, "user_id": 2417922, "user_type": "registered", "accept_rate": 0, "profile_image": "https://www.gravatar.com/avatar/5cee40c36bc502473b0976b339a8b8da?s=256&d=identicon&r=PG", "display_name": "Mark Lavin", "link": "https://stackoverflow.com/users/2417922/mark-lavin"}, "is_answered": false, "view_count": 215, "answer_count": 0, "score": 3, "last_activity_date": 1624886689, "creation_date": 1624639406, "last_edit_date": 1624651304, "question_id": 68134486, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68134486/why-does-tensorflow-bernoulli-distribution-always-return-0", "title": "Why does Tensorflow Bernoulli distribution always return 0?", "body": "<p>I am working on classifying texts based on word occurrences.  One of the\nsteps is to estimate the probability of a particular text for each\npossible class.  To do this, I am given NSAMPLES of texts from a\nvocabulary of NFEATURES words, each labelled with\none of NLABELS class labels.  From this, I construct a binary\noccurrence matrix where entry(sample,feature) is 1 iff text &quot;sample&quot;\ncontains the word encoded by &quot;feature&quot;.</p>\n<p>From the occurrence matrix, we can construct a matrix of conditional\nprobabilities and then smooth this so the probabilities are neither\n0.0 or 1.0, using the following code (copied from Coursera notebook):</p>\n<pre><code>def laplace_smoothing(labels, binary_data, n_classes):\n    # Compute the parameter estimates (adjusted fraction of documents in class that contain word)\n    n_words = binary_data.shape[1]\n    alpha = 1 # parameters for Laplace smoothing\n    theta = np.zeros([n_classes, n_words]) # stores parameter values - prob. word given class\n    for c_k in range(n_classes): # 0, 1, ..., 19\n        class_mask = (labels == c_k)\n        N = class_mask.sum() # number of articles in class\n        theta[c_k, :] = (binary_data[class_mask, :].sum(axis=0) + alpha)/(N + alpha*2)\n    return theta\n</code></pre>\n<p>To see the problem, here is code to mock up inputs and call for the\nresult:</p>\n<pre><code>import tensorflow_probability as tfp\ntfd = tfp.distributions\n\nNSAMPLES = 2000   # Size of corpus\nNFEATURES = 10000 # Number of words in corpus\nNLABELS = 10      # Number of classes\nONE_PROB = 0.02   # Probability that binary_datum will be 1\n\ndef mock_binary_data( nsamples, nfeatures, one_prob ):\n    binary_data = ( np.random.uniform( 0, 1, ( nsamples, nfeatures ) ) &lt; one_prob ).astype( 'int32' )\n    return binary_data\n\ndef mock_labels( nsamples, nlabels ):\n    labels = np.random.randint( 0, nlabels, nsamples )\n    return labels\n\nbinary_data = mock_binary_data( NSAMPLES, NFEATURES, ONE_PROB )\nlabels = mock_labels( NSAMPLES, NLABELS )\nsmoothed_data = laplace_smoothing( labels, binary_data, NLABELS )\n\nbernoulli = tfd.Independent( tfd.Bernoulli( probs = smoothed_data ), reinterpreted_batch_ndims = 1 )\n\ntest_random_data = mock_binary_data( 1, NFEATURES, ONE_PROB )[ 0 ]\nbernoulli.prob( test_random_data )\n</code></pre>\n<p>When I execute this, I get:</p>\n<pre><code>&lt;tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt;\n</code></pre>\n<p>that is, all the probabilities are zero.  Some step here is incorrect, can you\nplease help me find it?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 117}]
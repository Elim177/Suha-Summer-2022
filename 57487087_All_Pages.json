[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "reinforcement-learning", "q-learning"], "owner": {"account_id": 4799885, "reputation": 1006, "user_id": 3876796, "user_type": "registered", "accept_rate": 61, "profile_image": "https://www.gravatar.com/avatar/6baa712de45532fa50e686c8e26debee?s=256&d=identicon&r=PG&f=1", "display_name": "Othmane", "link": "https://stackoverflow.com/users/3876796/othmane"}, "is_answered": false, "view_count": 354, "answer_count": 1, "score": 0, "last_activity_date": 1625654257, "creation_date": 1565746111, "last_edit_date": 1565800075, "question_id": 57487087, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57487087/c51-reinforcement-learning-algorithm-extremely-slow", "title": "C51 reinforcement learning algorithm extremely slow", "body": "<p>I am applying reinforcement learning on a time series prediction problem. Until now, I have implemented a dueling DDQN algorithm with LSTM which seems to give some pretty good results, though sometimes slow to converge depending on the exact problem.\nI have then used C51 distributional reinforcement learning to compare the performance (which I expect to lead to better good results).</p>\n\n<p>I have slightly adapted the C51 google code <a href=\"https://github.com/google/dopamine\" rel=\"nofollow noreferrer\">dopamine</a> to be integrated into my code (network and training part). I am also using double Q learning for selection of next state action (which original code does not use). But, the problem is that it is really very very slow to execute. For comparaison, my previous dueling DDQN used to take 3.5 hours to train for 50000 episodes, however C51 algorithm has now taken almost 10 hours and yet has only reached 3000 episodes.</p>\n\n<p>I am wondering if there is something wrong with my adaptation of the code, or if C51 algorithm is really that slow. I am using an NVidia Geforce RTX 2080 Ti.</p>\n\n<p>Here is the network part:</p>\n\n<pre><code>#network part\nself.weights_initializer = tf.contrib.slim.variance_scaling_initializer(factor=1.0 / np.sqrt(3.0), mode='FAN_IN', uniform=True)\nself.net = tf.contrib.slim.fully_connected(\n  self.rnn, # output of an LSTM\n  num_actions * num_atoms,\n  activation_fn=None,\n  weights_initializer=self.weights_initializer)\n\nself.logits = tf.reshape(self.net, [-1, num_actions, num_atoms])\nself.probabilities = tf.contrib.layers.softmax(self.logits)\nself.q_values = tf.reduce_sum(self._support * self.probabilities, axis=2)\n\nself.predict = tf.argmax(self.q_values,1)\n\nself.actions = tf.placeholder(shape=[None],dtype=tf.int32)    \n\nself.target_distribution = tf.placeholder(shape=[None,num_atoms],dtype=tf.float32)\n\n# size of indices: batch_size x 1.\nself.indices = tf.range(tf.shape(self.logits)[0])[:, None]\n# size of reshaped_actions: batch_size x 2.\nself.reshaped_actions = tf.concat([self.indices, self.actions[:, None]], 1)\n# For each element of the batch, fetch the logits for its selected action.\nself.chosen_action_logits = tf.gather_nd(self.logits,\n                                self.reshaped_actions)\n\nself.td_error = tf.nn.softmax_cross_entropy_with_logits(labels=self.target_distribution,logits=self.chosen_action_logits)\n\n\n# divide by the real length of episodes instead of averaging which is incorrect\nself.loss = tf.cast(tf.reduce_sum(self.td_error), tf.float64) / tf.cast(tf.reduce_sum(self.seq_len), tf.float64)\n\nif apply_grad_clipping:\n   # calculate gradients and clip them to handle outliers\n   tvars = tf.trainable_variables()\n   grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), grad_clipping)\n   self.updateModel = optimizer.apply_gradients(\n        zip(grads, tvars),\n        name=\"updateModel\")\nelse:\n   self.updateModel = optimizer.minimize(self.loss, name=\"updateModel\")\n</code></pre>\n\n<p>And here's the training part:</p>\n\n<pre><code># training part\nif i &gt;= pre_train_episodes:\n        #Reset the lstm's hidden state\n        state_train = np.zeros((num_layers, 2, batch_size, h_size))\n        #Get a random batch of experiences.\n        trainBatch = myBuffer.sample(batch_size)\n        #Below we perform the Double-DQN update to the target Q-values\n        num_samples = batch_size*trace_length\n        # size of rewards: batch_size x 1\n        rewards = trainBatch[:,2][:, None]\n\n        # size of tiled_support: batch_size x num_atoms\n        tiled_support = tf.tile(mainQN._support, [num_samples])\n        tiled_support = tf.reshape(tiled_support, [num_samples, num_atoms])\n\n        # size of target_support: batch_size x num_atoms\n        is_terminal_multiplier = -(np.array(trainBatch[:,4]) - 1)\n        # Incorporate terminal state to discount factor.\n        # size of gamma_with_terminal: batch_size x 1\n        gamma_with_terminal = gamma * is_terminal_multiplier\n        gamma_with_terminal = gamma_with_terminal[:, None]\n\n        target_support = rewards + gamma_with_terminal * tiled_support\n\n\n        next_qt_argmax = sess.run([mainQN.predict], feed_dict={\\\n                            mainQN.scalarInput:np.vstack(trainBatch[:,3]),\\\n                            mainQN.trainLength:trace_length,mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n        next_qt_argmax = np.reshape(next_qt_argmax, [-1, 1])\n        probabilities = sess.run(targetQN.probabilities, feed_dict={\\\n                            targetQN.scalarInput:np.vstack(trainBatch[:,3]),\\\n                            targetQN.trainLength:trace_length,targetQN.state_in:state_train,targetQN.batch_size:batch_size})\n        batch_indices = np.arange(num_samples)[:, None]\n        batch_indexed_next_qt_argmax = np.concatenate([batch_indices, next_qt_argmax], axis=1)\n\n\n        # size of next_probabilities: batch_size x num_atoms\n        next_probabilities = tf.gather_nd(probabilities, batch_indexed_next_qt_argmax)\n\n\n        target_distribution = project_distribution(target_support, next_probabilities, mainQN._support)\n        target_distribution = target_distribution.eval()\n\n        loss, _, _ = sess.run([mainQN.loss, mainQN.check_ops, mainQN.updateModel], \\\n                            feed_dict={mainQN.scalarInput:np.vstack(trainBatch[:,0]),mainQN.target_distribution:target_distribution,\\\n                            mainQN.actions:trainBatch[:,1],mainQN.trainLength:trace_length,\\\n                            mainQN.state_in:state_train,mainQN.batch_size:batch_size})\n\n        # perform soft/hard update frequently\n        if i % update_target_freq == 0 or update_target_freq == 1 or softUpdate == True:\n            updateTarget(targetOps,sess)\n</code></pre>\n\n<p>auxiliary function:</p>\n\n<pre><code># function used above to project the distribution on the provided support\ndef project_distribution(supports, weights, target_support,\n                     validate_args=False):\n\"\"\"Projects a batch of (support, weights) onto target_support.\nBased on equation (7) in (Bellemare et al., 2017):\nhttps://arxiv.org/abs/1707.06887\nIn the rest of the comments we will refer to this equation simply as Eq7.\nThis code is not easy to digest, so we will use a running example to  clarify\nwhat is going on, with the following sample inputs:\n* supports =       [[0, 2, 4, 6, 8],\n                    [1, 3, 4, 5, 6]]\n* weights =        [[0.1, 0.6, 0.1, 0.1, 0.1],\n                    [0.1, 0.2, 0.5, 0.1, 0.1]]\n* target_support = [4, 5, 6, 7, 8]\nIn the code below, comments preceded with 'Ex:' will be referencing the above\nvalues.\nArgs:\nsupports: Tensor of shape (batch_size, num_dims) defining supports for the\n  distribution.\nweights: Tensor of shape (batch_size, num_dims) defining weights on the\n  original support points. Although for the CategoricalDQN agent these\n  weights are probabilities, it is not required that they are.\ntarget_support: Tensor of shape (num_dims) defining support of the projected\n  distribution. The values must be monotonically increasing. Vmin and Vmax\n  will be inferred from the first and last elements of this tensor,\n  respectively. The values in this tensor must be equally spaced.\n  validate_args: Whether we will verify the contents of the\n  target_support parameter.\n  Returns:\n     A Tensor of shape (batch_size, num_dims) with the projection of a batch of\n(support, weights) onto target_support.\n  Raises:\n    ValueError: If target_support has no dimensions, or if shapes of supports,\n  weights, and target_support are incompatible.\n \"\"\"\ntarget_support_deltas = target_support[1:] - target_support[:-1]\n# delta_z = `\\Delta z` in Eq7.\ndelta_z = target_support_deltas[0]\nvalidate_deps = []\nsupports.shape.assert_is_compatible_with(weights.shape)\nsupports[0].shape.assert_is_compatible_with(target_support.shape)\ntarget_support.shape.assert_has_rank(1)\nif validate_args:\n# Assert that supports and weights have the same shapes.\nvalidate_deps.append(\n    tf.Assert(\n        tf.reduce_all(tf.equal(tf.shape(supports), tf.shape(weights))),\n        [supports, weights]))\n# Assert that elements of supports and target_support have the same shape.\nvalidate_deps.append(\n    tf.Assert(\n        tf.reduce_all(\n            tf.equal(tf.shape(supports)[1], tf.shape(target_support))),\n        [supports, target_support]))\n# Assert that target_support has a single dimension.\nvalidate_deps.append(\n    tf.Assert(\n        tf.equal(tf.size(tf.shape(target_support)), 1), [target_support]))\n# Assert that the target_support is monotonically increasing.\nvalidate_deps.append(\n    tf.Assert(tf.reduce_all(target_support_deltas &gt; 0), [target_support]))\n# Assert that the values in target_support are equally spaced.\nvalidate_deps.append(\n    tf.Assert(\n        tf.reduce_all(tf.equal(target_support_deltas, delta_z)),\n        [target_support]))\n\nwith tf.control_dependencies(validate_deps):\n# Ex: `v_min, v_max = 4, 8`.\nv_min, v_max = target_support[0], target_support[-1]\n# Ex: `batch_size = 2`.\nbatch_size = tf.shape(supports)[0]\n# `N` in Eq7.\n# Ex: `num_dims = 5`.\nnum_dims = tf.shape(target_support)[0]\n# clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n# Ex: `clipped_support = [[[ 4.  4.  4.  6.  8.]]\n#                         [[ 4.  4.  4.  5.  6.]]]`.\nclipped_support = tf.clip_by_value(supports, v_min, v_max)[:, None, :]\n# Ex: `tiled_support = [[[[ 4.  4.  4.  6.  8.]\n#                         [ 4.  4.  4.  6.  8.]\n#                         [ 4.  4.  4.  6.  8.]\n#                         [ 4.  4.  4.  6.  8.]\n#                         [ 4.  4.  4.  6.  8.]]\n#                        [[ 4.  4.  4.  5.  6.]\n#                         [ 4.  4.  4.  5.  6.]\n#                         [ 4.  4.  4.  5.  6.]\n#                         [ 4.  4.  4.  5.  6.]\n#                         [ 4.  4.  4.  5.  6.]]]]`.\ntiled_support = tf.tile([clipped_support], [1, 1, num_dims, 1])\n# Ex: `reshaped_target_support = [[[ 4.]\n#                                  [ 5.]\n#                                  [ 6.]\n#                                  [ 7.]\n#                                  [ 8.]]\n#                                 [[ 4.]\n#                                  [ 5.]\n#                                  [ 6.]\n#                                  [ 7.]\n#                                  [ 8.]]]`.\nreshaped_target_support = tf.tile(target_support[:, None], [batch_size, 1])\nreshaped_target_support = tf.reshape(reshaped_target_support,\n                                     [batch_size, num_dims, 1])\n# numerator = `|clipped_support - z_i|` in Eq7.\n# Ex: `numerator = [[[[ 0.  0.  0.  2.  4.]\n#                     [ 1.  1.  1.  1.  3.]\n#                     [ 2.  2.  2.  0.  2.]\n#                     [ 3.  3.  3.  1.  1.]\n#                     [ 4.  4.  4.  2.  0.]]\n#                    [[ 0.  0.  0.  1.  2.]\n#                     [ 1.  1.  1.  0.  1.]\n#                     [ 2.  2.  2.  1.  0.]\n#                     [ 3.  3.  3.  2.  1.]\n#                     [ 4.  4.  4.  3.  2.]]]]`.\nnumerator = tf.abs(tiled_support - reshaped_target_support)\nquotient = 1 - (numerator / delta_z)\n# clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n# Ex: `clipped_quotient = [[[[ 1.  1.  1.  0.  0.]\n#                            [ 0.  0.  0.  0.  0.]\n#                            [ 0.  0.  0.  1.  0.]\n#                            [ 0.  0.  0.  0.  0.]\n#                            [ 0.  0.  0.  0.  1.]]\n#                           [[ 1.  1.  1.  0.  0.]\n#                            [ 0.  0.  0.  1.  0.]\n#                            [ 0.  0.  0.  0.  1.]\n#                            [ 0.  0.  0.  0.  0.]\n#                            [ 0.  0.  0.  0.  0.]]]]`.\nclipped_quotient = tf.clip_by_value(quotient, 0, 1)\n# Ex: `weights = [[ 0.1  0.6  0.1  0.1  0.1]\n#                 [ 0.1  0.2  0.5  0.1  0.1]]`.\nweights = weights[:, None, :]\n# inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x', \\pi(x'))`\n# in Eq7.\n# Ex: `inner_prod = [[[[ 0.1  0.6  0.1  0.  0. ]\n#                      [ 0.   0.   0.   0.  0. ]\n#                      [ 0.   0.   0.   0.1 0. ]\n#                      [ 0.   0.   0.   0.  0. ]\n#                      [ 0.   0.   0.   0.  0.1]]\n#                     [[ 0.1  0.2  0.5  0.  0. ]\n#                      [ 0.   0.   0.   0.1 0. ]\n#                      [ 0.   0.   0.   0.  0.1]\n#                      [ 0.   0.   0.   0.  0. ]\n#                      [ 0.   0.   0.   0.  0. ]]]]`.\ninner_prod = clipped_quotient * weights\n# Ex: `projection = [[ 0.8 0.0 0.1 0.0 0.1]\n#                    [ 0.8 0.1 0.1 0.0 0.0]]`.\nprojection = tf.reduce_sum(inner_prod, 3)\nprojection = tf.reshape(projection, [batch_size, num_dims])\nreturn projection\n</code></pre>\n\n<p>Thank you in advance!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 89}]
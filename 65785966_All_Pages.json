[{"items": [{"tags": ["python", "tensorflow", "pytorch", "tensorflow2.0"], "owner": {"account_id": 15435556, "reputation": 355, "user_id": 11135757, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/6c44eec0f7fdb44cbec9e6289a633592?s=256&d=identicon&r=PG&f=1", "display_name": "Sphinx", "link": "https://stackoverflow.com/users/11135757/sphinx"}, "is_answered": true, "view_count": 75, "answer_count": 1, "score": 0, "last_activity_date": 1611036021, "creation_date": 1611034332, "question_id": 65785966, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65785966/tensorflow-autodiff-slower-than-pytorchs-counterpart", "title": "tensorflow autodiff slower than pytorch&#39;s counterpart", "body": "<p>I am using tensorflow 2.0 and trying to evaluate gradients for backpropagating to a simple feedforward neural network. Here's how my model looks like:</p>\n<pre><code>def __init__(self, input_size, output_size):\n        inputs = tf.keras.Input(shape=(input_size,))\n        hidden_layer1 = tf.keras.layers.Dense(30, activation='relu')(inputs)\n        outputs = tf.keras.layers.Dense(output_size)(hidden_layer1)\n        self.model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n        self.loss_function = tf.keras.losses.Huber()\n</code></pre>\n<p>The forward pass to this network is fine but when I use gradient tape to train the model, it is at least 10x slower than PyTorch.\nTraining function:</p>\n<pre><code>def learn_modified_x(self, inputs, targets, actions):\n        with tf.GradientTape() as tape:\n            predictions = self.model(inputs)\n            predictions_for_action = gather_single_along_axis(predictions, actions)\n            loss = self.loss_function(targets, predictions_for_action)\n\n        grads = tape.gradient(loss, self.model.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n</code></pre>\n<p>I tried commenting lines to find what is actually causing the problem. I discovered that tape.gradient is a significant contributor to this situation.</p>\n<p>Any idea?</p>\n<p>PyTorch implementation</p>\n<pre><code>    def __init__(self, input_size, nb_action):\n        super(Network, self).__init__()\n        self.input_size = input_size\n        self.nb_action = nb_action\n        self.fc1 = nn.Linear(input_size, 30)\n        self.fc2 = nn.Linear(30, nb_action)\n    \n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        q_values = self.fc2(x)\n        return q_values\n\n    def learn(self, batch_state, batch_next_state, batch_reward, batch_action):\n        outputs = self.model(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n        next_outputs = self.model(batch_next_state).detach().max(1)[0]\n        target = self.gamma*next_outputs + batch_reward\n        td_loss = F.smooth_l1_loss(outputs, target)\n        self.optimizer.zero_grad()\n        td_loss.backward(retain_variables = True)\n        self.optimizer.step()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 225}]
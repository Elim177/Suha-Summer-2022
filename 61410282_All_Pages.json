[{"items": [{"tags": ["python", "python-3.x", "tensorflow", "tensorflow2.0", "gradienttape"], "owner": {"account_id": 18227110, "reputation": 55, "user_id": 13266681, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d3481f2710eb0ca218c5150760b18cff?s=256&d=identicon&r=PG&f=1", "display_name": "MichiTrain", "link": "https://stackoverflow.com/users/13266681/michitrain"}, "is_answered": true, "view_count": 949, "accepted_answer_id": 61411060, "answer_count": 1, "score": 2, "last_activity_date": 1587739514, "creation_date": 1587737167, "question_id": 61410282, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61410282/problem-computing-partial-derivatives-with-gradienttape-in-tensorflow2", "title": "Problem computing partial derivatives with GradientTape() in TensorFlow2", "body": "<p>i have problems in the computation of gradients using automatic differentiation in TensorFlow. Basically i want to create a neural network which has just one output-value f and get an input of two values (x,t). The network should act like a mathematical function, so in this case f(x,t) where x and t are the input-variables and i want to compute partial derivatives, for example <code>df_dx, d2f/dx2</code> or <code>df_dt</code>. I need those partial derivatives later for a specific loss-function.\nHere is my simplified code:</p>\n\n<pre><code>import numpy as np\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras import Model\n\n\nclass MyModel(Model):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.flatten = Flatten(input_shape=(2, 1))\n        self.d1 = Dense(28)\n        self.f = Dense(1)\n\n    def call(self, y):\n        y = self.flatten(y)\n        y = self.d1(y)\n        y = self.f(y)\n        return y\n\nif __name__ == \"__main__\":\n\n    #inp contains the input-variables (x,t)\n    inp = np.random.rand(1,2,1)\n    inp_tf = tf.convert_to_tensor(inp, np.float32)   \n\n    #Create a Model\n    model = MyModel()\n\n    #Here comes the important part:\n    x = inp_tf[0][0]\n    t = inp_tf[0][1]\n\n    with tf.GradientTape(persistent=True) as tape:\n        tape.watch(inp_tf[0][0])\n        tape.watch(inp_tf)\n        f = model(inp_tf)\n\n    df_dx = tape.gradient(f, inp_tf[0][0])  #Derivative df_dx\n    grad_f = tape.gradient(f, inp_tf)\n\n    tf.print(f)         #--&gt; [[-0.0968768075]]\n    tf.print(df_dx)     #--&gt; None\n    tf.print(grad_f)    #--&gt; [[[0.284864038]\n                        #      [-0.243642956]]]\n</code></pre>\n\n<p>What i expected was that i get <code>df_dx = [0.284864038]</code> (the first component of grad_f), but it results in <code>None</code>. My questions are:</p>\n\n<ol>\n<li>Is it possible to get partial derivatives of f to only one input-variable?</li>\n<li>If yes: What i have to change in my code that the computation df_dx doesn't result <code>None</code>?</li>\n</ol>\n\n<p>What i think could do is to modify the architecture of the <code>class MyModel</code> that i use two different Inputlayer (one for x and one for t) so that i can call the model like <code>f = model(x,t)</code> but that seems unnatural for me and i think there should be an easier way.</p>\n\n<hr>\n\n<p>Another point is that i don't get an Error when i change the input_shape of the Flattenlayer for example to <code>self.flatten = Flatten(input_shape=(5,1)</code> but my inputvector has shape(1,2,1), so i expect to get an error but that's not the case, why? I'm grateful for your help :)</p>\n\n<hr>\n\n<p>I use the following configurations:</p>\n\n<ul>\n<li>Visual Studio Code with Python-Extension as IDE</li>\n<li>Python-Version: 3.7.6</li>\n<li>TensorFlow-Version: 2.1.0</li>\n<li>Keras-Version: 2.2.4-tf</li>\n</ul>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 137}]
[{"items": [{"tags": ["python", "tensorflow", "random", "keras", "reproducible-research"], "owner": {"account_id": 7277007, "reputation": 17010, "user_id": 10133797, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ElNKG.png?s=256&g=1", "display_name": "OverLordGoldDragon", "link": "https://stackoverflow.com/users/10133797/overlordgolddragon"}, "is_answered": false, "view_count": 240, "answer_count": 0, "score": 2, "last_activity_date": 1578685735, "creation_date": 1572733745, "last_edit_date": 1578685735, "question_id": 58675856, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58675856/why-does-stacking-cnn-wreck-reproducibility-even-with-seed-cpu", "title": "Why does stacking CNN wreck reproducibility (even with seed &amp; CPU)?", "body": "<p><strong>REPRODUCIBLE</strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ipt = Input(batch_shape=batch_shape)\nx   = Conv2D(6, (8, 8), strides=(2, 2), activation='relu')(ipt)\nx   = Flatten()(x)\nout = Dense(6, activation='softmax')(x)  \n</code></pre>\n\n<p><strong>NOT REPRODUCIBLE</strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ipt = Input(batch_shape=batch_shape)\nx   = Conv2D(6, (8, 8), strides=(2, 2), activation='relu')(ipt)\nx   = Conv2D(6, (8, 8), strides=(2, 2), activation='relu')(x)\nx   = Flatten()(x)\nout = Dense(6, activation='softmax')(x)\n</code></pre>\n\n<hr>\n\n<p>The difference amplifies substantially when using a larger model, and actual data instead of random noise - up to <strong>30% difference in accuracy</strong> (relative) <strong>within a single small epoch</strong>. Environment setup, considered sources, and full minimal reproducible example below. <a href=\"https://github.com/tensorflow/tensorflow/issues/33942\" rel=\"nofollow noreferrer\">Relevant Git</a></p>\n\n<p>What is the problem, and how to fix it?</p>\n\n<hr>\n\n<p><strong>POSSIBLE SOURCES</strong>: (<strong>[x]</strong> = ruled out)</p>\n\n<ul>\n<li><strong>[x]</strong> TF2 vs. TF1; Keras 2.3.0+ vs. Keras 2.2.5 (tested both)</li>\n<li><strong>[x]</strong> Random seeds (<code>numpy</code>, <code>tf</code>, <code>random</code>, <code>PYTHONHASHSEED</code>)</li>\n<li><strong>[x]</strong> Data values / shuffling (same values, no shuffling)</li>\n<li><strong>[x]</strong> Weight initializations (same values)</li>\n<li><strong>[x]</strong> GPU usage (used CPU)</li>\n<li><strong>[x]</strong> CPU multithreading (used single thread; also see below's 'further')</li>\n<li><strong>[x]</strong> Numeric imprecision (used float64; further, extent of discrepancy too large for num. impr.)</li>\n<li><strong>[x]</strong> Bad CUDA install (all <a href=\"https://docs.nvidia.com/cuda/archive/10.0/cuda-installation-guide-microsoft-windows/index.html\" rel=\"nofollow noreferrer\">official guide</a> tests passed, TF detects GPU &amp; CUDA)</li>\n</ul>\n\n<hr>\n\n<p><strong>ENVIRONMENT</strong>: </p>\n\n<ul>\n<li>CUDA 10.0.130, cuDNN 7.6.0, Windows 10, GTX 1070</li>\n<li>Python 3.7.4, Spyder 3.3.6, Anaconda 3.0 2019.10</li>\n<li>Anaconda Powershell Prompt terminal to set <code>PYTHONHASHSEED</code> and start Spyder</li>\n</ul>\n\n<hr>\n\n<p><strong>OBSERVATIONS</strong>:</p>\n\n<ul>\n<li><code>float64</code> vs. <code>float32</code> - no noticeable difference</li>\n<li>CPU vs. GPU - no noticeable difference</li>\n<li>Non-reproducible also for <code>Conv1D</code></li>\n<li>Reproducible for <code>Dense</code> replacing <code>Conv</code>; other layers not tested</li>\n<li>For a <a href=\"https://pastebin.com/VSV7jv7x\" rel=\"nofollow noreferrer\">larger model</a>, which is still 'small', loss variance is substantial within a single epoch:</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>one_epoch_loss = [1.6814, 1.6018, 1.6577, 1.6789, 1.6878, 1.7022, 1.6689]\none_epoch_acc  = [0.2630, 0.3213, 0.2991, 0.3185, 0.2583, 0.2463, 0.2815]\n</code></pre>\n\n<hr>\n\n<p><strong>CODE</strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>batch_shape = (32, 64, 64, 3)\nnum_samples = 1152\n\nipt = Input(batch_shape=batch_shape)\nx   = Conv2D(6, (8, 8), strides=(2, 2), activation='relu')(ipt)\nx   = Conv2D(6, (8, 8), strides=(2, 2), activation='relu')(x)\nx   = Flatten()(x)\nout = Dense(6, activation='softmax')(x)\nmodel = Model(ipt, out)\nmodel.compile('adam', 'sparse_categorical_crossentropy')\n\nX = np.random.randn(num_samples, *batch_shape[1:])\ny = np.random.randint(0, 6, (num_samples, 1))\n\nreset_seeds()\nmodel.fit(x_train, y_train, epochs=5, shuffle=False)\n</code></pre>\n\n<hr>\n\n<p><strong>Imports / setup</strong>:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['PYTHONHASHSEED'] = '0'\nos.environ['CUDA_VISIBLE_DEVICES'] = '-1'\nimport numpy as np\nnp.random.seed(1)\nimport random\nrandom.seed(2)\n\nimport tensorflow as tf\nsession_conf = tf.ConfigProto(\n      intra_op_parallelism_threads=1,\n      inter_op_parallelism_threads=1)\nsess = tf.Session(config=session_conf) # single-threading; TF1-only\n\ndef reset_seeds():\n    np.random.seed(1)\n    random.seed(2)\n    if tf.__version__[0] == '2':\n        tf.random.set_seed(3)\n    else:\n        tf.set_random_seed(3)\n    print(\"RANDOM SEEDS RESET\")\nreset_seeds()\n\nfrom keras.layers import Input, Dense, Conv2D, Flatten\nfrom keras.models import Model\nimport keras.backend as K\n\nK.set_floatx('float64')\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 256}]
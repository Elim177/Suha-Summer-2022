[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 26373, "reputation": 43816, "user_id": 68571, "user_type": "registered", "accept_rate": 79, "profile_image": "https://i.stack.imgur.com/Yw9Lg.png?s=256&g=1", "display_name": "VansFannel", "link": "https://stackoverflow.com/users/68571/vansfannel"}, "is_answered": false, "view_count": 418, "answer_count": 0, "score": 1, "last_activity_date": 1494241349, "creation_date": 1494088142, "last_edit_date": 1494241349, "question_id": 43822818, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/43822818/attempt-to-reuse-rnncell-with-a-different-variable-scope", "title": "Attempt to reuse RNNCell with a different variable scope", "body": "<p>I'm using Tensorflow 1.1.0 with GPU support and I have this function:</p>\n\n<pre><code>def get_init_cell(batch_size, rnn_size, keep_prob=0.75, layers=2):\n    \"\"\"\n    Create an RNN Cell and initialize it.\n    :param batch_size: Size of batches\n    :param rnn_size: Size of RNNs\n    :return: Tuple (cell, initialize state)\n    \"\"\"\n    # Basic LSTM cell\n    # lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size, forget_bias=0.0, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n\n    # Add drop to the cell\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n\n    # Stack multiple LSTM layers\n    rnn_cell = tf.contrib.rnn.MultiRNNCell([drop for _ in range(layers)], state_is_tuple=True)\n\n    # Getting an initial state of zeros\n    initial_state = rnn_cell.zero_state(batch_size, tf.float32)\n\n    # Set the name\n    initial_state = tf.identity(initial_state, 'initial_state')\n\n    return rnn_cell, initial_state\n</code></pre>\n\n<p>The previous code works perfectly with Tensorflow 1.0.0 if I use <code>lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)</code> but now I have changed the way I get the lstm cell. And I get this error:</p>\n\n<blockquote>\n  <p>ValueError: Attempt to reuse RNNCell\n   with a different variable scope than its\n  first use.  First use of cell was with scope\n  'rnn/multi_rnn_cell/cell_0/basic_lstm_cell', this attempt is with\n  scope 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell'.  Please create a\n  new instance of the cell if you would like it to use a different set\n  of weights.  If before you were using:\n  MultiRNNCell([BasicLSTMCell(...)] * num_layers), change to:\n  MultiRNNCell([BasicLSTMCell(...) for _ in range(num_layers)]).  If\n  before you were using the same cell instance as both the forward and\n  reverse cell of a bidirectional RNN, simply create two instances (one\n  for forward, one for reverse).  In May 2017, we will start\n  transitioning this cell's behavior to use existing stored weights, if\n  any, when it is called with scope=None (which can lead to silent model\n  degradation, so this error will remain until then.)</p>\n</blockquote>\n\n<p>I have changed <code>get_init_cell</code> with this one:</p>\n\n<pre><code>def lstm_cell(rnn_size, keep_prob=0.75):\n    # Basic LSTM cell\n    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size, reuse=tf.get_variable_scope().reuse)\n\n    # Add drop to the cell\n    return tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n\ndef get_init_cell(batch_size, rnn_size, keep_prob=0.75, layers=2):\n    \"\"\"\n    Create an RNN Cell and initialize it.\n    :param batch_size: Size of batches\n    :param rnn_size: Size of RNNs\n    :return: Tuple (cell, initialize state)\n    \"\"\"    \n    # Stack multiple LSTM layers\n    rnn_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(layers)])\n\n    # Getting an initial state of zeros\n    initial_state = rnn_cell.zero_state(batch_size, tf.float32)\n\n    # Set the name\n    initial_state = tf.identity(initial_state, 'initial_state')\n\n    return rnn_cell, initial_state\n</code></pre>\n\n<p>And I still getting the same error.</p>\n\n<p>I have also try it with the same error:</p>\n\n<pre><code>def get_init_cell(batch_size, rnn_size, keep_prob=0.75, layers=2):\n    \"\"\"\n    Create an RNN Cell and initialize it.\n    :param batch_size: Size of batches\n    :param rnn_size: Size of RNNs\n    :return: Tuple (cell, initialize state)\n    \"\"\"    \n\n    def lstm_cell():\n        # Basic LSTM cell\n        return tf.contrib.rnn.BasicLSTMCell(rnn_size, reuse=tf.get_variable_scope().reuse)        \n\n    def attn_cell():\n        return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=keep_prob)\n\n    # Stack multiple LSTM layers\n    rnn_cell = tf.contrib.rnn.MultiRNNCell([attn_cell() for _ in range(layers)], state_is_tuple=True)\n\n    # Getting an initial state of zeros\n    initial_state = rnn_cell.zero_state(batch_size, tf.float32)\n\n    # Set the name\n    initial_state = tf.identity(initial_state, 'initial_state')\n\n    return rnn_cell, initial_state\n</code></pre>\n\n<p>Any idea?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 296}]
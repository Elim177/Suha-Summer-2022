[{"items": [{"tags": ["python", "numpy", "tensorflow", "machine-learning", "tensorflow2.0"], "owner": {"account_id": 9183870, "reputation": 175, "user_id": 7091135, "user_type": "registered", "profile_image": "https://lh5.googleusercontent.com/-KgAhAHh26-I/AAAAAAAAAAI/AAAAAAAAABk/XoaviWudNQ0/photo.jpg?sz=256", "display_name": "Alexander Lyapin", "link": "https://stackoverflow.com/users/7091135/alexander-lyapin"}, "is_answered": true, "view_count": 720, "accepted_answer_id": 63920691, "answer_count": 1, "score": 2, "last_activity_date": 1600261515, "creation_date": 1600252530, "last_edit_date": 1600254905, "question_id": 63918192, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63918192/how-to-properly-apply-gradients-in-tensorflow-v2-0", "title": "How to properly apply gradients in TensorFlow v2.0", "body": "<p>I'm trying to reproduce some neural transfer tutorial which previously based on <code>TensorFlow v1.x</code> but using <code>TensorFlow v2.0</code>.\nFor some reason, I can't apply gradients with <code>Adam optimizer</code> due to, I believe, some data format.\nThe source is quite large so I tried to give just most relevant lines, but if someone asks about full code I will add it here.</p>\n<p>Here is the optimizer</p>\n<pre class=\"lang-py prettyprint-override\"><code>    opt = tf.optimizers.Adam(learning_rate = 5, beta_1 = 0.99, epsilon = 1e-1)\n</code></pre>\n<p>And when I'm trying to apply gradients to initial variables using</p>\n<pre class=\"lang-py prettyprint-override\"><code>    opt.apply_gradients(zip(grads, init_image))\n</code></pre>\n<p>I'm getting error</p>\n<pre><code>!opt.apply_gradients(zip(grads, init_image))\n*** TypeError: zip argument #2 must support iteration\n</code></pre>\n<p>These are the zip arguments:</p>\n<pre><code>init_image\n&lt;tf.Variable 'Variable:0' shape=(1, 345, 512, 3) dtype=float32, numpy=\narray([[[[...]]]], dtype=float32)&gt;\n\ngrads\n&lt;tf.Tensor: shape=(1, 345, 512, 3), dtype=float32, numpy=\narray([[[[...]]]], dtype=float32)&gt;\n</code></pre>\n<p>As you can see, the shape is the same as it should be, values inside <code>numpy</code> are some real values, not <code>nans</code> or something like that.</p>\n<p>The gradients are calculated with:</p>\n<pre class=\"lang-py prettyprint-override\"><code>    with tf.GradientTape() as tape: \n         ...computing all_loss...\n    total_loss = all_loss[0]\n    grads = tape.gradient(total_loss, init_image)\n</code></pre>\n<p>Any suggestions please.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 56}]
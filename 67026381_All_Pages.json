[{"items": [{"tags": ["tensorflow", "keras", "tensorflow2.0", "tensor", "batch-normalization"], "owner": {"account_id": 9968983, "reputation": 2869, "user_id": 7375754, "user_type": "registered", "accept_rate": 77, "profile_image": "https://www.gravatar.com/avatar/830f6d236ab0502a68fd9b9daf7f729f?s=256&d=identicon&r=PG&f=1", "display_name": "Jane Sully", "link": "https://stackoverflow.com/users/7375754/jane-sully"}, "is_answered": true, "view_count": 402, "accepted_answer_id": 67035100, "answer_count": 1, "score": 3, "last_activity_date": 1618341900, "creation_date": 1617992317, "last_edit_date": 1618057018, "question_id": 67026381, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67026381/how-to-specify-linear-transformation-in-tensorflow", "title": "How to specify linear transformation in tensorflow?", "body": "<p>I want to perform a simple linear transformation on a layer x, so the output of the transformation is <code>y = a*x + b</code>. I am working with images, so x is 3-dimensional <code>(height * width * channels)</code>. Then <code>a</code> is a scale vector of size <code>c</code>, where <code>c</code> is the number of channels, and it has a single scale parameter for each channel dimension of x. Similarly, <code>b</code> is a shift vector of size and it has a single shift parameter for each channel dimension of <code>x</code>. This is a simple variation of normalization without normalizing the batch statistics.</p>\n<p>Here is an example:</p>\n<pre><code># TODO: learn gamma and beta parameters\nx = tf.keras.layers.Conv2D(filters=num_filters_e1,\n    kernel_size=5,\n    strides=2,\n    padding='same')(input)\nx = tf.keras.layers.Multiply()([x, gamma]) # scale by gamma along channel dim\nx = tf.keras.layers.Add()([x, beta]) # shift with beta along channel dim\ny = tf.keras.layers.ReLU()(x) # apply activation after transformation\n</code></pre>\n<p>I'm not sure about how to obtain gamma and beta. These are supposed to be parameters learned by the model during training, but I'm not sure how to construct or specify them. Typically I just specify layers (either convolutional or dense) to learn weights, but I'm not sure what layer to use here and what the layer should take in as input. Do I have to somehow initialize a vector of ones and then learn the weights to transform those into gamma and beta?</p>\n<p>Even if it is possible to do this with TensorFlow's <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization\" rel=\"nofollow noreferrer\">batchnorm layer</a> (which is still useful to know), I would like to learn how to implement this scaling/shifting from scratch. Thank you!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 170}]
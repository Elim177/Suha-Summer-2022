[{"items": [{"tags": ["python", "tensorflow", "keras", "deep-learning", "tensorflow2.0"], "owner": {"account_id": 17364858, "reputation": 31, "user_id": 12580264, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/9e5a35383e613d7206efa522b4e62f6a?s=256&d=identicon&r=PG&f=1", "display_name": "Agostino Dorano", "link": "https://stackoverflow.com/users/12580264/agostino-dorano"}, "is_answered": false, "view_count": 184, "answer_count": 1, "score": 0, "last_activity_date": 1605516091, "creation_date": 1605439601, "last_edit_date": 1605442530, "question_id": 64843826, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64843826/how-to-customize-what-happen-in-fit-method-in-tensorflow-2-0", "title": "How to customize what happen in fit method in tensorflow 2.0", "body": "<p>I'm studing how to customize what happen in fit method with tensorflow 2.0 and I'm following this link: <a href=\"https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit\" rel=\"nofollow noreferrer\">https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit</a>, but I noticed some difference in code at previous link; in particular it shown first this way to train model:</p>\n<pre><code>with tf.GradientTape() as tape:\n           y_pred = self(x, training=True)  # Forward pass\n           # Compute the loss value\n           # (the loss function is configured in `compile()`)\n           loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n           print(loss)\n</code></pre>\n<p>and after this one:</p>\n<pre><code>with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute our own loss\n            loss = keras.losses.mean_squared_error(y, y_pred)\n            print(loss.shape)\n</code></pre>\n<p>As you can see the difference is in the computation of loss function: in the first block they use a compiled loss function, whereas in the second block loss is computed with <code>mean_squared_error</code> function.\nMy doubts arise when I check the shape of the loss: in first case it is a scalar like this: <code>(tf.Tensor(0.21193008, shape=(), dtype=float32)</code> while in the second the print statement give me a tensor of shape <code>(32,)</code>.\nMy idea it is that in second case the mean over batch isn't computed and I don't know why.\nI solved using the <code>tf.reduce_mean</code> but I'm not sure if it is correct. However the loss should be a scalar and therefore I don't understand how the second mode works without <code>tf.reduce_mean</code> and if the usage of this statement is correct.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 87}]
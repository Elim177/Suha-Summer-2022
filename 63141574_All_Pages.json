[{"items": [{"tags": ["matplotlib", "jupyter-notebook", "reinforcement-learning"], "owner": {"account_id": 1484715, "reputation": 1099, "user_id": 1393214, "user_type": "registered", "accept_rate": 85, "profile_image": "https://www.gravatar.com/avatar/63c9e6d4bcd174aba5445c7fc62acfb1?s=256&d=identicon&r=PG&f=1", "display_name": "User", "link": "https://stackoverflow.com/users/1393214/user"}, "is_answered": false, "view_count": 56, "answer_count": 1, "score": 0, "last_activity_date": 1595967522, "creation_date": 1595964948, "question_id": 63141574, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63141574/matplotlib-plot-does-not-show-up-in-jupyter-notebook", "title": "matplotlib plot does not show up in jupyter notebook", "body": "<p>I'd like to reproduce the learning curve below (in jupyter notebook) which corresponds to the example of the page 637 of the Geron book.</p>\n<p><a href=\"https://i.stack.imgur.com/DD2AG.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/DD2AG.png\" alt=\"enter image description here\" /></a></p>\n<p>In particular, here is the code I use:</p>\n<pre><code>%pylab inline\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tf import keras\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import deque\nimport gym\n\nenv = gym.make(&quot;CartPole-v1&quot;)\ninput_shape = [4]\nn_outputs = 2\nreplay_buffer = deque(maxlen = 1000)\nbatch_size = 32\ndiscount_factor = 0.95\n\nmodel = keras.models.sequential([\n    keras.layers.Dense(32, activation=&quot;elu&quot;, input_shape = input_shape),\n    keras.layers.Dense(32, activation=&quot;elu&quot;),\n    keras.layers.Dense(n_outputs)\n])\n\ndef epsilon_greedy_policy(state, epsilon=0):\n    if np.random.rand() &lt; epsilon:\n        return np.random.randint(2)\n    else:\n        Q_values = model.predict(state[np.newaxis])\n        return np.argmax(Q_values[0])\n\ndef sample_experiences(batch_size):\n    indices = np.random.randint(len(replay_buffer), size = batch_size)\n    batch = [replay_buffer[index] for index in indices]\n    states, actions, rewards, next_states, dones = [\n        np.array([experience[field_index] for experience in batch])\n        for field_index in range(5)]\n    return states, actions, rewards, next_states, dones\n\ndef play_one_step(env, state, epsilon):\n    action = epsilon_greedy_policy(state, epsilon)\n    next_state, reward, done, info = env.step(action)\n    replay_buffer.append((state, actions, reward, next_state, done))\n    return next_state, reward, done, info\n\noptimizer = keras.optimizer.Adam(lr=1e-3)\nloss_fn = keras.losses.mean_squared_error\n\ndef training_step(batch_size):\n    experiences = sample_experiences(batch_size)\n    states, actions, rewards, next_states, dones = experiences\n    next_Q_values = model.predict(next_states)\n    max_next_Q_values = np.max(next_Q_values, axis=1)\n    target_Q_values = (rewards + (1-dones) * discount_factor 8 max_next_Q_values)\n    mask = tf.one_hot(actions, n_outputs)\n    with tf.GradientTape as tape:\n        all_Q_values = model(states)\n        Q_values = tf.reduce_mean(all_Q_values * mask, axis = 1, keepdims = True)\n        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.appy_gradients(zip(grads, model.trainable_variables))\n\nep = []\nrew = []\n    \nfor episode in range(600):\n    obs = env.reset()\n    for step in range(200):\n        epsilon = max(1 - episode / 500, 0.01)\n        obs, reward, done, info = play_one_step(env, obs, epsilon)\n        ep.append(epsilon)\n        rew.append(reward)\n        if done:\n            break\n    if episode &gt; 50:\n        training_step(batch_size)\n\nplt.plot(ep, rew)\nplt.show()\n</code></pre>\n<p>The code is exactly as represented in the book except the following points.</p>\n<p>1- I added</p>\n<pre><code>%pylab inline\n%matplotlib inline\n</code></pre>\n<p>2- I added <code>ep</code> and <code>rew</code> lists in the training loop to later plot their content.</p>\n<p>However, when I run the above, nothing happens. can anyone guide me through this?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 71}]
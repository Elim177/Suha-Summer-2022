[{"items": [{"tags": ["tensorflow", "machine-learning", "keras"], "owner": {"account_id": 19439210, "reputation": 53, "user_id": 14218002, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/d4ffbb3577b2989b8a8e94b9cb737478?s=256&d=identicon&r=PG&f=1", "display_name": "dhn", "link": "https://stackoverflow.com/users/14218002/dhn"}, "is_answered": false, "view_count": 133, "answer_count": 1, "score": 0, "last_activity_date": 1607398136, "creation_date": 1606834068, "last_edit_date": 1607398136, "question_id": 65092750, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65092750/transformers-for-action-recognition-resource-exhausted", "title": "Transformers for action recognition: Resource exhausted", "body": "<p>I am trying to adapt a transformers code from <a href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\" rel=\"nofollow noreferrer\">https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb</a> and use it in action recognition. But I keep getting Resource exhausted: OOM when allocating. I have a RTX Titan which has 24gb. I find it very strange to be running into this kinda of problem. My dataset is composed of 1000 actions with variable length frame (N) where each frame contains 84 float32 points (x, y). I combine N and points to form a fairly big 1d tensor for every action. Only 1 action has tensor length 40K all the others are around 10K, 20K etc.\nMy batch = 1, num_layers = 2, d_model = 32, dff = 64, num_heads = 2.</p>\n<p>A couple of batches are able to run before giving me this error.</p>\n<blockquote>\n<p>EPOCH: 0\ntf.Tensor([[336.655 324.18  310.146 ... 252.652 260.521 260.083]],\nshape=(1, 15960), dtype=float32) tf.Tensor([[783 499 572  19 784]],\nshape=(1, 5), dtype=int64)\nEpoch 1 Batch 0 Loss 6.3220 Accuracy 0.2500</p>\n<p>tf.Tensor([[323.237 320.201 310.713 ... 223.767 226.396 226.396]],\nshape=(1, 13020), dtype=float32) tf.Tensor([[783  42  50 784]],\nshape=(1, 4), dtype=int64)\nEpoch 1 Batch 1 Loss 6.2927 Accuracy 0.2917</p>\n<p>tf.Tensor([[343.387 331.561 316.581 ... 263.883 260.453 255.308]],\nshape=(1, 12096), dtype=float32) tf.Tensor([[783  46 784]], shape=(1,\n3), dtype=int64)\nEpoch 1 Batch 2 Loss 6.1787 Accuracy 0.3611</p>\n<p>tf.Tensor([[320.014 317.322 306.94  ... 219.537 220.311 221.472]],\nshape=(1, 10332), dtype=float32) tf.Tensor([[783 334 784]], shape=(1,\n3), dtype=int64)\nEpoch 1 Batch 3 Loss 6.1479 Accuracy 0.3958</p>\n</blockquote>\n<blockquote>\n<p>tf.Tensor([[224.648 218.128 208.176 ... 188.814 191.243 196.101]],\nshape=(1, 27300), dtype=float32) tf.Tensor([[783 350 784]], shape=(1,\n3), dtype=int64)</p>\n</blockquote>\n<p>Below is the error I am getting:</p>\n<blockquote>\n<p>2 root error(s) found.   (0) Resource exhausted:  OOM when allocating\ntensor with shape[1,2,31332,31332] and type float on\n/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc<br />\n[[node\ntransformer_1/encoder_2/encoder_layer_5/multi_head_attention_16/Softmax\n(defined at :32) ]] Hint: If you want\nto see a list of allocated tensors when OOM happens, add\nreport_tensor_allocations_upon_oom to RunOptions for current\nallocation info.</p>\n<p>[[gradient_tape/transformer_1/encoder_2/embedding_4/embedding_lookup/Reshape/_280]]\nHint: If you want to see a list of allocated tensors when OOM happens,\nadd report_tensor_allocations_upon_oom to RunOptions for current\nallocation info.</p>\n<p>(1) Resource exhausted:  OOM when allocating tensor with\nshape[1,2,31332,31332] and type float on\n/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc<br />\n[[node\ntransformer_1/encoder_2/encoder_layer_5/multi_head_attention_16/Softmax\n(defined at :32) ]] Hint: If you want\nto see a list of allocated tensors when OOM happens, add\nreport_tensor_allocations_upon_oom to RunOptions for current\nallocation info.</p>\n<p>0 successful operations. 0 derived errors ignored.\n[Op:__inference_train_step_20907]</p>\n<p>Errors may have originated from an input operation. Input Source\noperations connected to node\ntransformer_1/encoder_2/encoder_layer_5/multi_head_attention_16/Softmax:\ntransformer_1/encoder_2/encoder_layer_5/multi_head_attention_16/add\n(defined at :28)</p>\n<p>Input Source operations connected to node\ntransformer_1/encoder_2/encoder_layer_5/multi_head_attention_16/Softmax:\ntransformer_1/encoder_2/encoder_layer_5/multi_head_attention_16/add\n(defined at :28)</p>\n<p>Function call stack: train_step -&gt; train_step</p>\n</blockquote>\n<p>::UPDATED PROBLEM::</p>\n<p>So I reduced my tensor input shape. I was able to run it without resource exhaustion BUT I am running into another problem. I keep getting:</p>\n<blockquote>\n<p>2 root error(s) found.<br />\n(0) Invalid argument:  indices[0,1923] = -1\nis not in [0, 12936)   [[node\ntransformer_1/encoder_2/embedding_4/embedding_lookup (defined at\n:24) ]]   (1) Invalid argument:\nindices[0,1923] = -1 is not in [0, 12936)      [[node\ntransformer_1/encoder_2/embedding_4/embedding_lookup (defined at\n:24) ]]<br />\n[[transformer_1/encoder_2/embedding_4/embedding_lookup/_24]] 0\nsuccessful operations. 0 derived errors ignored.\n[Op:__inference_train_step_17044]</p>\n<p>Function call stack: train_step -&gt; train_step</p>\n</blockquote>\n<p>if my tensor inputs are less than 3000 elements, I can run successfully run it but higher I get the above error. Has anyone run into this kinda of problem ? I have no idea what error means or how to fix it :(</p>\n<p>any help again is appreciated</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 127}]
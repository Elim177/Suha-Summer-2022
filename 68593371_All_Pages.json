[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 20104244, "reputation": 13, "user_id": 14742134, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Gg0Pdo4uptiMkiRVYktzPd84wk11qX8cusXesDx=k-s256", "display_name": "Oskar Zdrojewski", "link": "https://stackoverflow.com/users/14742134/oskar-zdrojewski"}, "is_answered": false, "view_count": 65, "answer_count": 1, "score": 0, "last_activity_date": 1627685494, "creation_date": 1627658179, "last_edit_date": 1627683658, "question_id": 68593371, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68593371/why-is-tensorflow-unable-to-make-a-gradient-here", "title": "Why is Tensorflow unable to make a Gradient here?", "body": "<p>I get this warning (twice):\nWARNING:tensorflow:Gradients do not exist for variables ['my_dense_layer/why is this of not work:0'] when minimizing the loss.</p>\n<p>I want to train the weights in self.wm. But apparently it just doesn't work in this case.\nHow this works is just basically a remaping and reshaping to get a Matrix with which to calculate the output. I think it's still just a linear transformation in the end.\nwith tf.GradientTape I am getting an approperiate result. But when I run through a network I get a warning. Maybe somethings wrong with the way it's executed?</p>\n<pre><code>class MyDenseLayer(tf.keras.layers.Layer):\n    def __init__(self, num_outputs):\n        super(MyDenseLayer, self).__init__()\n        self.num_outputs = num_outputs\n\n    def build(self, input_shape):\n        self.wm=self.add_weight(&quot;why is this of not work&quot;,shape=[1,4])\n        self.mapper = tf.constant([[[float(i == k + (2 * (j ** 2))) for i in range(self.num_outputs)] for j in range(4)] for k in range(input_shape[1])],dtype='float32')\n        self.kernel = tf.matmul(self.wm, self.mapper)\n        self.kern = tf.reshape(self.kernel, (input_shape[1],self.num_outputs))\n\n    def call(self, inputs, **kwargs):\n        f=tf.matmul(inputs,self.kern)\n        return f\n\n</code></pre>\n<pre><code>\nmodel = krs.Sequential(\n    [\n        layers.Dense(512,activation='relu'),\n        layers.Dense(256,activation='sigmoid'),\n        layers.Flatten(),\n        customlayers.MyDenseLayer(64),\n        layers.Dense(10),\n    ]\n)\n\nmodel.compile(\n    loss=krs.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=krs.optimizers.Adam(learning_rate=0.001),\n    metrics=[&quot;accuracy&quot;],\n)\n</code></pre>\n<p>EDIT: Removed Numpy</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 195}]
[{"items": [{"tags": ["tensorflow", "lstm"], "owner": {"account_id": 2008558, "reputation": 89, "user_id": 1797679, "user_type": "registered", "accept_rate": 50, "profile_image": "https://www.gravatar.com/avatar/9bda95f86230bd7507c1c4ec315cb59b?s=256&d=identicon&r=PG", "display_name": "arrhhh", "link": "https://stackoverflow.com/users/1797679/arrhhh"}, "is_answered": true, "view_count": 1887, "accepted_answer_id": 43682170, "answer_count": 1, "score": 2, "last_activity_date": 1493388309, "creation_date": 1477422528, "question_id": 40247850, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/40247850/rnn-and-lstm-implementation-in-tensorflow", "title": "RNN and LSTM implementation in tensorflow", "body": "<p>I have been trying to learn how to code up an RNN and LSTM in tensorflow. I found an example online on this blog post</p>\n\n<p><a href=\"http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html\" rel=\"nofollow\">http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html</a></p>\n\n<p>Below are the snippets which I am having trouble understanding for an LSTM network to be used eventually for char-rnn generation</p>\n\n<pre><code>    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n\n    embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n    rnn_inputs = [tf.squeeze(i) for i in tf.split(1,\n                            num_steps, tf.nn.embedding_lookup(embeddings, x))]\n</code></pre>\n\n<h3>Different Section of the Code Now where the weights are defined</h3>\n\n<pre><code> with tf.variable_scope('softmax'):\n         W = tf.get_variable('W', [state_size, num_classes])\n         b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n\n y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n</code></pre>\n\n<p>x is the data to be fed, and y is the set of labels. In the lstm equations we have a series of gates, x(t) gets multiplied by a series and prev_hidden_state gets multiplied by some set of weights, biases are added and non-liniearities are applied. </p>\n\n<h2>Here are the doubts I have</h2>\n\n<ul>\n<li>In this case only one weight matrix is defined does that mean that\nworks for both x(t) and prev_hidden_state as well.</li>\n<li>For the embeddings matrix I know it has to be multiplied by the\nweight matrix but why is the first dimension num_classes</li>\n<li>For the rnn_inputs we are using squeeze which removes dimensions of 1\nbut why would I want to do that in a one-hot-encoding.</li>\n<li>Also from the splits I understand that we are unrolling the x of\ndimension (batch_size X num_steps) into discrete (batch_size X 1)\nvectors and then passing these values through the network is this\nright</li>\n</ul>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 33}]
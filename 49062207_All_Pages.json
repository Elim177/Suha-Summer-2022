[{"items": [{"tags": ["tensorflow", "tf-slim"], "owner": {"account_id": 97477, "reputation": 4181, "user_id": 264410, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/98a0fcc9a9390a0863a901945c45212c?s=256&d=identicon&r=PG", "display_name": "michael", "link": "https://stackoverflow.com/users/264410/michael"}, "is_answered": false, "view_count": 401, "answer_count": 1, "score": 0, "last_activity_date": 1519961055, "creation_date": 1519959974, "question_id": 49062207, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/49062207/why-do-i-have-to-reshape-inputs-from-tf-train-batch-to-use-with-slim-full", "title": "why do I have to reshape `inputs` from `tf.train.batch()` to use with `slim.fully_connected()?", "body": "<p>Why do I get this error for <code>slim.fully_connected()</code>?</p>\n\n<p><code>ValueError: Input 0 of layer fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [32]</code></p>\n\n<p>my input is <code>Tensor(\"batch:0\", shape=(32,), dtype=float32)</code> from <code>tf.train.batch()</code></p>\n\n<pre><code>  inputs, labels = tf.train.batch(\n        [input, label],\n        batch_size=batch_size,\n        num_threads=1,\n        capacity=2 * batch_size)\n</code></pre>\n\n<p>if I reshape the input to <code>(32,1)</code> it works fine.</p>\n\n<pre><code>inputs, targets = load_batch(train_dataset)\nprint(\"inputs:\", inputs, \"targets:\", targets)\n# inputs: Tensor(\"batch:0\", shape=(32,), dtype=float32) targets: Tensor(\"batch:1\", shape=(32,), dtype=float32)\n\ninputs = tf.reshape(inputs, [-1,1])\ntargets = tf.reshape(targets, [-1,1])\n</code></pre>\n\n<p>The examples in <a href=\"https://github.com/tensorflow/models/blob/master/research/slim/slim_walkthrough.ipynb\" rel=\"nofollow noreferrer\">slim walkthrough</a> seem to work without explicitly reshaping after <code>load_batch()</code></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 61}]
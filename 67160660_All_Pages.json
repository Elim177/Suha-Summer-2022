[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 12031545, "reputation": 11, "user_id": 8801619, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/41df4ec0dc375a311f02b3807b44acd2?s=256&d=identicon&r=PG&f=1", "display_name": "Yige", "link": "https://stackoverflow.com/users/8801619/yige"}, "is_answered": false, "view_count": 73, "answer_count": 0, "score": 1, "last_activity_date": 1618836203, "creation_date": 1618829719, "last_edit_date": 1618836203, "question_id": 67160660, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67160660/tensorflow-2-3-0-valueerror-no-gradients-provided-for-any-variable-training", "title": "tensorflow 2.3.0: ValueError: No gradients provided for any variable: (training procedure)", "body": "<p>I am trying to build a hierarchical model for sequence prediction. My input is a sequence (372*49) and my output is also a sequence (372*2). The details of the model are as follows.</p>\n<pre><code>class Local(tf.keras.layers.Layer):\n    def __init__(self, latent_dim=128, \n                intermediate_dim=256, name='local', **kwargs):\n        super(Local, self).__init__(name=name, **kwargs)\n        self.cnn = tf.keras.layers.Conv2D(filters=64, kernel_size=(7,1), padding='valid', \n                                                 input_shape=(7,7,1), activation='relu')\n        self.bn = tf.keras.layers.BatchNormalization()\n        self.dropout = tf.keras.layers.Dropout(0.2)\n        self.reshape = tf.keras.layers.Reshape((7,64))\n        self.lstm = tf.keras.layers.LSTM(200, return_sequences=False, stateful=False)\n        self.dense_1 = tf.keras.layers.Dense(intermediate_dim, activation='relu')\n        self.dense_2 = tf.keras.layers.Dense(latent_dim, activation='relu')\n        \n    def call(self, batch_inputs):\n        cnn_bn = self.bn(self.cnn(batch_inputs))\n        cnn_drop = self.dropout(cnn_bn)\n        lstm_inp = self.reshape(cnn_drop)\n        lstm_out = self.lstm(lstm_inp)\n        lstm_drop = self.dropout(lstm_out)\n        dense_1 = self.dropout(self.dense_1(lstm_drop))\n        local_out = self.dense_2(dense_1)\n        return local_out\n\nclass Global(tf.keras.layers.Layer):\n    def __init__(self, max_subs = 372, latent_dim = 128):\n        super(Global, self).__init__()\n        self.max_subs = max_subs\n        self.bottom_inp_dim = latent_dim\n        self.mask = tf.keras.layers.Masking(mask_value= -1, input_shape = (self.max_subs, self.bottom_inp_dim))\n        self.rnn = tf.keras.layers.LSTM(128, return_sequences = True)\n        self.dense_1 = tf.keras.layers.Dense(64, activation = 'relu') #tf.keras.layers.BatchNormalization(),\n        self.dense_2 = tf.keras.layers.Dense(32, activation = 'relu')\n        self.out = tf.keras.layers.Dense(2, activation = 'sigmoid')\n        \n    def call(self, batch_inputs):\n        x = batch_inputs\n        mask = self.mask(x)\n        h = self.rnn(mask)\n        h_d_1 = self.dense_1(h)\n        h_d_2 = self.dense_2(h_d_1)\n        y = self.out(h_d_2)\n        return y\n\nclass Seq(tf.keras.Model):\n    def __init__(self, latent_dim=128, name='seq', **kwargs):\n        super(Seq, self).__init__(name=name, **kwargs)\n        self.local_out_dim = latent_dim\n        self.local = Local()\n        self.pred = Global()\n\n        \n    def call(self, batch_inputs):\n        batch_output = np.zeros((batch_inputs.shape[0], batch_inputs.shape[1], 2))\n        for j in range(0, batch_inputs.shape[0]):\n            temp = np.zeros((batch_inputs.shape[1], self.local_out_dim))\n            for i in range(0,batch_inputs.shape[1]):\n                inp = batch_inputs[j,i,:]\n                inp = tf.reshape(inp, [1, 7, 7, 1])\n                temp[i,:] = self.local(inp)\n            seqinp = tf.reshape(temp, [1, batch_inputs.shape[1], self.local_out_dim])\n            batch_output[j,:,:] = self.pred(seqinp)\n\n        return batch_output\n</code></pre>\n<p>The <strong>Local</strong> layer is utilized to extract the local dependency within an input item, and the <strong>Global</strong> layer is utilize to learn sequential dependency between input items of a certain sequence.</p>\n<pre><code>t = np.random.random(size=(128, 372, 49))\nte = np.random.random(size=(128, 372, 2))\nseq = Seq()\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((t, te))\ntrain_dataset = train_dataset.shuffle(buffer_size=128).batch(4)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmse_loss_fn = tf.keras.losses.MeanSquaredError()\nloss_metric = tf.keras.metrics.Mean()\nvae = Seq()\n\nfor epoch in range(3):\n    print('Start of epoch %d' % (epoch,))\n\n    for (step, (x_batch_train, y_batch_train)) in enumerate(train_dataset):\n        with tf.GradientTape() as tape:\n            pred = seq(x_batch_train)\n            \n            loss = mse_loss_fn (y_batch_train, pred)\n        \n        grads = tape.gradient(loss, seq.trainable_variables)\n        optimizer.apply_gradients(zip(grads, seq.trainable_variables))\n        loss_metric(loss)\n    \n    if step % 100 == 0:\n        print('step %s: mean loss = %s' % (step, loss_metric.result()))\n</code></pre>\n<p>The gradient is None when optimize loss function, and the error is:</p>\n<pre><code>---&gt; 32         optimizer.apply_gradients(zip(grads, seq.trainable_variables))\n     33         loss_metric(loss)\n     34 \n\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)\n    511       ValueError: If none of the variables have gradients.\n    512     &quot;&quot;&quot;\n--&gt; 513     grads_and_vars = _filter_grads(grads_and_vars)\n    514     var_list = [v for (_, v) in grads_and_vars]\n    515 \n\n~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py in _filter_grads(grads_and_vars)\n   1269   if not filtered:\n   1270     raise ValueError(&quot;No gradients provided for any variable: %s.&quot; %\n-&gt; 1271                      ([v.name for _, v in grads_and_vars],))\n   1272   if vars_with_empty_grads:\n   1273     logging.warning(\n\nValueError: No gradients provided for any variable: ['seq/local/conv2d_52/kernel:0', 'seq/local/conv2d_52/bias:0', 'seq/local/batch_normalization_52/gamma:0', 'seq/local/batch_normalization_52/beta:0', 'seq/local/lstm_97/lstm_cell_97/kernel:0', 'seq/local/lstm_97/lstm_cell_97/recurrent_kernel:0', 'seq/local/lstm_97/lstm_cell_97/bias:0', 'seq/local/dense_244/kernel:0', 'seq/local/dense_244/bias:0', 'seq/local/dense_245/kernel:0', 'seq/local/dense_245/bias:0', 'seq/global_45/lstm_98/lstm_cell_98/kernel:0', 'seq/global_45/lstm_98/lstm_cell_98/recurrent_kernel:0', 'seq/global_45/lstm_98/lstm_cell_98/bias:0', 'seq/global_45/dense_246/kernel:0', 'seq/global_45/dense_246/bias:0', 'seq/global_45/dense_247/kernel:0', 'seq/global_45/dense_247/bias:0', 'seq/global_45/dense_248/kernel:0', 'seq/global_45/dense_248/bias:0'].\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 162}]
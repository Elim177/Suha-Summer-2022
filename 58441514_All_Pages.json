[{"items": [{"tags": ["python", "tensorflow", "keras", "performance-testing", "tensorflow2.0"], "owner": {"account_id": 7277007, "reputation": 17146, "user_id": 10133797, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/ElNKG.png?s=256&g=1", "display_name": "OverLordGoldDragon", "link": "https://stackoverflow.com/users/10133797/overlordgolddragon"}, "is_answered": true, "view_count": 37015, "accepted_answer_id": 58653632, "answer_count": 2, "score": 192, "last_activity_date": 1639742382, "creation_date": 1571351313, "last_edit_date": 1617473667, "question_id": 58441514, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/58441514/why-is-tensorflow-2-much-slower-than-tensorflow-1", "title": "Why is TensorFlow 2 much slower than TensorFlow 1?", "body": "<p>It's been cited by many users as the reason for switching to Pytorch, but I've yet to find a justification/explanation for sacrificing the most important practical quality, speed, for eager execution.</p>\n<p>Below is code benchmarking performance, TF1 vs. TF2 - with TF1 running anywhere from <strong>47% to 276% faster</strong>.</p>\n<p>My question is: <em>what is it, at the graph or hardware level, that yields such a significant slowdown?</em></p>\n<hr>\n<p>Looking for a detailed answer - am already familiar with broad concepts. <a href=\"https://github.com/tensorflow/tensorflow/issues/33487\" rel=\"noreferrer\">Relevant Git</a></p>\n<p><strong>Specs</strong>: CUDA 10.0.130, cuDNN 7.4.2, Python 3.7.4, Windows 10, GTX 1070</p>\n<hr>\n<p><strong>Benchmark results</strong>:</p>\n<img src=\"https://i.stack.imgur.com/ayBCS.png\" width=\"530\">\n<hr>\n<p><strong>UPDATE</strong>: Disabling Eager Execution per below code does <em>not</em> help. The behavior, however, is inconsistent: sometimes running in graph mode helps considerably, other times it runs <em>slower</em> relative to Eager.</p>\n<hr>\n<p><strong>Benchmark code</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code># use tensorflow.keras... to benchmark tf.keras; used GPU for all above benchmarks\nfrom keras.layers import Input, Dense, LSTM, Bidirectional, Conv1D\nfrom keras.layers import Flatten, Dropout\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport keras.backend as K\nimport numpy as np\nfrom time import time\n\nbatch_shape = (32, 400, 16)\nX, y = make_data(batch_shape)\n\nmodel_small = make_small_model(batch_shape)\nmodel_small.train_on_batch(X, y)  # skip first iteration which builds graph\ntimeit(model_small.train_on_batch, 200, X, y)\n\nK.clear_session()  # in my testing, kernel was restarted instead\n\nmodel_medium = make_medium_model(batch_shape)\nmodel_medium.train_on_batch(X, y)  # skip first iteration which builds graph\ntimeit(model_medium.train_on_batch, 10, X, y)\n</code></pre>\n<hr>\n<p><strong>Functions used</strong>:</p>\n<pre class=\"lang-py prettyprint-override\"><code>def timeit(func, iterations, *args):\n    t0 = time()\n    for _ in range(iterations):\n        func(*args)\n    print(&quot;Time/iter: %.4f sec&quot; % ((time() - t0) / iterations))\n\ndef make_small_model(batch_shape):\n    ipt   = Input(batch_shape=batch_shape)\n    x     = Conv1D(128, 400, strides=4, padding='same')(ipt)\n    x     = Flatten()(x)\n    x     = Dropout(0.5)(x)\n    x     = Dense(64, activation='relu')(x)\n    out   = Dense(1,  activation='sigmoid')(x)\n    model = Model(ipt, out)\n    model.compile(Adam(lr=1e-4), 'binary_crossentropy')\n    return model\n\ndef make_medium_model(batch_shape):\n    ipt   = Input(batch_shape=batch_shape)\n    x     = Bidirectional(LSTM(512, activation='relu', return_sequences=True))(ipt)\n    x     = LSTM(512, activation='relu', return_sequences=True)(x)\n    x     = Conv1D(128, 400, strides=4, padding='same')(x)\n    x     = Flatten()(x)\n    x     = Dense(256, activation='relu')(x)\n    x     = Dropout(0.5)(x)\n    x     = Dense(128, activation='relu')(x)\n    x     = Dense(64,  activation='relu')(x)\n    out   = Dense(1,   activation='sigmoid')(x)\n    model = Model(ipt, out)\n    model.compile(Adam(lr=1e-4), 'binary_crossentropy')\n    return model\n    \ndef make_data(batch_shape):\n    return np.random.randn(*batch_shape), np.random.randint(0, 2, (batch_shape[0], 1))\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 296}]
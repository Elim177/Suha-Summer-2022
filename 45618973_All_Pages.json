[{"items": [{"tags": ["tensorflow", "delay", "batch-normalization"], "owner": {"account_id": 10716056, "reputation": 2138, "user_id": 7886651, "user_type": "registered", "accept_rate": 76, "profile_image": "https://i.stack.imgur.com/zfb59.jpg?s=256&g=1", "display_name": "I. A", "link": "https://stackoverflow.com/users/7886651/i-a"}, "is_answered": false, "view_count": 99, "answer_count": 0, "score": 0, "last_activity_date": 1502382409, "creation_date": 1502382409, "question_id": 45618973, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/45618973/is-batch-normalization-a-good-practice-when-images-belong-to-one-class", "title": "Is Batch Normalization a good practice when images belong to one class?", "body": "<p>Assume I am training a Model to classify images of number 8 only from the MNIST dataset. Is it a good practice to use <strong>Batch Normalization</strong> in this case? </p>\n\n<p>Maybe it is good to use when images are heterogeneous, that is, belong to different classes, e.g. birds, dogs, cats, and so on. In other words, if the samples belong to the same class initially, then the network would adapt to the hidden distributions at each layer since they are not going to vary a lot (Maybe at the beginning while the network is training), but later, the weights would learn. But giving a network images of different classes, then it makes sense that hidden distribution will vary a lot from one image to the other (that is, from one class to the other). </p>\n\n<p>I am asking this question since I am trying to train a model with Batch Normalization enabled at one time and disabled another time. And since the model has been trained on images that belong to the same class, the model seemed to take more time for training with batch Normalization. </p>\n\n<p>P.S I am building my model with tensorflow and here is the <strong>Batch Norm</strong> layer that I'm using (from: <a href=\"https://r2rt.com/implementing-batch-normalization-in-tensorflow.html\" rel=\"nofollow noreferrer\">https://r2rt.com/implementing-batch-normalization-in-tensorflow.html</a>):</p>\n\n<pre><code>def batch_norm_wrapper(inputs, is_training, convlayer, decay = 0.999):\n\n    scale = tf.Variable(tf.ones([inputs.get_shape()[-1]]))\n    beta = tf.Variable(tf.zeros([inputs.get_shape()[-1]]))\n    pop_mean = tf.Variable(tf.zeros([inputs.get_shape()[-1]]), trainable=False)\n    pop_var = tf.Variable(tf.ones([inputs.get_shape()[-1]]), trainable=False)\n\n    if is_training:\n        if convlayer:\n            batch_mean, batch_var = tf.nn.moments(inputs, [0, 1, 2])\n        else:\n            batch_mean, batch_var = tf.nn.moments(inputs, [0])\n\n        train_mean = tf.assign(pop_mean,\n                               pop_mean * decay + batch_mean * (1 - decay))\n        train_var = tf.assign(pop_var,\n                              pop_var * decay + batch_var * (1 - decay))\n        with tf.control_dependencies([train_mean, train_var]):\n            return tf.nn.batch_normalization(inputs,\n                batch_mean, batch_var, beta, scale, epsilon)\n    else:\n        return tf.nn.batch_normalization(inputs,\n            pop_mean, pop_var, beta, scale, epsilon)\n</code></pre>\n\n<p>Maybe there is a mistake in this layer. Not sure. </p>\n\n<p>Any help is much appreciated!!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 71}]
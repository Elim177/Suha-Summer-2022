[{"items": [{"tags": ["python", "numpy", "tensorflow", "neural-network"], "owner": {"account_id": 13791835, "reputation": 147, "user_id": 9954063, "user_type": "registered", "profile_image": "https://graph.facebook.com/2236761996363870/picture?type=large", "display_name": "Sai Raghava", "link": "https://stackoverflow.com/users/9954063/sai-raghava"}, "is_answered": false, "view_count": 258, "answer_count": 0, "score": 0, "last_activity_date": 1566928868, "creation_date": 1566914358, "last_edit_date": 1566928868, "question_id": 57676349, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/57676349/how-to-pass-a-tensor-variable-to-feed-dict", "title": "How to pass a tensor variable to feed_dict?", "body": "<p>I have basic neural network which has a function run(). which takes the input and provides the output of the neural network. In the function body, I would like to <code>feed_dict</code> weights when running a session to calculate the output. Since these weights are <code>tf.compat.v1.variable</code> I am having an error <code>ValueError: Setting an array element with sequence</code></p>\n\n<p>I have initially thought it was my input that is causing the error and hence, I have made sure it has same dimensions as weights to use <code>tf.multiply</code> and since the error continued, I made sure to convert the numpy array into a tensor object hoping that might be causing the error. But ti still does not work.</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def run(self, carollis_input):\n        self.normalization(carollis_input)\n        c_in = np.array(carollis_input)\n        c_input = tf.compat.v1.convert_to_tensor(c_in, tf.float64)\n        #'finding the output of the input layer'\n        #with tf.Session() as sess1_2:\n        input_weight = tf.compat.v1.placeholder(tf.float64, shape=(self.neurons, 1))\n        input_bias = tf.compat.v1.placeholder(tf.float64, shape=(self.neurons, 1))\n        hidden_weight = tf.compat.v1.placeholder(tf.float64, shape=(self.neurons, 1))\n        hidden_bias =tf.compat.v1.placeholder(tf.float64, shape=(self.neurons, 1))\n        output_weight = tf.compat.v1.placeholder(tf.float64, shape=(4, 1))\n        output_bias = tf.compat.v1.placeholder(tf.float64, shape=(self.neurons, 1))\n\n        knowledge_input = tf.add(tf.multiply(c_input, input_weight), input_bias)\n        with tf.Session() as ses1:\n            ses1.run(knowledge_input, feed_dict={input_weight: self.weight_in, input_bias:self.bias_in})\n        knowledge_hidden = tf.nn.leaky_relu(knowledge_input, alpha=0.01) \n        #'calculating the output of hidden layer'\n        knowledge_hidden_output = 3.14*(tf.add(tf.multiply(knowledge_hidden, hidden_weight), hidden_bias))#input function of hidden layer\n        knowledge_hidden_out = tf.nn.leaky_relu(knowledge_hidden_output, alpha=0.01, name='leaky_relu')\n\n        with tf.Session() as ses2:\n            ses2.run(knowledge_hidden_out, feed_dict={hidden_weight: self.weight_hid, hidden_bias:self.bias_hid})\n        #'calculating the input of output layer'\n        tf.reshape(knowledge_hidden_out, [4, 2])#for quadrant method\n        in_out = tf.add(tf.multiply(knowledge_hidden_out, output_weight), output_bias)\n        with tf.Session() as ses3:\n            ses3.run(in_out, feed_dict={output_weight: self.weight_out, output_bias: self.bias_out})\n        #'finding the softmax output of the neurons'\n        softmax_output = np.array(4)\n        softmax_output = self.out_softmax(in_out)  # this gives the softmax output and stores it in the newly created array\n        return softmax_output\n</code></pre>\n\n<p>After going through other answers , I believe that error might be cause because I was trying to feed in a tensor variable i.e. weights and biases to the feed_dict. If thats the problem, how can i solve it, please kindly help me. The exact error is </p>\n\n<pre><code>File \"/home/microbot/catkin_ws/src/spider/spider_control/knowledge_transfer.py\", line 85, in run\n    ses1.run(knowledge_input, feed_dict={input_weight: self.weight_in, input_bias:self.bias_in})\n  File \"/home/microbot/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 950, in run\n    run_metadata_ptr)\n  File \"/home/microbot/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1142, in _run\n    np_val = np.asarray(subfeed_val, dtype=subfeed_dtype)\n  File \"/home/microbot/.local/lib/python3.6/site-packages/numpy/core/_asarray.py\", line 85, in asarray\n    return array(a, dtype, copy=False, order=order)\nValueError: setting an array element with a sequence\n</code></pre>\n\n<p>EDIT:\nThe weights are initialized inside the class initialiser i.e. </p>\n\n<pre><code>def __init__(self, neurons, layers):\n        self.neurons = neurons\n        self.layers =  layers\n        self.weight_initer = tf.truncated_normal_initializer(mean=0.0, stddev=0.01)\n        self.weight_in = tf.get_variable(name=\"Weight_input\", dtype=tf.float64, shape=[self.neurons, 1], initializer=self.weight_initer)\n        self.weight_initer1 = tf.truncated_normal_initializer(mean=1.0, stddev=0.01)\n        self.weight_hid = tf.get_variable(name=\"Weight_hidden\", dtype=tf.float64, shape=[self.neurons, 1], initializer=self.weight_initer1)\n        self.weight_initer2 = tf.truncated_normal_initializer(mean=2.0, stddev=0.01)\n        self.weight_out = tf.get_variable(name=\"Weight_output\", dtype=tf.float64, shape=[4, 2], initializer=self.weight_initer2)\n        self.bias_initer =tf.truncated_normal_initializer(mean=0.1, stddev=0.01)\n        self.bias_in  =tf.get_variable(name=\"Bias_input\", dtype=tf.float64, shape=[self.neurons, 1], initializer=self.bias_initer)\n        self.bias_initer1 =tf.truncated_normal_initializer(mean=0.2, stddev=0.01)\n        self.bias_hid = tf.get_variable(name=\"Bias_hidden\", dtype=tf.float64, shape=[self.neurons, 1], initializer=self.bias_initer1)\n        self.bias_initer2 =tf.truncated_normal_initializer(mean=0.3, stddev=0.01)\n        self.bias_out = tf.get_variable(name=\"Bias_output\", dtype=tf.float64, shape=[4, 1], initializer=self.bias_initer2)\n\n</code></pre>\n\n<p>Although now I have been having two different confusing, one with respective to scope of the graph and other to the weight initialising. Since I have defined the weight tensors in the initialiser, would this be the defaul graph and the graph that i am trying to build inside the run function be secondary ? </p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 261}]
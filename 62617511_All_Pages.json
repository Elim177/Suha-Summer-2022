[{"items": [{"tags": ["tensorflow", "keras", "tpu"], "owner": {"account_id": 3948330, "reputation": 5043, "user_id": 3259896, "user_type": "registered", "accept_rate": 60, "profile_image": "https://www.gravatar.com/avatar/641c30a7b383022f22b53c8cedb04e3f?s=256&d=identicon&r=PG&f=1", "display_name": "SantoshGupta7", "link": "https://stackoverflow.com/users/3259896/santoshgupta7"}, "is_answered": false, "view_count": 999, "answer_count": 0, "score": 3, "last_activity_date": 1620930720, "creation_date": 1593310520, "last_edit_date": 1593418449, "question_id": 62617511, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62617511/attributeerror-tensor-name-is-meaningless-when-eager-execution-is-enabled-wh", "title": "&quot;AttributeError: Tensor.name is meaningless when eager execution is enabled.&quot; when training on TPU at &quot;self.optimizer.apply_gradients&quot;", "body": "<p>I have code that works fine on GPU, but for TPU, I get at error starting at :</p>\n<p><code>self.optimizer.apply_gradients(zip(gradients, trainable_vars))</code></p>\n<p>Which says <code>AttributeError: Tensor.name is meaningless when eager execution is enabled.</code></p>\n<p>I have a custom model, which isn't too much different than the Keras default</p>\n<pre><code>class CustomModel(tf.keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x = data\n        y = tf.Variable(tf.constant([1.0], dtype=tf.float32))\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n\n        # Compute gradients\n        trainable_vars = self.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n        # Update metrics (includes the metric that tracks the loss)\n        self.compiled_metrics.update_state(y, y_pred)\n        # Return a dict mapping metric names to current value\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>\n<p>This is the full error message</p>\n<pre><code>Epoch 1/3\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-19-00fb5a641066&gt; in &lt;module&gt;()\n      5         validation_steps=val_steps,\n      6         validation_freq=1,\n----&gt; 7         callbacks=callbacks)\n\n10 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\n     64   def _method_wrapper(self, *args, **kwargs):\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\n---&gt; 66       return method(self, *args, **kwargs)\n     67 \n     68     # Running inside `run_distribute_coordinator` already.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n    846                 batch_size=batch_size):\n    847               callbacks.on_train_batch_begin(step)\n--&gt; 848               tmp_logs = train_function(iterator)\n    849               # Catch OutOfRangeError for Datasets of unknown size.\n    850               # This blocks until the batch has finished executing.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n    578         xla_context.Exit()\n    579     else:\n--&gt; 580       result = self._call(*args, **kwds)\n    581 \n    582     if tracing_count == self._get_tracing_count():\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\n    625       # This is the first call of __call__, so we have to initialize.\n    626       initializers = []\n--&gt; 627       self._initialize(args, kwds, add_initializers_to=initializers)\n    628     finally:\n    629       # At this point we know that the initialization is complete (or less\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\n    504     self._concrete_stateful_fn = (\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n--&gt; 506             *args, **kwds))\n    507 \n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n   2444       args, kwargs = None, None\n   2445     with self._lock:\n-&gt; 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\n   2447     return graph_function\n   2448 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\n   2775 \n   2776       self._function_cache.missed.add(call_context_key)\n-&gt; 2777       graph_function = self._create_graph_function(args, kwargs)\n   2778       self._function_cache.primary[cache_key] = graph_function\n   2779       return graph_function, args, kwargs\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n   2665             arg_names=arg_names,\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\n-&gt; 2667             capture_by_value=self._capture_by_value),\n   2668         self._function_attributes,\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n    979         _, original_func = tf_decorator.unwrap(python_func)\n    980 \n--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)\n    982 \n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n    440         # the function a weak reference to itself to avoid a reference cycle.\n--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\n    443 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\n    966           except Exception as e:  # pylint:disable=broad-except\n    967             if hasattr(e, &quot;ag_error_metadata&quot;):\n--&gt; 968               raise e.ag_error_metadata.to_exception(e)\n    969             else:\n    970               raise\n\nAttributeError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    &lt;ipython-input-6-490916a676f3&gt;:18 train_step  *\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n    /usr/local/lib/python3.6/dist-packages/tensorflow_addons/optimizers/weight_decay_optimizers.py:149 apply_gradients  *\n        return super().apply_gradients(grads_and_vars, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:472 apply_gradients  **\n        grads_and_vars = _filter_grads(grads_and_vars)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 _filter_grads\n        ([v.name for v in vars_with_empty_grads]))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 &lt;listcomp&gt;\n        ([v.name for v in vars_with_empty_grads]))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1123 name\n        &quot;Tensor.name is meaningless when eager execution is enabled.&quot;)\n\n    AttributeError: Tensor.name is meaningless when eager execution is enabled.\n</code></pre>\n<p>The full code can be here</p>\n<p><a href=\"https://colab.research.google.com/drive/1PqAAa0-Dh9cZfLjLQGuqt5zPWBXqZTn6?usp=sharing\" rel=\"nofollow noreferrer\">https://colab.research.google.com/drive/1PqAAa0-Dh9cZfLjLQGuqt5zPWBXqZTn6?usp=sharing</a></p>\n<p>I am wonder if there's some aspect of TPU training that I am missing, since I am getting this error only when training over a TPU.</p>\n<p>Here are some possibly related issues on Github</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/issues/33045\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/tensorflow/issues/33045</a></p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/issues/34635\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/tensorflow/issues/34635</a></p>\n<p>EDIT:</p>\n<p>I noticed that Tensorflow has changed how they define <code>train_step</code>, <a href=\"https://github.com/tensorflow/tensorflow/blob/2434d2401399e3973d2f704f977bd6ad2d029ca7/tensorflow/python/keras/engine/training.py#L716\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/tensorflow/blob/2434d2401399e3973d2f704f977bd6ad2d029ca7/tensorflow/python/keras/engine/training.py#L716</a></p>\n<p>so I updated my custom model to match it.</p>\n<pre><code>from tensorflow.python.keras.mixed_precision.experimental import loss_scale_optimizer as lso\nfrom tensorflow.python.distribute import parameter_server_strategy\n\ndef _minimize(strategy, tape, optimizer, loss, trainable_variables):\n    with tape:\n        if isinstance(optimizer, lso.LossScaleOptimizer):\n            loss = optimizer.get_scaled_loss(loss)\n\n    gradients = tape.gradient(loss, trainable_variables)\n    gradients = [(ClipIfNotNone(grad)) for grad in gradients]\n    gradients = [(ClipIfNotNone2(grad)) for grad in gradients]\n    # Whether to aggregate gradients outside of optimizer. This requires support\n    # of the optimizer and doesn't work with ParameterServerStrategy and\n    # CentralStroageStrategy.\n    aggregate_grads_outside_optimizer = (\n        optimizer._HAS_AGGREGATE_GRAD and  # pylint: disable=protected-access\n        not isinstance(strategy.extended,\n                        parameter_server_strategy.ParameterServerStrategyExtended))\n\n    if aggregate_grads_outside_optimizer:\n        # We aggregate gradients before unscaling them, in case a subclass of\n        # LossScaleOptimizer all-reduces in fp16. All-reducing in fp16 can only be\n        # done on scaled gradients, not unscaled gradients, for numeric stability.\n        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n                                                    trainable_variables))\n    if isinstance(optimizer, lso.LossScaleOptimizer):\n        gradients = optimizer.get_unscaled_gradients(gradients)\n    gradients = optimizer._clip_gradients(gradients)  # pylint: disable=protected-access\n    if trainable_variables:\n        if aggregate_grads_outside_optimizer:\n            optimizer.apply_gradients(\n                zip(gradients, trainable_variables),\n                experimental_aggregate_gradients=False)\n        else:\n            optimizer.apply_gradients(zip(gradients, trainable_variables))\n\nclass CustomModel(tf.keras.Model):\n    def train_step(self, data):\n        # Unpack the data. Its structure depends on your model and\n        # on what you pass to `fit()`.\n        x = data\n        y = tf.constant([1.0], dtype=tf.float32)\n        sample_weight = None\n\n        with tf.GradientTape() as tape:\n            y_pred = self(x, training=True)  # Forward pass\n            # Compute the loss value\n            # (the loss function is configured in `compile()`)\n            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n        \n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n                self.trainable_variables)\n\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n        return {m.name: m.result() for m in self.metrics}\n</code></pre>\n<p>However, the result is pretty much the same</p>\n<pre><code>    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    &lt;ipython-input-8-823751185253&gt;:53 train_step  *\n        _minimize(self.distribute_strategy, tape, self.optimizer, loss,\n    &lt;ipython-input-8-823751185253&gt;:24 _minimize  *\n        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients  **\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 _filter_grads\n        ([v.name for v in vars_with_empty_grads]))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 &lt;listcomp&gt;\n        ([v.name for v in vars_with_empty_grads]))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1123 name\n        &quot;Tensor.name is meaningless when eager execution is enabled.&quot;)\n\n    AttributeError: Tensor.name is meaningless when eager execution is enabled.\n</code></pre>\n<p>Edit2:</p>\n<p>I tried not making a custom <code>train_step</code> altogether, and just extend the tf.keras.Model class. Still getting the same issue.</p>\n<p>This is what my custom model looks like</p>\n<pre><code>class Dora_A(tf.keras.Model):\n    def __init__(self):\n        super(Dora_A, self).__init__()\n        self.bioRoberta = TFRobertaModel.from_pretrained('allenai/biomed_roberta_base', from_pt=True)\n\n        self.Q_Tlayer0 = deepcopy(self.bioRoberta.layers[0].encoder.layer[11])\n        self.Q_Tlayer0._name = self.Q_Tlayer0._name + 'Query'\n        self.P_Tlayer0 = deepcopy(self.bioRoberta.layers[0].encoder.layer[11])\n        self.P_Tlayer0._name = self.P_Tlayer0._name + 'Passage'\n\n        self.Q_Tlayer1 = deepcopy(self.bioRoberta.layers[0].encoder.layer[11])\n        self.Q_Tlayer1._name = self.Q_Tlayer1._name + 'Query'\n        self.P_Tlayer1 = deepcopy(self.bioRoberta.layers[0].encoder.layer[11])\n        self.P_Tlayer1._name = self.P_Tlayer1._name + 'Passage'\n\n        self.Q_Tlayer2 = deepcopy(self.bioRoberta.layers[0].encoder.layer[11])\n        self.Q_Tlayer2._name = self.Q_Tlayer2._name + 'Query'\n        self.P_Tlayer2 = deepcopy(self.bioRoberta.layers[0].encoder.layer[11])\n        self.P_Tlayer2._name = self.P_Tlayer2._name + 'Passage'\n\n        self.Q_Tlayer3 = deepcopy(self.bioRoberta.layers[0].encoder.layer[11])\n        self.Q_Tlayer3._name = self.Q_Tlayer3._name + 'Query'\n        self.P_Tlayer3 = deepcopy(self.bioRoberta.layers[0].encoder.layer[11])\n        self.P_Tlayer3._name = self.P_Tlayer3._name + 'Passage'\n\n        self.Q_Tlayer3.intermediate.intermediate_act_fn = tf.keras.activations.tanh\n        self.P_Tlayer3.intermediate.intermediate_act_fn = tf.keras.activations.tanh\n\n        # self.Q_Tlayer0.set_weights(self.Q_Tlayer3.get_weights())\n        # self.P_Tlayer0.set_weights(self.P_Tlayer3.get_weights())\n\n        # self.Q_Tlayer1.set_weights(self.Q_Tlayer3.get_weights())\n        # self.P_Tlayer1.set_weights(self.P_Tlayer3.get_weights())\n\n        # self.Q_Tlayer2.set_weights(self.Q_Tlayer3.get_weights())\n        # self.P_Tlayer2.set_weights(self.P_Tlayer3.get_weights())\n\n        self.Q_ff_1 = tf.keras.layers.Dense(768, activation='swish',  name='qffPost_n1')\n        self.P_ff_1 = tf.keras.layers.Dense(768, activation='swish',  name='pffPost_n1')\n\n        self.Q_ff_2 = tf.keras.layers.Dense(768, activation='tanh',  name='qffPost_n2')\n        self.P_ff_2 = tf.keras.layers.Dense(768, activation='tanh',  name='pffPost_n2')\n\n    def call(self, inputIds):\n        queryInputs, passageInputs = inputIds\n\n        Q_outputs = self.bioRoberta(queryInputs)[0]\n        P_outputs = self.bioRoberta(passageInputs)[0]\n\n        Q_outputs = self.Q_Tlayer0((Q_outputs, None, None))[0]\n        P_outputs = self.P_Tlayer0((P_outputs, None, None))[0]\n\n        Q_outputs = self.Q_Tlayer1((Q_outputs, None, None))[0]\n        P_outputs = self.P_Tlayer1((P_outputs, None, None))[0]\n\n        Q_outputs = self.Q_Tlayer2((Q_outputs, None, None))[0]\n        P_outputs = self.P_Tlayer2((P_outputs, None, None))[0]\n\n        Q_outputs = self.Q_Tlayer3((Q_outputs, None, None))[0]\n        P_outputs = self.P_Tlayer3((P_outputs, None, None))[0]       \n\n        Q_outputs = tf.concat([\n                        Q_outputs[:, 0], #cls, NOT from ff layer after last hidden state since it seems to be untrained in roberta\n                        tf.reduce_mean(Q_outputs[:, 1:-1], axis=1), # pooled except CLS and SEP\n                        tf.math.reduce_max(Q_outputs[:, 1:-1], axis=1),\n                        tf.math.reduce_min(Q_outputs[:, 1:-1], axis=1),\n                        tf.math.reduce_variance(Q_outputs[:, 1:-1], axis=1),\n                        tf.math.reduce_logsumexp(Q_outputs[:, 1:-1], axis=1),\n                        Q_outputs[:, -1] # sep, get from hidden state \n                        ],axis=1) \n        \n        P_outputs = tf.concat([\n                        P_outputs[:, 0], #cls, NOT from ff layer after last hidden state since it seems to be untrained in roberta\n                        tf.reduce_mean(P_outputs[:, 1:-1], axis=1), # pooled except CLS and SEP\n                        tf.math.reduce_max(P_outputs[:, 1:-1], axis=1),\n                        tf.math.reduce_min(P_outputs[:, 1:-1], axis=1),\n                        tf.math.reduce_variance(P_outputs[:, 1:-1], axis=1),\n                        tf.math.reduce_logsumexp(P_outputs[:, 1:-1], axis=1),\n                        P_outputs[:, -1] # sep, get from hidden state \n                        ],axis=1)\n\n        Q_outputs = Dropout(0.10)(Q_outputs)\n        P_outputs = Dropout(0.10)(P_outputs)\n\n        Q_outputs = self.Q_ff_1(Q_outputs) \n        P_outputs = self.P_ff_1(P_outputs) \n\n        Q_outputs = self.Q_ff_2(Q_outputs) \n        P_outputs = self.P_ff_2(P_outputs) \n\n        dotProductMatrix = tf.linalg.matmul(Q_outputs, P_outputs, transpose_b=True, name='mm')\n\n        return dotProductMatrix\n</code></pre>\n<p>This is the error message that I got for training</p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-23-d78edec93dcb&gt; in &lt;module&gt;()\n      1 model.fit(train_datasetFinal,\n      2         epochs=epochs,\n----&gt; 3         callbacks=callbacks)\n      4 \n      5 # else:\n\n10 frames\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)\n     64   def _method_wrapper(self, *args, **kwargs):\n     65     if not self._in_multi_worker_mode():  # pylint: disable=protected-access\n---&gt; 66       return method(self, *args, **kwargs)\n     67 \n     68     # Running inside `run_distribute_coordinator` already.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\n    846                 batch_size=batch_size):\n    847               callbacks.on_train_batch_begin(step)\n--&gt; 848               tmp_logs = train_function(iterator)\n    849               # Catch OutOfRangeError for Datasets of unknown size.\n    850               # This blocks until the batch has finished executing.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)\n    578         xla_context.Exit()\n    579     else:\n--&gt; 580       result = self._call(*args, **kwds)\n    581 \n    582     if tracing_count == self._get_tracing_count():\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)\n    625       # This is the first call of __call__, so we have to initialize.\n    626       initializers = []\n--&gt; 627       self._initialize(args, kwds, add_initializers_to=initializers)\n    628     finally:\n    629       # At this point we know that the initialization is complete (or less\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)\n    504     self._concrete_stateful_fn = (\n    505         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n--&gt; 506             *args, **kwds))\n    507 \n    508     def invalid_creator_scope(*unused_args, **unused_kwds):\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)\n   2444       args, kwargs = None, None\n   2445     with self._lock:\n-&gt; 2446       graph_function, _, _ = self._maybe_define_function(args, kwargs)\n   2447     return graph_function\n   2448 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)\n   2775 \n   2776       self._function_cache.missed.add(call_context_key)\n-&gt; 2777       graph_function = self._create_graph_function(args, kwargs)\n   2778       self._function_cache.primary[cache_key] = graph_function\n   2779       return graph_function, args, kwargs\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)\n   2665             arg_names=arg_names,\n   2666             override_flat_arg_shapes=override_flat_arg_shapes,\n-&gt; 2667             capture_by_value=self._capture_by_value),\n   2668         self._function_attributes,\n   2669         # Tell the ConcreteFunction to clean up its graph once it goes out of\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\n    979         _, original_func = tf_decorator.unwrap(python_func)\n    980 \n--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)\n    982 \n    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)\n    439         # __wrapped__ allows AutoGraph to swap in a converted function. We give\n    440         # the function a weak reference to itself to avoid a reference cycle.\n--&gt; 441         return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    442     weak_wrapped_fn = weakref.ref(wrapped_fn)\n    443 \n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)\n    966           except Exception as e:  # pylint:disable=broad-except\n    967             if hasattr(e, &quot;ag_error_metadata&quot;):\n--&gt; 968               raise e.ag_error_metadata.to_exception(e)\n    969             else:\n    970               raise\n\nAttributeError: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:174 run  **\n        return self.extended.tpu_run(fn, args, kwargs, options)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:867 tpu_run\n        return func(args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:934 tpu_function\n        padding_spec=padding_spec)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:893 replicate\n        padding_spec=padding_spec)[1]\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu.py:1280 split_compile_and_replicate\n        outputs = computation(*computation_inputs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:896 replicated_fn\n        result[0] = fn(*replica_args, **replica_kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:541 train_step  **\n        self.trainable_variables)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1804 _minimize\n        trainable_variables))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:521 _aggregate_gradients\n        filtered_grads_and_vars = _filter_grads(grads_and_vars)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 _filter_grads\n        ([v.name for v in vars_with_empty_grads]))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1223 &lt;listcomp&gt;\n        ([v.name for v in vars_with_empty_grads]))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1123 name\n        &quot;Tensor.name is meaningless when eager execution is enabled.&quot;)\n\n    AttributeError: Tensor.name is meaningless when eager execution is enabled.\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 289}]
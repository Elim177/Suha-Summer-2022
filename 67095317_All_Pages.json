[{"items": [{"tags": ["python", "tensorflow"], "owner": {"account_id": 4441934, "reputation": 1962, "user_id": 3616293, "user_type": "registered", "accept_rate": 35, "profile_image": "https://www.gravatar.com/avatar/cf7556b4227065cec9496375d64fea3d?s=256&d=identicon&r=PG&f=1", "display_name": "Arun", "link": "https://stackoverflow.com/users/3616293/arun"}, "is_answered": true, "view_count": 725, "answer_count": 1, "score": 1, "last_activity_date": 1618421707, "creation_date": 1618416577, "last_edit_date": 1618418379, "question_id": 67095317, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/67095317/tensorflow-sgd-decay-parameter", "title": "TensorFlow SGD decay parameter", "body": "<p>I am using TensorFlow 2.4.1 and Python3.8 for Computer Vision based CNN models such as VGG-18, ResNet-18/34, etc. My question is specific to weight decay declaration. There are two ways of defining it:</p>\n<ol>\n<li>The first is by declaring it for each layer using 'kernel_regularizer' parameter for 'Conv2D' layer</li>\n<li>The second is by using 'decay' parameter in TF SGD optimizer</li>\n</ol>\n<p>Example codes are:</p>\n<pre><code>weight_decay = 0.0005\n\nConv2D(\n    filters = 64, kernel_size = (3, 3),\n    activation='relu', kernel_initializer = tf.initializers.he_normal(),\n    strides = (1, 1), padding = 'same',\n    kernel_regularizer = regularizers.l2(weight_decay),\n)\n# NOTE: this 'kernel_regularizer' parameter is used for all of the conv layers in ResNet-18/34 and VGG-18 models\n\noptimizer = tf.keras.optimizers.SGD(learning_rate = 0.01, decay = lr_decay, momentum = 0.9)\n</code></pre>\n<p>My question is:</p>\n<ol>\n<li>Are these two techniques for using weight decay doing the same thing? If yes, only one should be used to avoid redundancy</li>\n<li>If not, does using both of these weight decay definitions add twice the weight decay? Because too much of regularization would push even the helpful weights towards zero and therefore in essence, any model will not learn the desired function.</li>\n</ol>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 167}]
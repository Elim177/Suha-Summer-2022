[{"items": [{"tags": ["python", "tensorflow", "machine-learning", "keras", "deep-learning"], "owner": {"account_id": 15116032, "reputation": 30058, "user_id": 10908375, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/L7f8w.jpg?s=256&g=1", "display_name": "Nicolas Gervais", "link": "https://stackoverflow.com/users/10908375/nicolas-gervais"}, "is_answered": true, "view_count": 5200, "accepted_answer_id": 62043823, "answer_count": 1, "score": 5, "last_activity_date": 1606756912, "creation_date": 1576704035, "last_edit_date": 1606756912, "question_id": 59400128, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59400128/warningtensorflowlayer-my-model-is-casting-an-input-tensor-from-dtype-float64", "title": "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer&#39;s dtype of float32, which is new behavior in TensorFlow 2", "body": "<p>Before my Tensorflow neural network starts training, the following warning prints out:</p>\n<blockquote>\n<p>WARNING:tensorflow:Layer my_model is casting an input tensor from\ndtype float64 to the layer's dtype of float32, which is new behavior\nin TensorFlow 2.  The layer has dtype float32 because it's dtype\ndefaults to floatx. If you intended to run this layer in float32, you\ncan safely ignore this warning.</p>\n</blockquote>\n<blockquote>\n<p>If in doubt, this warning is likely\nonly an issue if you are porting a TensorFlow 1.X model to TensorFlow\n2. To change all layers to have dtype float64 by default, call <code>tf.keras.backend.set_floatx('float64')</code>.</p>\n</blockquote>\n<blockquote>\n<p>To change just this layer,\npass dtype='float64' to the layer constructor. If you are the author\nof this layer, you can disable autocasting by passing autocast=False\nto the base Layer constructor.</p>\n</blockquote>\n<p>Now, based on the error message, <strong>I am able to silence this error message</strong> by setting the backend to <code>'float64'</code>. But, I would like to get to the bottom of this and set the right <code>dtypes</code> manually.</p>\n<p>Full code:</p>\n<pre class=\"lang-py prettyprint-override\"><code>import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Concatenate\nfrom tensorflow.keras import Model\nfrom sklearn.datasets import load_iris\niris, target = load_iris(return_X_y=True)\n\nX = iris[:, :3]\ny = iris[:, 3]\n\nds = tf.data.Dataset.from_tensor_slices((X, y)).shuffle(25).batch(8)\n\nclass MyModel(Model):\n  def __init__(self):\n    super(MyModel, self).__init__()\n    self.d0 = Dense(16, activation='relu')\n    self.d1 = Dense(32, activation='relu')\n    self.d2 = Dense(1, activation='linear')\n\n  def call(self, x):\n    x = self.d0(x)\n    x = self.d1(x)\n    x = self.d2(x)\n    return x\n\nmodel = MyModel()\n\nloss_object = tf.keras.losses.MeanSquaredError()\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n\nloss = tf.keras.metrics.Mean(name='loss')\nerror = tf.keras.metrics.MeanSquaredError()\n\n@tf.function\ndef train_step(inputs, targets):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs)\n        run_loss = loss_object(targets, predictions)\n    gradients = tape.gradient(run_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    loss(run_loss)\n    error(predictions, targets)\n\nfor epoch in range(10):\n  for data, labels in ds:\n    train_step(data, labels)\n\n  template = 'Epoch {:&gt;2}, Loss: {:&gt;7.4f}, MSE: {:&gt;6.2f}'\n  print(template.format(epoch+1,\n                        loss.result(),\n                        error.result()*100))\n  # Reset the metrics for the next epoch\n  loss.reset_states()\n  error.reset_states()\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 95}]
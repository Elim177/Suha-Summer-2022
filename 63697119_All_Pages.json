[{"items": [{"tags": ["tensorflow", "keras"], "owner": {"account_id": 6432279, "reputation": 732, "user_id": 6159217, "user_type": "registered", "accept_rate": 85, "profile_image": "https://www.gravatar.com/avatar/cf02c33ce95856d3094552a5ce7f7fba?s=256&d=identicon&r=PG&f=1", "display_name": "IntegrateThis", "link": "https://stackoverflow.com/users/6159217/integratethis"}, "is_answered": true, "view_count": 1686, "accepted_answer_id": 63777756, "answer_count": 1, "score": 1, "last_activity_date": 1599704985, "creation_date": 1599006833, "last_edit_date": 1599190518, "question_id": 63697119, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/63697119/tensorflow-gpu-2-4-vram-issue", "title": "Tensorflow-GPU 2.4 VRAM issue", "body": "<p>I am trying to run tensorflow-gpu version 2.4.0-dev20200828 (a tf-nightly build) for a convolutional neural network implementation. Some other details:</p>\n<ul>\n<li>The version of python is Python 3.8.5.</li>\n<li>Running Windows 10</li>\n<li>Using an nVidia RTX 2080 which has 8 GB VRAM</li>\n<li>Cuda Version 11.1</li>\n</ul>\n<p>The following snippet is what I run:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow import keras\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  try:\n    tf.config.experimental.set_virtual_device_configuration(\n        gpus[0],\n        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), &quot;Physical GPUs,&quot;, len(logical_gpus), &quot;Logical GPUs&quot;)\n  except RuntimeError as e:\n    # Virtual devices must be set before GPUs have been initialized\n    print(e)\n\nvgg_16 = keras.applications.VGG16(include_top=False, input_shape=(600, 600, 3))\nrandom_image = np.random.rand(1, 600, 600, 3)\noutput = vgg_16(random_image)\n</code></pre>\n<p>The code for the memory configuration was taken from answers from <a href=\"https://stackoverflow.com/questions/41117740/tensorflow-crashes-with-cublas-status-alloc-failed\">here</a></p>\n<p>The issue I am having is that my GPU has 8GB of VRAM, and I need to be able to run the CNN with relatively large image batch sizes. The example is executed on a single image, but surprisingly I seem to only be able to increase the batch size to about 2-3 600 by 600 images. The code taken as per the comments says that it:</p>\n<p><em>Restrict TensorFlow to only allocate 1GB of memory on the first GPU</em>, which is clearly not ideal.</p>\n<p>On the one hand if I allocate more, say 4000MB, I get errors such as:</p>\n<pre><code>E tensorflow/stream_executor/cuda/cuda_dnn.cc:325] Could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED\n</code></pre>\n<p>If I leave it as 1024 MB, I get messages like:</p>\n<pre><code>Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.25GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n</code></pre>\n<p>Any insights/resources on how to understand this issue much appreciated. I'd be willing to switch to another version of tensorflow/python/cuda if necessary, but ultimately I just want to have a deeper understanding of what this issue is.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 272}]
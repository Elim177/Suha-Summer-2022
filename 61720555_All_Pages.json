[{"items": [{"tags": ["python", "tensorflow", "keras", "nlp", "tensorflow-datasets"], "owner": {"account_id": 15116032, "reputation": 30058, "user_id": 10908375, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/L7f8w.jpg?s=256&g=1", "display_name": "Nicolas Gervais", "link": "https://stackoverflow.com/users/10908375/nicolas-gervais"}, "is_answered": true, "view_count": 691, "answer_count": 1, "score": 3, "last_activity_date": 1589179048, "creation_date": 1589157322, "last_edit_date": 1589179048, "question_id": 61720555, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61720555/handling-text-data-with-tensorflow-2-error-attempted-to-pad-to-a-smaller-size", "title": "Handling text data with Tensorflow 2 &quot;ERROR: Attempted to pad to a smaller size than the input element&quot;", "body": "<p>I'm getting this error while iterating through a tokenized text dataset:</p>\n\n<blockquote>\n  <p>tensorflow.python.framework.errors_impl.DataLossError: Attempted to pad to a smaller size than the input element</p>\n</blockquote>\n\n<p>It probably stems from the fact that I simply cannot understand the <code>padded_shapes=([None], (1,))</code> argument. Here's the <a href=\"https://drive.google.com/file/d/1N8WCMci_jpDHwCVgSED-B9yts-q9_Bb5/view\" rel=\"nofollow noreferrer\">dataset</a> and the code:</p>\n\n<pre><code>import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom collections import Counter\n\nos.chdir('/home/nicolas/Documents/Datasets')\n\ndata = tf.data.experimental.CsvDataset('rotten_tomatoes_string.csv',\n                                       record_defaults=[tf.string, tf.string],\n                                       header=True, select_cols=[0, 1])  \n\nreview_tokenizer= tfds.features.text.Tokenizer()\nvocabulary = Counter()\n\nfor ix, (_, review) in enumerate(data):\n    review = tf.strings.lower(review)\n    tokens = review_tokenizer.tokenize(review.numpy())\n    vocabulary.update(tokens)\n\nvocabulary = [key for (key, value) in vocabulary.items() if value &gt;= 5 and len(key) &gt; 2]\n\nratings = {'rotten', 'fresh'}\nreview_encoder = tfds.features.text.TokenTextEncoder(vocabulary)\nfreshness_encoder = tfds.features.text.TokenTextEncoder(ratings)\n\ndef encode(freshness, review):\n    review = review_encoder.encode(review.numpy())\n    freshness = freshness_encoder.encode(freshness.numpy())\n    return freshness, review\n\ndef tf_map(freshness, review):\n    encoded_freshness, review = tf.py_function(encode,\n                                               inp=[freshness, review],\n                                               Tout=[tf.int32, tf.int32])\n    review.set_shape([None])\n    encoded_freshness.set_shape(1,)\n    return encoded_freshness, review\n\n\nTAKE = 480_000\n\ntrain_data = data.map(tf_map).take(1000)\ntrain_data = train_data.padded_batch(8, padded_shapes=([None], (1,)))\n\nnext(iter(train_data))\n</code></pre>\n\n<p>How can I deal with this?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 233}]
[{"items": [{"tags": ["keras", "reinforcement-learning", "tensorflow2.0"], "owner": {"account_id": 6679353, "reputation": 1409, "user_id": 5153056, "user_type": "registered", "accept_rate": 43, "profile_image": "https://www.gravatar.com/avatar/62e53654c189ce34cb3a3d8f6f967a0c?s=256&d=identicon&r=PG", "display_name": "Roberto Aureli", "link": "https://stackoverflow.com/users/5153056/roberto-aureli"}, "is_answered": true, "view_count": 369, "answer_count": 1, "score": 2, "last_activity_date": 1580838522, "creation_date": 1579779781, "question_id": 59877781, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59877781/ddpg-tensroflow-2-actor-update", "title": "DDPG (Tensroflow 2) actor update", "body": "<p>I'm facing a big problem with the implementation in tensorflow 2 of a DDPG agent.\nWhile the update of the critic network is clear and simple (just do a gradient descent over the loss) the update of the actor is a little bit harder.</p>\n\n<p>This is my implementation of the \"actor_update\" function:</p>\n\n<pre><code>def actor_train(self, minibatch):\n    s_batch, _, _, _, _ = minibatch\n    with tf.GradientTape() as tape1:\n        with tf.GradientTape() as tape2:\n            mu = self.actor_network(s_batch)\n            q = self.critic_network([s_batch, mu])\n        mu_grad = tape1.gradient(mu, self.actor_network.trainable_weights)\n    q_grad = tape2.gradient(q, self.actor_network.trainable_weights)\n\n    x = np.array(q_grad)*np.array(mu_grad)\n    x /= -len(minibatch)\n    self.actor_optimizer.apply_gradients(zip(x, self.actor_network.trainable_weights))\n</code></pre>\n\n<p>As stated by the paper, the optimization is the product of two gradients: one is the gradient of the Q function wrt the actions and the other is the gradient of the actor function wrt the weights.</p>\n\n<p>Starting all the nets with weights taken by an uniform distribution between -1e-3 and 1e-3, the actor seems to not update it weights.\nInstead, plotting the result of the critic (using MountainCarContinous as test env) shows a little choerence with the data.</p>\n\n<p>This is the code of the critic for completeness:</p>\n\n<pre><code>def critic_train(self, minibatch):\n    s_batch, a_batch, r_batch, s_1_batch, t_batch = minibatch\n\n    mu_prime = np.array(self.actor_target_network(s_1_batch))\n    q_prime = self.critic_target_network([s_1_batch, mu_prime])\n    ys = r_batch + self.GAMMA * (1 - t_batch) * q_prime\n\n\n    with tf.GradientTape() as tape:\n        predicted_qs = self.critic_network([s_batch, a_batch])\n        loss = tf.keras.losses.MSE(ys, predicted_qs)\n        dloss = tape.gradient(loss, self.critic_network.trainable_weights)\n\n    self.critic_optimizer.apply_gradients(zip(dloss, self.critic_network.trainable_weights))\n</code></pre>\n\n<p>As an extra, the actor seems to saturate after a winning episode. (Means it get stuck on +1 or -1 for every input).</p>\n\n<p>Where is the problem? Is the update function right? Or is it only an hyperparameters tuning problem?</p>\n\n<p>This is the repo is someone want to have a better view of the problem: <a href=\"https://github.com/bebbo203/DDPG\" rel=\"nofollow noreferrer\">Github repo</a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 41}]
[{"items": [{"tags": ["python", "tensorflow", "tensorflow2.0", "tensorboard"], "owner": {"account_id": 2417041, "reputation": 1331, "user_id": 2110869, "user_type": "registered", "accept_rate": 89, "profile_image": "https://i.stack.imgur.com/thUI8.jpg?s=256&g=1", "display_name": "reubenjohn", "link": "https://stackoverflow.com/users/2110869/reubenjohn"}, "is_answered": true, "view_count": 2532, "answer_count": 1, "score": 1, "last_activity_date": 1586702667, "creation_date": 1586697247, "last_edit_date": 1586697698, "question_id": 61172053, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/61172053/tensorboard-graph-with-custom-training-loop-does-not-include-my-model", "title": "Tensorboard Graph with custom training loop does not include my Model", "body": "<p>I have created my own loop as shown in the TF 2 migration guide <a href=\"https://www.tensorflow.org/guide/migrate#write_your_own_loop\" rel=\"nofollow noreferrer\">here</a>.<br>\nI am currently able to see the graph for only the <code>--- VISIBLE ---</code> section of the code below. How do I make my model (defined in the <code>---NOT VISIBLE---</code> section) visible in tensorboard?</p>\n\n<p>If I was not using a custom training loop, I could have gone with the <a href=\"https://www.tensorflow.org/tensorboard/graphs#train_the_model_and_log_data\" rel=\"nofollow noreferrer\">documented</a> <code>model.fit approach</code>:</p>\n\n<pre><code>model.fit(..., callbacks=[keras.callbacks.TensorBoard(log_dir=logdir)])\n</code></pre>\n\n<p>In TF 1, the approach used to be quite straightforward:</p>\n\n<pre><code>tf.compat.v1.summary.FileWriter(LOGDIR, sess.graph)\n</code></pre>\n\n<p>The Tensorboard migration guide clearly states (<a href=\"https://www.tensorflow.org/tensorboard/migrate#additional_tips\" rel=\"nofollow noreferrer\">here</a>) that:</p>\n\n<blockquote>\n  <p>No direct writing of tf.compat.v1.Graph - instead use @tf.function and trace functions</p>\n</blockquote>\n\n<pre><code>configure_default_gpus()\ntf.summary.trace_on(graph=True)\nK = tf.keras\ndataset = sanity_dataset(BATCH_SIZE)\n\n#-------------------------- NOT VISIBLE -----------------------------------------\nmodel = K.models.Sequential([\n    K.layers.Flatten(input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)),\n    K.layers.Dense(10, activation=K.layers.LeakyReLU()),\n    K.layers.Dense(IMG_WIDTH * IMG_HEIGHT * IMG_CHANNELS, activation=K.layers.LeakyReLU()),\n    K.layers.Reshape((IMG_WIDTH, IMG_HEIGHT, IMG_CHANNELS)),\n])\n#--------------------------------------------------------------------------------\n\noptimizer = tf.keras.optimizers.Adam()\nloss_fn = K.losses.Huber()\n\n\n@tf.function\ndef train_step(inputs, targets):\n    with tf.GradientTape() as tape:\n        predictions = model(inputs, training=True)\n#-------------------------- VISIBLE ---------------------------------------------\n        pred_loss = loss_fn(targets, predictions)\n\n    gradients = tape.gradient(pred_loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n#--------------------------------------------------------------------------------\n    return pred_loss, predictions\n\n\nwith tf.summary.create_file_writer(LOG_DIR).as_default() as writer:\n    for epoch in range(5):\n        for step, (input_batch, target_batch) in enumerate(dataset):\n            total_loss, predictions = train_step(input_batch, target_batch)\n\n            if step == 0:\n                tf.summary.trace_export(name=\"all\", step=step, profiler_outdir=LOG_DIR)\n            tf.summary.scalar('loss', total_loss, step=step)\n            writer.flush()\nwriter.close()\n</code></pre>\n\n<p>There's a <a href=\"https://stackoverflow.com/questions/60639731/tensorboard-for-custom-training-loop-in-tensorflow-2\">similar unanswered question</a> where the OP was unable to view any graph.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 120}]
[{"items": [{"tags": ["python", "tensorflow", "time-series", "tensorflow2.0", "tensorflow-datasets"], "owner": {"account_id": 1539759, "reputation": 379, "user_id": 1434693, "user_type": "registered", "accept_rate": 70, "profile_image": "https://www.gravatar.com/avatar/bb209eff018c97b0070f3168f08d2ab2?s=256&d=identicon&r=PG", "display_name": "Milan Jain", "link": "https://stackoverflow.com/users/1434693/milan-jain"}, "is_answered": false, "view_count": 210, "answer_count": 1, "score": 0, "last_activity_date": 1625642663, "creation_date": 1613787007, "last_edit_date": 1613967183, "question_id": 66287320, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66287320/tensorflow-2-0-flat-map-to-flatten-dataset-of-dataset-returns-cardinality-2", "title": "Tensorflow 2.0: flat_map() to flatten Dataset of Dataset returns cardinality -2", "body": "<p>I am trying to run the following code (as given in Tensorflow documentation) to create windows of my data and then flatten the dataset of datasets.</p>\n<pre><code>window_size = 5\n\nwindows = range_ds.window(window_size, shift=1)\nfor sub_ds in windows.take(5):\n    print(sub_ds)\n\nflat_windows = windows.flat_map(lambda x: x)\n</code></pre>\n<p>The problem is that <code>flat_windows.cardinality().numpy()</code> returns cardinality to be -2 which is creating problem for me during training. I tried looking for ways to set_cardinality of a dataset but couldn't find anything. I also tried other ways of flattening a dataset of datasets, but again no success.</p>\n<p><strong>Edit-1:</strong> The problem with the training is that the shape is unknown (at Linear and Dense layers) when I am training a subclass model (given below). The model trains well when I train the model eagerly (through <code>tf.config.run_functions_eagerly(True)</code>) but that is slow. Therefore I want the input data to be known for the model training.</p>\n<h1>Neural Network</h1>\n<pre><code>class NeuralNetworkModel(tf.keras.Model): \n    def __init__(self):\n        super(NeuralNetworkModel, self).__init__()\n        self.encoder = Encoder()        \n    \n    def train_step(self, inputs):       \n        X        = inputs[0]\n        Y        = inputs[1] \n        \n        with tf.GradientTape() as tape:\n            enc_X    = self.encoder(X)\n            enc_Y    = self.encoder(Y)    \n\n            # loss:        \n            loss   = tf.norm(enc_Y - enc_X, axis = [0, 1], ord = 'fro')\n                \n        # Compute gradients\n        trainable_vars = self.encoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_vars)\n\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # Compute our own metrics\n        loss_tracker.update_state(loss)\n        \n        # Return a dict mapping metric names to current value.\n        # Note that it will include the loss (tracked in self.metrics).\n        return {&quot;loss&quot;: loss_tracker.result()}\n        \n    @property\n    def metrics(self):\n        # We list our `Metric` objects here so that `reset_states()` can be\n        # called automatically at the start of each epoch\n        # or at the start of `evaluate()`.\n        # If you don't implement this property, you have to call\n        # `reset_states()` yourself at the time of your choosing.\n        return [loss_tracker]\n    \n    def test_step(self, inputs):       \n        X = inputs[0]\n        Y = inputs[1] \n\n        Psi_X    = self.encoder(X)\n        Psi_Y    = self.encoder(Y)    \n\n        # loss:        \n        loss   = tf.norm(Psi_Y - Psi_X, axis = [0, 1], ord = 'fro')\n\n        # Compute our own metrics\n        loss_tracker.update_state(loss)\n        \n        # Return a dict mapping metric names to current value.\n        # Note that it will include the loss (tracked in self.metrics).\n        return {&quot;loss&quot;: loss_tracker.result()}\n        \nclass Encoder(tf.keras.Model):\n    def __init__(self):\n        super(Encoder, self).__init__(dtype = 'float64', name = 'Encoder')\n        self.input_layer   = DenseLayer(128)\n        self.hidden_layer1 = DenseLayer(128)\n        self.hidden_layer2 = DenseLayer(64)        \n        self.hidden_layer3 = DenseLayer(64)\n        self.output_layer  = LinearLayer(64)\n        \n    def call(self, input_data, training):\n        fx = self.input_layer(input_data)        \n        fx = self.hidden_layer1(fx)\n        fx = self.hidden_layer2(fx)\n        fx = self.hidden_layer3(fx)\n        return self.output_layer(fx)    \n\nclass LinearLayer(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(LinearLayer, self).__init__(dtype = 'float64')\n        self.units = units\n\n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.w = self.add_weight(shape = (input_dim, self.units), \n                             initializer = &quot;random_normal&quot;, \n                             trainable = True)\n        self.b = self.add_weight(shape = (self.units,),    \n                             initializer = tf.zeros_initializer(),\n                             trainable = True)\n\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w) + self.b\n\nclass DenseLayer(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(DenseLayer, self).__init__(dtype = 'float64')\n        self.units = units\n    \n    def build(self, input_shape):\n        input_dim = input_shape[-1]\n        self.w = self.add_weight(shape = (input_dim, self.units), \n                             initializer = &quot;random_normal&quot;, \n                             trainable = True)\n        self.b = self.add_weight(shape = (self.units,),    \n                             initializer = tf.zeros_initializer(),\n                             trainable = True)\n\n    def call(self, inputs):\n        x = tf.matmul(inputs, self.w) + self.b\n        return tf.nn.elu(x)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 202}]
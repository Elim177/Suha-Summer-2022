[{"items": [{"tags": ["tensorflow", "lstm", "dropout"], "owner": {"account_id": 10716056, "reputation": 2138, "user_id": 7886651, "user_type": "registered", "accept_rate": 76, "profile_image": "https://i.stack.imgur.com/zfb59.jpg?s=256&g=1", "display_name": "I. A", "link": "https://stackoverflow.com/users/7886651/i-a"}, "is_answered": true, "view_count": 769, "accepted_answer_id": 62032206, "answer_count": 1, "score": 2, "last_activity_date": 1590579833, "creation_date": 1590526694, "question_id": 62031302, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62031302/how-to-apply-monte-carlo-dropout-in-tensorflow-for-an-lstm-if-batch-normalizat", "title": "How to apply Monte Carlo Dropout, in tensorflow, for an LSTM if batch normalization is part of the model?", "body": "<p>I have a model composed of 3 LSTM layers followed by a batch norm layer and finally dense layer. Here is the code:</p>\n\n<pre><code>def build_uncomplied_model(hparams):\n    inputs = tf.keras.Input(shape=(None, hparams[\"n_features\"]))\n    x = return_RNN(hparams[\"rnn_type\"])(hparams[\"cell_size_1\"], return_sequences=True, recurrent_dropout=hparams['dropout'])(inputs)\n    x = return_RNN(hparams[\"rnn_type\"])(hparams[\"cell_size_2\"], return_sequences=True)(x)\n    x = return_RNN(hparams[\"rnn_type\"])(hparams[\"cell_size_3\"], return_sequences=True)(x)\n    x = layers.BatchNormalization()(x)\n    outputs = layers.TimeDistributed(layers.Dense(hparams[\"n_features\"]))(x)\n\n    model = tf.keras.Model(inputs, outputs, name=RNN_type + \"_model\")\n    return model\n</code></pre>\n\n<p>Now I am aware that to apply MCDropout, we can apply the following code:</p>\n\n<pre><code>y_predict = np.stack([my_model(X_test, training=True) for x in range(100)])\ny_proba = y_predict.mean(axis=0)\n</code></pre>\n\n<p>However, setting <code>training = True</code> will force the batch norm layer to overfit the testing dataset. </p>\n\n<p>Additionally, building a custom Dropout layer while setting training to True isn't a solution in my case because I am using LSTM.</p>\n\n<pre><code>class MCDropout(tf.keras.layers.Dropout):\n    def call(self, inputs):\n        return super().call(inputs, training=True)\n</code></pre>\n\n<p>Any help is much appreciated!!</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 59}]
[{"items": [{"tags": ["tensorflow", "convolution"], "owner": {"user_type": "does_not_exist", "display_name": "user6449618"}, "is_answered": true, "view_count": 593, "accepted_answer_id": 37748225, "answer_count": 1, "score": 0, "last_activity_date": 1465567373, "creation_date": 1465556532, "last_edit_date": 1465567373, "question_id": 37746583, "content_license": "CC BY-SA 3.0", "link": "https://stackoverflow.com/questions/37746583/tensorflow-tutorial-of-convolution-scale-of-logit", "title": "tensorflow tutorial of convolution, scale of logit", "body": "<p>I am trying to edit my own model by adding some code to cifar10.py and here is the question.</p>\n\n<p>In cifar10.py, the [tutorial][1] says:</p>\n\n<blockquote>\n  <p><strong>EXERCISE:</strong> The output of inference are un-normalized logits. Try editing the network architecture to return normalized predictions using tf.nn.softmax().</p>\n</blockquote>\n\n<p>So I directly input the output from \"local4\" to <code>tf.nn.softmax()</code>. This gives me the <em>scaled</em> logits which means the sum of all logits is 1.</p>\n\n<p>But in the loss function, the cifar10.py code uses:</p>\n\n<pre><code>tf.nn.sparse_softmax_cross_entropy_with_logits()\n</code></pre>\n\n<p>and description of this function says </p>\n\n<blockquote>\n  <p><strong>WARNING:</strong> This op expects unscaled logits, since it performs a softmax on logits internally for efficiency. Do not call this op with the output of softmax, as it will produce incorrect results.</p>\n</blockquote>\n\n<p>Also, according to the description, logits as input to above funtion must have the shape [batch_size, num_classes] and it means logits should be unscaled softmax, like sample code calculate unnormalized softmaxlogit as follow.</p>\n\n<pre><code>  # softmax, i.e. softmax(WX + b)\n  with tf.variable_scope('softmax_linear') as scope:\n    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES],\n                                          stddev=1/192.0, wd=0.0)\n    biases = _variable_on_cpu('biases', [NUM_CLASSES],\n                              tf.constant_initializer(0.0))\n    softmax_linear = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n    _activation_summary(softmax_linear)\n</code></pre>\n\n<p>Does this mean I don't have to use <code>tf.nn.softmax</code> in the code?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 116}]
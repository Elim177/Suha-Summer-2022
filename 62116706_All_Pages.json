[{"items": [{"tags": ["python-3.x", "tensorflow"], "owner": {"account_id": 16506887, "reputation": 21, "user_id": 11926707, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/54b1572b91fed948325dad9c13406e8b?s=256&d=identicon&r=PG&f=1", "display_name": "shuntw", "link": "https://stackoverflow.com/users/11926707/shuntw"}, "is_answered": false, "view_count": 309, "answer_count": 1, "score": 0, "last_activity_date": 1590987168, "creation_date": 1590930444, "question_id": 62116706, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62116706/tf-reduce-mean-is-not-working-in-tf-function", "title": "tf.reduce_mean is not working in tf.function", "body": "<p><strong>code below</strong></p>\n\n<hr>\n\n<p>@tf.function</p>\n\n<p>def train_step(self, batch):</p>\n\n<pre><code>with tf.GradientTape() as disc_tape, tf.GradientTape() as infe_tape, tf.GradientTape() as gener_tape:\n  loss_encoder, loss_decoder, loss_disc = self.compute_loss(batch)\n\n  grad_encoder = gener_tape.gradient(loss_encoder, self.model.inference_net.trainable_variables)\n  grad_decoder = infe_tape.gradient(loss_decoder, self.model.generative_net.trainable_variables)\n  grad_disc = disc_tape.gradient(loss_disc, self.model.discriminator.trainable_variables)\n\n  self.infe_optimizer.apply_gradients(zip(grad_encoder, self.model.inference_net.trainable_variables))\n  self.genr_optimizer.apply_gradients(zip(grad_decoder, self.model.generative_net.trainable_variables))\n  self.disc_optimizer.apply_gradients(zip(grad_disc, self.model.discriminator.trainable_variables))\n\n  return loss_encoder, loss_decoder, loss_disc\n</code></pre>\n\n<hr>\n\n<hr>\n\n<p>@tf.function</p>\n\n<p>def train(self, epoch):</p>\n\n<pre><code>loss_history = tf.zeros([3,0])\nfor batch in self.train_dataset:\n  loss_encoder, loss_decoder, loss_disc = self.train_step(batch)\n  loss_each = tf.expand_dims(tf.stack([loss_encoder, loss_decoder, loss_disc], axis=0), 1)\n  loss_history = tf.concat([loss_history, loss_each], axis=1)\n\nmean_loss_encoder, mean_loss_decoder, mean_loss_disc = tf.reduce_mean(loss_history, axis=1)\ntf.summary.scalar('encoder loss', data=mean_loss_encoder, step=epoch, description='train_loss')\ntf.summary.scalar('decoder loss', data=mean_loss_decoder, step=epoch, description='train_loss')\ntf.summary.scalar('discriminator loss', data=mean_loss_disc, step=epoch, description='train_loss')\nprint('encoder training loss %.5f' % mean_loss_encoder, 'decoder training loss %.5f' % mean_loss_decoder, 'discriminator training loss %.5f' % mean_loss_disc)\n</code></pre>\n\n<hr>\n\n<hr>\n\n<p>@tf.function</p>\n\n<p>def validation_step(self, batch):\n     return self.compute_loss(batch, training=tf.constant(False, tf.bool))</p>\n\n<hr>\n\n<hr>\n\n<p>@tf.function</p>\n\n<p>def validation(self, epoch):\n    '''produce mean loss for epoch and append to self.val_loss_monior'''</p>\n\n<pre><code>loss_history = tf.zeros([3,0])\nfor batch in self.validation_dataset:\n    loss_encoder, loss_decoder, loss_disc = self.validation_step(batch)\n    loss_each = tf.expand_dims(tf.concat([loss_encoder, loss_decoder, loss_disc], axis=0), 1)\n    loss_history = tf.concat([loss_history, loss_each], axis=1) \nmean_loss_each = tf.reduce_mean(loss_history, axis=1)\nprint('encoder validation loss %.5f' % mean_loss_each[0], 'decoder validation loss %.5f' % mean_loss_each[1], 'discriminator validation loss %.5f' % mean_loss_each[2])\n\ntf.summary.scalar('encoder loss', data=mean_loss_each[0], step=epoch, description='val_loss')\ntf.summary.scalar('decoder loss', data=mean_loss_each[1], step=epoch, description='val_loss')\ntf.summary.scalar('discriminator loss', data=mean_loss_each[2], step=epoch, description='val_loss')\n\n# self.val_loss_monior: (3, self.patience), each row store history loss for each network\nloss_each = tf.expand_dims(mean_loss_each, 1)\nif epoch + 1 &gt; self.patience:\n  is_less =  self.val_loss_monior &lt; loss_each\n  is_less_encoder, is_less_decoder, is_less_discriminator = list(map(lambda x: tf.raw_ops.Any(input=tf.cast(tf.squeeze(x), tf.bool), axis=0), tf.split(is_less, 3, axis=0)))\n\n  if not is_less_encoder:\n     self.infe_optimizer.learning_rate *= self.decay_rate\n  if not is_less_decoder:\n     self.genr_optimizer.learning_rate *= self.decay_rate\n  if not is_less_discriminator:\n     self.disc_optimizer.learning_rate *= self.decay_rate\n\nself.val_loss_monior = tf.slice(self.val_loss_monior, [0, 1], [3, self.patience - 1])\nself.val_loss_monior = tf.concat([self.val_loss_monior, loss_each], axis=1)\n\ntf.summary.scalar('encoder learning rate', data=self.infe_optimizer.learning_rate, step=epoch)\ntf.summary.scalar('decoder learning rate', data=self.genr_optimizer.learning_rate, step=epoch)\ntf.summary.scalar('discriminator learning rate', data=self.disc_optimizer.learning_rate, step=epoch)\n</code></pre>\n\n<hr>\n\n<hr>\n\n<p>@tf.function</p>\n\n<p>def fit(self):</p>\n\n<pre><code>self.val_loss_monitor = tf.zeros([3, self.patience])\nfor epoch in range(self.epochs):\n  start = time.time()\n  epoch = tf.cast(epoch, tf.int64)\n  self.train(epoch)\n  self.validation(epoch)\n\n  print ('Time for epoch {} is {} sec'.format(epoch, time.time()-start))\n  display.clear_output(wait=True)\n  if (epoch + 1) % 5 == 0:\n    self.generate_images(epoch)\n</code></pre>\n\n<hr>\n\n<p><strong>error massage below</strong></p>\n\n<hr>\n\n<p>OperatorNotAllowedInGraphError: in user code:</p>\n\n<pre><code>&lt;ipython-input-5-8af39e045349&gt;:152 fit  *\n    self.train(epoch)\n&lt;ipython-input-5-8af39e045349&gt;:90 train  *\n    mean_loss_encoder, mean_loss_decoder, mean_loss_disc = tf.reduce_mean(loss_history, axis=1)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:561 __iter__\n    self._disallow_iteration()\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:554 _disallow_iteration\n    self._disallow_when_autograph_enabled(\"iterating over `tf.Tensor`\")\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:532 _disallow_when_autograph_enabled\n    \" decorating it directly with @tf.function.\".format(task))\n\nOperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.\n</code></pre>\n\n<hr>\n\n<p>The problem should be </p>\n\n<hr>\n\n<p>mean_loss_encoder, mean_loss_decoder, mean_loss_disc = tf.reduce_mean(loss_history, axis=1)</p>\n\n<hr>\n\n<p>Error is about iterating over <code>tf.Tensor</code> is not allowed.\nIs reduce_mean not working in tf.function?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 157}]
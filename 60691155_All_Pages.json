[{"items": [{"tags": ["python-3.x", "keras", "deep-learning", "lstm", "tensorflow2.0"], "owner": {"account_id": 11684920, "reputation": 179, "user_id": 8963151, "user_type": "registered", "profile_image": "https://graph.facebook.com/1920065991647720/picture?type=large", "display_name": "Chompakorn CChaichot", "link": "https://stackoverflow.com/users/8963151/chompakorn-cchaichot"}, "is_answered": true, "view_count": 81, "accepted_answer_id": 60694487, "answer_count": 1, "score": 1, "last_activity_date": 1584286100, "creation_date": 1584262245, "question_id": 60691155, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/60691155/graph-disconnected-when-implementing-custom-lstm", "title": "Graph Disconnected when implementing custom LSTM", "body": "<p>I've been trying to write my own LSTM for customization. However, an error occurred when I try to call my code using Keras. The error said the graph was disconnected on <code>c_prev</code>, but <code>c_prev</code> was used as LSTM's cell initializer. So I'm not sure if it's something wrong with my code or the way I call the model. Any help is appreciated.</p>\n\n<p>My environment:</p>\n\n<ul>\n<li>Python 3.7.6</li>\n<li>Tensorflow 2.1.0 (installed via pip)</li>\n<li>Mac Mojave</li>\n</ul>\n\n<pre class=\"lang-py prettyprint-override\"><code>class EtienneLSTM(tf.keras.layers.Layer):\n    def __init__(self, units, activation='tanh', recurrent_activation='sigmoid',\n    kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', \n    use_bias=True, unit_forget_bias=True, \n    kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n    kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n    # dropout=0.0, recurrent_dropout=0.0,\n    return_sequences=False, return_state=False, go_backwards=False, use_batchnorm=False):\n        super(EtienneLSTM, self).__init__()\n        self.units = units #\n\n        self.activation = tf.keras.layers.Activation(activation) #\n        self.recurrent_activation = tf.keras.layers.Activation(recurrent_activation) #\n\n        self.use_bias = use_bias #\n\n        self.kernel_initializer = kernel_initializer #\n        self.recurrent_initializer =  recurrent_initializer #\n        self.bias_initializer = bias_initializer #\n        self.unit_forget_bias = unit_forget_bias #\n        if self.unit_forget_bias:\n            self.bias_initializer = 'zeros'\n\n        self.kernel_regularizer = kernel_regularizer #\n        self.recurrent_regularizer = recurrent_regularizer #\n        self.bias_regularizer = bias_regularizer #\n        self.activity_regularizer = activity_regularizer\n\n        self.kernel_constraint = kernel_constraint #\n        self.recurrent_constraint = recurrent_constraint #\n        self.bias_constraint = bias_constraint #\n\n        # self.dropout = dropout\n        # self.recurrent_dropout = recurrent_dropout\n\n        self.return_sequences = return_sequences #\n        self.return_state = return_state #\n        self.go_backwards = go_backwards #\n\n        self.use_batchnorm = use_batchnorm\n        if self.use_batchnorm:\n            self.batchnorm_f = tf.keras.layers.BatchNormalization()\n            self.batchnorm_i = tf.keras.layers.BatchNormalization()\n            self.batchnorm_o = tf.keras.layers.BatchNormalization()\n            self.batchnorm_c = tf.keras.layers.BatchNormalization()\n\n    def build(self, input_shape):\n        # forgot gate\n        self.Wf = self.add_weight(shape=(input_shape[-1], self.units), initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, trainable=True)\n        self.Uf = self.add_weight(shape=(self.units, self.units), initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint, trainable=True)\n        if self.unit_forget_bias:\n            self.bf = self.add_weight(shape=(self.units,), initializer='ones', regularizer=self.bias_regularizer, constraint=self.bias_constraint, trainable=True)\n        else:\n            self.bf = self.add_weight(shape=(self.units,), initializer=self.bias_initializer, regularizer=self.bias_regularizer, trainable=True)\n        # input gate\n        self.Wi = self.add_weight(shape=(input_shape[-1], self.units), initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, trainable=True)\n        self.Ui = self.add_weight(shape=(self.units, self.units), initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint, trainable=True)\n        if self.use_bias:\n            self.bi = self.add_weight(shape=(self.units,), initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint, trainable=True)\n\n        # output gate\n        self.Wo = self.add_weight(shape=(input_shape[-1], self.units), initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, trainable=True)\n        self.Uo = self.add_weight(shape=(self.units, self.units), initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint, trainable=True)\n        if self.use_bias:\n            self.bo = self.add_weight(shape=(self.units,), initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint, trainable=True)\n\n        # context\n        self.Wc = self.add_weight(shape=(input_shape[-1], self.units), initializer=self.kernel_initializer, regularizer=self.kernel_regularizer, constraint=self.kernel_constraint, trainable=True)\n        self.Uc = self.add_weight(shape=(self.units, self.units), initializer=self.recurrent_initializer, regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint, trainable=True)\n        if self.use_bias:\n            self.bc = self.add_weight(shape=(self.units,), initializer=self.bias_initializer, regularizer=self.bias_regularizer, constraint=self.bias_constraint, trainable=True)\n\n    def _inp_gate(self, x, hidden):\n        return self.recurrent_activation(tf.matmul(x, self.Wi) + tf.matmul(hidden, self.Ui) + self.bi)\n\n    def _new_mem(self, x, hidden):\n        return self.activation(tf.matmul(x, self.Wc) + tf.matmul(hidden, self.Uc) + self.bc)\n\n    def _forget_gate(self, x, hidden):\n        return self.recurrent_activation(tf.matmul(x, self.Wf) + tf.matmul(hidden, self.Uf) + self.bf)\n\n    def _update_cell(self, c_prev, c_tilde, f_t, i_t):\n        return (f_t * c_prev) + (i_t * c_tilde)\n\n    def _out_gate(self, x, hidden, ct):\n        ot = self.recurrent_activation(tf.matmul(x, self.Wo) + tf.matmul(hidden, self.Uo) + self.bo)\n        return ot * self.activation(ct)\n\n    def call(self, x, hidden, c_prev):\n        if self.go_backwards: x = x[:,:,::-1]\n\n        f_t = self._forget_gate(x, hidden)\n        i_t = self._inp_gate(x, hidden)\n        c_tilde = self._new_mem(x, hidden)\n        c_t = self._update_cell(c_prev, c_tilde, f_t, i_t)\n        h_t = self._out_gate(x, hidden, c_t)\n\n        # if self.return_state:\n        #     return h_t, c_t\n        # if self.return_sequences:\n        #     return h_t\n        return h_t\ntf.keras.backend.clear_session()\n\ndef get_LSTM():\n    inp = tf.keras.layers.Input(shape=(200, 40))\n    out = tf.keras.layers.LSTM(32)(inp)\n    return tf.keras.Model(inp, out)\n\ndef get_EtienneLSTM():\n    inp = tf.keras.layers.Input(shape=(200, 40))\n    h0 = tf.keras.layers.Input(shape=(32,), name='h0')\n    c0 = tf.keras.layers.Input(shape=(32,), name='c0')\n    out = EtienneLSTM(32)(inp, h0, c0)\n    return tf.keras.Model(inp, out)\n\nmodel_tf = get_LSTM()\nmodel_etienne = get_EtienneLSTM()\n\n</code></pre>\n\n<p>Here is my error message:</p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n in \n     14 \n     15 model_tf = get_LSTM()\n---&gt; 16 model_etienne = get_EtienneLSTM()\n\n in get_EtienneLSTM()\n     11     c0 = tf.keras.layers.Input(shape=(32,), name='c0')\n     12     out = EtienneLSTM(32)(inp, h0, c0)\n---&gt; 13     return tf.keras.Model(inp, out)\n     14 \n     15 model_tf = get_LSTM()\n\n~/.env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py in __init__(self, *args, **kwargs)\n    144 \n    145   def __init__(self, *args, **kwargs):\n--&gt; 146     super(Model, self).__init__(*args, **kwargs)\n    147     _keras_api_gauge.get_cell('model').set(True)\n    148     # initializing _distribution_strategy here since it is possible to call\n\n~/.env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in __init__(self, *args, **kwargs)\n    167         'inputs' in kwargs and 'outputs' in kwargs):\n    168       # Graph network\n--&gt; 169       self._init_graph_network(*args, **kwargs)\n    170     else:\n    171       # Subclassed network\n\n~/.env/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)\n    455     self._self_setattr_tracking = False  # pylint: disable=protected-access\n    456     try:\n--&gt; 457       result = method(self, *args, **kwargs)\n    458     finally:\n    459       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access\n\n~/.env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in _init_graph_network(self, inputs, outputs, name, **kwargs)\n    322     # Keep track of the network's nodes and layers.\n    323     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n--&gt; 324         self.inputs, self.outputs)\n    325     self._network_nodes = nodes\n    326     self._nodes_by_depth = nodes_by_depth\n\n~/.env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py in _map_graph_network(inputs, outputs)\n   1674                              'The following previous layers '\n   1675                              'were accessed without issue: ' +\n-&gt; 1676                              str(layers_with_complete_input))\n   1677         for x in nest.flatten(node.output_tensors):\n   1678           computable_tensors.add(id(x))\n\nValueError: Graph disconnected: cannot obtain value for tensor Tensor(\"c0:0\", shape=(None, 32), dtype=float32) at layer \"c0\". The following previous layers were accessed without issue: ['input_2']\n</code></pre>\n\n<p>Thank you for your help.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 90}]
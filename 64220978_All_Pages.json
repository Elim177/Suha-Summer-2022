[{"items": [{"tags": ["python-3.x", "tensorflow", "deep-learning", "quantization"], "owner": {"account_id": 19669314, "reputation": 11, "user_id": 14398875, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/ef2123e0fd561d8e48816d69494d51a4?s=256&d=identicon&r=PG&f=1", "display_name": "bibek", "link": "https://stackoverflow.com/users/14398875/bibek"}, "is_answered": false, "view_count": 184, "answer_count": 0, "score": 1, "last_activity_date": 1601968401, "creation_date": 1601968401, "question_id": 64220978, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64220978/port-pytorch-code-to-tf2-0-equivalent-of-x-batch-requires-grad-true-in-tf2-0", "title": "Port pytorch code to tf2.0: equivalent of x_batch.requires_grad = True in tf2.0?", "body": "<p>I am trying to port pytorch code to tf2 code for this <a href=\"https://github.com/amirgholami/ZeroQ/blob/master/classification/uniform_test.py\" rel=\"nofollow noreferrer\">repo</a>.\nThe overall logic of the code is as follows:</p>\n<ul>\n<li>sample data from Gaussian distribution with zero mean and unit variance</li>\n<li>extract mean and variance from batch-normalization layers of trained model</li>\n<li>input sampled data into model and get outputs from each conv-layers(before BN layers)</li>\n<li>calculate L2-loss between outputs and extracted means and variances</li>\n<li>update the sampled data via Optimizer</li>\n</ul>\n<p>My issue is that in order to update the data, it has be made trainable and therefore I converted to <code>x_batch</code> to <code>x_batch = tf.Variable(x_batch, trainable=True)</code> but Variables in <code>tf</code> are not iterable and hence there is an a problem when updating the weights via <code>optimizer.apply_gradients(zip(gradients, x_batch))</code>.</p>\n<p>In pytorch, it's relatively simple and can done simple by</p>\n<pre><code>for x_batch in dataloader:    \n    x_batch.requires_grad = True\n    .\n    .\n    .\n    # update the distilled data\n    loss.backward()\n    optimizer.step()\n</code></pre>\n<p>My attempt in tf2.0 is below.</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation, Input, MaxPooling2D, Dropout, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow import keras\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.layers import BatchNormalization\nimport numpy as np\n\ndef vgg_block(x, filters, layers, name, weight_decay):\n    for i in range(layers):\n        x = Conv2D(filters, (3, 3), padding='same', kernel_initializer='he_normal',\n                     kernel_regularizer=regularizers.l2(weight_decay), name=f'{name}_conv_{i}')(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n    return x\n\n\ndef vgg8(x, weight_decay=1e-4, is_classifier=False):\n    x = vgg_block(x, 16, 2, 'block_1', weight_decay)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = vgg_block(x, 32, 2, 'block_2', weight_decay)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = vgg_block(x, 64, 2, 'block_3', weight_decay)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Flatten()(x)\n    x = Dense(512, kernel_initializer='he_normal', activation='relu', name = 'dense_1')(x)\n    x = Dense(10)(x)\n    return x\n    \ndef l2_loss(A, B):\n    &quot;&quot;&quot;\n    L-2 loss between A and B normalized by length.\n    Shape of A should be (features_num, ), shape of B should be (batch_size, features_num)\n    &quot;&quot;&quot;\n    # pytorch : (A - B).norm()**2 / B.size(0)\n    diff = A-B\n    l2loss = tf.nn.l2_loss(diff)  # sum(t ** 2) / 2\n    return l2loss / B.shape[0]\n    \n\ninputs = Input((32, 32, 3))\nmodel = Model(inputs, vgg8(inputs))\n\neps = 1.0e-6\nbn_stats = []\n\nfor layer in model.layers:\n    if isinstance(layer, BatchNormalization):\n        bn_gamma, bn_beta, bn_mean, bn_var = layer.get_weights()\n        #print(bn_mean.shape, bn_var.shape)  # tf.reshape(w, [-1]) \n        bn_stats.append((bn_mean, tf.math.sqrt(bn_var+eps)))\n\n    \n\nextractor = tf.keras.models.Model(inputs=model.inputs,\n                        outputs=[layer.output for layer in model.layers if isinstance(layer, Conv2D)])\n\n\nclass UniformDataset(keras.utils.Sequence):\n    &quot;&quot;&quot;\n    get random uniform samples with mean 0 and variance 1\n    &quot;&quot;&quot;\n    def __init__(self, length, size):\n        self.length = length\n        self.size = size\n    \n    def __len__(self):\n        return self.length\n    \n    def __getitem__(self, idx):\n        sample = tf.random.normal(self.size)\n        return sample\n\ndata = UniformDataset(length=100, size=(32, 32,3))\n\n# convert to tf.data iterator\ntrain_iter = iter(data)\ntrain_data = []\nfor x in train_iter:\n    train_data.append(x)\ntrain_data = tf.stack(train_data, axis=0)\ntrain_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n\nrefined_gaussian = []\niterations = 500\n\nfor x_batch in train_dataset.batch(32):\n    x_batch = tf.Variable(x_batch, trainable=True)  # make x_batch trainable\n    \n    outputs = extractor(x_batch)\n    \n    input_mean = tf.zeros([1,3]) \n    input_std = tf.ones([1,3])   \n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.5)\n\n    with tf.GradientTape(persistent=True) as tape:\n        for it in range(iterations):\n            mean_loss = 0\n            std_loss = 0\n\n            for cnt, (bn_stat, output) in enumerate(zip(bn_stats, outputs)):  \n                output = tf.reshape(output, [output.shape[0], output.shape[-1], -1])\n                tmp_mean = tf.math.reduce_mean(output, axis=2)\n                tmp_std = tf.math.reduce_std(output, axis=2) + eps\n                bn_mean, bn_std = bn_stat[0], bn_stat[1]\n                mean_loss += l2_loss(bn_mean, tmp_mean)\n                std_loss += l2_loss(bn_std, tmp_std)\n            \n            #print('mean_loss', mean_loss, 'std_loss', std_loss)\n            x_reshape = tf.reshape(x_train, [x_train.shape[0], 3, -1])\n            tmp_mean = tf.math.reduce_mean(x_reshape, axis=2)\n            tmp_std = tf.math.reduce_std(x_reshape, axis=2) + eps\n\n            mean_loss += l2_loss(input_mean, tmp_mean)\n            std_loss += l2_loss(input_std, tmp_std)\n            loss = mean_loss + std_loss\n            gradients = tape.gradient(loss, x_batch)\n\n            optimizer.apply_gradients(zip(gradients, x_batch))\n            \n        refined_gaussian.append(x_batch)\n        \n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 174}]
[{"items": [{"tags": ["python", "tensorflow", "keras", "tensorflow2.0"], "owner": {"account_id": 6956681, "reputation": 1056, "user_id": 5337505, "user_type": "registered", "accept_rate": 67, "profile_image": "https://lh5.googleusercontent.com/-F4WMV67MG-o/AAAAAAAAAAI/AAAAAAAABGA/b-vMtZ0piXI/photo.jpg?sz=256", "display_name": "Siladittya", "link": "https://stackoverflow.com/users/5337505/siladittya"}, "is_answered": true, "view_count": 367, "accepted_answer_id": 62428706, "answer_count": 1, "score": 0, "last_activity_date": 1593009584, "creation_date": 1592318704, "last_edit_date": 1592333051, "question_id": 62411242, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62411242/tf-keras-backend-clip-not-giving-correct-results", "title": "tf.keras.backend.clip not giving correct results", "body": "<p><code>tf.keras.backend.clip</code> is not clipping the tensors</p>\n\n<p>When I use <code>tf.keras.backend.clip</code> inside this function</p>\n\n<pre><code>def grads_ds(model_ds, ds_inputs,y_true,cw):\n    print(y_true)\n    with tf.GradientTape() as ds_tape:\n        y_pred = model_ds(ds_inputs)\n        print(y_pred.numpy())\n        logits_1 = -1*y_true*K.log(y_pred)*cw[:,0]\n        logits_0 = -1*(1-y_true)*K.log(1-y_pred)*cw[:,1]\n        loss = logits_1 + logits_0\n        loss_value_ds = K.sum(loss)\n\n    ds_grads = ds_tape.gradient(loss_value_ds,model_ds.trainable_variables,unconnected_gradients=tf.UnconnectedGradients.NONE)\n    for g in ds_grads:\n        g = tf.keras.backend.clip(g,min_grad,max_grad)\n    return loss_value_ds, ds_grads\n</code></pre>\n\n<p>THe value of the gradients remain the same (unclipped).</p>\n\n<p>When I use <code>tf.keras.backend.clip</code> inside the custom training loop, same way</p>\n\n<pre><code>for g in ds_grads:\n    g = tf.keras.backend.clip(g,min_grad,max_grad)\n</code></pre>\n\n<p>it doesn't work. The gradient applied to the variables are not clipped.</p>\n\n<p>However, if I print <code>g</code> within the loop, then it shows the clipped value.</p>\n\n<p>Can't understand where the problem is.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 32}]
[{"items": [{"tags": ["tensorflow", "keras", "parallel-processing", "generative-adversarial-network", "multi-gpu"], "owner": {"account_id": 6094529, "reputation": 371, "user_id": 4755781, "user_type": "registered", "profile_image": "https://i.stack.imgur.com/81kKj.jpg?s=256&g=1", "display_name": "Giriraj Pawar", "link": "https://stackoverflow.com/users/4755781/giriraj-pawar"}, "is_answered": false, "view_count": 454, "answer_count": 0, "score": 4, "last_activity_date": 1616344385, "creation_date": 1615893787, "last_edit_date": 1616176332, "question_id": 66654250, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66654250/how-to-modify-the-keras-cyclegan-example-code-to-run-parallelly-on-gpus-using-tf", "title": "How to modify the Keras CycleGAN example code to run parallelly on GPUs using tf.strategy", "body": "<p>Here is the example of CycleGAN from the Keras\n<a href=\"https://keras.io/examples/generative/cyclegan/\" rel=\"nofollow noreferrer\">CycleGAN Example Using Keras.</a></p>\n<p>Here is my modified implementation to use multiple GPUs. To implement the custom training I have used a reference <a href=\"https://www.tensorflow.org/tutorials/distribute/custom_training\" rel=\"nofollow noreferrer\">Custom training with tf.distribute.Strategy</a></p>\n<p>I want an example of CycleGAN from the Keras to run fast using GPUs. As further I need to process and train a huge amount of data. As well as CycleGAN uses multiple loss functions <code>train_step</code> will return 4 types of losses, currently, I am just returning one for easier understanding. Still, the training on GPUs is dead slow. I am not able to find the reason behind this.</p>\n<p>Am I using <code>tf.distribute.Strategy</code> wrongly?</p>\n<pre><code>&quot;&quot;&quot;\nTitle: CycleGAN\nAuthor: [A_K_Nain](https://twitter.com/A_K_Nain)\nDate created: 2020/08/12\nLast modified: 2020/08/12\nDescription: Implementation of CycleGAN.\n&quot;&quot;&quot;\n\n&quot;&quot;&quot;\n## CycleGAN\nCycleGAN is a model that aims to solve the image-to-image translation\nproblem. The goal of the image-to-image translation problem is to learn the\nmapping between an input image and an output image using a training set of\naligned image pairs. However, obtaining paired examples isn't always feasible.\nCycleGAN tries to learn this mapping without requiring paired input-output images,\nusing cycle-consistent adversarial networks.\n- [Paper](https://arxiv.org/pdf/1703.10593.pdf)\n- [Original implementation](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n&quot;&quot;&quot;\n\n&quot;&quot;&quot;\n## Setup\n&quot;&quot;&quot;\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport tensorflow_addons as tfa\nimport tensorflow_datasets as tfds\n\ntfds.disable_progress_bar()\nautotune = tf.data.experimental.AUTOTUNE\n\n# Create a MirroredStrategy.\nstrategy = tf.distribute.MirroredStrategy()\nprint('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n\n&quot;&quot;&quot;\n## Prepare the dataset\nIn this example, we will be using the\n[horse to zebra](https://www.tensorflow.org/datasets/catalog/cycle_gan#cycle_ganhorse2zebra)\ndataset.\n&quot;&quot;&quot;\n\n# Load the horse-zebra dataset using tensorflow-datasets.\ndataset, _ = tfds.load(&quot;cycle_gan/horse2zebra&quot;, with_info=True, as_supervised=True)\ntrain_horses, train_zebras = dataset[&quot;trainA&quot;], dataset[&quot;trainB&quot;]\ntest_horses, test_zebras = dataset[&quot;testA&quot;], dataset[&quot;testB&quot;]\n\n# Define the standard image size.\norig_img_size = (286, 286)\n# Size of the random crops to be used during training.\ninput_img_size = (256, 256, 3)\n# Weights initializer for the layers.\nkernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n# Gamma initializer for instance normalization.\ngamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\nbuffer_size = 256\nbatch_size = 1\n\n\ndef normalize_img(img):\n    img = tf.cast(img, dtype=tf.float32)\n    # Map values in the range [-1, 1]\n    return (img / 127.5) - 1.0\n\n\ndef preprocess_train_image(img, label):\n    # Random flip\n    img = tf.image.random_flip_left_right(img)\n    # Resize to the original size first\n    img = tf.image.resize(img, [*orig_img_size])\n    # Random crop to 256X256\n    img = tf.image.random_crop(img, size=[*input_img_size])\n    # Normalize the pixel values in the range [-1, 1]\n    img = normalize_img(img)\n    return img\n\n\ndef preprocess_test_image(img, label):\n    # Only resizing and normalization for the test images.\n    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n    img = normalize_img(img)\n    return img\n\n\n&quot;&quot;&quot;\n## Create `Dataset` objects\n&quot;&quot;&quot;\nBATCH_SIZE_PER_REPLICA = batch_size\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync  \n\n\n# Apply the preprocessing operations to the training data\ntrain_horses = (\n    train_horses.map(preprocess_train_image, num_parallel_calls=autotune)\n    .cache()\n    .shuffle(buffer_size)\n    .batch(GLOBAL_BATCH_SIZE)\n)\ntrain_zebras = (\n    train_zebras.map(preprocess_train_image, num_parallel_calls=autotune)\n    .cache()\n    .shuffle(buffer_size)\n    .batch(GLOBAL_BATCH_SIZE)\n)\n\n# Apply the preprocessing operations to the test data\ntest_horses = (\n    test_horses.map(preprocess_test_image, num_parallel_calls=autotune)\n    .cache()\n    .shuffle(buffer_size)\n    .batch(GLOBAL_BATCH_SIZE)\n)\ntest_zebras = (\n    test_zebras.map(preprocess_test_image, num_parallel_calls=autotune)\n    .cache()\n    .shuffle(buffer_size)\n    .batch(GLOBAL_BATCH_SIZE)\n)\n\n# Visualize some samples\n\n_, ax = plt.subplots(4, 2, figsize=(10, 15))\nfor i, samples in enumerate(zip(train_horses.take(4), train_zebras.take(4))):\n    horse = (((samples[0][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n    zebra = (((samples[1][0] * 127.5) + 127.5).numpy()).astype(np.uint8)\n    ax[i, 0].imshow(horse)\n    ax[i, 1].imshow(zebra)\nplt.show()\nplt.savefig('Visualize_Some_Samples')\nplt.close()     \n\n\n# Building blocks used in the CycleGAN generators and discriminators\n\nclass ReflectionPadding2D(layers.Layer):\n    &quot;&quot;&quot;Implements Reflection Padding as a layer.\n    Args:\n        padding(tuple): Amount of padding for the\n        spatial dimensions.\n    Returns:\n        A padded tensor with the same type as the input tensor.\n    &quot;&quot;&quot;\n\n    def __init__(self, padding=(1, 1), **kwargs):\n        self.padding = tuple(padding)\n        super(ReflectionPadding2D, self).__init__(**kwargs)\n\n    def call(self, input_tensor, mask=None):\n        padding_width, padding_height = self.padding\n        padding_tensor = [\n            [0, 0],\n            [padding_height, padding_height],\n            [padding_width, padding_width],\n            [0, 0],\n        ]\n        return tf.pad(input_tensor, padding_tensor, mode=&quot;REFLECT&quot;)\n\n\ndef residual_block(\n    x,\n    activation,\n    kernel_initializer=kernel_init,\n    kernel_size=(3, 3),\n    strides=(1, 1),\n    padding=&quot;valid&quot;,\n    gamma_initializer=gamma_init,\n    use_bias=False,\n):\n    dim = x.shape[-1]\n    input_tensor = x\n\n    x = ReflectionPadding2D()(input_tensor)\n    x = layers.Conv2D(\n        dim,\n        kernel_size,\n        strides=strides,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=use_bias,\n    )(x)\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    x = activation(x)\n\n    x = ReflectionPadding2D()(x)\n    x = layers.Conv2D(\n        dim,\n        kernel_size,\n        strides=strides,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=use_bias,\n    )(x)\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    x = layers.add([input_tensor, x])\n    return x\n\n\ndef downsample(\n    x,\n    filters,\n    activation,\n    kernel_initializer=kernel_init,\n    kernel_size=(3, 3),\n    strides=(2, 2),\n    padding=&quot;same&quot;,\n    gamma_initializer=gamma_init,\n    use_bias=False,\n):\n    x = layers.Conv2D(\n        filters,\n        kernel_size,\n        strides=strides,\n        kernel_initializer=kernel_initializer,\n        padding=padding,\n        use_bias=use_bias,\n    )(x)\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    if activation:\n        x = activation(x)\n    return x\n\n\ndef upsample(\n    x,\n    filters,\n    activation,\n    kernel_size=(3, 3),\n    strides=(2, 2),\n    padding=&quot;same&quot;,\n    kernel_initializer=kernel_init,\n    gamma_initializer=gamma_init,\n    use_bias=False,\n):\n    x = layers.Conv2DTranspose(\n        filters,\n        kernel_size,\n        strides=strides,\n        padding=padding,\n        kernel_initializer=kernel_initializer,\n        use_bias=use_bias,\n    )(x)\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    if activation:\n        x = activation(x)\n    return x\n\n\n\n\ndef get_resnet_generator(\n    filters=64,\n    num_downsampling_blocks=2,\n    num_residual_blocks=9,\n    num_upsample_blocks=2,\n    gamma_initializer=gamma_init,\n    name=None,\n):\n    img_input = layers.Input(shape=input_img_size, name=name + &quot;_img_input&quot;)\n    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n        x\n    )\n    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n    x = layers.Activation(&quot;relu&quot;)(x)\n\n    # Downsampling\n    for _ in range(num_downsampling_blocks):\n        filters *= 2\n        x = downsample(x, filters=filters, activation=layers.Activation(&quot;relu&quot;))\n\n    # Residual blocks\n    for _ in range(num_residual_blocks):\n        x = residual_block(x, activation=layers.Activation(&quot;relu&quot;))\n\n    # Upsampling\n    for _ in range(num_upsample_blocks):\n        filters //= 2\n        x = upsample(x, filters, activation=layers.Activation(&quot;relu&quot;))\n\n    # Final block\n    x = ReflectionPadding2D(padding=(3, 3))(x)\n    x = layers.Conv2D(3, (7, 7), padding=&quot;valid&quot;)(x)\n    x = layers.Activation(&quot;tanh&quot;)(x)\n\n    model = keras.models.Model(img_input, x, name=name)\n    return model\n\n\n&quot;&quot;&quot;\n## Build the discriminators\nThe discriminators implement the following architecture:\n`C64-&gt;C128-&gt;C256-&gt;C512`\n&quot;&quot;&quot;\n\ndef get_discriminator(\n    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n):\n    img_input = layers.Input(shape=input_img_size, name=name + &quot;_img_input&quot;)\n    x = layers.Conv2D(\n        filters,\n        (4, 4),\n        strides=(2, 2),\n        padding=&quot;same&quot;,\n        kernel_initializer=kernel_initializer,\n    )(img_input)\n    x = layers.LeakyReLU(0.2)(x)\n\n    num_filters = filters\n    for num_downsample_block in range(3):\n        num_filters *= 2\n        if num_downsample_block &lt; 2:\n            x = downsample(\n                x,\n                filters=num_filters,\n                activation=layers.LeakyReLU(0.2),\n                kernel_size=(4, 4),\n                strides=(2, 2),\n            )\n        else:\n            x = downsample(\n                x,\n                filters=num_filters,\n                activation=layers.LeakyReLU(0.2),\n                kernel_size=(4, 4),\n                strides=(1, 1),\n            )\n\n    x = layers.Conv2D(\n        1, (4, 4), strides=(1, 1), padding=&quot;same&quot;, kernel_initializer=kernel_initializer\n    )(x)\n\n    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n    return model\n\n\n\n&quot;&quot;&quot;\n## Build the CycleGAN model\n&quot;&quot;&quot;\n\nclass CycleGan(keras.Model):\n    def __init__(\n        self,\n        generator_G,\n        generator_F,\n        discriminator_X,\n        discriminator_Y,\n        lambda_cycle=10.0,\n        lambda_identity=0.5,\n    ):\n        super(CycleGan, self).__init__()\n        self.gen_G = generator_G\n        self.gen_F = generator_F\n        self.disc_X = discriminator_X\n        self.disc_Y = discriminator_Y\n        self.lambda_cycle = lambda_cycle\n        self.lambda_identity = lambda_identity\n\n    def compile(\n        self,\n        gen_G_optimizer,\n        gen_F_optimizer,\n        disc_X_optimizer,\n        disc_Y_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.gen_G_optimizer = gen_G_optimizer\n        self.gen_F_optimizer = gen_F_optimizer\n        self.disc_X_optimizer = disc_X_optimizer\n        self.disc_Y_optimizer = disc_Y_optimizer\n        self.generator_loss_fn = gen_loss_fn\n        self.discriminator_loss_fn = disc_loss_fn\n        #self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n        #self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        # x is Horse and y is zebra\n        real_x, real_y = batch_data\n\n        with tf.GradientTape(persistent=True) as tape:\n            # Horse to fake zebra\n            fake_y = self.gen_G(real_x, training=True)\n            # Zebra to fake horse -&gt; y2x\n            fake_x = self.gen_F(real_y, training=True)\n\n            # Cycle (Horse to fake zebra to fake horse): x -&gt; y -&gt; x\n            cycled_x = self.gen_F(fake_y, training=True)\n            # Cycle (Zebra to fake horse to fake zebra) y -&gt; x -&gt; y\n            cycled_y = self.gen_G(fake_x, training=True)\n\n            # Identity mapping\n            same_x = self.gen_F(real_x, training=True)\n            same_y = self.gen_G(real_y, training=True)\n\n            # Discriminator output\n            disc_real_x = self.disc_X(real_x, training=True)\n            disc_fake_x = self.disc_X(fake_x, training=True)\n\n            disc_real_y = self.disc_Y(real_y, training=True)\n            disc_fake_y = self.disc_Y(fake_y, training=True)\n\n            # Generator adverserial loss\n            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n\n            # Generator cycle loss\n            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n\n            # Generator identity loss\n            id_loss_G = (\n                self.identity_loss_fn(real_y, same_y)\n                * self.lambda_cycle\n                * self.lambda_identity\n            )\n            id_loss_F = (\n                self.identity_loss_fn(real_x, same_x)\n                * self.lambda_cycle\n                * self.lambda_identity\n            )\n\n            # Total generator loss\n            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n\n            # Discriminator loss\n            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n\n        # Get the gradients for the generators\n        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n\n        # Get the gradients for the discriminators\n        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n\n        # Update the weights of the generators\n        self.gen_G_optimizer.apply_gradients(\n            zip(grads_G, self.gen_G.trainable_variables)\n        )\n        self.gen_F_optimizer.apply_gradients(\n            zip(grads_F, self.gen_F.trainable_variables)\n        )\n\n        # Update the weights of the discriminators\n        self.disc_X_optimizer.apply_gradients(\n            zip(disc_X_grads, self.disc_X.trainable_variables)\n        )\n        self.disc_Y_optimizer.apply_gradients(\n            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n        )\n\n        return total_loss_G\n        # return [total_loss_G, total_loss_F, disc_X_loss, disc_Y_loss]\n        \n\n\n# Open a strategy scope.\nwith strategy.scope():\n   mae_loss_fn = keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)\n    \n    # Loss function for evaluating cycle consistency loss\n    def cycle_loss_fn(real, cycled):\n        cycle_loss = mae_loss_fn(real, cycled)\n        cycle_loss = tf.nn.compute_average_loss(cycle_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n        return cycle_loss\n         \n\n    # Loss function for evaluating identity mapping loss\n    def identity_loss_fn(real, same):\n        identity_loss = mae_loss_fn(real, same)\n        identity_loss = tf.nn.compute_average_loss(identity_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n        return identity_loss\n\n    # Loss function for evaluating adversarial loss\n    adv_loss_fn = keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n\n    # Define the loss function for the generators\n    def generator_loss_fn(fake):\n        fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n        fake_loss = tf.nn.compute_average_loss(fake_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n        return fake_loss\n\n\n    # Define the loss function for the discriminators\n    def discriminator_loss_fn(real, fake):\n        real_loss = adv_loss_fn(tf.ones_like(real), real)\n        fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n        real_loss = tf.nn.compute_average_loss(real_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n        fake_loss = tf.nn.compute_average_loss(fake_loss, global_batch_size=GLOBAL_BATCH_SIZE)  \n        return (real_loss + fake_loss) * 0.5\n\n    # Get the generators\n    gen_G = get_resnet_generator(name=&quot;generator_G&quot;)\n    gen_F = get_resnet_generator(name=&quot;generator_F&quot;)\n\n    # Get the discriminators\n    disc_X = get_discriminator(name=&quot;discriminator_X&quot;)\n    disc_Y = get_discriminator(name=&quot;discriminator_Y&quot;)\n\n\n\n    # Create cycle gan model\n    cycle_gan_model = CycleGan(\n        generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n    )\n    optimizer = keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)    \n    # Compile the model\n    cycle_gan_model.compile(\n        gen_G_optimizer=optimizer,\n        gen_F_optimizer=optimizer,\n        disc_X_optimizer=optimizer,\n        disc_Y_optimizer=optimizer,\n        gen_loss_fn=generator_loss_fn,\n        disc_loss_fn=discriminator_loss_fn,\n        cycle_loss_fn=cycle_loss_fn,\n        identity_loss_fn=identity_loss_fn\n    )\n\n\ntrain_dist_dataset = strategy.experimental_distribute_dataset(\n    tf.data.Dataset.zip((train_horses, \n    train_zebras)))\n\n# `run` replicates the provided computation and runs it\n# with the distributed input.\n@tf.function\ndef distributed_train_step(dataset_inputs):\n  per_replica_losses = strategy.run(cycle_gan_model.train_step, args=(dataset_inputs,))\n  return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n                         axis=None)\n\n&quot;&quot;&quot;\n## Train the end-to-end model\n&quot;&quot;&quot;\nfor epoch in range(1):\n    # TRAIN LOOP\n    all_loss = 0.0\n    num_batches = 0.0\n    for one_batch in train_dist_dataset:\n        all_loss +=  distributed_train_step(one_batch)\n        num_batches += 1\n    train_loss = all_loss/num_batches\n    print(train_loss)\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 183}]
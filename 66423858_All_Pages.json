[{"items": [{"tags": ["python", "c++", "tensorflow"], "owner": {"account_id": 5964559, "reputation": 18377, "user_id": 4832499, "user_type": "registered", "accept_rate": 79, "profile_image": "https://www.gravatar.com/avatar/76532994f6efa8d7dc8a3bb5b7b262a8?s=256&d=identicon&r=PG&f=1", "display_name": "Passer By", "link": "https://stackoverflow.com/users/4832499/passer-by"}, "is_answered": false, "view_count": 85, "answer_count": 0, "score": 1, "last_activity_date": 1614608708, "creation_date": 1614608708, "question_id": 66423858, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66423858/persist-gradienttape-after-construction", "title": "Persist GradientTape after Construction", "body": "<p>Is it possible to set persistence of a <code>GradientTape</code> after its construction?</p>\n<p>The use case is when I have code that I don't control pass a non-persistent tape to me, with which I must take two gradients with dependencies between them.</p>\n<pre class=\"lang-py prettyprint-override\"><code># I don't control this\nwith tf.GradientTape() as tape:\n    y = f(x)\n    z = g(y)\n\n# I control this\ndzdx = tape.gradient(z, x)\nresult = tape.gradient(z, y, output_gradients=dzdx)  # not persistent\n</code></pre>\n<p>I've pondered the possibility of simply setting <code>tape._persistent</code> before calling <code>gradient</code>, but the persistence gets <a href=\"https://github.com/tensorflow/tensorflow/blob/v2.4.1/tensorflow/python/eager/backprop.py#L856\" rel=\"nofollow noreferrer\">passed all the way</a> to C++ code at construction, which probably won't like being inconsistent with the Python code</p>\n<pre class=\"lang-py prettyprint-override\"><code>def push_new_tape(persistent=False, watch_accessed_variables=True):\n  &quot;&quot;&quot;Pushes a new tape onto the tape stack.&quot;&quot;&quot;\n  tape = pywrap_tfe.TFE_Py_TapeSetNew(persistent, watch_accessed_variables)\n  return Tape(tape)\n</code></pre>\n<p>Which ultimately <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/eager/tape.h#L125\" rel=\"nofollow noreferrer\">ends up at</a></p>\n<pre class=\"lang-cpp prettyprint-override\"><code>template &lt;typename Gradient, typename BackwardFunction, typename TapeTensor&gt;\nclass GradientTape {\n public:\n  // If `persistent` is true, GradientTape will not eagerly delete backward\n  // functions (and hence the tensors they keep alive). Instead, everything\n  // is deleted in ~GradientTape. Persistent GradientTapes are useful when\n  // users want to compute multiple gradients over the same tape.\n  explicit GradientTape(bool persistent) : persistent_(persistent) {}\n  ~GradientTape() {\n    for (const auto&amp; pair : op_tape_) {\n      pair.second.backward_function_deleter(pair.second.backward_function);\n    }\n  }\n\n  // ...\n\n  bool IsPersistent() const { return persistent_; }\n</code></pre>\n<p>Where <code>IsPersistent</code> is the only public interface to <code>persistent_</code>. I can't tell how exactly <code>persistent_</code> is used, and that sounds like a memory leak in waiting.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 197}]
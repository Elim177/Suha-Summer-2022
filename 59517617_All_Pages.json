[{"items": [{"tags": ["tensorflow", "keras", "tensorflow2.0"], "owner": {"account_id": 2376160, "reputation": 1485, "user_id": 2079445, "user_type": "registered", "accept_rate": 45, "profile_image": "https://www.gravatar.com/avatar/b3054d111aa00407c8d8000957ad9958?s=256&d=identicon&r=PG", "display_name": "ling", "link": "https://stackoverflow.com/users/2079445/ling"}, "is_answered": true, "view_count": 303, "accepted_answer_id": 59518370, "answer_count": 1, "score": 0, "last_activity_date": 1577657601, "creation_date": 1577604960, "last_edit_date": 1577653213, "question_id": 59517617, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/59517617/how-to-obtain-the-encoder-from-the-make-csv-dataset", "title": "How to obtain the encoder from the make_csv_dataset?", "body": "<p>I used this code from the tutorial:</p>\n\n<pre><code>def get_train_dataset(file_path, **kwargs):\n  dataset = tf.data.experimental.make_csv_dataset(\n      file_path,\n      batch_size=10, # Artificially small to make examples easier to show.\n      label_name=LABEL_COLUMN,\n      na_value=\"?\",\n      num_epochs=1,\n      ignore_errors=True,\n      select_columns= CSV_COLUMNS,\n      **kwargs)\n  return dataset \n</code></pre>\n\n<p>Then I created the train set:</p>\n\n<pre><code>raw_train_data = get_train_dataset(train_file_path)\n</code></pre>\n\n<p>to train the model.</p>\n\n<p>The question is how to get the encoder used for the training data for encoding new text?</p>\n\n<p>I loaded the new data, but this doesn't use the same encoder as the training data:</p>\n\n<pre><code>raw_test_data = get_test_dataset(new_data_file_path)\n</code></pre>\n\n<p>How to obtain the original encoder when using tf.data.experimental.make_csv_dataset?</p>\n\n<p>EDIT:</p>\n\n<pre><code>    train_file_path = \"./train.csv\"\ntest_file_path = \"./test.csv\"\n\nLABEL_COLUMN = 'target'\nCSV_COLUMNS = ['text', 'target']\n\ndef get_train_dataset(file_path, **kwargs):\n  dataset = tf.data.experimental.make_csv_dataset(\n      file_path,\n      batch_size=10, # Artificially small to make examples easier to show.\n      label_name=LABEL_COLUMN,\n      na_value=\"?\",\n      num_epochs=1,\n      ignore_errors=True,\n      select_columns= CSV_COLUMNS,\n      **kwargs)\n  return dataset\n\ndef get_test_dataset(file_path, **kwargs):\n  dataset = tf.data.experimental.make_csv_dataset(\n      file_path,\n      batch_size=10, # Artificially small to make examples easier to show.\n      na_value=\"?\",\n      num_epochs=1,\n      ignore_errors=True,\n      **kwargs)\n  return dataset\n\nsample_submission = pd.read_csv(\"./sample_submission.csv\")\n\nraw_train_data = get_train_dataset(train_file_path)\nraw_test_data = get_test_dataset(test_file_path)\n\ndef extract_train_tensor(example, label):\n    print(example)\n    return example['text'], label\n\ndef extract_test_tensor(example):\n    print(example)\n    return example['text']\n\ntest_data = raw_test_data.map(lambda ex: extract_test_tensor(ex))\ntest_data_size = len(list(test_data))\nprint(\"test size: \", test_data_size)\n\ntrain_data_all = raw_train_data.map(lambda ex, label: extract_train_tensor(ex, label))\ntrain_data_all = train_data_all.shuffle(10000)\nprint(train_data_all)\ntrain_data_size = len(list(train_data_all))\nprint(\"train size: \", train_data_size)\ntrain_size = int(0.7 * train_data_size)\nval_size = int(0.3 * train_data_size)\ntrain_data = train_data_all.take(train_size)\nval_data = train_data_all.skip(train_size)\n\nembedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\n                           dtype=tf.string, trainable=True)\nmodel = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_data,\n                    epochs=20,\n                    validation_data=val_data,\n                    verbose=1)\nimport numpy as np\npredictions = model.predict(test_data)\npredictions = np.where(predictions &gt; 0.5, 1, 0)\nsample_submission['target'] = predictions\nprint(predictions)\n</code></pre>\n\n<p>The two calls to get_train_dataset() and get_test_dataset() to generate train and test data. The train data is split into train and validation sets and the accuracy is great. However, the test data accuracy is very low. Both data sets are strings of text and I didn't do any encoding.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 93}]
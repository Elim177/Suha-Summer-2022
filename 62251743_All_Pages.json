[{"items": [{"tags": ["tensorflow", "keras", "gpu"], "owner": {"account_id": 2212212, "reputation": 87, "user_id": 1953737, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/12435886a59521557beb9a0f0fd35c57?s=256&d=identicon&r=PG", "display_name": "user1953737", "link": "https://stackoverflow.com/users/1953737/user1953737"}, "is_answered": false, "view_count": 146, "answer_count": 0, "score": 2, "last_activity_date": 1591564357, "creation_date": 1591564357, "question_id": 62251743, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/62251743/gpus-utilization-in-fit-and-custom-training-function-with-gradienttape-is-very-d", "title": "GPUs utilization in fit and custom training function with GradientTape is very different", "body": "<p>When I trained a model with model.fit with my own data generator function in tensorflow keras, I noticed GPU utilization could be almost 100%. This is because the model I trained is a good one regarding the parallelization.</p>\n\n<p>Nonetheless if I built a train function, something like the following function, I noticed the utilization is very different:</p>\n\n<pre><code>@tf.function\ndef train_step(input, output):\n  with tf.GradientTape() as tape:\n    predictions = model(input)\n    loss = loss_object(output, predictions)\n  gradients = tape.gradient(loss, model.trainable_variables)\n  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n  train_loss(loss)\n</code></pre>\n\n<p>In detail, I noticed it goes from 0% to almost 100%, and then goes down to 0% again for a moment between updates. I guess between two updates, data needs to transfer from CPU to GPU in the first place. This explains why it goes from 100% to 0% for some moment.</p>\n\n<p>But this leads to a question to me is: How to make it more optimal for a custom training function with tf.GradientTape so that the GPUs is almost 100% all the time (as with model.fit)?</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 182}]
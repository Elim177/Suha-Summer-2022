[{"items": [{"tags": ["python", "tensorflow", "gradienttape"], "owner": {"account_id": 20577072, "reputation": 11, "user_id": 15103996, "user_type": "registered", "profile_image": "https://lh3.googleusercontent.com/a-/AOh14Ggh85LkkiMW1pkzQLwS-6b8s_eEJcww4TJ-cWSf=k-s256", "display_name": "Yong KIM", "link": "https://stackoverflow.com/users/15103996/yong-kim"}, "is_answered": true, "view_count": 710, "answer_count": 1, "score": 1, "last_activity_date": 1611911079, "creation_date": 1611907348, "last_edit_date": 1611911079, "question_id": 65950732, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/65950732/tf-tape-gradient-returns-none-for-my-numerical-function-model", "title": "tf.tape.gradient() returns None for my numerical function model", "body": "<p>I'm trying to use <code>tf.GradientTape()</code>.\nBut the problem is <code>tape.gradient</code>returns <code>None</code>, so that the error output (<code>TypeError : unsupported operand type(s) for *: 'float' and 'NoneType'</code>) popped up.\nAs you can see in my code, <code>dloss_dparams = tape.gradient(Cost, [XX,YY])</code> returns none.\nI don't know What is the problem. Does anyone why it behaves like this?</p>\n<pre><code>import tensorflow as tf\nimport math\nimport numpy as np\nfrom numpy import loadtxt\nimport scipy.integrate as inte\nfrom numpy import asarray\nfrom numpy import savetxt\n\nDPA_l=7.68e+5 #Displacements/ion/cm\nNa = 6.10e+22 # atomic density atoms/cm3\nc=0.0424\nS = math.pi*(0.05)**2\nrmax=0.23#cm\nrmin=-0.23\nm=100\nn=100\nNdpa =1\nNinitial=1\ndx = (rmax-rmin)/m\ndy = (rmax-rmin)/n\n\ndfInd = loadtxt('intensity_2c_re2.csv', delimiter =',')\ndfRi = loadtxt('p2c_t,ri,sd_re3.csv', delimiter =',')\nFnd = np.full((len(dfInd),m+1,n+1),np.nan)\nDPAnd = np.full((len(dfInd),m+1,n+1),np.nan)\nLnd = np.full((len(dfInd),m+1,n+1),np.nan)\n\ndef I_V(bb):\n    return 2.2e-8+bb*1e-9\ndef T_D(aa):\n    return 7.9e-2 +aa*1e-3\n\nVnd = np.full((len(dfInd),m+1,n+1),np.nan)\nAnd = np.full((len(dfInd),m+1,n+1),np.nan)\nA2nd = np.full((len(dfInd),m+1,n+1),np.nan)\nf_x = np.full((len(dfInd),1),np.nan)\nf_y = np.full((len(dfInd),1),np.nan)\nSSB = np.full((len(dfInd),1),0)\nF0nd =np.full((len(dfInd),2),np.nan)\n\n#initial value\nfor i in range(len(dfInd)): \n    F0nd[i,1] = (0.762*dfInd[i,2]*dfInd[i,0]/S)*0.1*np.sqrt(2)/(2*(np.sqrt(math.pi))*math.erf(1/(5*c*2**2.5))*c)\n    F0nd[i,0]= dfInd[i,0]\n \nf_x[0,0] = inte.quad(lambda x : np.math.exp((-(x-(rmax))**2)/(2*c**2)),0,rmax-rmin)[0]\nf_y[0,0] = inte.quad(lambda y : np.exp((-(y-(rmax))**2)/(2*c**2)),0,rmax-rmin)[0]\nF0nd[0,1] = (0.762*dfInd[0,2]*dfInd[1,0]/S)*0.1*math.sqrt(2)/(2*(math.sqrt(math.pi))*math.erf(1/(5*c*2**2.5))*c) ###\ucd94\uac00 12/29\nDPAnd[0,:,:] = 0\nLnd[0,:,:] = 0\nVnd[0,:,:] = 0\n\nfor j in range(m+1):\n    for k in range(n+1):\n        Fnd[0,j,k] = math.sqrt(F0nd[0,1])*np.math.exp(-(((j*dx-(rmax))**2)/(2*c**2)))*math.sqrt(F0nd[0,1])*np.exp(-(((k*dy-(rmax))**2)/(2*c**2)))\n        And[0,j,k] = ((np.exp((-(j*dx-(rmax))**2)/(2*c**2))+np.exp((-((j+1)*dx-(rmax))**2)/(2*c**2)))*0.5*dx/f_x[0])*((np.exp((-(k*dy-(rmax))**2)/(2*c**2))+np.exp((-((k+1)*dy-(rmax))**2)/(2*c**2)))*0.5*dy/f_y[0])\n        A2nd[0,j,k] = (1-Vnd[0,j,k])*And[0,j,k]\n\n\ndef predictions(XX,YY):\n    i=0\n    v=1\n    aa=0.0\n    bb=0.0\n    \n    f_x[i+1,0] = inte.quad(lambda x : np.exp((-(x-(rmax-XX))**2)/(2*c**2)),0,rmax-rmin)[0]  #f_x[1,0] #f_x[21,0]\n    f_y[i+1,0] = inte.quad(lambda y : np.exp((-(y-(rmax-YY))**2)/(2*c**2)),0,rmax-rmin)[0]  #f_y[1,0] #f_y[21,0]\n    for j in range(m+1):\n        for k in range(n+1):\n            Fnd[i+1,j,k] = F0nd[i+1,1]*np.exp(-(((j*dx-(rmax-XX))**2)/(2*c**2)))*np.exp(-(((k*dy-(rmax-YY))**2)/(2*c**2)))\n            And[i+1,j,k] = ((np.exp((-(j*dx-(rmax-XX))**2)/(2*c**2))+np.exp((-((j+1)*dx-(rmax-XX))**2)/(2*c**2)))*0.5*dx/f_x[i+1])*((np.exp((-(k*dy-(rmax-YY))**2)/(2*c**2))+np.exp((-((k+1)*dy-(rmax-YY))**2)/(2*c**2)))*0.5*dy/f_y[i+1])\n            DPAnd[i+1,j,k] = DPAnd[i,j,k] + DPA_l*Fnd[i,j,k]/Na\n                \n            if  DPAnd[i+1,j,k] &lt; T_D(aa) :\n                Lnd[i+1,j,k] = DPAnd[i+1,j,k]/(T_D(aa))\n            elif DPAnd[i+1,j,k]&gt;=T_D(aa) :\n                Lnd[i+1,j,k] = 1.0\n                    \n            if Lnd[i+1,j,k]&lt;1:\n                A2nd[i+1,j,k] = (1.0-Lnd[i+1,j,k])*And[i+1,j,k]*0.79 + 1.0*And[i+1,j,k]*0.21\n            elif Lnd[i+1,j,k]==1.0:\n                A2nd[i+1,j,k] = (1.0-Lnd[i+1,j,k])*And[i+1,j,k]\n                    \n    SSB = I_V(bb)*(((A2nd).sum(axis=2)).sum(axis=1))\n    loss = (SSB[v]-dfRi[v-1,1])\n    return loss\n\n\nXX = tf.Variable(0.1)\nYY = tf.Variable(0.1)\nlearning_rate = 0.01\nfor j in range(100):\n    with tf.GradientTape() as tape:\n        tape.watch([XX,YY])\n        Cost = tf.reduce_mean(tf.square(predictions(XX,YY)))\n        print('Cost=',Cost)\n    dloss_dparams = tape.gradient(Cost, [XX,YY])\n    print('dloss_dparams[0] =',dloss_dparams[0],', dloss_dparams[1] =',dloss_dparams[1])\n    XX.assign_sub(learning_rate * dloss_dparams[0])\n    YY.aasign_sub(learning_rate * dloss_dparams[1])\n\n    if i%10 ==0 :\n        print(&quot;{:5} I {:10.4f} I {:10.4} I {:10.6f}&quot;.format(i, XX.numpy(), YY.numpy(), cost))    \n</code></pre>\n<p>output is that</p>\n<pre><code>Cost= tf.Tensor(2.803658208007937e-17, shape=(), dtype=float64)\ndloss_dparams[0] = None , dloss_dparams[1] = None\nTraceback (most recent call last):\nFile &quot;C:/Users/USER/Downloads/tensorflow_gradient descent14.py&quot;, line 103, in &lt;module&gt;\nXX.assign_sub(learning_rate * dloss_dparams[0])\nTypeError: unsupported operand type(s) for *: 'float' and 'NoneType'\n</code></pre>\n<p>the relation plot with Variables(XX,YY) and Cost is below, just in case.\n<a href=\"https://i.stack.imgur.com/QMRzr.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/QMRzr.png\" alt=\"enter image description here\" /></a></p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 221}]
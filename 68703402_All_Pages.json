[{"items": [{"tags": ["python", "deep-learning", "pytorch", "tensorflow2.0"], "owner": {"account_id": 19052928, "reputation": 31, "user_id": 13910718, "user_type": "registered", "profile_image": "https://lh6.googleusercontent.com/-eC4snBvXvPg/AAAAAAAAAAI/AAAAAAAAAAA/AMZuucnNd56DlJHh8wmPedoJXpVW_owwYw/photo.jpg?sz=256", "display_name": "Shijia li", "link": "https://stackoverflow.com/users/13910718/shijia-li"}, "is_answered": false, "view_count": 130, "answer_count": 0, "score": 2, "last_activity_date": 1628452772, "creation_date": 1628445401, "last_edit_date": 1628452772, "question_id": 68703402, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/68703402/why-is-the-tensorflow-2-x-with-tf-function-twice-as-fast-as-pytorch", "title": "Why is the TensorFlow 2.x with @tf.function twice as fast as pytorch?", "body": "<p>On small models or small datasets, <strong>tf2 with @tf.function is twice as fast as pytorch</strong>.</p>\n<p>Why Pytorch is slower than Tensorflow on small datasets/models? Am I setting it wrong? Is there any way to speed up pytorch?</p>\n<p>TF2 version</p>\n<pre><code>import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport numpy as np\nimport time\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\ndims = 600\nDATA_SIZE = 10000\nbatchsz = 100\nepochs = 10\nlr=1e-3\ninputs = np.random.random([DATA_SIZE, dims]).astype(np.float32)\ntargets =np.random.random([DATA_SIZE, dims]).astype(np.float32)\ntic_total=time.time()\nmodel=keras.Sequential([layers.Dense(dims),\n                        layers.ReLU(),\n                        layers.Dense(dims),\n                        layers.ReLU(),\n                        layers.Dense(dims),\n                        layers.ReLU(),\n                        layers.Dense(dims),\n                        layers.ReLU(),\n                        layers.Dense(dims),\n                        layers.ReLU(),\n                        layers.Dense(dims)])\n\n\ntrain_db = tf.data.Dataset.from_tensor_slices((inputs,targets))  # [w*h,C]\ntrain_db = train_db.shuffle(DATA_SIZE).batch(batchsz)\noptimizer = tf.optimizers.Adam(lr)\n\n@tf.function\ndef bp():\n    with tf.GradientTape() as tape:\n        loss = tf.reduce_mean(tf.square(model(data[0]) - data[1]))\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n\nfor epoch in range(epochs):\n    tic_epoch=time.time()\n    for step, data in enumerate(train_db):\n        bp()\n    toc_epoch = time.time()\n    print('time per epoch:', toc_epoch - tic_epoch)\ntoc_total = time.time()\nprint('time elapsed:',toc_total-tic_total)\n</code></pre>\n<p>Results:</p>\n<pre><code>time per epoch: 1.2558062076568604\ntime per epoch: 0.2627861499786377\ntime per epoch: 0.25072169303894043\ntime per epoch: 0.250835657119751\ntime per epoch: 0.2861459255218506\ntime per epoch: 0.26630401611328125\ntime per epoch: 0.24439024925231934\ntime per epoch: 0.23915791511535645\ntime per epoch: 0.25443601608276367\ntime per epoch: 0.2899332046508789\ntime elapsed: 4.2830810546875\n</code></pre>\n<p>Pytorch version</p>\n<pre><code>import numpy as np\nimport torch.utils\nimport torch.utils.data\nfrom torch import nn\nimport time\n\ndims = 600\nDATA_SIZE = 10000\nbatchsz = 100\nepochs = 10\nlr = 1e-3\ninputs = np.random.random([DATA_SIZE, dims]).astype(np.float32)\ntargets = np.random.random([DATA_SIZE, dims]).astype(np.float32)\ntic_total = time.time()\nmodel = nn.Sequential(\n    nn.Linear(dims, dims),\n    nn.ReLU(),\n    nn.Linear(dims, dims),\n    nn.ReLU(),\n    nn.Linear(dims, dims),\n    nn.ReLU(),\n    nn.Linear(dims, dims),\n    nn.ReLU(),\n    nn.Linear(dims, dims),\n    nn.ReLU(),\n    nn.Linear(dims, dims)\n).cuda()\n\ntrain_x = torch.tensor(inputs).cuda()\ntrain_y = torch.tensor(targets).cuda()\n\ntrain_db = torch.utils.data.TensorDataset(train_x, train_y)\ntrain_db = torch.utils.data.DataLoader(train_db, batch_size=batchsz, shuffle=True)\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nmodel.train()\nfor epoch in range(epochs):\n    tic_epoch = time.time()\n    for step, data in enumerate(train_db):\n        loss = (model(data[0]) - data[1]).square().mean()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    toc_epoch = time.time()\n    print('time per epoch:', toc_epoch - tic_epoch)\ntoc_total = time.time()\nprint('time elapsed:', toc_total - tic_total)\n</code></pre>\n<p>Results:</p>\n<pre><code>time per epoch: 1.1492640972137451\ntime per epoch: 0.4739706516265869\ntime per epoch: 0.4949944019317627\ntime per epoch: 0.48531651496887207\ntime per epoch: 0.4747319221496582\ntime per epoch: 0.4962129592895508\ntime per epoch: 0.6802644729614258\ntime per epoch: 0.5593955516815186\ntime per epoch: 0.4838066101074219\ntime per epoch: 0.4937736988067627\ntime elapsed: 8.071050882339478\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 192}]
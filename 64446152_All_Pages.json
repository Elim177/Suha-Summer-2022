[{"items": [{"tags": ["python", "tensorflow", "keras"], "owner": {"account_id": 12407130, "reputation": 176, "user_id": 9041483, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/59df4513b52491907c604fd4a028b965?s=256&d=identicon&r=PG&f=1", "display_name": "Danil Kononyhin", "link": "https://stackoverflow.com/users/9041483/danil-kononyhin"}, "is_answered": true, "view_count": 922, "accepted_answer_id": 64451370, "answer_count": 1, "score": 1, "last_activity_date": 1603278899, "creation_date": 1603200373, "question_id": 64446152, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/64446152/custom-learning-rate-scheduler-tf2-and-keras", "title": "Custom learning rate scheduler TF2 and Keras", "body": "<p>I am trying to write custom learning rate scheduler: cosine annealing with warm-up.\nBut I can't use it neither in Keras, nor in Tensorflow.\nBelow is the code:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n\ndef make_linear_lr(min_lr, max_lr, number_of_steps):\n    def gen_lr(step):\n        return (max_lr - min_lr) / number_of_steps * step + min_lr\n    return gen_lr\n\ndef make_cosine_anneal_lr(learning_rate, alpha, decay_steps):\n    def gen_lr(global_step):\n        global_step = min(global_step, decay_steps)\n        cosine_decay = 0.5 * (1 + np.cos(np.pi * global_step / decay_steps))\n        decayed = (1 - alpha) * cosine_decay + alpha\n        decayed_learning_rate = learning_rate * decayed\n        return decayed_learning_rate\n    return gen_lr\n\ndef make_cosine_annealing_with_warmup(min_lr, max_lr, number_of_steps, alpha, decay_steps):\n    gen_lr_1 = make_linear_lr(min_lr, max_lr, number_of_steps)\n    gen_lr_2 = make_cosine_anneal_lr(max_lr, alpha, decay_steps)\n    def gen_lr(global_step):\n        if global_step &lt; number_of_steps:\n            return gen_lr_1(global_step)\n        else:\n            return gen_lr_2(global_step - number_of_steps)\n        \n    return gen_lr\n\nclass CosineAnnealingWithWarmUP(tf.keras.optimizers.schedules.LearningRateSchedule):\n  def __init__(self, min_lr, max_lr, number_of_steps, alpha, decay_steps):\n    super(CosineAnnealingWithWarmUP, self).__init__()\n    self.gen_lr_ca =  make_cosine_annealing_with_warmup(min_lr, max_lr, number_of_steps, alpha, decay_steps)\n  def __call__(self, step):\n    return tf.cast(self.gen_lr_ca(step), tf.float32)\n\nlearning_rate_fn = CosineAnnealingWithWarmUP(.0000001, 0.01, 10_000, 0, 150_000)\noptimizer=tf.keras.optimizers.SGD(\n    learning_rate=learning_rate_fn, \n    momentum=0.95)\n</code></pre>\n<p>I use this function in TensorFlow to train my model:</p>\n<pre><code>def get_model_train_step_function(model, optimizer, vars_to_fine_tune, batch_size):\n  @tf.function\n  def train_step_fn(image_tensors,\n                    groundtruth_boxes_list,\n                    groundtruth_classes_list):\n    shapes = tf.constant(batch_size * [[640, 640, 3]], dtype=tf.int32)\n    model.provide_groundtruth(\n        groundtruth_boxes_list=groundtruth_boxes_list,\n        groundtruth_classes_list=groundtruth_classes_list)\n    with tf.GradientTape() as tape:\n      preprocessed_images = tf.concat(\n          [model.preprocess(\n              image_tensor\n           )[0]\n           for image_tensor in image_tensors], axis=0)\n      prediction_dict = model.predict(preprocessed_images, shapes)\n      losses_dict = model.loss(prediction_dict, shapes)\n      total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n      gradients = tape.gradient(total_loss, vars_to_fine_tune)\n      optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n    return total_loss\n\n  return train_step_fn \n</code></pre>\n<p>When I try to use it with TensorFlow, passing optimizer in get_model_train_step_function \u2014 it works if I remove @tf.function decorator. But it doesn't work with @tf.function, the error says:\nOperatorNotAllowedInGraphError: using a <code>tf.Tensor</code> as a Python <code>bool</code> is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.</p>\n<p>How should I write my custom learning rate scheduler? Also, I would like to use this Schedule with Keras. But it doesn't work there at all.</p>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 162}]
[{"items": [{"tags": ["python", "numpy", "tensorflow", "keras", "keras-tuner"], "owner": {"account_id": 20102528, "reputation": 31, "user_id": 14740723, "user_type": "registered", "profile_image": "https://www.gravatar.com/avatar/dbb87094b3107e6fb1f376fe0aed4128?s=256&d=identicon&r=PG&f=1", "display_name": "Christian Pommer", "link": "https://stackoverflow.com/users/14740723/christian-pommer"}, "is_answered": false, "view_count": 51, "answer_count": 1, "score": 0, "last_activity_date": 1613157825, "creation_date": 1613046704, "question_id": 66154846, "content_license": "CC BY-SA 4.0", "link": "https://stackoverflow.com/questions/66154846/custom-traing-loop-with-multiple-model-pass-through", "title": "Custom Traing Loop with multiple model pass through", "body": "<p>Dear stackoverflow members,</p>\n<p>I am currently trying to implement my own keras tuner training loop. In this loop I want to pass the input variable multiple times through the model in example:</p>\n<pre><code>Y = Startvalue\nfor i in range(x):\n   Y = model(Y)\n</code></pre>\n<p>I want to see if this method creates more stable simulations for my self feedback problem.\nWhen I implement it I get an OOM error even when I do not loop. This error does not occur when I just do it normally.\nMy Class example (the OOM error occurs when i switch logits for logits2:</p>\n<pre><code>class MyTuner(kt.Tuner):\n    def run_trial(self, trial, train_ds, validation_data):\n\n        model = self.hypermodel.build(trial.hyperparameters)\n\n        optimizer = tf.keras.optimizers.Adam()\n        epoch_loss_metric = tf.keras.metrics.MeanSquaredError()\n\n        def microbatch(T_IN, A_IN, D_IN):\n            OUT_T = []\n            OUT_A = []\n            for i in range(len(T_IN)):\n                A_IN_R = tf.expand_dims(tf.squeeze(A_IN[i]), 0)\n                T_IN_R = tf.expand_dims(tf.squeeze(T_IN[i]), 0)\n                D_IN_R = tf.expand_dims(tf.squeeze(D_IN[i]), 0)\n                (OUT_T_R, OUT_A_R) = model((A_IN_R, T_IN_R, D_IN_R))\n                OUT_T.append(tf.squeeze(OUT_T_R))\n                OUT_A.append(tf.squeeze(OUT_A_R))\n            return(tf.squeeze(tf.stack(OUT_T)), tf.squeeze(tf.stack(OUT_A)))\n\n        def run_train_step(data):\n            T_IN = tf.dtypes.cast(data[0][0], 'float32')\n            A_IN = tf.dtypes.cast(data[0][1], 'float32')\n            D_IN = tf.dtypes.cast(data[0][2], 'float32')\n            A_Ta = tf.dtypes.cast(data[1][0], 'float32')\n            T_Ta = tf.dtypes.cast(data[1][1], 'float32')\n            mse = tf.keras.losses.MeanSquaredError()\n\n            with tf.GradientTape() as tape:\n                logits2 = microbatch(T_IN, A_IN, D_IN)\n\n                logits = model([A_IN, T_IN, D_IN])\n                loss   = mse((T_Ta, A_Ta), logits2)\n                # Add any regularization losses.\n                if model.losses:\n                    loss += tf.math.add_n(model.losses)\n                gradients = tape.gradient(loss, model.trainable_variables)\n            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n            epoch_loss_metric.update_state((T_Ta, A_Ta), logits2)\n            return loss\n\n        for epoch in range(1000):\n            print('Epoch: {}'.format(epoch))\n\n            self.on_epoch_begin(trial, model, epoch, logs={})\n            for batch, data in enumerate(train_ds):\n                self.on_batch_begin(trial, model, batch, logs={})\n                batch_loss = float(run_train_step(data))\n                self.on_batch_end(trial, model, batch, logs={'loss': batch_loss})\n\n                if batch % 100 == 0:\n                    loss = epoch_loss_metric.result().numpy()\n                    print('Batch: {}, Average Loss: {}'.format(batch, loss))\n\n            epoch_loss = epoch_loss_metric.result().numpy()\n            self.on_epoch_end(trial, model, epoch, logs={'loss': epoch_loss})\n            epoch_loss_metric.reset_states()\n    ````\n\n</code></pre>\n"}], "has_more": false, "quota_max": 300, "quota_remaining": 211}]